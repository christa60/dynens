{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Resampling training and validating data sets...\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "4500 test samples\n",
      "y_train shape: (50000, 1)\n",
      "4500 valid samples\n",
      "Building model...\n",
      "ResNet20v1\n",
      "Preparing callbacks...\n",
      "Using real-time data augmentation.\n",
      "Training...\n",
      "Epoch 1/5\n",
      "\n",
      " Optimizer iteration 0, batch 0\n",
      "\n",
      " Learning rate 0.001, Model learning rate 0.0010000000474974513\n",
      "  1/391 [..............................] - ETA: 1:09:31 - loss: 5.1286 - acc: 0.0703\n",
      " Optimizer iteration 1, batch 1\n",
      "\n",
      " Learning rate 0.0009999993511109622, Model learning rate 0.0009999993490055203\n",
      "\n",
      " Optimizer iteration 2, batch 2\n",
      "\n",
      " Learning rate 0.0009999974044455327, Model learning rate 0.0009999973699450493\n",
      "  3/391 [..............................] - ETA: 23:13 - loss: 4.2290 - acc: 0.0807  \n",
      " Optimizer iteration 3, batch 3\n",
      "\n",
      " Learning rate 0.0009999941600087644, Model learning rate 0.0009999941103160381\n",
      "\n",
      " Optimizer iteration 4, batch 4\n",
      "\n",
      " Learning rate 0.0009999896178090784, Model learning rate 0.0009999895701184869\n",
      "  5/391 [..............................] - ETA: 13:59 - loss: 3.7404 - acc: 0.1125\n",
      " Optimizer iteration 5, batch 5\n",
      "\n",
      " Learning rate 0.000999983777858264, Model learning rate 0.0009999837493523955\n",
      "  6/391 [..............................] - ETA: 11:40 - loss: 3.5722 - acc: 0.1172\n",
      " Optimizer iteration 6, batch 6\n",
      "\n",
      " Learning rate 0.0009999766401714793, Model learning rate 0.000999976648017764\n",
      "  7/391 [..............................] - ETA: 10:01 - loss: 3.3968 - acc: 0.1362\n",
      " Optimizer iteration 7, batch 7\n",
      "\n",
      " Learning rate 0.0009999682047672506, Model learning rate 0.0009999681496992707\n",
      "  8/391 [..............................] - ETA: 8:47 - loss: 3.2764 - acc: 0.1406 \n",
      " Optimizer iteration 8, batch 8\n",
      "\n",
      " Learning rate 0.0009999584716674725, Model learning rate 0.000999958487227559\n",
      "\n",
      " Optimizer iteration 9, batch 9\n",
      "\n",
      " Learning rate 0.000999947440897408, Model learning rate 0.0009999474277719855\n",
      " 10/391 [..............................] - ETA: 7:03 - loss: 3.1127 - acc: 0.1469\n",
      " Optimizer iteration 10, batch 10\n",
      "\n",
      " Learning rate 0.0009999351124856874, Model learning rate 0.0009999350877478719\n",
      "\n",
      " Optimizer iteration 11, batch 11\n",
      "\n",
      " Learning rate 0.0009999214864643102, Model learning rate 0.0009999214671552181\n",
      " 12/391 [..............................] - ETA: 5:54 - loss: 2.9884 - acc: 0.1595\n",
      " Optimizer iteration 12, batch 12\n",
      "\n",
      " Learning rate 0.0009999065628686437, Model learning rate 0.0009999065659940243\n",
      "\n",
      " Optimizer iteration 13, batch 13\n",
      "\n",
      " Learning rate 0.0009998903417374227, Model learning rate 0.0009998903842642903\n",
      " 14/391 [>.............................] - ETA: 5:04 - loss: 2.8880 - acc: 0.1708\n",
      " Optimizer iteration 14, batch 14\n",
      "\n",
      " Learning rate 0.00099987282311275, Model learning rate 0.0009998728055506945\n",
      "\n",
      " Optimizer iteration 15, batch 15\n",
      "\n",
      " Learning rate 0.0009998540070400965, Model learning rate 0.0009998540626838803\n",
      " 16/391 [>.............................] - ETA: 4:26 - loss: 2.8108 - acc: 0.1772\n",
      " Optimizer iteration 16, batch 16\n",
      "\n",
      " Learning rate 0.0009998338935683, Model learning rate 0.0009998339228332043\n",
      " 17/391 [>.............................] - ETA: 4:11 - loss: 2.7903 - acc: 0.1815\n",
      " Optimizer iteration 17, batch 17\n",
      "\n",
      " Learning rate 0.0009998124827495663, Model learning rate 0.0009998125024139881\n",
      " 18/391 [>.............................] - ETA: 3:58 - loss: 2.7546 - acc: 0.1858\n",
      " Optimizer iteration 18, batch 18\n",
      "\n",
      " Learning rate 0.0009997897746394685, Model learning rate 0.0009997898014262319\n",
      " 19/391 [>.............................] - ETA: 3:46 - loss: 2.7213 - acc: 0.1904\n",
      " Optimizer iteration 19, batch 19\n",
      "\n",
      " Learning rate 0.0009997657692969464, Model learning rate 0.0009997658198699355\n",
      "\n",
      " Optimizer iteration 20, batch 20\n",
      "\n",
      " Learning rate 0.0009997404667843074, Model learning rate 0.0009997404413297772\n",
      " 21/391 [>.............................] - ETA: 3:25 - loss: 2.6686 - acc: 0.2013\n",
      " Optimizer iteration 21, batch 21\n",
      "\n",
      " Learning rate 0.000999713867167226, Model learning rate 0.0009997138986364007\n",
      " 22/391 [>.............................] - ETA: 3:16 - loss: 2.6524 - acc: 0.2031\n",
      " Optimizer iteration 22, batch 22\n",
      "\n",
      " Learning rate 0.0009996859705147423, Model learning rate 0.0009996859589591622\n",
      " 23/391 [>.............................] - ETA: 3:07 - loss: 2.6298 - acc: 0.2069\n",
      " Optimizer iteration 23, batch 23\n",
      "\n",
      " Learning rate 0.0009996567768992641, Model learning rate 0.0009996567387133837\n",
      " 24/391 [>.............................] - ETA: 3:00 - loss: 2.6060 - acc: 0.2103\n",
      " Optimizer iteration 24, batch 24\n",
      "\n",
      " Learning rate 0.000999626286396565, Model learning rate 0.000999626237899065\n",
      " 25/391 [>.............................] - ETA: 2:54 - loss: 2.5852 - acc: 0.2112\n",
      " Optimizer iteration 25, batch 25\n",
      "\n",
      " Learning rate 0.0009995944990857848, Model learning rate 0.0009995944565162063\n",
      "\n",
      " Optimizer iteration 26, batch 26\n",
      "\n",
      " Learning rate 0.0009995614150494292, Model learning rate 0.0009995613945648074\n",
      " 27/391 [=>............................] - ETA: 2:42 - loss: 2.5513 - acc: 0.2173\n",
      " Optimizer iteration 27, batch 27\n",
      "\n",
      " Learning rate 0.0009995270343733697, Model learning rate 0.0009995270520448685\n",
      " 28/391 [=>............................] - ETA: 2:37 - loss: 2.5388 - acc: 0.2193\n",
      " Optimizer iteration 28, batch 28\n",
      "\n",
      " Learning rate 0.000999491357146843, Model learning rate 0.0009994913125410676\n",
      " 29/391 [=>............................] - ETA: 2:32 - loss: 2.5211 - acc: 0.2225\n",
      " Optimizer iteration 29, batch 29\n",
      "\n",
      " Learning rate 0.0009994543834624518, Model learning rate 0.0009994544088840485\n",
      " 30/391 [=>............................] - ETA: 2:27 - loss: 2.4997 - acc: 0.2276\n",
      " Optimizer iteration 30, batch 30\n",
      "\n",
      " Learning rate 0.0009994161134161633, Model learning rate 0.0009994161082431674\n",
      " 31/391 [=>............................] - ETA: 2:23 - loss: 2.4842 - acc: 0.2314\n",
      " Optimizer iteration 31, batch 31\n",
      "\n",
      " Learning rate 0.0009993765471073093, Model learning rate 0.0009993765270337462\n",
      " 32/391 [=>............................] - ETA: 2:19 - loss: 2.4691 - acc: 0.2339\n",
      " Optimizer iteration 32, batch 32\n",
      "\n",
      " Learning rate 0.0009993356846385866, Model learning rate 0.000999335665255785\n",
      " 33/391 [=>............................] - ETA: 2:15 - loss: 2.4527 - acc: 0.2377\n",
      " Optimizer iteration 33, batch 33\n",
      "\n",
      " Learning rate 0.000999293526116056, Model learning rate 0.0009992935229092836\n",
      " 34/391 [=>............................] - ETA: 2:11 - loss: 2.4369 - acc: 0.2424\n",
      " Optimizer iteration 34, batch 34\n",
      "\n",
      " Learning rate 0.000999250071649142, Model learning rate 0.0009992500999942422\n",
      " 35/391 [=>............................] - ETA: 2:08 - loss: 2.4212 - acc: 0.2467\n",
      " Optimizer iteration 35, batch 35\n",
      "\n",
      " Learning rate 0.0009992053213506334, Model learning rate 0.0009992052800953388\n",
      " 36/391 [=>............................] - ETA: 2:05 - loss: 2.4079 - acc: 0.2489\n",
      " Optimizer iteration 36, batch 36\n",
      "\n",
      " Learning rate 0.000999159275336682, Model learning rate 0.0009991592960432172\n",
      " 37/391 [=>............................] - ETA: 2:02 - loss: 2.4009 - acc: 0.2496\n",
      " Optimizer iteration 37, batch 37\n",
      "\n",
      " Learning rate 0.0009991119337268031, Model learning rate 0.0009991119150072336\n",
      "\n",
      " Optimizer iteration 38, batch 38\n",
      "\n",
      " Learning rate 0.0009990632966438743, Model learning rate 0.00099906325340271\n",
      " 39/391 [=>............................] - ETA: 1:56 - loss: 2.3806 - acc: 0.2538\n",
      " Optimizer iteration 39, batch 39\n",
      "\n",
      " Learning rate 0.0009990133642141358, Model learning rate 0.0009990133112296462\n",
      " 40/391 [==>...........................] - ETA: 1:54 - loss: 2.3693 - acc: 0.2566\n",
      " Optimizer iteration 40, batch 40\n",
      "\n",
      " Learning rate 0.0009989621365671902, Model learning rate 0.0009989620884880424\n",
      " 41/391 [==>...........................] - ETA: 1:51 - loss: 2.3560 - acc: 0.2591\n",
      " Optimizer iteration 41, batch 41\n",
      "\n",
      " Learning rate 0.0009989096138360014, Model learning rate 0.0009989095851778984\n",
      "\n",
      " Optimizer iteration 42, batch 42\n",
      "\n",
      " Learning rate 0.0009988557961568955, Model learning rate 0.0009988558012992144\n",
      " 43/391 [==>...........................] - ETA: 1:47 - loss: 2.3359 - acc: 0.2644\n",
      " Optimizer iteration 43, batch 43\n",
      "\n",
      " Learning rate 0.000998800683669559, Model learning rate 0.0009988007368519902\n",
      " 44/391 [==>...........................] - ETA: 1:45 - loss: 2.3261 - acc: 0.2669\n",
      " Optimizer iteration 44, batch 44\n",
      "\n",
      " Learning rate 0.0009987442765170397, Model learning rate 0.0009987442754209042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45/391 [==>...........................] - ETA: 1:43 - loss: 2.3157 - acc: 0.2694\n",
      " Optimizer iteration 45, batch 45\n",
      "\n",
      " Learning rate 0.0009986865748457456, Model learning rate 0.000998686533421278\n",
      "\n",
      " Optimizer iteration 46, batch 46\n",
      "\n",
      " Learning rate 0.000998627578805444, Model learning rate 0.0009986276272684336\n",
      " 47/391 [==>...........................] - ETA: 1:39 - loss: 2.2950 - acc: 0.2753\n",
      " Optimizer iteration 47, batch 47\n",
      "\n",
      " Learning rate 0.0009985672885492634, Model learning rate 0.0009985673241317272\n",
      " 48/391 [==>...........................] - ETA: 1:37 - loss: 2.2860 - acc: 0.2772\n",
      " Optimizer iteration 48, batch 48\n",
      "\n",
      " Learning rate 0.0009985057042336898, Model learning rate 0.0009985057404264808\n",
      " 49/391 [==>...........................] - ETA: 1:35 - loss: 2.2795 - acc: 0.2803\n",
      " Optimizer iteration 49, batch 49\n",
      "\n",
      " Learning rate 0.000998442826018569, Model learning rate 0.0009984428761526942\n",
      " 50/391 [==>...........................] - ETA: 1:33 - loss: 2.2721 - acc: 0.2819\n",
      " Optimizer iteration 50, batch 50\n",
      "\n",
      " Learning rate 0.000998378654067105, Model learning rate 0.0009983786148950458\n",
      " 51/391 [==>...........................] - ETA: 1:32 - loss: 2.2641 - acc: 0.2834\n",
      " Optimizer iteration 51, batch 51\n",
      "\n",
      " Learning rate 0.0009983131885458597, Model learning rate 0.000998313189484179\n",
      " 52/391 [==>...........................] - ETA: 1:30 - loss: 2.2569 - acc: 0.2861\n",
      " Optimizer iteration 52, batch 52\n",
      "\n",
      " Learning rate 0.0009982464296247522, Model learning rate 0.0009982464835047722\n",
      " 53/391 [===>..........................] - ETA: 1:29 - loss: 2.2511 - acc: 0.2879\n",
      " Optimizer iteration 53, batch 53\n",
      "\n",
      " Learning rate 0.0009981783774770593, Model learning rate 0.0009981783805415034\n",
      "\n",
      " Optimizer iteration 54, batch 54\n",
      "\n",
      " Learning rate 0.0009981090322794144, Model learning rate 0.0009981089970096946\n",
      " 55/391 [===>..........................] - ETA: 1:26 - loss: 2.2354 - acc: 0.2916\n",
      " Optimizer iteration 55, batch 55\n",
      "\n",
      " Learning rate 0.0009980383942118066, Model learning rate 0.0009980384493246675\n",
      " 56/391 [===>..........................] - ETA: 1:25 - loss: 2.2308 - acc: 0.2916\n",
      " Optimizer iteration 56, batch 56\n",
      "\n",
      " Learning rate 0.0009979664634575808, Model learning rate 0.0009979665046557784\n",
      " 57/391 [===>..........................] - ETA: 1:23 - loss: 2.2244 - acc: 0.2926\n",
      " Optimizer iteration 57, batch 57\n",
      "\n",
      " Learning rate 0.0009978932402034376, Model learning rate 0.0009978932794183493\n",
      "\n",
      " Optimizer iteration 58, batch 58\n",
      "\n",
      " Learning rate 0.000997818724639432, Model learning rate 0.00099781877361238\n",
      " 59/391 [===>..........................] - ETA: 1:21 - loss: 2.2114 - acc: 0.2952\n",
      " Optimizer iteration 59, batch 59\n",
      "\n",
      " Learning rate 0.0009977429169589732, Model learning rate 0.0009977428708225489\n",
      " 60/391 [===>..........................] - ETA: 1:20 - loss: 2.2055 - acc: 0.2964\n",
      " Optimizer iteration 60, batch 60\n",
      "\n",
      " Learning rate 0.0009976658173588243, Model learning rate 0.0009976658038794994\n",
      " 61/391 [===>..........................] - ETA: 1:19 - loss: 2.1985 - acc: 0.2987\n",
      " Optimizer iteration 61, batch 61\n",
      "\n",
      " Learning rate 0.0009975874260391019, Model learning rate 0.00099758745636791\n",
      " 62/391 [===>..........................] - ETA: 1:17 - loss: 2.1939 - acc: 0.2998\n",
      " Optimizer iteration 62, batch 62\n",
      "\n",
      " Learning rate 0.0009975077432032749, Model learning rate 0.0009975077118724585\n",
      " 63/391 [===>..........................] - ETA: 1:16 - loss: 2.1876 - acc: 0.3013\n",
      " Optimizer iteration 63, batch 63\n",
      "\n",
      " Learning rate 0.0009974267690581644, Model learning rate 0.0009974268032237887\n",
      " 64/391 [===>..........................] - ETA: 1:15 - loss: 2.1817 - acc: 0.3032\n",
      " Optimizer iteration 64, batch 64\n",
      "\n",
      " Learning rate 0.0009973445038139437, Model learning rate 0.000997344497591257\n",
      " 65/391 [===>..........................] - ETA: 1:14 - loss: 2.1747 - acc: 0.3041\n",
      " Optimizer iteration 65, batch 65\n",
      "\n",
      " Learning rate 0.0009972609476841367, Model learning rate 0.0009972609113901854\n",
      "\n",
      " Optimizer iteration 66, batch 66\n",
      "\n",
      " Learning rate 0.000997176100885618, Model learning rate 0.0009971760446205735\n",
      " 67/391 [====>.........................] - ETA: 1:12 - loss: 2.1639 - acc: 0.3064\n",
      " Optimizer iteration 67, batch 67\n",
      "\n",
      " Learning rate 0.000997089963638612, Model learning rate 0.0009970900136977434\n",
      " 68/391 [====>.........................] - ETA: 1:11 - loss: 2.1596 - acc: 0.3071\n",
      " Optimizer iteration 68, batch 68\n",
      "\n",
      " Learning rate 0.0009970025361666932, Model learning rate 0.0009970025857910514\n",
      " 69/391 [====>.........................] - ETA: 1:11 - loss: 2.1567 - acc: 0.3080\n",
      " Optimizer iteration 69, batch 69\n",
      "\n",
      " Learning rate 0.0009969138186967843, Model learning rate 0.0009969137609004974\n",
      "\n",
      " Optimizer iteration 70, batch 70\n",
      "\n",
      " Learning rate 0.0009968238114591566, Model learning rate 0.0009968237718567252\n",
      " 71/391 [====>.........................] - ETA: 1:09 - loss: 2.1455 - acc: 0.3122\n",
      " Optimizer iteration 71, batch 71\n",
      "\n",
      " Learning rate 0.0009967325146874287, Model learning rate 0.000996732502244413\n",
      " 72/391 [====>.........................] - ETA: 1:08 - loss: 2.1413 - acc: 0.3134\n",
      " Optimizer iteration 72, batch 72\n",
      "\n",
      " Learning rate 0.0009966399286185665, Model learning rate 0.0009966399520635605\n",
      " 73/391 [====>.........................] - ETA: 1:07 - loss: 2.1373 - acc: 0.3139\n",
      " Optimizer iteration 73, batch 73\n",
      "\n",
      " Learning rate 0.0009965460534928825, Model learning rate 0.0009965460048988461\n",
      " 74/391 [====>.........................] - ETA: 1:06 - loss: 2.1327 - acc: 0.3155\n",
      " Optimizer iteration 74, batch 74\n",
      "\n",
      " Learning rate 0.0009964508895540349, Model learning rate 0.0009964508935809135\n",
      " 75/391 [====>.........................] - ETA: 1:06 - loss: 2.1302 - acc: 0.3151\n",
      " Optimizer iteration 75, batch 75\n",
      "\n",
      " Learning rate 0.000996354437049027, Model learning rate 0.000996354385279119\n",
      " 76/391 [====>.........................] - ETA: 1:05 - loss: 2.1282 - acc: 0.3161\n",
      " Optimizer iteration 76, batch 76\n",
      "\n",
      " Learning rate 0.0009962566962282066, Model learning rate 0.0009962567128241062\n",
      " 77/391 [====>.........................] - ETA: 1:04 - loss: 2.1243 - acc: 0.3170\n",
      " Optimizer iteration 77, batch 77\n",
      "\n",
      " Learning rate 0.0009961576673452655, Model learning rate 0.0009961576433852315\n",
      "\n",
      " Optimizer iteration 78, batch 78\n",
      "\n",
      " Learning rate 0.000996057350657239, Model learning rate 0.0009960572933778167\n",
      " 79/391 [=====>........................] - ETA: 1:02 - loss: 2.1146 - acc: 0.3188\n",
      " Optimizer iteration 79, batch 79\n",
      "\n",
      " Learning rate 0.0009959557464245042, Model learning rate 0.0009959557792171836\n",
      " 80/391 [=====>........................] - ETA: 1:02 - loss: 2.1095 - acc: 0.3203\n",
      " Optimizer iteration 80, batch 80\n",
      "\n",
      " Learning rate 0.000995852854910781, Model learning rate 0.0009958528680726886\n",
      " 81/391 [=====>........................] - ETA: 1:01 - loss: 2.1052 - acc: 0.3214\n",
      " Optimizer iteration 81, batch 81\n",
      "\n",
      " Learning rate 0.00099574867638313, Model learning rate 0.0009957486763596535\n",
      " 82/391 [=====>........................] - ETA: 1:00 - loss: 2.1013 - acc: 0.3223\n",
      " Optimizer iteration 82, batch 82\n",
      "\n",
      " Learning rate 0.0009956432111119522, Model learning rate 0.0009956432040780783\n",
      " 83/391 [=====>........................] - ETA: 1:00 - loss: 2.0983 - acc: 0.3227\n",
      " Optimizer iteration 83, batch 83\n",
      "\n",
      " Learning rate 0.000995536459370989, Model learning rate 0.000995536451227963\n",
      " 84/391 [=====>........................] - ETA: 59s - loss: 2.0939 - acc: 0.3238 \n",
      " Optimizer iteration 84, batch 84\n",
      "\n",
      " Learning rate 0.0009954284214373204, Model learning rate 0.0009954284178093076\n",
      "\n",
      " Optimizer iteration 85, batch 85\n",
      "\n",
      " Learning rate 0.0009953190975913646, Model learning rate 0.000995319103822112\n",
      " 86/391 [=====>........................] - ETA: 58s - loss: 2.0865 - acc: 0.3255\n",
      " Optimizer iteration 86, batch 86\n",
      "\n",
      " Learning rate 0.0009952084881168783, Model learning rate 0.0009952085092663765\n",
      " 87/391 [=====>........................] - ETA: 57s - loss: 2.0841 - acc: 0.3267\n",
      " Optimizer iteration 87, batch 87\n",
      "\n",
      " Learning rate 0.0009950965933009544, Model learning rate 0.0009950966341421008\n",
      " 88/391 [=====>........................] - ETA: 57s - loss: 2.0800 - acc: 0.3281\n",
      " Optimizer iteration 88, batch 88\n",
      "\n",
      " Learning rate 0.000994983413434022, Model learning rate 0.0009949833620339632\n",
      " 89/391 [=====>........................] - ETA: 56s - loss: 2.0773 - acc: 0.3292\n",
      " Optimizer iteration 89, batch 89\n",
      "\n",
      " Learning rate 0.000994868948809846, Model learning rate 0.0009948689257726073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90/391 [=====>........................] - ETA: 56s - loss: 2.0758 - acc: 0.3299\n",
      " Optimizer iteration 90, batch 90\n",
      "\n",
      " Learning rate 0.0009947531997255255, Model learning rate 0.0009947532089427114\n",
      "\n",
      " Optimizer iteration 91, batch 91\n",
      "\n",
      " Learning rate 0.0009946361664814943, Model learning rate 0.0009946362115442753\n",
      " 92/391 [======>.......................] - ETA: 55s - loss: 2.0684 - acc: 0.3316\n",
      " Optimizer iteration 92, batch 92\n",
      "\n",
      " Learning rate 0.0009945178493815181, Model learning rate 0.0009945178171619773\n",
      " 93/391 [======>.......................] - ETA: 54s - loss: 2.0635 - acc: 0.3329\n",
      " Optimizer iteration 93, batch 93\n",
      "\n",
      " Learning rate 0.000994398248732696, Model learning rate 0.000994398258626461\n",
      " 94/391 [======>.......................] - ETA: 54s - loss: 2.0604 - acc: 0.3338\n",
      " Optimizer iteration 94, batch 94\n",
      "\n",
      " Learning rate 0.000994277364845458, Model learning rate 0.0009942774195224047\n",
      "\n",
      " Optimizer iteration 95, batch 95\n",
      "\n",
      " Learning rate 0.0009941551980335653, Model learning rate 0.0009941551834344864\n",
      " 96/391 [======>.......................] - ETA: 53s - loss: 2.0557 - acc: 0.3353\n",
      " Optimizer iteration 96, batch 96\n",
      "\n",
      " Learning rate 0.0009940317486141082, Model learning rate 0.0009940317831933498\n",
      " 97/391 [======>.......................] - ETA: 52s - loss: 2.0516 - acc: 0.3363\n",
      " Optimizer iteration 97, batch 97\n",
      "\n",
      " Learning rate 0.0009939070169075071, Model learning rate 0.0009939069859683514\n",
      " 98/391 [======>.......................] - ETA: 52s - loss: 2.0476 - acc: 0.3372\n",
      " Optimizer iteration 98, batch 98\n",
      "\n",
      " Learning rate 0.00099378100323751, Model learning rate 0.0009937810245901346\n",
      " 99/391 [======>.......................] - ETA: 51s - loss: 2.0447 - acc: 0.3381\n",
      " Optimizer iteration 99, batch 99\n",
      "\n",
      " Learning rate 0.0009936537079311926, Model learning rate 0.000993653666228056\n",
      "100/391 [======>.......................] - ETA: 51s - loss: 2.0416 - acc: 0.3389\n",
      " Optimizer iteration 100, batch 100\n",
      "\n",
      " Learning rate 0.0009935251313189565, Model learning rate 0.000993525143712759\n",
      "\n",
      " Optimizer iteration 101, batch 101\n",
      "\n",
      " Learning rate 0.00099339527373453, Model learning rate 0.0009933952242136002\n",
      "102/391 [======>.......................] - ETA: 50s - loss: 2.0347 - acc: 0.3405\n",
      " Optimizer iteration 102, batch 102\n",
      "\n",
      " Learning rate 0.0009932641355149653, Model learning rate 0.000993264140561223\n",
      "103/391 [======>.......................] - ETA: 49s - loss: 2.0318 - acc: 0.3417\n",
      " Optimizer iteration 103, batch 103\n",
      "\n",
      " Learning rate 0.0009931317170006398, Model learning rate 0.000993131659924984\n",
      "104/391 [======>.......................] - ETA: 49s - loss: 2.0291 - acc: 0.3425\n",
      " Optimizer iteration 104, batch 104\n",
      "\n",
      " Learning rate 0.0009929980185352525, Model learning rate 0.0009929980151355267\n",
      "105/391 [=======>......................] - ETA: 49s - loss: 2.0255 - acc: 0.3434\n",
      " Optimizer iteration 105, batch 105\n",
      "\n",
      " Learning rate 0.0009928630404658254, Model learning rate 0.0009928630897775292\n",
      "\n",
      " Optimizer iteration 106, batch 106\n",
      "\n",
      " Learning rate 0.0009927267831427017, Model learning rate 0.00099272676743567\n",
      "107/391 [=======>......................] - ETA: 48s - loss: 2.0189 - acc: 0.3453\n",
      " Optimizer iteration 107, batch 107\n",
      "\n",
      " Learning rate 0.0009925892469195452, Model learning rate 0.0009925892809405923\n",
      "108/391 [=======>......................] - ETA: 47s - loss: 2.0159 - acc: 0.3466\n",
      " Optimizer iteration 108, batch 108\n",
      "\n",
      " Learning rate 0.0009924504321533387, Model learning rate 0.0009924503974616528\n",
      "109/391 [=======>......................] - ETA: 47s - loss: 2.0131 - acc: 0.3468\n",
      " Optimizer iteration 109, batch 109\n",
      "\n",
      " Learning rate 0.0009923103392043835, Model learning rate 0.000992310349829495\n",
      "\n",
      " Optimizer iteration 110, batch 110\n",
      "\n",
      " Learning rate 0.000992168968436299, Model learning rate 0.000992169021628797\n",
      "111/391 [=======>......................] - ETA: 46s - loss: 2.0072 - acc: 0.3483\n",
      " Optimizer iteration 111, batch 111\n",
      "\n",
      " Learning rate 0.0009920263202160206, Model learning rate 0.0009920262964442372\n",
      "112/391 [=======>......................] - ETA: 46s - loss: 2.0044 - acc: 0.3490\n",
      " Optimizer iteration 112, batch 112\n",
      "\n",
      " Learning rate 0.0009918823949138004, Model learning rate 0.0009918824071064591\n",
      "\n",
      " Optimizer iteration 113, batch 113\n",
      "\n",
      " Learning rate 0.000991737192903204, Model learning rate 0.000991737237200141\n",
      "114/391 [=======>......................] - ETA: 45s - loss: 1.9996 - acc: 0.3498\n",
      " Optimizer iteration 114, batch 114\n",
      "\n",
      " Learning rate 0.0009915907145611115, Model learning rate 0.0009915906703099608\n",
      "115/391 [=======>......................] - ETA: 44s - loss: 1.9956 - acc: 0.3514\n",
      " Optimizer iteration 115, batch 115\n",
      "\n",
      " Learning rate 0.0009914429602677162, Model learning rate 0.0009914429392665625\n",
      "116/391 [=======>......................] - ETA: 44s - loss: 1.9925 - acc: 0.3524\n",
      " Optimizer iteration 116, batch 116\n",
      "\n",
      " Learning rate 0.0009912939304065219, Model learning rate 0.000991293927654624\n",
      "117/391 [=======>......................] - ETA: 44s - loss: 1.9900 - acc: 0.3533\n",
      " Optimizer iteration 117, batch 117\n",
      "\n",
      " Learning rate 0.0009911436253643444, Model learning rate 0.0009911436354741454\n",
      "\n",
      " Optimizer iteration 118, batch 118\n",
      "\n",
      " Learning rate 0.0009909920455313088, Model learning rate 0.0009909920627251267\n",
      "119/391 [========>.....................] - ETA: 43s - loss: 1.9843 - acc: 0.3551\n",
      " Optimizer iteration 119, batch 119\n",
      "\n",
      " Learning rate 0.000990839191300849, Model learning rate 0.000990839209407568\n",
      "120/391 [========>.....................] - ETA: 43s - loss: 1.9815 - acc: 0.3561\n",
      " Optimizer iteration 120, batch 120\n",
      "\n",
      " Learning rate 0.0009906850630697068, Model learning rate 0.0009906850755214691\n",
      "\n",
      " Optimizer iteration 121, batch 121\n",
      "\n",
      " Learning rate 0.0009905296612379307, Model learning rate 0.0009905296610668302\n",
      "122/391 [========>.....................] - ETA: 42s - loss: 1.9742 - acc: 0.3585\n",
      " Optimizer iteration 122, batch 122\n",
      "\n",
      " Learning rate 0.0009903729862088748, Model learning rate 0.000990372966043651\n",
      "\n",
      " Optimizer iteration 123, batch 123\n",
      "\n",
      " Learning rate 0.0009902150383891979, Model learning rate 0.000990214990451932\n",
      "124/391 [========>.....................] - ETA: 42s - loss: 1.9703 - acc: 0.3592\n",
      " Optimizer iteration 124, batch 124\n",
      "\n",
      " Learning rate 0.0009900558181888627, Model learning rate 0.0009900558507069945\n",
      "125/391 [========>.....................] - ETA: 41s - loss: 1.9685 - acc: 0.3598\n",
      " Optimizer iteration 125, batch 125\n",
      "\n",
      " Learning rate 0.0009898953260211339, Model learning rate 0.0009898953139781952\n",
      "126/391 [========>.....................] - ETA: 41s - loss: 1.9655 - acc: 0.3610\n",
      " Optimizer iteration 126, batch 126\n",
      "\n",
      " Learning rate 0.000989733562302578, Model learning rate 0.0009897336130961776\n",
      "\n",
      " Optimizer iteration 127, batch 127\n",
      "\n",
      " Learning rate 0.0009895705274530619, Model learning rate 0.000989570515230298\n",
      "128/391 [========>.....................] - ETA: 40s - loss: 1.9608 - acc: 0.3627\n",
      " Optimizer iteration 128, batch 128\n",
      "\n",
      " Learning rate 0.0009894062218957515, Model learning rate 0.0009894062532112002\n",
      "\n",
      " Optimizer iteration 129, batch 129\n",
      "\n",
      " Learning rate 0.0009892406460571114, Model learning rate 0.0009892405942082405\n",
      "130/391 [========>.....................] - ETA: 40s - loss: 1.9565 - acc: 0.3635\n",
      " Optimizer iteration 130, batch 130\n",
      "\n",
      " Learning rate 0.0009890738003669028, Model learning rate 0.0009890737710520625\n",
      "\n",
      " Optimizer iteration 131, batch 131\n",
      "\n",
      " Learning rate 0.000988905685258183, Model learning rate 0.0009889056673273444\n",
      "132/391 [=========>....................] - ETA: 39s - loss: 1.9513 - acc: 0.3651\n",
      " Optimizer iteration 132, batch 132\n",
      "\n",
      " Learning rate 0.0009887363011673045, Model learning rate 0.0009887362830340862\n",
      "\n",
      " Optimizer iteration 133, batch 133\n",
      "\n",
      " Learning rate 0.0009885656485339128, Model learning rate 0.000988565618172288\n",
      "134/391 [=========>....................] - ETA: 38s - loss: 1.9467 - acc: 0.3662\n",
      " Optimizer iteration 134, batch 134\n",
      "\n",
      " Learning rate 0.0009883937278009466, Model learning rate 0.0009883936727419496\n",
      "\n",
      " Optimizer iteration 135, batch 135\n",
      "\n",
      " Learning rate 0.0009882205394146362, Model learning rate 0.000988220563158393\n",
      "136/391 [=========>....................] - ETA: 38s - loss: 1.9403 - acc: 0.3682\n",
      " Optimizer iteration 136, batch 136\n",
      "\n",
      " Learning rate 0.000988046083824501, Model learning rate 0.0009880460565909743\n",
      "137/391 [=========>....................] - ETA: 37s - loss: 1.9380 - acc: 0.3687\n",
      " Optimizer iteration 137, batch 137\n",
      "\n",
      " Learning rate 0.0009878703614833507, Model learning rate 0.0009878703858703375\n",
      "138/391 [=========>....................] - ETA: 37s - loss: 1.9358 - acc: 0.3687\n",
      " Optimizer iteration 138, batch 138\n",
      "\n",
      " Learning rate 0.0009876933728472826, Model learning rate 0.0009876933181658387\n",
      "\n",
      " Optimizer iteration 139, batch 139\n",
      "\n",
      " Learning rate 0.0009875151183756806, Model learning rate 0.0009875150863081217\n",
      "140/391 [=========>....................] - ETA: 37s - loss: 1.9307 - acc: 0.3705\n",
      " Optimizer iteration 140, batch 140\n",
      "\n",
      " Learning rate 0.000987335598531214, Model learning rate 0.0009873355738818645\n",
      "\n",
      " Optimizer iteration 141, batch 141\n",
      "\n",
      " Learning rate 0.0009871548137798368, Model learning rate 0.0009871547808870673\n",
      "142/391 [=========>....................] - ETA: 36s - loss: 1.9272 - acc: 0.3717\n",
      " Optimizer iteration 142, batch 142\n",
      "\n",
      " Learning rate 0.0009869727645907857, Model learning rate 0.00098697270732373\n",
      "\n",
      " Optimizer iteration 143, batch 143\n",
      "\n",
      " Learning rate 0.0009867894514365802, Model learning rate 0.0009867894696071744\n",
      "144/391 [==========>...................] - ETA: 36s - loss: 1.9237 - acc: 0.3727\n",
      " Optimizer iteration 144, batch 144\n",
      "\n",
      " Learning rate 0.0009866048747930194, Model learning rate 0.0009866048349067569\n",
      "\n",
      " Optimizer iteration 145, batch 145\n",
      "\n",
      " Learning rate 0.0009864190351391822, Model learning rate 0.000986419036053121\n",
      "146/391 [==========>...................] - ETA: 35s - loss: 1.9179 - acc: 0.3745\n",
      " Optimizer iteration 146, batch 146\n",
      "\n",
      " Learning rate 0.0009862319329574263, Model learning rate 0.0009862319566309452\n",
      "\n",
      " Optimizer iteration 147, batch 147\n",
      "\n",
      " Learning rate 0.0009860435687333857, Model learning rate 0.0009860435966402292\n",
      "148/391 [==========>...................] - ETA: 34s - loss: 1.9134 - acc: 0.3758\n",
      " Optimizer iteration 148, batch 148\n",
      "\n",
      " Learning rate 0.0009858539429559703, Model learning rate 0.0009858539560809731\n",
      "149/391 [==========>...................] - ETA: 34s - loss: 1.9111 - acc: 0.3765\n",
      " Optimizer iteration 149, batch 149\n",
      "\n",
      " Learning rate 0.0009856630561173648, Model learning rate 0.000985663034953177\n",
      "\n",
      " Optimizer iteration 150, batch 150\n",
      "\n",
      " Learning rate 0.000985470908713026, Model learning rate 0.0009854709496721625\n",
      "151/391 [==========>...................] - ETA: 34s - loss: 1.9084 - acc: 0.3769\n",
      " Optimizer iteration 151, batch 151\n",
      "\n",
      " Learning rate 0.000985277501241684, Model learning rate 0.0009852774674072862\n",
      "152/391 [==========>...................] - ETA: 33s - loss: 1.9067 - acc: 0.3775\n",
      " Optimizer iteration 152, batch 152\n",
      "\n",
      " Learning rate 0.0009850828342053382, Model learning rate 0.0009850828209891915\n",
      "\n",
      " Optimizer iteration 153, batch 153\n",
      "\n",
      " Learning rate 0.000984886908109258, Model learning rate 0.0009848868940025568\n",
      "154/391 [==========>...................] - ETA: 33s - loss: 1.9023 - acc: 0.3789\n",
      " Optimizer iteration 154, batch 154\n",
      "\n",
      " Learning rate 0.000984689723461981, Model learning rate 0.000984689686447382\n",
      "\n",
      " Optimizer iteration 155, batch 155\n",
      "\n",
      " Learning rate 0.0009844912807753104, Model learning rate 0.0009844913147389889\n",
      "156/391 [==========>...................] - ETA: 32s - loss: 1.8981 - acc: 0.3805\n",
      " Optimizer iteration 156, batch 156\n",
      "\n",
      " Learning rate 0.0009842915805643156, Model learning rate 0.0009842915460467339\n",
      "\n",
      " Optimizer iteration 157, batch 157\n",
      "\n",
      " Learning rate 0.0009840906233473297, Model learning rate 0.0009840906132012606\n",
      "158/391 [===========>..................] - ETA: 32s - loss: 1.8944 - acc: 0.3817\n",
      " Optimizer iteration 158, batch 158\n",
      "\n",
      " Learning rate 0.0009838884096459487, Model learning rate 0.0009838883997872472\n",
      "\n",
      " Optimizer iteration 159, batch 159\n",
      "\n",
      " Learning rate 0.0009836849399850291, Model learning rate 0.0009836849058046937\n",
      "160/391 [===========>..................] - ETA: 32s - loss: 1.8908 - acc: 0.3829\n",
      " Optimizer iteration 160, batch 160\n",
      "\n",
      " Learning rate 0.0009834802148926882, Model learning rate 0.000983480247668922\n",
      "161/391 [===========>..................] - ETA: 31s - loss: 1.8896 - acc: 0.3832\n",
      " Optimizer iteration 161, batch 161\n",
      "\n",
      " Learning rate 0.0009832742349003014, Model learning rate 0.0009832741925492883\n",
      "\n",
      " Optimizer iteration 162, batch 162\n",
      "\n",
      " Learning rate 0.0009830670005425012, Model learning rate 0.0009830669732764363\n",
      "163/391 [===========>..................] - ETA: 31s - loss: 1.8843 - acc: 0.3850\n",
      " Optimizer iteration 163, batch 163\n",
      "\n",
      " Learning rate 0.0009828585123571763, Model learning rate 0.0009828584734350443\n",
      "164/391 [===========>..................] - ETA: 31s - loss: 1.8816 - acc: 0.3862\n",
      " Optimizer iteration 164, batch 164\n",
      "\n",
      " Learning rate 0.0009826487708854692, Model learning rate 0.000982648809440434\n",
      "\n",
      " Optimizer iteration 165, batch 165\n",
      "\n",
      " Learning rate 0.0009824377766717758, Model learning rate 0.0009824377484619617\n",
      "166/391 [===========>..................] - ETA: 30s - loss: 1.8786 - acc: 0.3869\n",
      " Optimizer iteration 166, batch 166\n",
      "\n",
      " Learning rate 0.0009822255302637435, Model learning rate 0.0009822255233302712\n",
      "\n",
      " Optimizer iteration 167, batch 167\n",
      "\n",
      " Learning rate 0.0009820120322122695, Model learning rate 0.0009820120176300406\n",
      "168/391 [===========>..................] - ETA: 30s - loss: 1.8767 - acc: 0.3873\n",
      " Optimizer iteration 168, batch 168\n",
      "\n",
      " Learning rate 0.0009817972830715002, Model learning rate 0.00098179723136127\n",
      "\n",
      " Optimizer iteration 169, batch 169\n",
      "\n",
      " Learning rate 0.0009815812833988292, Model learning rate 0.000981581280939281\n",
      "170/391 [============>.................] - ETA: 29s - loss: 1.8738 - acc: 0.3881\n",
      " Optimizer iteration 170, batch 170\n",
      "\n",
      " Learning rate 0.0009813640337548953, Model learning rate 0.000981364049948752\n",
      "171/391 [============>.................] - ETA: 29s - loss: 1.8723 - acc: 0.3887\n",
      " Optimizer iteration 171, batch 171\n",
      "\n",
      " Learning rate 0.0009811455347035827, Model learning rate 0.0009811455383896828\n",
      "172/391 [============>.................] - ETA: 29s - loss: 1.8707 - acc: 0.3891\n",
      " Optimizer iteration 172, batch 172\n",
      "\n",
      " Learning rate 0.0009809257868120176, Model learning rate 0.0009809257462620735\n",
      "\n",
      " Optimizer iteration 173, batch 173\n",
      "\n",
      " Learning rate 0.0009807047906505682, Model learning rate 0.000980704789981246\n",
      "174/391 [============>.................] - ETA: 28s - loss: 1.8687 - acc: 0.3895\n",
      " Optimizer iteration 174, batch 174\n",
      "\n",
      " Learning rate 0.0009804825467928423, Model learning rate 0.0009804825531318784\n",
      "175/391 [============>.................] - ETA: 28s - loss: 1.8673 - acc: 0.3899\n",
      " Optimizer iteration 175, batch 175\n",
      "\n",
      " Learning rate 0.000980259055815686, Model learning rate 0.0009802590357139707\n",
      "176/391 [============>.................] - ETA: 28s - loss: 1.8663 - acc: 0.3899\n",
      " Optimizer iteration 176, batch 176\n",
      "\n",
      " Learning rate 0.0009800343182991835, Model learning rate 0.0009800343541428447\n",
      "\n",
      " Optimizer iteration 177, batch 177\n",
      "\n",
      " Learning rate 0.000979808334826653, Model learning rate 0.0009798083920031786\n",
      "178/391 [============>.................] - ETA: 28s - loss: 1.8635 - acc: 0.3906\n",
      " Optimizer iteration 178, batch 178\n",
      "\n",
      " Learning rate 0.0009795811059846475, Model learning rate 0.0009795811492949724\n",
      "\n",
      " Optimizer iteration 179, batch 179\n",
      "\n",
      " Learning rate 0.000979352632362952, Model learning rate 0.0009793526260182261\n",
      "180/391 [============>.................] - ETA: 27s - loss: 1.8601 - acc: 0.3915\n",
      " Optimizer iteration 180, batch 180\n",
      "\n",
      " Learning rate 0.0009791229145545831, Model learning rate 0.0009791229385882616\n",
      "181/391 [============>.................] - ETA: 27s - loss: 1.8577 - acc: 0.3922\n",
      " Optimizer iteration 181, batch 181\n",
      "\n",
      " Learning rate 0.0009788919531557858, Model learning rate 0.000978891970589757\n",
      "\n",
      " Optimizer iteration 182, batch 182\n",
      "\n",
      " Learning rate 0.0009786597487660335, Model learning rate 0.0009786597220227122\n",
      "183/391 [=============>................] - ETA: 27s - loss: 1.8556 - acc: 0.3932\n",
      " Optimizer iteration 183, batch 183\n",
      "\n",
      " Learning rate 0.0009784263019880259, Model learning rate 0.0009784263093024492\n",
      "184/391 [=============>................] - ETA: 26s - loss: 1.8541 - acc: 0.3937\n",
      " Optimizer iteration 184, batch 184\n",
      "\n",
      " Learning rate 0.0009781916134276871, Model learning rate 0.0009781916160136461\n",
      "\n",
      " Optimizer iteration 185, batch 185\n",
      "\n",
      " Learning rate 0.0009779556836941646, Model learning rate 0.000977955642156303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/391 [=============>................] - ETA: 26s - loss: 1.8512 - acc: 0.3941\n",
      " Optimizer iteration 186, batch 186\n",
      "\n",
      " Learning rate 0.0009777185133998268, Model learning rate 0.0009777185041457415\n",
      "\n",
      " Optimizer iteration 187, batch 187\n",
      "\n",
      " Learning rate 0.0009774801031602629, Model learning rate 0.00097748008556664\n",
      "188/391 [=============>................] - ETA: 26s - loss: 1.8476 - acc: 0.3954\n",
      " Optimizer iteration 188, batch 188\n",
      "\n",
      " Learning rate 0.00097724045359428, Model learning rate 0.00097724050283432\n",
      "\n",
      " Optimizer iteration 189, batch 189\n",
      "\n",
      " Learning rate 0.0009769995653239022, Model learning rate 0.0009769995231181383\n",
      "190/391 [=============>................] - ETA: 25s - loss: 1.8450 - acc: 0.3960\n",
      " Optimizer iteration 190, batch 190\n",
      "\n",
      " Learning rate 0.0009767574389743681, Model learning rate 0.0009767574956640601\n",
      "\n",
      " Optimizer iteration 191, batch 191\n",
      "\n",
      " Learning rate 0.0009765140751741306, Model learning rate 0.00097651407122612\n",
      "192/391 [=============>................] - ETA: 25s - loss: 1.8418 - acc: 0.3970\n",
      " Optimizer iteration 192, batch 192\n",
      "\n",
      " Learning rate 0.000976269474554854, Model learning rate 0.0009762694826349616\n",
      "\n",
      " Optimizer iteration 193, batch 193\n",
      "\n",
      " Learning rate 0.0009760236377514128, Model learning rate 0.0009760236134752631\n",
      "194/391 [=============>................] - ETA: 24s - loss: 1.8385 - acc: 0.3980\n",
      " Optimizer iteration 194, batch 194\n",
      "\n",
      " Learning rate 0.0009757765654018904, Model learning rate 0.0009757765801623464\n",
      "195/391 [=============>................] - ETA: 24s - loss: 1.8370 - acc: 0.3986\n",
      " Optimizer iteration 195, batch 195\n",
      "\n",
      " Learning rate 0.0009755282581475768, Model learning rate 0.0009755282662808895\n",
      "196/391 [==============>...............] - ETA: 24s - loss: 1.8364 - acc: 0.3987\n",
      " Optimizer iteration 196, batch 196\n",
      "\n",
      " Learning rate 0.0009752787166329675, Model learning rate 0.0009752787300385535\n",
      "\n",
      " Optimizer iteration 197, batch 197\n",
      "\n",
      " Learning rate 0.0009750279415057616, Model learning rate 0.0009750279132276773\n",
      "198/391 [==============>...............] - ETA: 24s - loss: 1.8338 - acc: 0.3996\n",
      " Optimizer iteration 198, batch 198\n",
      "\n",
      " Learning rate 0.0009747759334168601, Model learning rate 0.0009747759322635829\n",
      "\n",
      " Optimizer iteration 199, batch 199\n",
      "\n",
      " Learning rate 0.0009745226930203639, Model learning rate 0.0009745226707309484\n",
      "200/391 [==============>...............] - ETA: 23s - loss: 1.8310 - acc: 0.4004\n",
      " Optimizer iteration 200, batch 200\n",
      "\n",
      " Learning rate 0.0009742682209735727, Model learning rate 0.0009742682450450957\n",
      "\n",
      " Optimizer iteration 201, batch 201\n",
      "\n",
      " Learning rate 0.0009740125179369832, Model learning rate 0.0009740125387907028\n",
      "202/391 [==============>...............] - ETA: 23s - loss: 1.8285 - acc: 0.4011\n",
      " Optimizer iteration 202, batch 202\n",
      "\n",
      " Learning rate 0.0009737555845742868, Model learning rate 0.0009737556101754308\n",
      "203/391 [==============>...............] - ETA: 23s - loss: 1.8276 - acc: 0.4011\n",
      " Optimizer iteration 203, batch 203\n",
      "\n",
      " Learning rate 0.0009734974215523685, Model learning rate 0.0009734974009916186\n",
      "204/391 [==============>...............] - ETA: 23s - loss: 1.8275 - acc: 0.4012\n",
      " Optimizer iteration 204, batch 204\n",
      "\n",
      " Learning rate 0.0009732380295413049, Model learning rate 0.0009732380276545882\n",
      "205/391 [==============>...............] - ETA: 23s - loss: 1.8256 - acc: 0.4021\n",
      " Optimizer iteration 205, batch 205\n",
      "\n",
      " Learning rate 0.0009729774092143626, Model learning rate 0.0009729774319566786\n",
      "\n",
      " Optimizer iteration 206, batch 206\n",
      "\n",
      " Learning rate 0.0009727155612479963, Model learning rate 0.0009727155556902289\n",
      "207/391 [==============>...............] - ETA: 22s - loss: 1.8222 - acc: 0.4030\n",
      " Optimizer iteration 207, batch 207\n",
      "\n",
      " Learning rate 0.000972452486321847, Model learning rate 0.000972452515270561\n",
      "208/391 [==============>...............] - ETA: 22s - loss: 1.8209 - acc: 0.4037\n",
      " Optimizer iteration 208, batch 208\n",
      "\n",
      " Learning rate 0.0009721881851187406, Model learning rate 0.0009721881942823529\n",
      "\n",
      " Optimizer iteration 209, batch 209\n",
      "\n",
      " Learning rate 0.0009719226583246855, Model learning rate 0.0009719226509332657\n",
      "210/391 [===============>..............] - ETA: 22s - loss: 1.8179 - acc: 0.4046\n",
      " Optimizer iteration 210, batch 210\n",
      "\n",
      " Learning rate 0.0009716559066288715, Model learning rate 0.0009716558852232993\n",
      "211/391 [===============>..............] - ETA: 22s - loss: 1.8159 - acc: 0.4053\n",
      " Optimizer iteration 211, batch 211\n",
      "\n",
      " Learning rate 0.0009713879307236677, Model learning rate 0.0009713879553601146\n",
      "212/391 [===============>..............] - ETA: 21s - loss: 1.8152 - acc: 0.4056\n",
      " Optimizer iteration 212, batch 212\n",
      "\n",
      " Learning rate 0.0009711187313046206, Model learning rate 0.0009711187449283898\n",
      "213/391 [===============>..............] - ETA: 21s - loss: 1.8127 - acc: 0.4063\n",
      " Optimizer iteration 213, batch 213\n",
      "\n",
      " Learning rate 0.0009708483090704523, Model learning rate 0.0009708483121357858\n",
      "\n",
      " Optimizer iteration 214, batch 214\n",
      "\n",
      " Learning rate 0.0009705766647230589, Model learning rate 0.0009705766569823027\n",
      "215/391 [===============>..............] - ETA: 21s - loss: 1.8100 - acc: 0.4070\n",
      " Optimizer iteration 215, batch 215\n",
      "\n",
      " Learning rate 0.0009703037989675086, Model learning rate 0.0009703037794679403\n",
      "216/391 [===============>..............] - ETA: 21s - loss: 1.8092 - acc: 0.4072\n",
      " Optimizer iteration 216, batch 216\n",
      "\n",
      " Learning rate 0.0009700297125120399, Model learning rate 0.0009700297378003597\n",
      "217/391 [===============>..............] - ETA: 21s - loss: 1.8075 - acc: 0.4077\n",
      " Optimizer iteration 217, batch 217\n",
      "\n",
      " Learning rate 0.0009697544060680594, Model learning rate 0.000969754415564239\n",
      "\n",
      " Optimizer iteration 218, batch 218\n",
      "\n",
      " Learning rate 0.0009694778803501403, Model learning rate 0.0009694778709672391\n",
      "219/391 [===============>..............] - ETA: 20s - loss: 1.8055 - acc: 0.4082\n",
      " Optimizer iteration 219, batch 219\n",
      "\n",
      " Learning rate 0.0009692001360760211, Model learning rate 0.000969200162217021\n",
      "220/391 [===============>..............] - ETA: 20s - loss: 1.8041 - acc: 0.4087\n",
      " Optimizer iteration 220, batch 220\n",
      "\n",
      " Learning rate 0.0009689211739666023, Model learning rate 0.0009689211728982627\n",
      "\n",
      " Optimizer iteration 221, batch 221\n",
      "\n",
      " Learning rate 0.0009686409947459458, Model learning rate 0.0009686410194262862\n",
      "222/391 [================>.............] - ETA: 20s - loss: 1.8016 - acc: 0.4099\n",
      " Optimizer iteration 222, batch 222\n",
      "\n",
      " Learning rate 0.0009683595991412725, Model learning rate 0.0009683595853857696\n",
      "\n",
      " Optimizer iteration 223, batch 223\n",
      "\n",
      " Learning rate 0.0009680769878829606, Model learning rate 0.0009680769871920347\n",
      "224/391 [================>.............] - ETA: 20s - loss: 1.7995 - acc: 0.4106\n",
      " Optimizer iteration 224, batch 224\n",
      "\n",
      " Learning rate 0.0009677931617045432, Model learning rate 0.0009677931666374207\n",
      "225/391 [================>.............] - ETA: 19s - loss: 1.7983 - acc: 0.4109\n",
      " Optimizer iteration 225, batch 225\n",
      "\n",
      " Learning rate 0.0009675081213427075, Model learning rate 0.0009675081237219274\n",
      "226/391 [================>.............] - ETA: 19s - loss: 1.7963 - acc: 0.4114\n",
      " Optimizer iteration 226, batch 226\n",
      "\n",
      " Learning rate 0.0009672218675372913, Model learning rate 0.000967221858445555\n",
      "227/391 [================>.............] - ETA: 19s - loss: 1.7952 - acc: 0.4118\n",
      " Optimizer iteration 227, batch 227\n",
      "\n",
      " Learning rate 0.0009669344010312829, Model learning rate 0.0009669344290159643\n",
      "228/391 [================>.............] - ETA: 19s - loss: 1.7940 - acc: 0.4121\n",
      " Optimizer iteration 228, batch 228\n",
      "\n",
      " Learning rate 0.0009666457225708174, Model learning rate 0.0009666457190178335\n",
      "229/391 [================>.............] - ETA: 19s - loss: 1.7925 - acc: 0.4125\n",
      " Optimizer iteration 229, batch 229\n",
      "\n",
      " Learning rate 0.0009663558329051762, Model learning rate 0.0009663558448664844\n",
      "230/391 [================>.............] - ETA: 19s - loss: 1.7907 - acc: 0.4132\n",
      " Optimizer iteration 230, batch 230\n",
      "\n",
      " Learning rate 0.0009660647327867839, Model learning rate 0.0009660647483542562\n",
      "\n",
      " Optimizer iteration 231, batch 231\n",
      "\n",
      " Learning rate 0.0009657724229712075, Model learning rate 0.0009657724294811487\n",
      "232/391 [================>.............] - ETA: 18s - loss: 1.7880 - acc: 0.4145\n",
      " Optimizer iteration 232, batch 232\n",
      "\n",
      " Learning rate 0.0009654789042171535, Model learning rate 0.0009654788882471621\n",
      "\n",
      " Optimizer iteration 233, batch 233\n",
      "\n",
      " Learning rate 0.000965184177286466, Model learning rate 0.0009651841828599572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234/391 [================>.............] - ETA: 18s - loss: 1.7857 - acc: 0.4153\n",
      " Optimizer iteration 234, batch 234\n",
      "\n",
      " Learning rate 0.0009648882429441257, Model learning rate 0.0009648882551118731\n",
      "\n",
      " Optimizer iteration 235, batch 235\n",
      "\n",
      " Learning rate 0.0009645911019582466, Model learning rate 0.0009645911050029099\n",
      "236/391 [=================>............] - ETA: 18s - loss: 1.7836 - acc: 0.4159\n",
      " Optimizer iteration 236, batch 236\n",
      "\n",
      " Learning rate 0.0009642927551000749, Model learning rate 0.0009642927325330675\n",
      "\n",
      " Optimizer iteration 237, batch 237\n",
      "\n",
      " Learning rate 0.0009639932031439866, Model learning rate 0.0009639931959100068\n",
      "238/391 [=================>............] - ETA: 17s - loss: 1.7819 - acc: 0.4168\n",
      " Optimizer iteration 238, batch 238\n",
      "\n",
      " Learning rate 0.0009636924468674854, Model learning rate 0.0009636924369260669\n",
      "\n",
      " Optimizer iteration 239, batch 239\n",
      "\n",
      " Learning rate 0.0009633904870512015, Model learning rate 0.0009633905137889087\n",
      "240/391 [=================>............] - ETA: 17s - loss: 1.7809 - acc: 0.4172\n",
      " Optimizer iteration 240, batch 240\n",
      "\n",
      " Learning rate 0.0009630873244788883, Model learning rate 0.0009630873100832105\n",
      "241/391 [=================>............] - ETA: 17s - loss: 1.7795 - acc: 0.4177\n",
      " Optimizer iteration 241, batch 241\n",
      "\n",
      " Learning rate 0.0009627829599374214, Model learning rate 0.000962782942224294\n",
      "\n",
      " Optimizer iteration 242, batch 242\n",
      "\n",
      " Learning rate 0.0009624773942167958, Model learning rate 0.0009624774102121592\n",
      "243/391 [=================>............] - ETA: 17s - loss: 1.7770 - acc: 0.4184\n",
      " Optimizer iteration 243, batch 243\n",
      "\n",
      " Learning rate 0.0009621706281101248, Model learning rate 0.0009621706558391452\n",
      "244/391 [=================>............] - ETA: 17s - loss: 1.7758 - acc: 0.4187\n",
      " Optimizer iteration 244, batch 244\n",
      "\n",
      " Learning rate 0.0009618626624136369, Model learning rate 0.000961862679105252\n",
      "\n",
      " Optimizer iteration 245, batch 245\n",
      "\n",
      " Learning rate 0.0009615534979266745, Model learning rate 0.0009615534800104797\n",
      "246/391 [=================>............] - ETA: 16s - loss: 1.7743 - acc: 0.4192\n",
      " Optimizer iteration 246, batch 246\n",
      "\n",
      " Learning rate 0.0009612431354516912, Model learning rate 0.0009612431167624891\n",
      "247/391 [=================>............] - ETA: 16s - loss: 1.7735 - acc: 0.4194\n",
      " Optimizer iteration 247, batch 247\n",
      "\n",
      " Learning rate 0.0009609315757942503, Model learning rate 0.0009609315893612802\n",
      "248/391 [==================>...........] - ETA: 16s - loss: 1.7720 - acc: 0.4201\n",
      " Optimizer iteration 248, batch 248\n",
      "\n",
      " Learning rate 0.0009606188197630223, Model learning rate 0.0009606188395991921\n",
      "\n",
      " Optimizer iteration 249, batch 249\n",
      "\n",
      " Learning rate 0.0009603048681697834, Model learning rate 0.0009603048674762249\n",
      "250/391 [==================>...........] - ETA: 16s - loss: 1.7697 - acc: 0.4210\n",
      " Optimizer iteration 250, batch 250\n",
      "\n",
      " Learning rate 0.0009599897218294122, Model learning rate 0.0009599897312000394\n",
      "251/391 [==================>...........] - ETA: 16s - loss: 1.7687 - acc: 0.4213\n",
      " Optimizer iteration 251, batch 251\n",
      "\n",
      " Learning rate 0.0009596733815598888, Model learning rate 0.0009596733725629747\n",
      "252/391 [==================>...........] - ETA: 15s - loss: 1.7680 - acc: 0.4214\n",
      " Optimizer iteration 252, batch 252\n",
      "\n",
      " Learning rate 0.0009593558481822921, Model learning rate 0.0009593558497726917\n",
      "253/391 [==================>...........] - ETA: 15s - loss: 1.7670 - acc: 0.4219\n",
      " Optimizer iteration 253, batch 253\n",
      "\n",
      " Learning rate 0.0009590371225207981, Model learning rate 0.0009590371046215296\n",
      "254/391 [==================>...........] - ETA: 15s - loss: 1.7663 - acc: 0.4222\n",
      " Optimizer iteration 254, batch 254\n",
      "\n",
      " Learning rate 0.0009587172054026768, Model learning rate 0.0009587171953171492\n",
      "255/391 [==================>...........] - ETA: 15s - loss: 1.7652 - acc: 0.4225\n",
      " Optimizer iteration 255, batch 255\n",
      "\n",
      " Learning rate 0.0009583960976582913, Model learning rate 0.0009583961218595505\n",
      "256/391 [==================>...........] - ETA: 15s - loss: 1.7643 - acc: 0.4227\n",
      " Optimizer iteration 256, batch 256\n",
      "\n",
      " Learning rate 0.0009580738001210944, Model learning rate 0.0009580738260410726\n",
      "257/391 [==================>...........] - ETA: 15s - loss: 1.7634 - acc: 0.4232\n",
      " Optimizer iteration 257, batch 257\n",
      "\n",
      " Learning rate 0.000957750313627628, Model learning rate 0.0009577503078617156\n",
      "258/391 [==================>...........] - ETA: 15s - loss: 1.7628 - acc: 0.4234\n",
      " Optimizer iteration 258, batch 258\n",
      "\n",
      " Learning rate 0.0009574256390175192, Model learning rate 0.0009574256255291402\n",
      "259/391 [==================>...........] - ETA: 14s - loss: 1.7623 - acc: 0.4237\n",
      " Optimizer iteration 259, batch 259\n",
      "\n",
      " Learning rate 0.0009570997771334791, Model learning rate 0.0009570997790433466\n",
      "260/391 [==================>...........] - ETA: 14s - loss: 1.7616 - acc: 0.4239\n",
      " Optimizer iteration 260, batch 260\n",
      "\n",
      " Learning rate 0.0009567727288213005, Model learning rate 0.0009567727101966739\n",
      "261/391 [===================>..........] - ETA: 14s - loss: 1.7605 - acc: 0.4241\n",
      " Optimizer iteration 261, batch 261\n",
      "\n",
      " Learning rate 0.0009564444949298558, Model learning rate 0.0009564444771967828\n",
      "\n",
      " Optimizer iteration 262, batch 262\n",
      "\n",
      " Learning rate 0.0009561150763110944, Model learning rate 0.0009561150800436735\n",
      "263/391 [===================>..........] - ETA: 14s - loss: 1.7591 - acc: 0.4246\n",
      " Optimizer iteration 263, batch 263\n",
      "\n",
      " Learning rate 0.0009557844738200408, Model learning rate 0.000955784460529685\n",
      "264/391 [===================>..........] - ETA: 14s - loss: 1.7585 - acc: 0.4249\n",
      " Optimizer iteration 264, batch 264\n",
      "\n",
      " Learning rate 0.0009554526883147925, Model learning rate 0.0009554526768624783\n",
      "265/391 [===================>..........] - ETA: 14s - loss: 1.7577 - acc: 0.4253\n",
      " Optimizer iteration 265, batch 265\n",
      "\n",
      " Learning rate 0.0009551197206565174, Model learning rate 0.0009551197290420532\n",
      "\n",
      " Optimizer iteration 266, batch 266\n",
      "\n",
      " Learning rate 0.0009547855717094515, Model learning rate 0.000954785558860749\n",
      "267/391 [===================>..........] - ETA: 13s - loss: 1.7559 - acc: 0.4259\n",
      " Optimizer iteration 267, batch 267\n",
      "\n",
      " Learning rate 0.0009544502423408973, Model learning rate 0.0009544502245262265\n",
      "268/391 [===================>..........] - ETA: 13s - loss: 1.7551 - acc: 0.4264\n",
      " Optimizer iteration 268, batch 268\n",
      "\n",
      " Learning rate 0.0009541137334212211, Model learning rate 0.0009541137260384858\n",
      "\n",
      " Optimizer iteration 269, batch 269\n",
      "\n",
      " Learning rate 0.0009537760458238505, Model learning rate 0.0009537760633975267\n",
      "270/391 [===================>..........] - ETA: 13s - loss: 1.7548 - acc: 0.4265\n",
      " Optimizer iteration 270, batch 270\n",
      "\n",
      " Learning rate 0.0009534371804252727, Model learning rate 0.0009534371783956885\n",
      "\n",
      " Optimizer iteration 271, batch 271\n",
      "\n",
      " Learning rate 0.0009530971381050319, Model learning rate 0.0009530971292406321\n",
      "272/391 [===================>..........] - ETA: 13s - loss: 1.7533 - acc: 0.4271\n",
      " Optimizer iteration 272, batch 272\n",
      "\n",
      " Learning rate 0.0009527559197457272, Model learning rate 0.0009527559159323573\n",
      "\n",
      " Optimizer iteration 273, batch 273\n",
      "\n",
      " Learning rate 0.0009524135262330098, Model learning rate 0.0009524135384708643\n",
      "274/391 [====================>.........] - ETA: 13s - loss: 1.7507 - acc: 0.4277\n",
      " Optimizer iteration 274, batch 274\n",
      "\n",
      " Learning rate 0.0009520699584555812, Model learning rate 0.0009520699386484921\n",
      "\n",
      " Optimizer iteration 275, batch 275\n",
      "\n",
      " Learning rate 0.0009517252173051911, Model learning rate 0.0009517252328805625\n",
      "276/391 [====================>.........] - ETA: 12s - loss: 1.7495 - acc: 0.4282\n",
      " Optimizer iteration 276, batch 276\n",
      "\n",
      " Learning rate 0.0009513793036766345, Model learning rate 0.0009513793047517538\n",
      "277/391 [====================>.........] - ETA: 12s - loss: 1.7483 - acc: 0.4286\n",
      " Optimizer iteration 277, batch 277\n",
      "\n",
      " Learning rate 0.0009510322184677493, Model learning rate 0.0009510322124697268\n",
      "278/391 [====================>.........] - ETA: 12s - loss: 1.7472 - acc: 0.4288\n",
      " Optimizer iteration 278, batch 278\n",
      "\n",
      " Learning rate 0.0009506839625794152, Model learning rate 0.0009506839560344815\n",
      "\n",
      " Optimizer iteration 279, batch 279\n",
      "\n",
      " Learning rate 0.0009503345369155494, Model learning rate 0.000950334535446018\n",
      "280/391 [====================>.........] - ETA: 12s - loss: 1.7457 - acc: 0.4295\n",
      " Optimizer iteration 280, batch 280\n",
      "\n",
      " Learning rate 0.0009499839423831061, Model learning rate 0.0009499839507043362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizer iteration 281, batch 281\n",
      "\n",
      " Learning rate 0.0009496321798920731, Model learning rate 0.0009496322018094361\n",
      "282/391 [====================>.........] - ETA: 12s - loss: 1.7444 - acc: 0.4299\n",
      " Optimizer iteration 282, batch 282\n",
      "\n",
      " Learning rate 0.0009492792503554695, Model learning rate 0.0009492792305536568\n",
      "283/391 [====================>.........] - ETA: 11s - loss: 1.7434 - acc: 0.4304\n",
      " Optimizer iteration 283, batch 283\n",
      "\n",
      " Learning rate 0.000948925154689344, Model learning rate 0.0009489251533523202\n",
      "284/391 [====================>.........] - ETA: 11s - loss: 1.7423 - acc: 0.4306\n",
      " Optimizer iteration 284, batch 284\n",
      "\n",
      " Learning rate 0.0009485698938127715, Model learning rate 0.0009485699119977653\n",
      "285/391 [====================>.........] - ETA: 11s - loss: 1.7411 - acc: 0.4309\n",
      " Optimizer iteration 285, batch 285\n",
      "\n",
      " Learning rate 0.0009482134686478518, Model learning rate 0.0009482134482823312\n",
      "286/391 [====================>.........] - ETA: 11s - loss: 1.7400 - acc: 0.4312\n",
      " Optimizer iteration 286, batch 286\n",
      "\n",
      " Learning rate 0.0009478558801197064, Model learning rate 0.0009478558786213398\n",
      "287/391 [=====================>........] - ETA: 11s - loss: 1.7386 - acc: 0.4316\n",
      " Optimizer iteration 287, batch 287\n",
      "\n",
      " Learning rate 0.0009474971291564764, Model learning rate 0.0009474971448071301\n",
      "288/391 [=====================>........] - ETA: 11s - loss: 1.7375 - acc: 0.4319\n",
      " Optimizer iteration 288, batch 288\n",
      "\n",
      " Learning rate 0.0009471372166893198, Model learning rate 0.0009471371886320412\n",
      "\n",
      " Optimizer iteration 289, batch 289\n",
      "\n",
      " Learning rate 0.0009467761436524099, Model learning rate 0.000946776126511395\n",
      "290/391 [=====================>........] - ETA: 11s - loss: 1.7364 - acc: 0.4324\n",
      " Optimizer iteration 290, batch 290\n",
      "\n",
      " Learning rate 0.0009464139109829321, Model learning rate 0.0009464139002375305\n",
      "\n",
      " Optimizer iteration 291, batch 291\n",
      "\n",
      " Learning rate 0.0009460505196210813, Model learning rate 0.0009460505098104477\n",
      "292/391 [=====================>........] - ETA: 10s - loss: 1.7349 - acc: 0.4331\n",
      " Optimizer iteration 292, batch 292\n",
      "\n",
      " Learning rate 0.0009456859705100606, Model learning rate 0.0009456859552301466\n",
      "293/391 [=====================>........] - ETA: 10s - loss: 1.7341 - acc: 0.4334\n",
      " Optimizer iteration 293, batch 293\n",
      "\n",
      " Learning rate 0.0009453202645960773, Model learning rate 0.0009453202364966273\n",
      "\n",
      " Optimizer iteration 294, batch 294\n",
      "\n",
      " Learning rate 0.000944953402828342, Model learning rate 0.0009449534118175507\n",
      "295/391 [=====================>........] - ETA: 10s - loss: 1.7326 - acc: 0.4340\n",
      " Optimizer iteration 295, batch 295\n",
      "\n",
      " Learning rate 0.0009445853861590646, Model learning rate 0.0009445853647775948\n",
      "296/391 [=====================>........] - ETA: 10s - loss: 1.7325 - acc: 0.4340\n",
      " Optimizer iteration 296, batch 296\n",
      "\n",
      " Learning rate 0.0009442162155434534, Model learning rate 0.0009442162117920816\n",
      "297/391 [=====================>........] - ETA: 10s - loss: 1.7314 - acc: 0.4346\n",
      " Optimizer iteration 297, batch 297\n",
      "\n",
      " Learning rate 0.0009438458919397112, Model learning rate 0.0009438458946533501\n",
      "\n",
      " Optimizer iteration 298, batch 298\n",
      "\n",
      " Learning rate 0.0009434744163090339, Model learning rate 0.0009434744133614004\n",
      "299/391 [=====================>........] - ETA: 9s - loss: 1.7290 - acc: 0.4353 \n",
      " Optimizer iteration 299, batch 299\n",
      "\n",
      " Learning rate 0.0009431017896156073, Model learning rate 0.0009431017679162323\n",
      "300/391 [======================>.......] - ETA: 9s - loss: 1.7283 - acc: 0.4357\n",
      " Optimizer iteration 300, batch 300\n",
      "\n",
      " Learning rate 0.0009427280128266049, Model learning rate 0.000942728016525507\n",
      "301/391 [======================>.......] - ETA: 9s - loss: 1.7271 - acc: 0.4359\n",
      " Optimizer iteration 301, batch 301\n",
      "\n",
      " Learning rate 0.0009423530869121855, Model learning rate 0.0009423531009815633\n",
      "\n",
      " Optimizer iteration 302, batch 302\n",
      "\n",
      " Learning rate 0.0009419770128454898, Model learning rate 0.0009419770212844014\n",
      "303/391 [======================>.......] - ETA: 9s - loss: 1.7266 - acc: 0.4361\n",
      " Optimizer iteration 303, batch 303\n",
      "\n",
      " Learning rate 0.00094159979160264, Model learning rate 0.0009415997774340212\n",
      "304/391 [======================>.......] - ETA: 9s - loss: 1.7260 - acc: 0.4363\n",
      " Optimizer iteration 304, batch 304\n",
      "\n",
      " Learning rate 0.0009412214241627343, Model learning rate 0.0009412214276380837\n",
      "305/391 [======================>.......] - ETA: 9s - loss: 1.7250 - acc: 0.4368\n",
      " Optimizer iteration 305, batch 305\n",
      "\n",
      " Learning rate 0.000940841911507847, Model learning rate 0.0009408419136889279\n",
      "\n",
      " Optimizer iteration 306, batch 306\n",
      "\n",
      " Learning rate 0.0009404612546230243, Model learning rate 0.0009404612355865538\n",
      "307/391 [======================>.......] - ETA: 9s - loss: 1.7237 - acc: 0.4375\n",
      " Optimizer iteration 307, batch 307\n",
      "\n",
      " Learning rate 0.0009400794544962828, Model learning rate 0.0009400794515386224\n",
      "308/391 [======================>.......] - ETA: 8s - loss: 1.7232 - acc: 0.4379\n",
      " Optimizer iteration 308, batch 308\n",
      "\n",
      " Learning rate 0.0009396965121186058, Model learning rate 0.0009396965033374727\n",
      "309/391 [======================>.......] - ETA: 8s - loss: 1.7222 - acc: 0.4382\n",
      " Optimizer iteration 309, batch 309\n",
      "\n",
      " Learning rate 0.0009393124284839423, Model learning rate 0.0009393124491907656\n",
      "310/391 [======================>.......] - ETA: 8s - loss: 1.7212 - acc: 0.4386\n",
      " Optimizer iteration 310, batch 310\n",
      "\n",
      " Learning rate 0.0009389272045892023, Model learning rate 0.0009389272308908403\n",
      "311/391 [======================>.......] - ETA: 8s - loss: 1.7202 - acc: 0.4388\n",
      " Optimizer iteration 311, batch 311\n",
      "\n",
      " Learning rate 0.0009385408414342564, Model learning rate 0.0009385408484376967\n",
      "312/391 [======================>.......] - ETA: 8s - loss: 1.7192 - acc: 0.4392\n",
      " Optimizer iteration 312, batch 312\n",
      "\n",
      " Learning rate 0.0009381533400219318, Model learning rate 0.0009381533600389957\n",
      "313/391 [=======================>......] - ETA: 8s - loss: 1.7187 - acc: 0.4394\n",
      " Optimizer iteration 313, batch 313\n",
      "\n",
      " Learning rate 0.0009377647013580102, Model learning rate 0.0009377647074870765\n",
      "\n",
      " Optimizer iteration 314, batch 314\n",
      "\n",
      " Learning rate 0.000937374926451225, Model learning rate 0.0009373749489895999\n",
      "315/391 [=======================>......] - ETA: 8s - loss: 1.7172 - acc: 0.4401\n",
      " Optimizer iteration 315, batch 315\n",
      "\n",
      " Learning rate 0.000936984016313259, Model learning rate 0.0009369840263389051\n",
      "316/391 [=======================>......] - ETA: 7s - loss: 1.7164 - acc: 0.4402\n",
      " Optimizer iteration 316, batch 316\n",
      "\n",
      " Learning rate 0.0009365919719587412, Model learning rate 0.0009365919977426529\n",
      "317/391 [=======================>......] - ETA: 7s - loss: 1.7158 - acc: 0.4405\n",
      " Optimizer iteration 317, batch 317\n",
      "\n",
      " Learning rate 0.000936198794405245, Model learning rate 0.0009361988049931824\n",
      "318/391 [=======================>......] - ETA: 7s - loss: 1.7145 - acc: 0.4409\n",
      " Optimizer iteration 318, batch 318\n",
      "\n",
      " Learning rate 0.0009358044846732847, Model learning rate 0.0009358045062981546\n",
      "319/391 [=======================>......] - ETA: 7s - loss: 1.7144 - acc: 0.4410\n",
      " Optimizer iteration 319, batch 319\n",
      "\n",
      " Learning rate 0.0009354090437863132, Model learning rate 0.0009354090434499085\n",
      "320/391 [=======================>......] - ETA: 7s - loss: 1.7132 - acc: 0.4413\n",
      " Optimizer iteration 320, batch 320\n",
      "\n",
      " Learning rate 0.0009350124727707196, Model learning rate 0.000935012474656105\n",
      "\n",
      " Optimizer iteration 321, batch 321\n",
      "\n",
      " Learning rate 0.0009346147726558265, Model learning rate 0.0009346147999167442\n",
      "322/391 [=======================>......] - ETA: 7s - loss: 1.7121 - acc: 0.4416\n",
      " Optimizer iteration 322, batch 322\n",
      "\n",
      " Learning rate 0.0009342159444738864, Model learning rate 0.0009342159610241652\n",
      "323/391 [=======================>......] - ETA: 7s - loss: 1.7111 - acc: 0.4421\n",
      " Optimizer iteration 323, batch 323\n",
      "\n",
      " Learning rate 0.0009338159892600807, Model learning rate 0.0009338160161860287\n",
      "324/391 [=======================>......] - ETA: 7s - loss: 1.7098 - acc: 0.4427\n",
      " Optimizer iteration 324, batch 324\n",
      "\n",
      " Learning rate 0.0009334149080525154, Model learning rate 0.000933414907194674\n",
      "325/391 [=======================>......] - ETA: 6s - loss: 1.7093 - acc: 0.4429\n",
      " Optimizer iteration 325, batch 325\n",
      "\n",
      " Learning rate 0.0009330127018922195, Model learning rate 0.000933012692257762\n",
      "326/391 [========================>.....] - ETA: 6s - loss: 1.7086 - acc: 0.4429\n",
      " Optimizer iteration 326, batch 326\n",
      "\n",
      " Learning rate 0.0009326093718231412, Model learning rate 0.0009326093713752925\n",
      "327/391 [========================>.....] - ETA: 6s - loss: 1.7079 - acc: 0.4431\n",
      " Optimizer iteration 327, batch 327\n",
      "\n",
      " Learning rate 0.0009322049188921467, Model learning rate 0.0009322049445472658\n",
      "328/391 [========================>.....] - ETA: 6s - loss: 1.7070 - acc: 0.4434\n",
      " Optimizer iteration 328, batch 328\n",
      "\n",
      " Learning rate 0.0009317993441490162, Model learning rate 0.0009317993535660207\n",
      "\n",
      " Optimizer iteration 329, batch 329\n",
      "\n",
      " Learning rate 0.0009313926486464419, Model learning rate 0.0009313926566392183\n",
      "330/391 [========================>.....] - ETA: 6s - loss: 1.7058 - acc: 0.4438\n",
      " Optimizer iteration 330, batch 330\n",
      "\n",
      " Learning rate 0.0009309848334400246, Model learning rate 0.0009309848537668586\n",
      "331/391 [========================>.....] - ETA: 6s - loss: 1.7049 - acc: 0.4444\n",
      " Optimizer iteration 331, batch 331\n",
      "\n",
      " Learning rate 0.0009305758995882716, Model learning rate 0.0009305758867412806\n",
      "332/391 [========================>.....] - ETA: 6s - loss: 1.7042 - acc: 0.4447\n",
      " Optimizer iteration 332, batch 332\n",
      "\n",
      " Learning rate 0.0009301658481525939, Model learning rate 0.0009301658719778061\n",
      "\n",
      " Optimizer iteration 333, batch 333\n",
      "\n",
      " Learning rate 0.0009297546801973025, Model learning rate 0.0009297546930611134\n",
      "334/391 [========================>.....] - ETA: 5s - loss: 1.7030 - acc: 0.4451\n",
      " Optimizer iteration 334, batch 334\n",
      "\n",
      " Learning rate 0.0009293423967896076, Model learning rate 0.0009293424081988633\n",
      "\n",
      " Optimizer iteration 335, batch 335\n",
      "\n",
      " Learning rate 0.0009289289989996133, Model learning rate 0.0009289290173910558\n",
      "336/391 [========================>.....] - ETA: 5s - loss: 1.7016 - acc: 0.4455\n",
      " Optimizer iteration 336, batch 336\n",
      "\n",
      " Learning rate 0.0009285144879003172, Model learning rate 0.0009285144624300301\n",
      "\n",
      " Optimizer iteration 337, batch 337\n",
      "\n",
      " Learning rate 0.0009280988645676059, Model learning rate 0.000928098859731108\n",
      "338/391 [========================>.....] - ETA: 5s - loss: 1.7001 - acc: 0.4461\n",
      " Optimizer iteration 338, batch 338\n",
      "\n",
      " Learning rate 0.0009276821300802534, Model learning rate 0.0009276821510866284\n",
      "339/391 [=========================>....] - ETA: 5s - loss: 1.6992 - acc: 0.4463\n",
      " Optimizer iteration 339, batch 339\n",
      "\n",
      " Learning rate 0.0009272642855199171, Model learning rate 0.0009272642782889307\n",
      "340/391 [=========================>....] - ETA: 5s - loss: 1.6986 - acc: 0.4466\n",
      " Optimizer iteration 340, batch 340\n",
      "\n",
      " Learning rate 0.0009268453319711362, Model learning rate 0.0009268453577533364\n",
      "\n",
      " Optimizer iteration 341, batch 341\n",
      "\n",
      " Learning rate 0.000926425270521328, Model learning rate 0.0009264252730645239\n",
      "342/391 [=========================>....] - ETA: 5s - loss: 1.6974 - acc: 0.4470\n",
      " Optimizer iteration 342, batch 342\n",
      "\n",
      " Learning rate 0.0009260041022607859, Model learning rate 0.0009260040824301541\n",
      "343/391 [=========================>....] - ETA: 4s - loss: 1.6963 - acc: 0.4475\n",
      " Optimizer iteration 343, batch 343\n",
      "\n",
      " Learning rate 0.0009255818282826756, Model learning rate 0.0009255818440578878\n",
      "344/391 [=========================>....] - ETA: 4s - loss: 1.6954 - acc: 0.4478\n",
      " Optimizer iteration 344, batch 344\n",
      "\n",
      " Learning rate 0.0009251584496830327, Model learning rate 0.0009251584415324032\n",
      "\n",
      " Optimizer iteration 345, batch 345\n",
      "\n",
      " Learning rate 0.0009247339675607605, Model learning rate 0.0009247339912690222\n",
      "346/391 [=========================>....] - ETA: 4s - loss: 1.6936 - acc: 0.4483\n",
      " Optimizer iteration 346, batch 346\n",
      "\n",
      " Learning rate 0.000924308383017626, Model learning rate 0.000924308376852423\n",
      "\n",
      " Optimizer iteration 347, batch 347\n",
      "\n",
      " Learning rate 0.0009238816971582578, Model learning rate 0.0009238817146979272\n",
      "348/391 [=========================>....] - ETA: 4s - loss: 1.6926 - acc: 0.4487\n",
      " Optimizer iteration 348, batch 348\n",
      "\n",
      " Learning rate 0.000923453911090143, Model learning rate 0.0009234538883902133\n",
      "349/391 [=========================>....] - ETA: 4s - loss: 1.6921 - acc: 0.4487\n",
      " Optimizer iteration 349, batch 349\n",
      "\n",
      " Learning rate 0.0009230250259236243, Model learning rate 0.0009230250143446028\n",
      "350/391 [=========================>....] - ETA: 4s - loss: 1.6914 - acc: 0.4489\n",
      " Optimizer iteration 350, batch 350\n",
      "\n",
      " Learning rate 0.0009225950427718975, Model learning rate 0.000922595034353435\n",
      "351/391 [=========================>....] - ETA: 4s - loss: 1.6908 - acc: 0.4491\n",
      " Optimizer iteration 351, batch 351\n",
      "\n",
      " Learning rate 0.0009221639627510075, Model learning rate 0.0009221639484167099\n",
      "352/391 [==========================>...] - ETA: 4s - loss: 1.6896 - acc: 0.4494\n",
      " Optimizer iteration 352, batch 352\n",
      "\n",
      " Learning rate 0.0009217317869798471, Model learning rate 0.0009217318147420883\n",
      "353/391 [==========================>...] - ETA: 3s - loss: 1.6887 - acc: 0.4497\n",
      " Optimizer iteration 353, batch 353\n",
      "\n",
      " Learning rate 0.0009212985165801529, Model learning rate 0.0009212985169142485\n",
      "354/391 [==========================>...] - ETA: 3s - loss: 1.6880 - acc: 0.4500\n",
      " Optimizer iteration 354, batch 354\n",
      "\n",
      " Learning rate 0.0009208641526765023, Model learning rate 0.0009208641713485122\n",
      "355/391 [==========================>...] - ETA: 3s - loss: 1.6874 - acc: 0.4502\n",
      " Optimizer iteration 355, batch 355\n",
      "\n",
      " Learning rate 0.0009204286963963111, Model learning rate 0.0009204287198372185\n",
      "356/391 [==========================>...] - ETA: 3s - loss: 1.6867 - acc: 0.4505\n",
      " Optimizer iteration 356, batch 356\n",
      "\n",
      " Learning rate 0.0009199921488698308, Model learning rate 0.0009199921623803675\n",
      "357/391 [==========================>...] - ETA: 3s - loss: 1.6858 - acc: 0.4508\n",
      " Optimizer iteration 357, batch 357\n",
      "\n",
      " Learning rate 0.0009195545112301446, Model learning rate 0.0009195544989779592\n",
      "358/391 [==========================>...] - ETA: 3s - loss: 1.6849 - acc: 0.4511\n",
      " Optimizer iteration 358, batch 358\n",
      "\n",
      " Learning rate 0.0009191157846131662, Model learning rate 0.0009191157878376544\n",
      "359/391 [==========================>...] - ETA: 3s - loss: 1.6843 - acc: 0.4514\n",
      " Optimizer iteration 359, batch 359\n",
      "\n",
      " Learning rate 0.0009186759701576349, Model learning rate 0.0009186759707517922\n",
      "360/391 [==========================>...] - ETA: 3s - loss: 1.6832 - acc: 0.4518\n",
      " Optimizer iteration 360, batch 360\n",
      "\n",
      " Learning rate 0.0009182350690051134, Model learning rate 0.0009182350477203727\n",
      "361/391 [==========================>...] - ETA: 3s - loss: 1.6825 - acc: 0.4520\n",
      " Optimizer iteration 361, batch 361\n",
      "\n",
      " Learning rate 0.0009177930822999859, Model learning rate 0.0009177930769510567\n",
      "362/391 [==========================>...] - ETA: 2s - loss: 1.6820 - acc: 0.4520\n",
      " Optimizer iteration 362, batch 362\n",
      "\n",
      " Learning rate 0.0009173500111894535, Model learning rate 0.0009173500002361834\n",
      "\n",
      " Optimizer iteration 363, batch 363\n",
      "\n",
      " Learning rate 0.0009169058568235323, Model learning rate 0.0009169058757834136\n",
      "364/391 [==========================>...] - ETA: 2s - loss: 1.6807 - acc: 0.4525\n",
      " Optimizer iteration 364, batch 364\n",
      "\n",
      " Learning rate 0.0009164606203550497, Model learning rate 0.0009164606453850865\n",
      "365/391 [===========================>..] - ETA: 2s - loss: 1.6798 - acc: 0.4529\n",
      " Optimizer iteration 365, batch 365\n",
      "\n",
      " Learning rate 0.0009160143029396422, Model learning rate 0.0009160143090412021\n",
      "366/391 [===========================>..] - ETA: 2s - loss: 1.6789 - acc: 0.4533\n",
      " Optimizer iteration 366, batch 366\n",
      "\n",
      " Learning rate 0.0009155669057357514, Model learning rate 0.0009155669249594212\n",
      "367/391 [===========================>..] - ETA: 2s - loss: 1.6782 - acc: 0.4536\n",
      " Optimizer iteration 367, batch 367\n",
      "\n",
      " Learning rate 0.0009151184299046221, Model learning rate 0.0009151184349320829\n",
      "368/391 [===========================>..] - ETA: 2s - loss: 1.6775 - acc: 0.4538\n",
      " Optimizer iteration 368, batch 368\n",
      "\n",
      " Learning rate 0.0009146688766102984, Model learning rate 0.0009146688971668482\n",
      "369/391 [===========================>..] - ETA: 2s - loss: 1.6771 - acc: 0.4540\n",
      " Optimizer iteration 369, batch 369\n",
      "\n",
      " Learning rate 0.0009142182470196212, Model learning rate 0.0009142182534560561\n",
      "370/391 [===========================>..] - ETA: 2s - loss: 1.6764 - acc: 0.4542\n",
      " Optimizer iteration 370, batch 370\n",
      "\n",
      " Learning rate 0.000913766542302225, Model learning rate 0.0009137665620073676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371/391 [===========================>..] - ETA: 2s - loss: 1.6760 - acc: 0.4544\n",
      " Optimizer iteration 371, batch 371\n",
      "\n",
      " Learning rate 0.0009133137636305345, Model learning rate 0.0009133137646131217\n",
      "372/391 [===========================>..] - ETA: 1s - loss: 1.6754 - acc: 0.4546\n",
      " Optimizer iteration 372, batch 372\n",
      "\n",
      " Learning rate 0.0009128599121797621, Model learning rate 0.0009128599194809794\n",
      "\n",
      " Optimizer iteration 373, batch 373\n",
      "\n",
      " Learning rate 0.000912404989127905, Model learning rate 0.0009124049684032798\n",
      "374/391 [===========================>..] - ETA: 1s - loss: 1.6745 - acc: 0.4550\n",
      " Optimizer iteration 374, batch 374\n",
      "\n",
      " Learning rate 0.0009119489956557415, Model learning rate 0.0009119489695876837\n",
      "375/391 [===========================>..] - ETA: 1s - loss: 1.6741 - acc: 0.4552\n",
      " Optimizer iteration 375, batch 375\n",
      "\n",
      " Learning rate 0.0009114919329468282, Model learning rate 0.0009114919230341911\n",
      "376/391 [===========================>..] - ETA: 1s - loss: 1.6734 - acc: 0.4555\n",
      " Optimizer iteration 376, batch 376\n",
      "\n",
      " Learning rate 0.000911033802187497, Model learning rate 0.0009110338287428021\n",
      "\n",
      " Optimizer iteration 377, batch 377\n",
      "\n",
      " Learning rate 0.000910574604566852, Model learning rate 0.0009105746285058558\n",
      "378/391 [============================>.] - ETA: 1s - loss: 1.6721 - acc: 0.4560\n",
      " Optimizer iteration 378, batch 378\n",
      "\n",
      " Learning rate 0.0009101143412767665, Model learning rate 0.0009101143223233521\n",
      "\n",
      " Optimizer iteration 379, batch 379\n",
      "\n",
      " Learning rate 0.0009096530135118797, Model learning rate 0.0009096530266106129\n",
      "380/391 [============================>.] - ETA: 1s - loss: 1.6707 - acc: 0.4564\n",
      " Optimizer iteration 380, batch 380\n",
      "\n",
      " Learning rate 0.0009091906224695935, Model learning rate 0.0009091906249523163\n",
      "381/391 [============================>.] - ETA: 1s - loss: 1.6703 - acc: 0.4566\n",
      " Optimizer iteration 381, batch 381\n",
      "\n",
      " Learning rate 0.00090872716935007, Model learning rate 0.0009087271755561233\n",
      "382/391 [============================>.] - ETA: 0s - loss: 1.6696 - acc: 0.4567\n",
      " Optimizer iteration 382, batch 382\n",
      "\n",
      " Learning rate 0.000908262655356228, Model learning rate 0.0009082626784220338\n",
      "\n",
      " Optimizer iteration 383, batch 383\n",
      "\n",
      " Learning rate 0.0009077970816937393, Model learning rate 0.000907797075342387\n",
      "384/391 [============================>.] - ETA: 0s - loss: 1.6680 - acc: 0.4573\n",
      " Optimizer iteration 384, batch 384\n",
      "\n",
      " Learning rate 0.0009073304495710266, Model learning rate 0.0009073304245248437\n",
      "385/391 [============================>.] - ETA: 0s - loss: 1.6674 - acc: 0.4575\n",
      " Optimizer iteration 385, batch 385\n",
      "\n",
      " Learning rate 0.0009068627601992598, Model learning rate 0.0009068627841770649\n",
      "\n",
      " Optimizer iteration 386, batch 386\n",
      "\n",
      " Learning rate 0.0009063940147923528, Model learning rate 0.0009063940378837287\n",
      "387/391 [============================>.] - ETA: 0s - loss: 1.6656 - acc: 0.4581\n",
      " Optimizer iteration 387, batch 387\n",
      "\n",
      " Learning rate 0.000905924214566961, Model learning rate 0.0009059241856448352\n",
      "388/391 [============================>.] - ETA: 0s - loss: 1.6645 - acc: 0.4586\n",
      " Optimizer iteration 388, batch 388\n",
      "\n",
      " Learning rate 0.0009054533607424769, Model learning rate 0.0009054533438757062\n",
      "\n",
      " Optimizer iteration 389, batch 389\n",
      "\n",
      " Learning rate 0.0009049814545410281, Model learning rate 0.0009049814543686807\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.6627 - acc: 0.4592\n",
      " Optimizer iteration 390, batch 390\n",
      "\n",
      " Learning rate 0.0009045084971874737, Model learning rate 0.0009045085171237588\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 1.6621 - acc: 0.4593 - val_loss: 1.8721 - val_acc: 0.4116\n",
      "\n",
      "Epoch 00001: saving model to /home/ubuntu/Projects/hybrid-ensemble/model/run_200/cifar10_ResNet20v1_model-0001.h5\n",
      "Epoch 2/5\n",
      "\n",
      " Optimizer iteration 391, batch 0\n",
      "\n",
      " Learning rate 0.0009040344899094011, Model learning rate 0.0009040344739332795\n",
      "  1/391 [..............................] - ETA: 16s - loss: 1.1662 - acc: 0.6094\n",
      " Optimizer iteration 392, batch 1\n",
      "\n",
      " Learning rate 0.0009035594339371228, Model learning rate 0.0009035594412125647\n",
      "\n",
      " Optimizer iteration 393, batch 2\n",
      "\n",
      " Learning rate 0.0009030833305036732, Model learning rate 0.0009030833025462925\n",
      "  3/391 [..............................] - ETA: 16s - loss: 1.2824 - acc: 0.5755\n",
      " Optimizer iteration 394, batch 3\n",
      "\n",
      " Learning rate 0.0009026061808448055, Model learning rate 0.0009026061743497849\n",
      "\n",
      " Optimizer iteration 395, batch 4\n",
      "\n",
      " Learning rate 0.0009021279861989884, Model learning rate 0.0009021279984153807\n",
      "  5/391 [..............................] - ETA: 16s - loss: 1.3011 - acc: 0.5766\n",
      " Optimizer iteration 396, batch 5\n",
      "\n",
      " Learning rate 0.0009016487478074031, Model learning rate 0.0009016487747430801\n",
      "\n",
      " Optimizer iteration 397, batch 6\n",
      "\n",
      " Learning rate 0.0009011684669139397, Model learning rate 0.0009011684451252222\n",
      "  7/391 [..............................] - ETA: 16s - loss: 1.3342 - acc: 0.5692\n",
      " Optimizer iteration 398, batch 7\n",
      "\n",
      " Learning rate 0.0009006871447651941, Model learning rate 0.0009006871259771287\n",
      "\n",
      " Optimizer iteration 399, batch 8\n",
      "\n",
      " Learning rate 0.0009002047826104651, Model learning rate 0.0009002047590911388\n",
      "  9/391 [..............................] - ETA: 16s - loss: 1.3391 - acc: 0.5677\n",
      " Optimizer iteration 400, batch 9\n",
      "\n",
      " Learning rate 0.0008997213817017506, Model learning rate 0.0008997214026749134\n",
      " 10/391 [..............................] - ETA: 16s - loss: 1.3517 - acc: 0.5617\n",
      " Optimizer iteration 401, batch 10\n",
      "\n",
      " Learning rate 0.0008992369432937451, Model learning rate 0.0008992369403131306\n",
      "\n",
      " Optimizer iteration 402, batch 11\n",
      "\n",
      " Learning rate 0.0008987514686438353, Model learning rate 0.0008987514884211123\n",
      " 12/391 [..............................] - ETA: 16s - loss: 1.3526 - acc: 0.5605\n",
      " Optimizer iteration 403, batch 12\n",
      "\n",
      " Learning rate 0.0008982649590120981, Model learning rate 0.0008982649305835366\n",
      "\n",
      " Optimizer iteration 404, batch 13\n",
      "\n",
      " Learning rate 0.0008977774156612968, Model learning rate 0.0008977774414233863\n",
      " 14/391 [>.............................] - ETA: 16s - loss: 1.3523 - acc: 0.5675\n",
      " Optimizer iteration 405, batch 14\n",
      "\n",
      " Learning rate 0.0008972888398568772, Model learning rate 0.0008972888463176787\n",
      " 15/391 [>.............................] - ETA: 17s - loss: 1.3564 - acc: 0.5677\n",
      " Optimizer iteration 406, batch 15\n",
      "\n",
      " Learning rate 0.0008967992328669654, Model learning rate 0.0008967992616817355\n",
      " 16/391 [>.............................] - ETA: 17s - loss: 1.3511 - acc: 0.5728\n",
      " Optimizer iteration 407, batch 16\n",
      "\n",
      " Learning rate 0.0008963085959623637, Model learning rate 0.000896308571100235\n",
      "\n",
      " Optimizer iteration 408, batch 17\n",
      "\n",
      " Learning rate 0.0008958169304165479, Model learning rate 0.0008958169491961598\n",
      " 18/391 [>.............................] - ETA: 17s - loss: 1.3683 - acc: 0.5694\n",
      " Optimizer iteration 409, batch 18\n",
      "\n",
      " Learning rate 0.0008953242375056634, Model learning rate 0.0008953242213465273\n",
      "\n",
      " Optimizer iteration 410, batch 19\n",
      "\n",
      " Learning rate 0.0008948305185085225, Model learning rate 0.0008948305039666593\n",
      " 20/391 [>.............................] - ETA: 16s - loss: 1.3658 - acc: 0.5711\n",
      " Optimizer iteration 411, batch 20\n",
      "\n",
      " Learning rate 0.0008943357747066003, Model learning rate 0.0008943357970565557\n",
      " 21/391 [>.............................] - ETA: 18s - loss: 1.3700 - acc: 0.5725\n",
      " Optimizer iteration 412, batch 21\n",
      "\n",
      " Learning rate 0.0008938400073840325, Model learning rate 0.0008938399842008948\n",
      " 22/391 [>.............................] - ETA: 18s - loss: 1.3620 - acc: 0.5735\n",
      " Optimizer iteration 413, batch 22\n",
      "\n",
      " Learning rate 0.0008933432178276107, Model learning rate 0.0008933432400226593\n",
      " 23/391 [>.............................] - ETA: 18s - loss: 1.3640 - acc: 0.5727\n",
      " Optimizer iteration 414, batch 23\n",
      "\n",
      " Learning rate 0.0008928454073267801, Model learning rate 0.0008928453898988664\n",
      " 24/391 [>.............................] - ETA: 18s - loss: 1.3628 - acc: 0.5732\n",
      " Optimizer iteration 415, batch 24\n",
      "\n",
      " Learning rate 0.000892346577173636, Model learning rate 0.000892346550244838\n",
      " 25/391 [>.............................] - ETA: 19s - loss: 1.3642 - acc: 0.5734\n",
      " Optimizer iteration 416, batch 25\n",
      "\n",
      " Learning rate 0.0008918467286629199, Model learning rate 0.0008918467210605741\n",
      " 26/391 [>.............................] - ETA: 19s - loss: 1.3636 - acc: 0.5733\n",
      " Optimizer iteration 417, batch 26\n",
      "\n",
      " Learning rate 0.0008913458630920168, Model learning rate 0.0008913458441384137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27/391 [=>............................] - ETA: 19s - loss: 1.3615 - acc: 0.5732\n",
      " Optimizer iteration 418, batch 27\n",
      "\n",
      " Learning rate 0.0008908439817609514, Model learning rate 0.0008908439776860178\n",
      " 28/391 [=>............................] - ETA: 19s - loss: 1.3598 - acc: 0.5728\n",
      " Optimizer iteration 419, batch 28\n",
      "\n",
      " Learning rate 0.0008903410859723847, Model learning rate 0.0008903410634957254\n",
      " 29/391 [=>............................] - ETA: 20s - loss: 1.3592 - acc: 0.5722\n",
      " Optimizer iteration 420, batch 29\n",
      "\n",
      " Learning rate 0.0008898371770316111, Model learning rate 0.0008898371597751975\n",
      " 30/391 [=>............................] - ETA: 20s - loss: 1.3595 - acc: 0.5716\n",
      " Optimizer iteration 421, batch 30\n",
      "\n",
      " Learning rate 0.0008893322562465546, Model learning rate 0.0008893322665244341\n",
      " 31/391 [=>............................] - ETA: 20s - loss: 1.3601 - acc: 0.5716\n",
      " Optimizer iteration 422, batch 31\n",
      "\n",
      " Learning rate 0.0008888263249277656, Model learning rate 0.0008888263255357742\n",
      " 32/391 [=>............................] - ETA: 20s - loss: 1.3618 - acc: 0.5698\n",
      " Optimizer iteration 423, batch 32\n",
      "\n",
      " Learning rate 0.0008883193843884169, Model learning rate 0.0008883193950168788\n",
      " 33/391 [=>............................] - ETA: 20s - loss: 1.3625 - acc: 0.5708\n",
      " Optimizer iteration 424, batch 33\n",
      "\n",
      " Learning rate 0.0008878114359443012, Model learning rate 0.000887811416760087\n",
      " 34/391 [=>............................] - ETA: 21s - loss: 1.3615 - acc: 0.5717\n",
      " Optimizer iteration 425, batch 34\n",
      "\n",
      " Learning rate 0.0008873024809138273, Model learning rate 0.0008873025071807206\n",
      " 35/391 [=>............................] - ETA: 21s - loss: 1.3589 - acc: 0.5725\n",
      " Optimizer iteration 426, batch 35\n",
      "\n",
      " Learning rate 0.0008867925206180165, Model learning rate 0.0008867924916557968\n",
      " 36/391 [=>............................] - ETA: 21s - loss: 1.3581 - acc: 0.5729\n",
      " Optimizer iteration 427, batch 36\n",
      "\n",
      " Learning rate 0.0008862815563804996, Model learning rate 0.0008862815448082983\n",
      " 37/391 [=>............................] - ETA: 21s - loss: 1.3613 - acc: 0.5722\n",
      " Optimizer iteration 428, batch 37\n",
      "\n",
      " Learning rate 0.0008857695895275126, Model learning rate 0.0008857696084305644\n",
      " 38/391 [=>............................] - ETA: 21s - loss: 1.3624 - acc: 0.5732\n",
      " Optimizer iteration 429, batch 38\n",
      "\n",
      " Learning rate 0.0008852566213878947, Model learning rate 0.000885256624314934\n",
      " 39/391 [=>............................] - ETA: 21s - loss: 1.3610 - acc: 0.5733\n",
      " Optimizer iteration 430, batch 39\n",
      "\n",
      " Learning rate 0.000884742653293083, Model learning rate 0.0008847426506690681\n",
      " 40/391 [==>...........................] - ETA: 21s - loss: 1.3619 - acc: 0.5713\n",
      " Optimizer iteration 431, batch 40\n",
      "\n",
      " Learning rate 0.0008842276865771108, Model learning rate 0.0008842276874929667\n",
      " 41/391 [==>...........................] - ETA: 21s - loss: 1.3633 - acc: 0.5705\n",
      " Optimizer iteration 432, batch 41\n",
      "\n",
      " Learning rate 0.0008837117225766032, Model learning rate 0.0008837117347866297\n",
      " 42/391 [==>...........................] - ETA: 21s - loss: 1.3620 - acc: 0.5712\n",
      " Optimizer iteration 433, batch 42\n",
      "\n",
      " Learning rate 0.0008831947626307734, Model learning rate 0.0008831947343423963\n",
      " 43/391 [==>...........................] - ETA: 21s - loss: 1.3618 - acc: 0.5714\n",
      " Optimizer iteration 434, batch 43\n",
      "\n",
      " Learning rate 0.0008826768080814205, Model learning rate 0.0008826768025755882\n",
      " 44/391 [==>...........................] - ETA: 21s - loss: 1.3611 - acc: 0.5723\n",
      " Optimizer iteration 435, batch 44\n",
      "\n",
      " Learning rate 0.0008821578602729241, Model learning rate 0.0008821578812785447\n",
      " 45/391 [==>...........................] - ETA: 21s - loss: 1.3585 - acc: 0.5733\n",
      " Optimizer iteration 436, batch 45\n",
      "\n",
      " Learning rate 0.0008816379205522428, Model learning rate 0.0008816379122436047\n",
      " 46/391 [==>...........................] - ETA: 21s - loss: 1.3552 - acc: 0.5746\n",
      " Optimizer iteration 437, batch 46\n",
      "\n",
      " Learning rate 0.000881116990268909, Model learning rate 0.00088111701188609\n",
      " 47/391 [==>...........................] - ETA: 21s - loss: 1.3541 - acc: 0.5738\n",
      " Optimizer iteration 438, batch 47\n",
      "\n",
      " Learning rate 0.0008805950707750268, Model learning rate 0.000880595063790679\n",
      " 48/391 [==>...........................] - ETA: 21s - loss: 1.3553 - acc: 0.5732\n",
      " Optimizer iteration 439, batch 48\n",
      "\n",
      " Learning rate 0.0008800721634252671, Model learning rate 0.0008800721843726933\n",
      " 49/391 [==>...........................] - ETA: 21s - loss: 1.3558 - acc: 0.5724\n",
      " Optimizer iteration 440, batch 49\n",
      "\n",
      " Learning rate 0.0008795482695768658, Model learning rate 0.0008795482572168112\n",
      " 50/391 [==>...........................] - ETA: 21s - loss: 1.3567 - acc: 0.5714\n",
      " Optimizer iteration 441, batch 50\n",
      "\n",
      " Learning rate 0.0008790233905896185, Model learning rate 0.0008790233987383544\n",
      " 51/391 [==>...........................] - ETA: 21s - loss: 1.3566 - acc: 0.5714\n",
      " Optimizer iteration 442, batch 51\n",
      "\n",
      " Learning rate 0.0008784975278258782, Model learning rate 0.0008784975507296622\n",
      " 52/391 [==>...........................] - ETA: 21s - loss: 1.3544 - acc: 0.5724\n",
      " Optimizer iteration 443, batch 52\n",
      "\n",
      " Learning rate 0.0008779706826505513, Model learning rate 0.0008779706549830735\n",
      " 53/391 [===>..........................] - ETA: 21s - loss: 1.3522 - acc: 0.5731\n",
      " Optimizer iteration 444, batch 53\n",
      "\n",
      " Learning rate 0.0008774428564310938, Model learning rate 0.0008774428279139102\n",
      " 54/391 [===>..........................] - ETA: 21s - loss: 1.3535 - acc: 0.5725\n",
      " Optimizer iteration 445, batch 54\n",
      "\n",
      " Learning rate 0.0008769140505375084, Model learning rate 0.0008769140695221722\n",
      " 55/391 [===>..........................] - ETA: 21s - loss: 1.3522 - acc: 0.5732\n",
      " Optimizer iteration 446, batch 55\n",
      "\n",
      " Learning rate 0.0008763842663423407, Model learning rate 0.0008763842633925378\n",
      "\n",
      " Optimizer iteration 447, batch 56\n",
      "\n",
      " Learning rate 0.0008758535052206749, Model learning rate 0.0008758535259403288\n",
      " 57/391 [===>..........................] - ETA: 21s - loss: 1.3508 - acc: 0.5728\n",
      " Optimizer iteration 448, batch 57\n",
      "\n",
      " Learning rate 0.0008753217685501317, Model learning rate 0.0008753217407502234\n",
      " 58/391 [===>..........................] - ETA: 21s - loss: 1.3516 - acc: 0.5723\n",
      " Optimizer iteration 449, batch 58\n",
      "\n",
      " Learning rate 0.0008747890577108633, Model learning rate 0.0008747890824452043\n",
      " 59/391 [===>..........................] - ETA: 21s - loss: 1.3501 - acc: 0.5727\n",
      " Optimizer iteration 450, batch 59\n",
      "\n",
      " Learning rate 0.0008742553740855505, Model learning rate 0.0008742553764022887\n",
      " 60/391 [===>..........................] - ETA: 21s - loss: 1.3476 - acc: 0.5732\n",
      " Optimizer iteration 451, batch 60\n",
      "\n",
      " Learning rate 0.0008737207190593994, Model learning rate 0.0008737207390367985\n",
      " 61/391 [===>..........................] - ETA: 21s - loss: 1.3470 - acc: 0.5739\n",
      " Optimizer iteration 452, batch 61\n",
      "\n",
      " Learning rate 0.0008731850940201369, Model learning rate 0.0008731851121410728\n",
      " 62/391 [===>..........................] - ETA: 21s - loss: 1.3446 - acc: 0.5748\n",
      " Optimizer iteration 453, batch 62\n",
      "\n",
      " Learning rate 0.000872648500358008, Model learning rate 0.0008726484957151115\n",
      " 63/391 [===>..........................] - ETA: 21s - loss: 1.3445 - acc: 0.5745\n",
      " Optimizer iteration 454, batch 63\n",
      "\n",
      " Learning rate 0.0008721109394657716, Model learning rate 0.0008721109479665756\n",
      " 64/391 [===>..........................] - ETA: 21s - loss: 1.3446 - acc: 0.5740\n",
      " Optimizer iteration 455, batch 64\n",
      "\n",
      " Learning rate 0.0008715724127386971, Model learning rate 0.0008715724106878042\n",
      " 65/391 [===>..........................] - ETA: 21s - loss: 1.3443 - acc: 0.5745\n",
      " Optimizer iteration 456, batch 65\n",
      "\n",
      " Learning rate 0.0008710329215745611, Model learning rate 0.0008710329420864582\n",
      " 66/391 [====>.........................] - ETA: 21s - loss: 1.3464 - acc: 0.5730\n",
      " Optimizer iteration 457, batch 66\n",
      "\n",
      " Learning rate 0.000870492467373643, Model learning rate 0.0008704924839548767\n",
      " 67/391 [====>.........................] - ETA: 21s - loss: 1.3474 - acc: 0.5722\n",
      " Optimizer iteration 458, batch 67\n",
      "\n",
      " Learning rate 0.0008699510515387221, Model learning rate 0.0008699510362930596\n",
      "\n",
      " Optimizer iteration 459, batch 68\n",
      "\n",
      " Learning rate 0.0008694086754750737, Model learning rate 0.0008694086573086679\n",
      " 69/391 [====>.........................] - ETA: 21s - loss: 1.3464 - acc: 0.5719\n",
      " Optimizer iteration 460, batch 69\n",
      "\n",
      " Learning rate 0.0008688653405904651, Model learning rate 0.0008688653470017016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70/391 [====>.........................] - ETA: 21s - loss: 1.3464 - acc: 0.5715\n",
      " Optimizer iteration 461, batch 70\n",
      "\n",
      " Learning rate 0.0008683210482951527, Model learning rate 0.0008683210471644998\n",
      " 71/391 [====>.........................] - ETA: 21s - loss: 1.3472 - acc: 0.5714\n",
      " Optimizer iteration 462, batch 71\n",
      "\n",
      " Learning rate 0.0008677758000018776, Model learning rate 0.0008677758160047233\n",
      " 72/391 [====>.........................] - ETA: 21s - loss: 1.3478 - acc: 0.5709\n",
      " Optimizer iteration 463, batch 72\n",
      "\n",
      " Learning rate 0.0008672295971258625, Model learning rate 0.0008672295953147113\n",
      " 73/391 [====>.........................] - ETA: 21s - loss: 1.3460 - acc: 0.5718\n",
      " Optimizer iteration 464, batch 73\n",
      "\n",
      " Learning rate 0.0008666824410848075, Model learning rate 0.0008666824433021247\n",
      " 74/391 [====>.........................] - ETA: 21s - loss: 1.3476 - acc: 0.5716\n",
      " Optimizer iteration 465, batch 74\n",
      "\n",
      " Learning rate 0.0008661343332988868, Model learning rate 0.0008661343599669635\n",
      " 75/391 [====>.........................] - ETA: 21s - loss: 1.3483 - acc: 0.5715\n",
      " Optimizer iteration 466, batch 75\n",
      "\n",
      " Learning rate 0.0008655852751907451, Model learning rate 0.0008655852871015668\n",
      "\n",
      " Optimizer iteration 467, batch 76\n",
      "\n",
      " Learning rate 0.0008650352681854933, Model learning rate 0.0008650352829135954\n",
      " 77/391 [====>.........................] - ETA: 21s - loss: 1.3440 - acc: 0.5726\n",
      " Optimizer iteration 468, batch 77\n",
      "\n",
      " Learning rate 0.0008644843137107057, Model learning rate 0.0008644842891953886\n",
      " 78/391 [====>.........................] - ETA: 21s - loss: 1.3439 - acc: 0.5725\n",
      " Optimizer iteration 469, batch 78\n",
      "\n",
      " Learning rate 0.0008639324131964155, Model learning rate 0.000863932422362268\n",
      " 79/391 [=====>........................] - ETA: 21s - loss: 1.3413 - acc: 0.5740\n",
      " Optimizer iteration 470, batch 79\n",
      "\n",
      " Learning rate 0.0008633795680751116, Model learning rate 0.0008633795659989119\n",
      "\n",
      " Optimizer iteration 471, batch 80\n",
      "\n",
      " Learning rate 0.0008628257797817344, Model learning rate 0.0008628257783129811\n",
      " 81/391 [=====>........................] - ETA: 21s - loss: 1.3420 - acc: 0.5739\n",
      " Optimizer iteration 472, batch 81\n",
      "\n",
      " Learning rate 0.0008622710497536725, Model learning rate 0.0008622710593044758\n",
      " 82/391 [=====>........................] - ETA: 21s - loss: 1.3420 - acc: 0.5744\n",
      " Optimizer iteration 473, batch 82\n",
      "\n",
      " Learning rate 0.0008617153794307588, Model learning rate 0.0008617153507657349\n",
      " 83/391 [=====>........................] - ETA: 21s - loss: 1.3426 - acc: 0.5744\n",
      " Optimizer iteration 474, batch 83\n",
      "\n",
      " Learning rate 0.000861158770255267, Model learning rate 0.0008611587691120803\n",
      " 84/391 [=====>........................] - ETA: 20s - loss: 1.3433 - acc: 0.5740\n",
      " Optimizer iteration 475, batch 84\n",
      "\n",
      " Learning rate 0.0008606012236719073, Model learning rate 0.0008606011979281902\n",
      " 85/391 [=====>........................] - ETA: 20s - loss: 1.3431 - acc: 0.5739\n",
      " Optimizer iteration 476, batch 85\n",
      "\n",
      " Learning rate 0.0008600427411278233, Model learning rate 0.0008600427536293864\n",
      " 86/391 [=====>........................] - ETA: 20s - loss: 1.3423 - acc: 0.5739\n",
      " Optimizer iteration 477, batch 86\n",
      "\n",
      " Learning rate 0.0008594833240725876, Model learning rate 0.0008594833198003471\n",
      " 87/391 [=====>........................] - ETA: 20s - loss: 1.3418 - acc: 0.5742\n",
      " Optimizer iteration 478, batch 87\n",
      "\n",
      " Learning rate 0.0008589229739581988, Model learning rate 0.0008589229546487331\n",
      " 88/391 [=====>........................] - ETA: 20s - loss: 1.3394 - acc: 0.5752\n",
      " Optimizer iteration 479, batch 88\n",
      "\n",
      " Learning rate 0.0008583616922390771, Model learning rate 0.0008583617163822055\n",
      " 89/391 [=====>........................] - ETA: 20s - loss: 1.3394 - acc: 0.5749\n",
      " Optimizer iteration 480, batch 89\n",
      "\n",
      " Learning rate 0.0008577994803720606, Model learning rate 0.0008577994885854423\n",
      " 90/391 [=====>........................] - ETA: 20s - loss: 1.3398 - acc: 0.5752\n",
      " Optimizer iteration 481, batch 90\n",
      "\n",
      " Learning rate 0.0008572363398164017, Model learning rate 0.0008572363294661045\n",
      " 91/391 [=====>........................] - ETA: 20s - loss: 1.3409 - acc: 0.5749\n",
      " Optimizer iteration 482, batch 91\n",
      "\n",
      " Learning rate 0.0008566722720337634, Model learning rate 0.000856672297231853\n",
      " 92/391 [======>.......................] - ETA: 20s - loss: 1.3402 - acc: 0.5748\n",
      " Optimizer iteration 483, batch 92\n",
      "\n",
      " Learning rate 0.0008561072784882155, Model learning rate 0.000856107275467366\n",
      " 93/391 [======>.......................] - ETA: 20s - loss: 1.3407 - acc: 0.5753\n",
      " Optimizer iteration 484, batch 93\n",
      "\n",
      " Learning rate 0.0008555413606462301, Model learning rate 0.0008555413805879653\n",
      " 94/391 [======>.......................] - ETA: 20s - loss: 1.3396 - acc: 0.5756\n",
      " Optimizer iteration 485, batch 94\n",
      "\n",
      " Learning rate 0.0008549745199766792, Model learning rate 0.000854974496178329\n",
      " 95/391 [======>.......................] - ETA: 20s - loss: 1.3383 - acc: 0.5760\n",
      " Optimizer iteration 486, batch 95\n",
      "\n",
      " Learning rate 0.0008544067579508291, Model learning rate 0.000854406738653779\n",
      "\n",
      " Optimizer iteration 487, batch 96\n",
      "\n",
      " Learning rate 0.0008538380760423383, Model learning rate 0.0008538380498066545\n",
      " 97/391 [======>.......................] - ETA: 20s - loss: 1.3374 - acc: 0.5760\n",
      " Optimizer iteration 488, batch 97\n",
      "\n",
      " Learning rate 0.0008532684757272526, Model learning rate 0.0008532684878446162\n",
      " 98/391 [======>.......................] - ETA: 20s - loss: 1.3362 - acc: 0.5765\n",
      " Optimizer iteration 489, batch 98\n",
      "\n",
      " Learning rate 0.0008526979584840015, Model learning rate 0.0008526979363523424\n",
      " 99/391 [======>.......................] - ETA: 20s - loss: 1.3360 - acc: 0.5764\n",
      " Optimizer iteration 490, batch 99\n",
      "\n",
      " Learning rate 0.0008521265257933948, Model learning rate 0.0008521265117451549\n",
      "100/391 [======>.......................] - ETA: 20s - loss: 1.3379 - acc: 0.5757\n",
      " Optimizer iteration 491, batch 100\n",
      "\n",
      " Learning rate 0.0008515541791386177, Model learning rate 0.0008515541558153927\n",
      "101/391 [======>.......................] - ETA: 20s - loss: 1.3379 - acc: 0.5758\n",
      " Optimizer iteration 492, batch 101\n",
      "\n",
      " Learning rate 0.0008509809200052286, Model learning rate 0.0008509809267707169\n",
      "102/391 [======>.......................] - ETA: 20s - loss: 1.3385 - acc: 0.5759\n",
      " Optimizer iteration 493, batch 102\n",
      "\n",
      " Learning rate 0.0008504067498811532, Model learning rate 0.0008504067664034665\n",
      "103/391 [======>.......................] - ETA: 20s - loss: 1.3397 - acc: 0.5758\n",
      " Optimizer iteration 494, batch 103\n",
      "\n",
      " Learning rate 0.0008498316702566827, Model learning rate 0.0008498316747136414\n",
      "\n",
      " Optimizer iteration 495, batch 104\n",
      "\n",
      " Learning rate 0.0008492556826244686, Model learning rate 0.0008492557099089026\n",
      "105/391 [=======>......................] - ETA: 19s - loss: 1.3383 - acc: 0.5765\n",
      " Optimizer iteration 496, batch 105\n",
      "\n",
      " Learning rate 0.0008486787884795188, Model learning rate 0.0008486788137815893\n",
      "106/391 [=======>......................] - ETA: 19s - loss: 1.3393 - acc: 0.5755\n",
      " Optimizer iteration 497, batch 106\n",
      "\n",
      " Learning rate 0.0008481009893191947, Model learning rate 0.0008481009863317013\n",
      "107/391 [=======>......................] - ETA: 19s - loss: 1.3394 - acc: 0.5753\n",
      " Optimizer iteration 498, batch 107\n",
      "\n",
      " Learning rate 0.0008475222866432064, Model learning rate 0.0008475222857668996\n",
      "\n",
      " Optimizer iteration 499, batch 108\n",
      "\n",
      " Learning rate 0.0008469426819536092, Model learning rate 0.0008469426538795233\n",
      "109/391 [=======>......................] - ETA: 19s - loss: 1.3384 - acc: 0.5756\n",
      " Optimizer iteration 500, batch 109\n",
      "\n",
      " Learning rate 0.0008463621767547997, Model learning rate 0.0008463621488772333\n",
      "110/391 [=======>......................] - ETA: 19s - loss: 1.3385 - acc: 0.5758\n",
      " Optimizer iteration 501, batch 110\n",
      "\n",
      " Learning rate 0.0008457807725535116, Model learning rate 0.0008457807707600296\n",
      "111/391 [=======>......................] - ETA: 19s - loss: 1.3386 - acc: 0.5755\n",
      " Optimizer iteration 502, batch 111\n",
      "\n",
      " Learning rate 0.0008451984708588121, Model learning rate 0.0008451984613202512\n",
      "\n",
      " Optimizer iteration 503, batch 112\n",
      "\n",
      " Learning rate 0.0008446152731820983, Model learning rate 0.0008446152787655592\n",
      "113/391 [=======>......................] - ETA: 19s - loss: 1.3391 - acc: 0.5754\n",
      " Optimizer iteration 504, batch 113\n",
      "\n",
      " Learning rate 0.0008440311810370921, Model learning rate 0.0008440311648882926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/391 [=======>......................] - ETA: 19s - loss: 1.3379 - acc: 0.5756\n",
      " Optimizer iteration 505, batch 114\n",
      "\n",
      " Learning rate 0.0008434461959398376, Model learning rate 0.0008434461778961122\n",
      "115/391 [=======>......................] - ETA: 19s - loss: 1.3382 - acc: 0.5760\n",
      " Optimizer iteration 506, batch 115\n",
      "\n",
      " Learning rate 0.0008428603194086966, Model learning rate 0.0008428603177890182\n",
      "\n",
      " Optimizer iteration 507, batch 116\n",
      "\n",
      " Learning rate 0.0008422735529643444, Model learning rate 0.0008422735263593495\n",
      "117/391 [=======>......................] - ETA: 19s - loss: 1.3357 - acc: 0.5768\n",
      " Optimizer iteration 508, batch 117\n",
      "\n",
      " Learning rate 0.0008416858981297663, Model learning rate 0.000841685920022428\n",
      "118/391 [========>.....................] - ETA: 19s - loss: 1.3357 - acc: 0.5771\n",
      " Optimizer iteration 509, batch 118\n",
      "\n",
      " Learning rate 0.0008410973564302533, Model learning rate 0.000841097382362932\n",
      "\n",
      " Optimizer iteration 510, batch 119\n",
      "\n",
      " Learning rate 0.0008405079293933986, Model learning rate 0.0008405079133808613\n",
      "120/391 [========>.....................] - ETA: 19s - loss: 1.3357 - acc: 0.5768\n",
      " Optimizer iteration 511, batch 120\n",
      "\n",
      " Learning rate 0.000839917618549093, Model learning rate 0.0008399176294915378\n",
      "121/391 [========>.....................] - ETA: 18s - loss: 1.3369 - acc: 0.5764\n",
      " Optimizer iteration 512, batch 121\n",
      "\n",
      " Learning rate 0.0008393264254295217, Model learning rate 0.0008393264142796397\n",
      "122/391 [========>.....................] - ETA: 19s - loss: 1.3372 - acc: 0.5765\n",
      " Optimizer iteration 513, batch 122\n",
      "\n",
      " Learning rate 0.0008387343515691595, Model learning rate 0.0008387343259528279\n",
      "\n",
      " Optimizer iteration 514, batch 123\n",
      "\n",
      " Learning rate 0.0008381413985047672, Model learning rate 0.0008381414227187634\n",
      "124/391 [========>.....................] - ETA: 18s - loss: 1.3375 - acc: 0.5759\n",
      " Optimizer iteration 515, batch 124\n",
      "\n",
      " Learning rate 0.0008375475677753881, Model learning rate 0.0008375475881621242\n",
      "125/391 [========>.....................] - ETA: 18s - loss: 1.3363 - acc: 0.5763\n",
      " Optimizer iteration 516, batch 125\n",
      "\n",
      " Learning rate 0.0008369528609223429, Model learning rate 0.0008369528804905713\n",
      "126/391 [========>.....................] - ETA: 18s - loss: 1.3357 - acc: 0.5764\n",
      " Optimizer iteration 517, batch 126\n",
      "\n",
      " Learning rate 0.0008363572794892267, Model learning rate 0.0008363572997041047\n",
      "\n",
      " Optimizer iteration 518, batch 127\n",
      "\n",
      " Learning rate 0.0008357608250219047, Model learning rate 0.0008357608458027244\n",
      "128/391 [========>.....................] - ETA: 18s - loss: 1.3357 - acc: 0.5760\n",
      " Optimizer iteration 519, batch 128\n",
      "\n",
      " Learning rate 0.0008351634990685079, Model learning rate 0.0008351635187864304\n",
      "129/391 [========>.....................] - ETA: 18s - loss: 1.3361 - acc: 0.5755\n",
      " Optimizer iteration 520, batch 129\n",
      "\n",
      " Learning rate 0.0008345653031794292, Model learning rate 0.0008345653186552227\n",
      "130/391 [========>.....................] - ETA: 18s - loss: 1.3359 - acc: 0.5760\n",
      " Optimizer iteration 521, batch 130\n",
      "\n",
      " Learning rate 0.0008339662389073197, Model learning rate 0.0008339662454091012\n",
      "\n",
      " Optimizer iteration 522, batch 131\n",
      "\n",
      " Learning rate 0.0008333663078070846, Model learning rate 0.0008333662990480661\n",
      "132/391 [=========>....................] - ETA: 18s - loss: 1.3350 - acc: 0.5761\n",
      " Optimizer iteration 523, batch 132\n",
      "\n",
      " Learning rate 0.0008327655114358782, Model learning rate 0.0008327655377797782\n",
      "133/391 [=========>....................] - ETA: 18s - loss: 1.3349 - acc: 0.5761\n",
      " Optimizer iteration 524, batch 133\n",
      "\n",
      " Learning rate 0.0008321638513531018, Model learning rate 0.0008321638451889157\n",
      "134/391 [=========>....................] - ETA: 18s - loss: 1.3342 - acc: 0.5764\n",
      " Optimizer iteration 525, batch 134\n",
      "\n",
      " Learning rate 0.0008315613291203976, Model learning rate 0.0008315613376908004\n",
      "\n",
      " Optimizer iteration 526, batch 135\n",
      "\n",
      " Learning rate 0.000830957946301646, Model learning rate 0.0008309579570777714\n",
      "136/391 [=========>....................] - ETA: 18s - loss: 1.3341 - acc: 0.5765\n",
      " Optimizer iteration 527, batch 136\n",
      "\n",
      " Learning rate 0.0008303537044629611, Model learning rate 0.0008303537033498287\n",
      "137/391 [=========>....................] - ETA: 18s - loss: 1.3336 - acc: 0.5766\n",
      " Optimizer iteration 528, batch 137\n",
      "\n",
      " Learning rate 0.0008297486051726862, Model learning rate 0.0008297485765069723\n",
      "138/391 [=========>....................] - ETA: 17s - loss: 1.3320 - acc: 0.5772\n",
      " Optimizer iteration 529, batch 138\n",
      "\n",
      " Learning rate 0.0008291426500013908, Model learning rate 0.0008291426347568631\n",
      "\n",
      " Optimizer iteration 530, batch 139\n",
      "\n",
      " Learning rate 0.0008285358405218655, Model learning rate 0.0008285358198918402\n",
      "140/391 [=========>....................] - ETA: 17s - loss: 1.3328 - acc: 0.5768\n",
      " Optimizer iteration 531, batch 140\n",
      "\n",
      " Learning rate 0.0008279281783091181, Model learning rate 0.0008279281901195645\n",
      "141/391 [=========>....................] - ETA: 17s - loss: 1.3317 - acc: 0.5774\n",
      " Optimizer iteration 532, batch 141\n",
      "\n",
      " Learning rate 0.0008273196649403702, Model learning rate 0.0008273196872323751\n",
      "142/391 [=========>....................] - ETA: 17s - loss: 1.3314 - acc: 0.5774\n",
      " Optimizer iteration 533, batch 142\n",
      "\n",
      " Learning rate 0.0008267103019950528, Model learning rate 0.0008267103112302721\n",
      "143/391 [=========>....................] - ETA: 17s - loss: 1.3310 - acc: 0.5774\n",
      " Optimizer iteration 534, batch 143\n",
      "\n",
      " Learning rate 0.000826100091054801, Model learning rate 0.0008261000621132553\n",
      "144/391 [==========>...................] - ETA: 17s - loss: 1.3320 - acc: 0.5771\n",
      " Optimizer iteration 535, batch 144\n",
      "\n",
      " Learning rate 0.0008254890337034519, Model learning rate 0.0008254890562966466\n",
      "145/391 [==========>...................] - ETA: 17s - loss: 1.3319 - acc: 0.5769\n",
      " Optimizer iteration 536, batch 145\n",
      "\n",
      " Learning rate 0.0008248771315270392, Model learning rate 0.0008248771191574633\n",
      "146/391 [==========>...................] - ETA: 17s - loss: 1.3332 - acc: 0.5764\n",
      " Optimizer iteration 537, batch 146\n",
      "\n",
      " Learning rate 0.0008242643861137891, Model learning rate 0.0008242643671110272\n",
      "\n",
      " Optimizer iteration 538, batch 147\n",
      "\n",
      " Learning rate 0.0008236507990541169, Model learning rate 0.0008236508001573384\n",
      "148/391 [==========>...................] - ETA: 17s - loss: 1.3328 - acc: 0.5767\n",
      " Optimizer iteration 539, batch 148\n",
      "\n",
      " Learning rate 0.0008230363719406223, Model learning rate 0.0008230363600887358\n",
      "149/391 [==========>...................] - ETA: 17s - loss: 1.3321 - acc: 0.5772\n",
      " Optimizer iteration 540, batch 149\n",
      "\n",
      " Learning rate 0.0008224211063680853, Model learning rate 0.0008224211051128805\n",
      "150/391 [==========>...................] - ETA: 17s - loss: 1.3321 - acc: 0.5770\n",
      " Optimizer iteration 541, batch 150\n",
      "\n",
      " Learning rate 0.0008218050039334624, Model learning rate 0.0008218049770221114\n",
      "151/391 [==========>...................] - ETA: 17s - loss: 1.3317 - acc: 0.5773\n",
      " Optimizer iteration 542, batch 151\n",
      "\n",
      " Learning rate 0.0008211880662358817, Model learning rate 0.0008211880922317505\n",
      "152/391 [==========>...................] - ETA: 16s - loss: 1.3315 - acc: 0.5774\n",
      " Optimizer iteration 543, batch 152\n",
      "\n",
      " Learning rate 0.00082057029487664, Model learning rate 0.0008205702761188149\n",
      "153/391 [==========>...................] - ETA: 16s - loss: 1.3316 - acc: 0.5774\n",
      " Optimizer iteration 544, batch 153\n",
      "\n",
      " Learning rate 0.0008199516914591976, Model learning rate 0.0008199517033062875\n",
      "154/391 [==========>...................] - ETA: 16s - loss: 1.3310 - acc: 0.5779\n",
      " Optimizer iteration 545, batch 154\n",
      "\n",
      " Learning rate 0.0008193322575891739, Model learning rate 0.0008193322573788464\n",
      "\n",
      " Optimizer iteration 546, batch 155\n",
      "\n",
      " Learning rate 0.0008187119948743449, Model learning rate 0.0008187119965441525\n",
      "156/391 [==========>...................] - ETA: 16s - loss: 1.3311 - acc: 0.5774\n",
      " Optimizer iteration 547, batch 156\n",
      "\n",
      " Learning rate 0.000818090904924637, Model learning rate 0.0008180909208022058\n",
      "157/391 [===========>..................] - ETA: 16s - loss: 1.3301 - acc: 0.5779\n",
      " Optimizer iteration 548, batch 157\n",
      "\n",
      " Learning rate 0.0008174689893521239, Model learning rate 0.0008174689719453454\n",
      "158/391 [===========>..................] - ETA: 16s - loss: 1.3298 - acc: 0.5782\n",
      " Optimizer iteration 549, batch 158\n",
      "\n",
      " Learning rate 0.0008168462497710226, Model learning rate 0.0008168462663888931\n",
      "\n",
      " Optimizer iteration 550, batch 159\n",
      "\n",
      " Learning rate 0.0008162226877976886, Model learning rate 0.0008162226877175272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/391 [===========>..................] - ETA: 16s - loss: 1.3317 - acc: 0.5773\n",
      " Optimizer iteration 551, batch 160\n",
      "\n",
      " Learning rate 0.0008155983050506122, Model learning rate 0.0008155982941389084\n",
      "161/391 [===========>..................] - ETA: 16s - loss: 1.3314 - acc: 0.5776\n",
      " Optimizer iteration 552, batch 161\n",
      "\n",
      " Learning rate 0.0008149731031504135, Model learning rate 0.0008149730856530368\n",
      "162/391 [===========>..................] - ETA: 16s - loss: 1.3309 - acc: 0.5777\n",
      " Optimizer iteration 553, batch 162\n",
      "\n",
      " Learning rate 0.0008143470837198393, Model learning rate 0.0008143470622599125\n",
      "\n",
      " Optimizer iteration 554, batch 163\n",
      "\n",
      " Learning rate 0.0008137202483837583, Model learning rate 0.0008137202239595354\n",
      "164/391 [===========>..................] - ETA: 16s - loss: 1.3294 - acc: 0.5783\n",
      " Optimizer iteration 555, batch 164\n",
      "\n",
      " Learning rate 0.0008130925987691568, Model learning rate 0.0008130925707519054\n",
      "165/391 [===========>..................] - ETA: 16s - loss: 1.3302 - acc: 0.5779\n",
      " Optimizer iteration 556, batch 165\n",
      "\n",
      " Learning rate 0.0008124641365051346, Model learning rate 0.0008124641608446836\n",
      "166/391 [===========>..................] - ETA: 16s - loss: 1.3302 - acc: 0.5780\n",
      " Optimizer iteration 557, batch 166\n",
      "\n",
      " Learning rate 0.0008118348632229007, Model learning rate 0.0008118348778225482\n",
      "167/391 [===========>..................] - ETA: 16s - loss: 1.3306 - acc: 0.5780\n",
      " Optimizer iteration 558, batch 167\n",
      "\n",
      " Learning rate 0.0008112047805557692, Model learning rate 0.0008112047798931599\n",
      "\n",
      " Optimizer iteration 559, batch 168\n",
      "\n",
      " Learning rate 0.0008105738901391552, Model learning rate 0.0008105738670565188\n",
      "169/391 [===========>..................] - ETA: 15s - loss: 1.3289 - acc: 0.5787\n",
      " Optimizer iteration 560, batch 169\n",
      "\n",
      " Learning rate 0.0008099421936105702, Model learning rate 0.0008099421975202858\n",
      "170/391 [============>.................] - ETA: 15s - loss: 1.3278 - acc: 0.5791\n",
      " Optimizer iteration 561, batch 170\n",
      "\n",
      " Learning rate 0.0008093096926096177, Model learning rate 0.0008093097130768001\n",
      "171/391 [============>.................] - ETA: 15s - loss: 1.3271 - acc: 0.5796\n",
      " Optimizer iteration 562, batch 171\n",
      "\n",
      " Learning rate 0.00080867638877799, Model learning rate 0.0008086764137260616\n",
      "\n",
      " Optimizer iteration 563, batch 172\n",
      "\n",
      " Learning rate 0.0008080422837594627, Model learning rate 0.0008080422994680703\n",
      "173/391 [============>.................] - ETA: 15s - loss: 1.3261 - acc: 0.5800\n",
      " Optimizer iteration 564, batch 173\n",
      "\n",
      " Learning rate 0.0008074073791998906, Model learning rate 0.0008074073703028262\n",
      "174/391 [============>.................] - ETA: 15s - loss: 1.3266 - acc: 0.5796\n",
      " Optimizer iteration 565, batch 174\n",
      "\n",
      " Learning rate 0.0008067716767472045, Model learning rate 0.0008067716844379902\n",
      "\n",
      " Optimizer iteration 566, batch 175\n",
      "\n",
      " Learning rate 0.0008061351780514057, Model learning rate 0.0008061351836659014\n",
      "176/391 [============>.................] - ETA: 15s - loss: 1.3270 - acc: 0.5795\n",
      " Optimizer iteration 567, batch 176\n",
      "\n",
      " Learning rate 0.0008054978847645622, Model learning rate 0.0008054978679865599\n",
      "177/391 [============>.................] - ETA: 15s - loss: 1.3270 - acc: 0.5795\n",
      " Optimizer iteration 568, batch 177\n",
      "\n",
      " Learning rate 0.0008048597985408047, Model learning rate 0.0008048597956076264\n",
      "178/391 [============>.................] - ETA: 15s - loss: 1.3269 - acc: 0.5796\n",
      " Optimizer iteration 569, batch 178\n",
      "\n",
      " Learning rate 0.0008042209210363216, Model learning rate 0.0008042209083214402\n",
      "179/391 [============>.................] - ETA: 15s - loss: 1.3262 - acc: 0.5799\n",
      " Optimizer iteration 570, batch 179\n",
      "\n",
      " Learning rate 0.0008035812539093556, Model learning rate 0.0008035812643356621\n",
      "\n",
      " Optimizer iteration 571, batch 180\n",
      "\n",
      " Learning rate 0.0008029407988201985, Model learning rate 0.0008029408054426312\n",
      "181/391 [============>.................] - ETA: 15s - loss: 1.3248 - acc: 0.5806\n",
      " Optimizer iteration 572, batch 181\n",
      "\n",
      " Learning rate 0.0008022995574311875, Model learning rate 0.0008022995316423476\n",
      "182/391 [============>.................] - ETA: 15s - loss: 1.3248 - acc: 0.5804\n",
      " Optimizer iteration 573, batch 182\n",
      "\n",
      " Learning rate 0.0008016575314067005, Model learning rate 0.0008016575593501329\n",
      "\n",
      " Optimizer iteration 574, batch 183\n",
      "\n",
      " Learning rate 0.0008010147224131523, Model learning rate 0.0008010147139430046\n",
      "184/391 [=============>................] - ETA: 14s - loss: 1.3238 - acc: 0.5810\n",
      " Optimizer iteration 575, batch 184\n",
      "\n",
      " Learning rate 0.0008003711321189895, Model learning rate 0.0008003711118362844\n",
      "185/391 [=============>................] - ETA: 14s - loss: 1.3239 - acc: 0.5810\n",
      " Optimizer iteration 576, batch 185\n",
      "\n",
      " Learning rate 0.000799726762194687, Model learning rate 0.0007997267530299723\n",
      "186/391 [=============>................] - ETA: 14s - loss: 1.3238 - acc: 0.5812\n",
      " Optimizer iteration 577, batch 186\n",
      "\n",
      " Learning rate 0.0007990816143127431, Model learning rate 0.0007990816375240684\n",
      "\n",
      " Optimizer iteration 578, batch 187\n",
      "\n",
      " Learning rate 0.0007984356901476755, Model learning rate 0.0007984357071109116\n",
      "188/391 [=============>................] - ETA: 14s - loss: 1.3223 - acc: 0.5819\n",
      " Optimizer iteration 579, batch 188\n",
      "\n",
      " Learning rate 0.0007977889913760163, Model learning rate 0.000797789019998163\n",
      "189/391 [=============>................] - ETA: 14s - loss: 1.3214 - acc: 0.5822\n",
      " Optimizer iteration 580, batch 189\n",
      "\n",
      " Learning rate 0.0007971415196763087, Model learning rate 0.0007971415179781616\n",
      "190/391 [=============>................] - ETA: 14s - loss: 1.3208 - acc: 0.5825\n",
      " Optimizer iteration 581, batch 190\n",
      "\n",
      " Learning rate 0.0007964932767291019, Model learning rate 0.0007964932592585683\n",
      "\n",
      " Optimizer iteration 582, batch 191\n",
      "\n",
      " Learning rate 0.0007958442642169468, Model learning rate 0.0007958442438393831\n",
      "192/391 [=============>................] - ETA: 14s - loss: 1.3215 - acc: 0.5823\n",
      " Optimizer iteration 583, batch 192\n",
      "\n",
      " Learning rate 0.0007951944838243916, Model learning rate 0.0007951944717206061\n",
      "193/391 [=============>................] - ETA: 14s - loss: 1.3210 - acc: 0.5827\n",
      " Optimizer iteration 584, batch 193\n",
      "\n",
      " Learning rate 0.0007945439372379782, Model learning rate 0.0007945439429022372\n",
      "194/391 [=============>................] - ETA: 14s - loss: 1.3215 - acc: 0.5825\n",
      " Optimizer iteration 585, batch 194\n",
      "\n",
      " Learning rate 0.0007938926261462366, Model learning rate 0.0007938925991766155\n",
      "195/391 [=============>................] - ETA: 14s - loss: 1.3208 - acc: 0.5827\n",
      " Optimizer iteration 586, batch 195\n",
      "\n",
      " Learning rate 0.0007932405522396812, Model learning rate 0.0007932405569590628\n",
      "\n",
      " Optimizer iteration 587, batch 196\n",
      "\n",
      " Learning rate 0.0007925877172108067, Model learning rate 0.0007925876998342574\n",
      "197/391 [==============>...............] - ETA: 13s - loss: 1.3214 - acc: 0.5829\n",
      " Optimizer iteration 588, batch 197\n",
      "\n",
      " Learning rate 0.0007919341227540828, Model learning rate 0.000791934144217521\n",
      "\n",
      " Optimizer iteration 589, batch 198\n",
      "\n",
      " Learning rate 0.0007912797705659507, Model learning rate 0.0007912797736935318\n",
      "199/391 [==============>...............] - ETA: 13s - loss: 1.3209 - acc: 0.5834\n",
      " Optimizer iteration 590, batch 199\n",
      "\n",
      " Learning rate 0.0007906246623448183, Model learning rate 0.0007906246464699507\n",
      "\n",
      " Optimizer iteration 591, batch 200\n",
      "\n",
      " Learning rate 0.0007899687997910558, Model learning rate 0.0007899688207544386\n",
      "201/391 [==============>...............] - ETA: 13s - loss: 1.3223 - acc: 0.5828\n",
      " Optimizer iteration 592, batch 201\n",
      "\n",
      " Learning rate 0.0007893121846069913, Model learning rate 0.0007893121801316738\n",
      "202/391 [==============>...............] - ETA: 13s - loss: 1.3230 - acc: 0.5823\n",
      " Optimizer iteration 593, batch 202\n",
      "\n",
      " Learning rate 0.0007886548184969063, Model learning rate 0.000788654841016978\n",
      "203/391 [==============>...............] - ETA: 13s - loss: 1.3222 - acc: 0.5825\n",
      " Optimizer iteration 594, batch 203\n",
      "\n",
      " Learning rate 0.0007879967031670313, Model learning rate 0.0007879966869950294\n",
      "\n",
      " Optimizer iteration 595, batch 204\n",
      "\n",
      " Learning rate 0.0007873378403255419, Model learning rate 0.0007873378344811499\n",
      "205/391 [==============>...............] - ETA: 13s - loss: 1.3214 - acc: 0.5829\n",
      " Optimizer iteration 596, batch 205\n",
      "\n",
      " Learning rate 0.0007866782316825535, Model learning rate 0.0007866782252676785\n",
      "\n",
      " Optimizer iteration 597, batch 206\n",
      "\n",
      " Learning rate 0.0007860178789501172, Model learning rate 0.0007860178593546152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/391 [==============>...............] - ETA: 13s - loss: 1.3212 - acc: 0.5831\n",
      " Optimizer iteration 598, batch 207\n",
      "\n",
      " Learning rate 0.000785356783842216, Model learning rate 0.000785356794949621\n",
      "208/391 [==============>...............] - ETA: 13s - loss: 1.3216 - acc: 0.5829\n",
      " Optimizer iteration 599, batch 208\n",
      "\n",
      " Learning rate 0.0007846949480747588, Model learning rate 0.0007846949738450348\n",
      "209/391 [===============>..............] - ETA: 13s - loss: 1.3208 - acc: 0.5834\n",
      " Optimizer iteration 600, batch 209\n",
      "\n",
      " Learning rate 0.0007840323733655779, Model learning rate 0.0007840323960408568\n",
      "210/391 [===============>..............] - ETA: 13s - loss: 1.3205 - acc: 0.5836\n",
      " Optimizer iteration 601, batch 210\n",
      "\n",
      " Learning rate 0.0007833690614344231, Model learning rate 0.000783369061537087\n",
      "211/391 [===============>..............] - ETA: 12s - loss: 1.3197 - acc: 0.5840\n",
      " Optimizer iteration 602, batch 211\n",
      "\n",
      " Learning rate 0.0007827050140029577, Model learning rate 0.0007827050285413861\n",
      "\n",
      " Optimizer iteration 603, batch 212\n",
      "\n",
      " Learning rate 0.0007820402327947542, Model learning rate 0.0007820402388460934\n",
      "213/391 [===============>..............] - ETA: 12s - loss: 1.3194 - acc: 0.5840\n",
      " Optimizer iteration 604, batch 213\n",
      "\n",
      " Learning rate 0.0007813747195352895, Model learning rate 0.0007813746924512088\n",
      "\n",
      " Optimizer iteration 605, batch 214\n",
      "\n",
      " Learning rate 0.0007807084759519405, Model learning rate 0.0007807084475643933\n",
      "215/391 [===============>..............] - ETA: 12s - loss: 1.3185 - acc: 0.5846\n",
      " Optimizer iteration 606, batch 215\n",
      "\n",
      " Learning rate 0.0007800415037739801, Model learning rate 0.0007800415041856468\n",
      "\n",
      " Optimizer iteration 607, batch 216\n",
      "\n",
      " Learning rate 0.0007793738047325717, Model learning rate 0.0007793738041073084\n",
      "217/391 [===============>..............] - ETA: 12s - loss: 1.3174 - acc: 0.5851\n",
      " Optimizer iteration 608, batch 217\n",
      "\n",
      " Learning rate 0.0007787053805607659, Model learning rate 0.000778705405537039\n",
      "\n",
      " Optimizer iteration 609, batch 218\n",
      "\n",
      " Learning rate 0.0007780362329934951, Model learning rate 0.0007780362502671778\n",
      "219/391 [===============>..............] - ETA: 12s - loss: 1.3172 - acc: 0.5849\n",
      " Optimizer iteration 610, batch 219\n",
      "\n",
      " Learning rate 0.0007773663637675694, Model learning rate 0.0007773663382977247\n",
      "220/391 [===============>..............] - ETA: 12s - loss: 1.3170 - acc: 0.5853\n",
      " Optimizer iteration 611, batch 220\n",
      "\n",
      " Learning rate 0.000776695774621672, Model learning rate 0.0007766957860440016\n",
      "221/391 [===============>..............] - ETA: 12s - loss: 1.3169 - acc: 0.5853\n",
      " Optimizer iteration 612, batch 221\n",
      "\n",
      " Learning rate 0.0007760244672963548, Model learning rate 0.0007760244770906866\n",
      "\n",
      " Optimizer iteration 613, batch 222\n",
      "\n",
      " Learning rate 0.0007753524435340334, Model learning rate 0.0007753524696454406\n",
      "223/391 [================>.............] - ETA: 12s - loss: 1.3162 - acc: 0.5857\n",
      " Optimizer iteration 614, batch 223\n",
      "\n",
      " Learning rate 0.0007746797050789834, Model learning rate 0.0007746797055006027\n",
      "224/391 [================>.............] - ETA: 12s - loss: 1.3161 - acc: 0.5858\n",
      " Optimizer iteration 615, batch 224\n",
      "\n",
      " Learning rate 0.0007740062536773351, Model learning rate 0.0007740062428638339\n",
      "225/391 [================>.............] - ETA: 11s - loss: 1.3161 - acc: 0.5858\n",
      " Optimizer iteration 616, batch 225\n",
      "\n",
      " Learning rate 0.0007733320910770693, Model learning rate 0.0007733320817351341\n",
      "226/391 [================>.............] - ETA: 11s - loss: 1.3161 - acc: 0.5855\n",
      " Optimizer iteration 617, batch 226\n",
      "\n",
      " Learning rate 0.0007726572190280134, Model learning rate 0.0007726572221145034\n",
      "227/391 [================>.............] - ETA: 11s - loss: 1.3157 - acc: 0.5858\n",
      " Optimizer iteration 618, batch 227\n",
      "\n",
      " Learning rate 0.0007719816392818353, Model learning rate 0.0007719816640019417\n",
      "228/391 [================>.............] - ETA: 11s - loss: 1.3158 - acc: 0.5859\n",
      " Optimizer iteration 619, batch 228\n",
      "\n",
      " Learning rate 0.0007713053535920402, Model learning rate 0.0007713053491897881\n",
      "229/391 [================>.............] - ETA: 11s - loss: 1.3156 - acc: 0.5860\n",
      " Optimizer iteration 620, batch 229\n",
      "\n",
      " Learning rate 0.0007706283637139657, Model learning rate 0.0007706283358857036\n",
      "230/391 [================>.............] - ETA: 11s - loss: 1.3158 - acc: 0.5860\n",
      " Optimizer iteration 621, batch 230\n",
      "\n",
      " Learning rate 0.000769950671404777, Model learning rate 0.000769950682297349\n",
      "\n",
      " Optimizer iteration 622, batch 231\n",
      "\n",
      " Learning rate 0.0007692722784234624, Model learning rate 0.0007692722720094025\n",
      "232/391 [================>.............] - ETA: 11s - loss: 1.3152 - acc: 0.5865\n",
      " Optimizer iteration 623, batch 232\n",
      "\n",
      " Learning rate 0.0007685931865308292, Model learning rate 0.0007685931632295251\n",
      "233/391 [================>.............] - ETA: 11s - loss: 1.3150 - acc: 0.5864\n",
      " Optimizer iteration 624, batch 233\n",
      "\n",
      " Learning rate 0.0007679133974894983, Model learning rate 0.0007679134141653776\n",
      "\n",
      " Optimizer iteration 625, batch 234\n",
      "\n",
      " Learning rate 0.0007672329130639005, Model learning rate 0.0007672329084016383\n",
      "235/391 [=================>............] - ETA: 11s - loss: 1.3145 - acc: 0.5867\n",
      " Optimizer iteration 626, batch 235\n",
      "\n",
      " Learning rate 0.0007665517350202715, Model learning rate 0.0007665517623536289\n",
      "236/391 [=================>............] - ETA: 11s - loss: 1.3140 - acc: 0.5870\n",
      " Optimizer iteration 627, batch 236\n",
      "\n",
      " Learning rate 0.0007658698651266467, Model learning rate 0.0007658698596060276\n",
      "237/391 [=================>............] - ETA: 11s - loss: 1.3140 - acc: 0.5870\n",
      " Optimizer iteration 628, batch 237\n",
      "\n",
      " Learning rate 0.000765187305152858, Model learning rate 0.0007651873165741563\n",
      "238/391 [=================>............] - ETA: 11s - loss: 1.3144 - acc: 0.5869\n",
      " Optimizer iteration 629, batch 238\n",
      "\n",
      " Learning rate 0.0007645040568705282, Model learning rate 0.000764504075050354\n",
      "239/391 [=================>............] - ETA: 10s - loss: 1.3142 - acc: 0.5867\n",
      " Optimizer iteration 630, batch 239\n",
      "\n",
      " Learning rate 0.0007638201220530663, Model learning rate 0.0007638201350346208\n",
      "\n",
      " Optimizer iteration 631, batch 240\n",
      "\n",
      " Learning rate 0.0007631355024756639, Model learning rate 0.0007631354965269566\n",
      "241/391 [=================>............] - ETA: 10s - loss: 1.3132 - acc: 0.5872\n",
      " Optimizer iteration 632, batch 241\n",
      "\n",
      " Learning rate 0.0007624501999152893, Model learning rate 0.0007624502177350223\n",
      "242/391 [=================>............] - ETA: 10s - loss: 1.3128 - acc: 0.5873\n",
      " Optimizer iteration 633, batch 242\n",
      "\n",
      " Learning rate 0.0007617642161506837, Model learning rate 0.0007617642404511571\n",
      "243/391 [=================>............] - ETA: 10s - loss: 1.3114 - acc: 0.5879\n",
      " Optimizer iteration 634, batch 243\n",
      "\n",
      " Learning rate 0.0007610775529623568, Model learning rate 0.0007610775646753609\n",
      "\n",
      " Optimizer iteration 635, batch 244\n",
      "\n",
      " Learning rate 0.0007603902121325811, Model learning rate 0.0007603901904076338\n",
      "245/391 [=================>............] - ETA: 10s - loss: 1.3110 - acc: 0.5883\n",
      " Optimizer iteration 636, batch 245\n",
      "\n",
      " Learning rate 0.0007597021954453886, Model learning rate 0.0007597021758556366\n",
      "246/391 [=================>............] - ETA: 10s - loss: 1.3112 - acc: 0.5884\n",
      " Optimizer iteration 637, batch 246\n",
      "\n",
      " Learning rate 0.0007590135046865651, Model learning rate 0.0007590135210193694\n",
      "247/391 [=================>............] - ETA: 10s - loss: 1.3113 - acc: 0.5883\n",
      " Optimizer iteration 638, batch 247\n",
      "\n",
      " Learning rate 0.0007583241416436461, Model learning rate 0.0007583241676911712\n",
      "248/391 [==================>...........] - ETA: 10s - loss: 1.3111 - acc: 0.5883\n",
      " Optimizer iteration 639, batch 248\n",
      "\n",
      " Learning rate 0.0007576341081059123, Model learning rate 0.000757634115871042\n",
      "249/391 [==================>...........] - ETA: 10s - loss: 1.3104 - acc: 0.5887\n",
      " Optimizer iteration 640, batch 249\n",
      "\n",
      " Learning rate 0.0007569434058643844, Model learning rate 0.0007569434237666428\n",
      "250/391 [==================>...........] - ETA: 10s - loss: 1.3100 - acc: 0.5887\n",
      " Optimizer iteration 641, batch 250\n",
      "\n",
      " Learning rate 0.0007562520367118186, Model learning rate 0.0007562520331703126\n",
      "251/391 [==================>...........] - ETA: 10s - loss: 1.3096 - acc: 0.5888\n",
      " Optimizer iteration 642, batch 251\n",
      "\n",
      " Learning rate 0.0007555600024427027, Model learning rate 0.0007555600022897124\n",
      "252/391 [==================>...........] - ETA: 10s - loss: 1.3094 - acc: 0.5888\n",
      " Optimizer iteration 643, batch 252\n",
      "\n",
      " Learning rate 0.0007548673048532504, Model learning rate 0.0007548673311248422\n",
      "253/391 [==================>...........] - ETA: 9s - loss: 1.3088 - acc: 0.5890 \n",
      " Optimizer iteration 644, batch 253\n",
      "\n",
      " Learning rate 0.000754173945741397, Model learning rate 0.0007541739614680409\n",
      "254/391 [==================>...........] - ETA: 9s - loss: 1.3086 - acc: 0.5890\n",
      " Optimizer iteration 645, batch 254\n",
      "\n",
      " Learning rate 0.0007534799269067953, Model learning rate 0.0007534799515269697\n",
      "\n",
      " Optimizer iteration 646, batch 255\n",
      "\n",
      " Learning rate 0.0007527852501508099, Model learning rate 0.0007527852430939674\n",
      "256/391 [==================>...........] - ETA: 9s - loss: 1.3091 - acc: 0.5888\n",
      " Optimizer iteration 647, batch 256\n",
      "\n",
      " Learning rate 0.0007520899172765136, Model learning rate 0.0007520898943766952\n",
      "257/391 [==================>...........] - ETA: 9s - loss: 1.3091 - acc: 0.5887\n",
      " Optimizer iteration 648, batch 257\n",
      "\n",
      " Learning rate 0.0007513939300886816, Model learning rate 0.0007513939053751528\n",
      "\n",
      " Optimizer iteration 649, batch 258\n",
      "\n",
      " Learning rate 0.0007506972903937878, Model learning rate 0.0007506972760893404\n",
      "259/391 [==================>...........] - ETA: 9s - loss: 1.3090 - acc: 0.5889\n",
      " Optimizer iteration 650, batch 259\n",
      "\n",
      " Learning rate 0.00075, Model learning rate 0.000750000006519258\n",
      "260/391 [==================>...........] - ETA: 9s - loss: 1.3090 - acc: 0.5888\n",
      " Optimizer iteration 651, batch 260\n",
      "\n",
      " Learning rate 0.0007493020607171743, Model learning rate 0.0007493020384572446\n",
      "261/391 [===================>..........] - ETA: 9s - loss: 1.3094 - acc: 0.5887\n",
      " Optimizer iteration 652, batch 261\n",
      "\n",
      " Learning rate 0.0007486034743568511, Model learning rate 0.0007486034883186221\n",
      "262/391 [===================>..........] - ETA: 9s - loss: 1.3089 - acc: 0.5889\n",
      " Optimizer iteration 653, batch 262\n",
      "\n",
      " Learning rate 0.0007479042427322508, Model learning rate 0.0007479042396880686\n",
      "263/391 [===================>..........] - ETA: 9s - loss: 1.3084 - acc: 0.5891\n",
      " Optimizer iteration 654, batch 263\n",
      "\n",
      " Learning rate 0.0007472043676582685, Model learning rate 0.0007472043507732451\n",
      "\n",
      " Optimizer iteration 655, batch 264\n",
      "\n",
      " Learning rate 0.0007465038509514688, Model learning rate 0.0007465038797818124\n",
      "265/391 [===================>..........] - ETA: 9s - loss: 1.3072 - acc: 0.5893\n",
      " Optimizer iteration 656, batch 265\n",
      "\n",
      " Learning rate 0.0007458026944300824, Model learning rate 0.0007458027102984488\n",
      "266/391 [===================>..........] - ETA: 9s - loss: 1.3073 - acc: 0.5893\n",
      " Optimizer iteration 657, batch 266\n",
      "\n",
      " Learning rate 0.0007451008999140005, Model learning rate 0.0007451009005308151\n",
      "267/391 [===================>..........] - ETA: 8s - loss: 1.3070 - acc: 0.5893\n",
      " Optimizer iteration 658, batch 267\n",
      "\n",
      " Learning rate 0.0007443984692247701, Model learning rate 0.0007443984504789114\n",
      "\n",
      " Optimizer iteration 659, batch 268\n",
      "\n",
      " Learning rate 0.0007436954041855892, Model learning rate 0.0007436954183503985\n",
      "269/391 [===================>..........] - ETA: 8s - loss: 1.3074 - acc: 0.5892\n",
      " Optimizer iteration 660, batch 269\n",
      "\n",
      " Learning rate 0.000742991706621303, Model learning rate 0.0007429916877299547\n",
      "\n",
      " Optimizer iteration 661, batch 270\n",
      "\n",
      " Learning rate 0.0007422873783583981, Model learning rate 0.0007422873750329018\n",
      "271/391 [===================>..........] - ETA: 8s - loss: 1.3075 - acc: 0.5891\n",
      " Optimizer iteration 662, batch 271\n",
      "\n",
      " Learning rate 0.0007415824212249977, Model learning rate 0.0007415824220515788\n",
      "272/391 [===================>..........] - ETA: 8s - loss: 1.3075 - acc: 0.5893\n",
      " Optimizer iteration 663, batch 272\n",
      "\n",
      " Learning rate 0.0007408768370508576, Model learning rate 0.0007408768287859857\n",
      "273/391 [===================>..........] - ETA: 8s - loss: 1.3081 - acc: 0.5889\n",
      " Optimizer iteration 664, batch 273\n",
      "\n",
      " Learning rate 0.0007401706276673615, Model learning rate 0.0007401706534437835\n",
      "\n",
      " Optimizer iteration 665, batch 274\n",
      "\n",
      " Learning rate 0.0007394637949075153, Model learning rate 0.0007394637796096504\n",
      "275/391 [====================>.........] - ETA: 8s - loss: 1.3078 - acc: 0.5891\n",
      " Optimizer iteration 666, batch 275\n",
      "\n",
      " Learning rate 0.0007387563406059432, Model learning rate 0.0007387563236989081\n",
      "276/391 [====================>.........] - ETA: 8s - loss: 1.3073 - acc: 0.5892\n",
      " Optimizer iteration 667, batch 276\n",
      "\n",
      " Learning rate 0.0007380482665988826, Model learning rate 0.0007380482857115567\n",
      "277/391 [====================>.........] - ETA: 8s - loss: 1.3068 - acc: 0.5894\n",
      " Optimizer iteration 668, batch 277\n",
      "\n",
      " Learning rate 0.0007373395747241791, Model learning rate 0.0007373395492322743\n",
      "278/391 [====================>.........] - ETA: 8s - loss: 1.3064 - acc: 0.5894\n",
      " Optimizer iteration 669, batch 278\n",
      "\n",
      " Learning rate 0.0007366302668212826, Model learning rate 0.0007366302888840437\n",
      "279/391 [====================>.........] - ETA: 8s - loss: 1.3062 - acc: 0.5897\n",
      " Optimizer iteration 670, batch 279\n",
      "\n",
      " Learning rate 0.000735920344731241, Model learning rate 0.0007359203300438821\n",
      "280/391 [====================>.........] - ETA: 8s - loss: 1.3058 - acc: 0.5898\n",
      " Optimizer iteration 671, batch 280\n",
      "\n",
      " Learning rate 0.0007352098102966978, Model learning rate 0.0007352097891271114\n",
      "281/391 [====================>.........] - ETA: 7s - loss: 1.3055 - acc: 0.5899\n",
      " Optimizer iteration 672, batch 281\n",
      "\n",
      " Learning rate 0.0007344986653618844, Model learning rate 0.0007344986661337316\n",
      "282/391 [====================>.........] - ETA: 7s - loss: 1.3052 - acc: 0.5898\n",
      " Optimizer iteration 673, batch 282\n",
      "\n",
      " Learning rate 0.0007337869117726176, Model learning rate 0.0007337869028560817\n",
      "283/391 [====================>.........] - ETA: 7s - loss: 1.3054 - acc: 0.5899\n",
      " Optimizer iteration 674, batch 283\n",
      "\n",
      " Learning rate 0.0007330745513762936, Model learning rate 0.0007330745575018227\n",
      "284/391 [====================>.........] - ETA: 7s - loss: 1.3053 - acc: 0.5897\n",
      " Optimizer iteration 675, batch 284\n",
      "\n",
      " Learning rate 0.0007323615860218843, Model learning rate 0.0007323615718632936\n",
      "285/391 [====================>.........] - ETA: 7s - loss: 1.3047 - acc: 0.5899\n",
      " Optimizer iteration 676, batch 285\n",
      "\n",
      " Learning rate 0.0007316480175599309, Model learning rate 0.0007316480041481555\n",
      "286/391 [====================>.........] - ETA: 7s - loss: 1.3048 - acc: 0.5899\n",
      " Optimizer iteration 677, batch 286\n",
      "\n",
      " Learning rate 0.0007309338478425404, Model learning rate 0.0007309338543564081\n",
      "287/391 [=====================>........] - ETA: 7s - loss: 1.3046 - acc: 0.5900\n",
      " Optimizer iteration 678, batch 287\n",
      "\n",
      " Learning rate 0.0007302190787233807, Model learning rate 0.0007302190642803907\n",
      "288/391 [=====================>........] - ETA: 7s - loss: 1.3049 - acc: 0.5898\n",
      " Optimizer iteration 679, batch 288\n",
      "\n",
      " Learning rate 0.0007295037120576749, Model learning rate 0.0007295036921277642\n",
      "289/391 [=====================>........] - ETA: 7s - loss: 1.3046 - acc: 0.5899\n",
      " Optimizer iteration 680, batch 289\n",
      "\n",
      " Learning rate 0.0007287877497021977, Model learning rate 0.0007287877378985286\n",
      "290/391 [=====================>........] - ETA: 7s - loss: 1.3047 - acc: 0.5899\n",
      " Optimizer iteration 681, batch 290\n",
      "\n",
      " Learning rate 0.0007280711935152691, Model learning rate 0.0007280712015926838\n",
      "291/391 [=====================>........] - ETA: 7s - loss: 1.3041 - acc: 0.5900\n",
      " Optimizer iteration 682, batch 291\n",
      "\n",
      " Learning rate 0.0007273540453567512, Model learning rate 0.000727354025002569\n",
      "292/391 [=====================>........] - ETA: 7s - loss: 1.3039 - acc: 0.5902\n",
      " Optimizer iteration 683, batch 292\n",
      "\n",
      " Learning rate 0.0007266363070880424, Model learning rate 0.0007266363245435059\n",
      "293/391 [=====================>........] - ETA: 7s - loss: 1.3035 - acc: 0.5904\n",
      " Optimizer iteration 684, batch 293\n",
      "\n",
      " Learning rate 0.0007259179805720726, Model learning rate 0.0007259179838001728\n",
      "294/391 [=====================>........] - ETA: 7s - loss: 1.3030 - acc: 0.5906\n",
      " Optimizer iteration 685, batch 294\n",
      "\n",
      " Learning rate 0.0007251990676732984, Model learning rate 0.0007251990609802306\n",
      "295/391 [=====================>........] - ETA: 6s - loss: 1.3031 - acc: 0.5907\n",
      " Optimizer iteration 686, batch 295\n",
      "\n",
      " Learning rate 0.0007244795702576989, Model learning rate 0.0007244795560836792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/391 [=====================>........] - ETA: 6s - loss: 1.3031 - acc: 0.5907\n",
      " Optimizer iteration 687, batch 296\n",
      "\n",
      " Learning rate 0.0007237594901927699, Model learning rate 0.0007237594691105187\n",
      "297/391 [=====================>........] - ETA: 6s - loss: 1.3029 - acc: 0.5908\n",
      " Optimizer iteration 688, batch 297\n",
      "\n",
      " Learning rate 0.0007230388293475197, Model learning rate 0.00072303885826841\n",
      "298/391 [=====================>........] - ETA: 6s - loss: 1.3027 - acc: 0.5908\n",
      " Optimizer iteration 689, batch 298\n",
      "\n",
      " Learning rate 0.0007223175895924637, Model learning rate 0.0007223176071420312\n",
      "299/391 [=====================>........] - ETA: 6s - loss: 1.3024 - acc: 0.5908\n",
      " Optimizer iteration 690, batch 299\n",
      "\n",
      " Learning rate 0.0007215957727996207, Model learning rate 0.0007215957739390433\n",
      "300/391 [======================>.......] - ETA: 6s - loss: 1.3028 - acc: 0.5907\n",
      " Optimizer iteration 691, batch 300\n",
      "\n",
      " Learning rate 0.0007208733808425063, Model learning rate 0.0007208733586594462\n",
      "301/391 [======================>.......] - ETA: 6s - loss: 1.3028 - acc: 0.5906\n",
      " Optimizer iteration 692, batch 301\n",
      "\n",
      " Learning rate 0.0007201504155961296, Model learning rate 0.000720150419510901\n",
      "302/391 [======================>.......] - ETA: 6s - loss: 1.3024 - acc: 0.5910\n",
      " Optimizer iteration 693, batch 302\n",
      "\n",
      " Learning rate 0.0007194268789369874, Model learning rate 0.0007194268982857466\n",
      "\n",
      " Optimizer iteration 694, batch 303\n",
      "\n",
      " Learning rate 0.00071870277274306, Model learning rate 0.000718702794983983\n",
      "304/391 [======================>.......] - ETA: 6s - loss: 1.3020 - acc: 0.5911\n",
      " Optimizer iteration 695, batch 304\n",
      "\n",
      " Learning rate 0.000717978098893805, Model learning rate 0.0007179781096056104\n",
      "305/391 [======================>.......] - ETA: 6s - loss: 1.3016 - acc: 0.5910\n",
      " Optimizer iteration 696, batch 305\n",
      "\n",
      " Learning rate 0.000717252859270155, Model learning rate 0.0007172528421506286\n",
      "306/391 [======================>.......] - ETA: 6s - loss: 1.3016 - acc: 0.5909\n",
      " Optimizer iteration 697, batch 306\n",
      "\n",
      " Learning rate 0.0007165270557545094, Model learning rate 0.0007165270508266985\n",
      "307/391 [======================>.......] - ETA: 6s - loss: 1.3013 - acc: 0.5910\n",
      " Optimizer iteration 698, batch 307\n",
      "\n",
      " Learning rate 0.0007158006902307321, Model learning rate 0.0007158006774261594\n",
      "308/391 [======================>.......] - ETA: 6s - loss: 1.3013 - acc: 0.5910\n",
      " Optimizer iteration 699, batch 308\n",
      "\n",
      " Learning rate 0.000715073764584146, Model learning rate 0.000715073780156672\n",
      "309/391 [======================>.......] - ETA: 5s - loss: 1.3013 - acc: 0.5911\n",
      " Optimizer iteration 700, batch 309\n",
      "\n",
      " Learning rate 0.000714346280701527, Model learning rate 0.0007143463008105755\n",
      "310/391 [======================>.......] - ETA: 5s - loss: 1.3011 - acc: 0.5911\n",
      " Optimizer iteration 701, batch 310\n",
      "\n",
      " Learning rate 0.0007136182404711008, Model learning rate 0.0007136182393878698\n",
      "\n",
      " Optimizer iteration 702, batch 311\n",
      "\n",
      " Learning rate 0.0007128896457825364, Model learning rate 0.000712889654096216\n",
      "312/391 [======================>.......] - ETA: 5s - loss: 1.3011 - acc: 0.5911\n",
      " Optimizer iteration 703, batch 312\n",
      "\n",
      " Learning rate 0.0007121604985269422, Model learning rate 0.000712160486727953\n",
      "313/391 [=======================>......] - ETA: 5s - loss: 1.3016 - acc: 0.5910\n",
      " Optimizer iteration 704, batch 313\n",
      "\n",
      " Learning rate 0.0007114308005968609, Model learning rate 0.0007114307954907417\n",
      "314/391 [=======================>......] - ETA: 5s - loss: 1.3017 - acc: 0.5908\n",
      " Optimizer iteration 705, batch 314\n",
      "\n",
      " Learning rate 0.0007107005538862646, Model learning rate 0.0007107005803845823\n",
      "\n",
      " Optimizer iteration 706, batch 315\n",
      "\n",
      " Learning rate 0.0007099697602905493, Model learning rate 0.0007099697832018137\n",
      "316/391 [=======================>......] - ETA: 5s - loss: 1.3012 - acc: 0.5910\n",
      " Optimizer iteration 707, batch 316\n",
      "\n",
      " Learning rate 0.0007092384217065314, Model learning rate 0.000709238403942436\n",
      "317/391 [=======================>......] - ETA: 5s - loss: 1.3015 - acc: 0.5909\n",
      " Optimizer iteration 708, batch 317\n",
      "\n",
      " Learning rate 0.0007085065400324407, Model learning rate 0.000708506559021771\n",
      "318/391 [=======================>......] - ETA: 5s - loss: 1.3009 - acc: 0.5911\n",
      " Optimizer iteration 709, batch 318\n",
      "\n",
      " Learning rate 0.0007077741171679173, Model learning rate 0.0007077741320244968\n",
      "\n",
      " Optimizer iteration 710, batch 319\n",
      "\n",
      " Learning rate 0.000707041155014006, Model learning rate 0.0007070411811582744\n",
      "320/391 [=======================>......] - ETA: 5s - loss: 1.3009 - acc: 0.5910\n",
      " Optimizer iteration 711, batch 320\n",
      "\n",
      " Learning rate 0.0007063076554731512, Model learning rate 0.0007063076482154429\n",
      "321/391 [=======================>......] - ETA: 5s - loss: 1.3004 - acc: 0.5912\n",
      " Optimizer iteration 712, batch 321\n",
      "\n",
      " Learning rate 0.0007055736204491922, Model learning rate 0.0007055735914036632\n",
      "322/391 [=======================>......] - ETA: 5s - loss: 1.2998 - acc: 0.5915\n",
      " Optimizer iteration 713, batch 322\n",
      "\n",
      " Learning rate 0.0007048390518473579, Model learning rate 0.0007048390689305961\n",
      "323/391 [=======================>......] - ETA: 4s - loss: 1.2993 - acc: 0.5917\n",
      " Optimizer iteration 714, batch 323\n",
      "\n",
      " Learning rate 0.0007041039515742625, Model learning rate 0.0007041039643809199\n",
      "324/391 [=======================>......] - ETA: 4s - loss: 1.2989 - acc: 0.5918\n",
      " Optimizer iteration 715, batch 324\n",
      "\n",
      " Learning rate 0.0007033683215379002, Model learning rate 0.0007033683359622955\n",
      "325/391 [=======================>......] - ETA: 4s - loss: 1.2985 - acc: 0.5920\n",
      " Optimizer iteration 716, batch 325\n",
      "\n",
      " Learning rate 0.0007026321636476397, Model learning rate 0.0007026321836747229\n",
      "326/391 [========================>.....] - ETA: 4s - loss: 1.2985 - acc: 0.5919\n",
      " Optimizer iteration 717, batch 326\n",
      "\n",
      " Learning rate 0.0007018954798142204, Model learning rate 0.0007018955075182021\n",
      "327/391 [========================>.....] - ETA: 4s - loss: 1.2979 - acc: 0.5922\n",
      " Optimizer iteration 718, batch 327\n",
      "\n",
      " Learning rate 0.0007011582719497466, Model learning rate 0.0007011582492850721\n",
      "328/391 [========================>.....] - ETA: 4s - loss: 1.2974 - acc: 0.5924\n",
      " Optimizer iteration 719, batch 328\n",
      "\n",
      " Learning rate 0.0007004205419676825, Model learning rate 0.0007004205253906548\n",
      "\n",
      " Optimizer iteration 720, batch 329\n",
      "\n",
      " Learning rate 0.0006996822917828477, Model learning rate 0.0006996822776272893\n",
      "330/391 [========================>.....] - ETA: 4s - loss: 1.2965 - acc: 0.5927\n",
      " Optimizer iteration 721, batch 330\n",
      "\n",
      " Learning rate 0.0006989435233114123, Model learning rate 0.0006989435059949756\n",
      "\n",
      " Optimizer iteration 722, batch 331\n",
      "\n",
      " Learning rate 0.000698204238470891, Model learning rate 0.0006982042104937136\n",
      "332/391 [========================>.....] - ETA: 4s - loss: 1.2966 - acc: 0.5927\n",
      " Optimizer iteration 723, batch 332\n",
      "\n",
      " Learning rate 0.0006974644391801395, Model learning rate 0.0006974644493311644\n",
      "\n",
      " Optimizer iteration 724, batch 333\n",
      "\n",
      " Learning rate 0.000696724127359348, Model learning rate 0.000696724106092006\n",
      "334/391 [========================>.....] - ETA: 4s - loss: 1.2966 - acc: 0.5927\n",
      " Optimizer iteration 725, batch 334\n",
      "\n",
      " Learning rate 0.0006959833049300376, Model learning rate 0.0006959832971915603\n",
      "\n",
      " Optimizer iteration 726, batch 335\n",
      "\n",
      " Learning rate 0.0006952419738150545, Model learning rate 0.0006952419644221663\n",
      "336/391 [========================>.....] - ETA: 4s - loss: 1.2961 - acc: 0.5930\n",
      " Optimizer iteration 727, batch 336\n",
      "\n",
      " Learning rate 0.0006945001359385651, Model learning rate 0.0006945001077838242\n",
      "337/391 [========================>.....] - ETA: 3s - loss: 1.2958 - acc: 0.5931\n",
      " Optimizer iteration 728, batch 337\n",
      "\n",
      " Learning rate 0.0006937577932260515, Model learning rate 0.0006937577854841948\n",
      "338/391 [========================>.....] - ETA: 3s - loss: 1.2954 - acc: 0.5932\n",
      " Optimizer iteration 729, batch 338\n",
      "\n",
      " Learning rate 0.0006930149476043058, Model learning rate 0.0006930149393156171\n",
      "339/391 [=========================>....] - ETA: 3s - loss: 1.2952 - acc: 0.5932\n",
      " Optimizer iteration 730, batch 339\n",
      "\n",
      " Learning rate 0.0006922716010014255, Model learning rate 0.0006922716274857521\n",
      "340/391 [=========================>....] - ETA: 3s - loss: 1.2953 - acc: 0.5932\n",
      " Optimizer iteration 731, batch 340\n",
      "\n",
      " Learning rate 0.0006915277553468083, Model learning rate 0.000691527733579278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341/391 [=========================>....] - ETA: 3s - loss: 1.2953 - acc: 0.5932\n",
      " Optimizer iteration 732, batch 341\n",
      "\n",
      " Learning rate 0.0006907834125711476, Model learning rate 0.0006907834322191775\n",
      "342/391 [=========================>....] - ETA: 3s - loss: 1.2955 - acc: 0.5929\n",
      " Optimizer iteration 733, batch 342\n",
      "\n",
      " Learning rate 0.0006900385746064268, Model learning rate 0.0006900385487824678\n",
      "343/391 [=========================>....] - ETA: 3s - loss: 1.2958 - acc: 0.5927\n",
      " Optimizer iteration 734, batch 343\n",
      "\n",
      " Learning rate 0.0006892932433859147, Model learning rate 0.0006892932578921318\n",
      "344/391 [=========================>....] - ETA: 3s - loss: 1.2954 - acc: 0.5928\n",
      " Optimizer iteration 735, batch 344\n",
      "\n",
      " Learning rate 0.0006885474208441603, Model learning rate 0.0006885474431328475\n",
      "\n",
      " Optimizer iteration 736, batch 345\n",
      "\n",
      " Learning rate 0.0006878011089169878, Model learning rate 0.0006878011045046151\n",
      "346/391 [=========================>....] - ETA: 3s - loss: 1.2957 - acc: 0.5928\n",
      " Optimizer iteration 737, batch 346\n",
      "\n",
      " Learning rate 0.0006870543095414918, Model learning rate 0.0006870543002150953\n",
      "\n",
      " Optimizer iteration 738, batch 347\n",
      "\n",
      " Learning rate 0.0006863070246560319, Model learning rate 0.0006863070302642882\n",
      "348/391 [=========================>....] - ETA: 3s - loss: 1.2952 - acc: 0.5931\n",
      " Optimizer iteration 739, batch 348\n",
      "\n",
      " Learning rate 0.0006855592562002281, Model learning rate 0.0006855592364445329\n",
      "349/391 [=========================>....] - ETA: 3s - loss: 1.2948 - acc: 0.5931\n",
      " Optimizer iteration 740, batch 349\n",
      "\n",
      " Learning rate 0.0006848110061149555, Model learning rate 0.0006848110351711512\n",
      "350/391 [=========================>....] - ETA: 2s - loss: 1.2944 - acc: 0.5933\n",
      " Optimizer iteration 741, batch 350\n",
      "\n",
      " Learning rate 0.0006840622763423391, Model learning rate 0.0006840622518211603\n",
      "351/391 [=========================>....] - ETA: 2s - loss: 1.2940 - acc: 0.5934\n",
      " Optimizer iteration 742, batch 351\n",
      "\n",
      " Learning rate 0.0006833130688257489, Model learning rate 0.0006833130610175431\n",
      "352/391 [==========================>...] - ETA: 2s - loss: 1.2941 - acc: 0.5935\n",
      " Optimizer iteration 743, batch 352\n",
      "\n",
      " Learning rate 0.0006825633855097954, Model learning rate 0.0006825634045526385\n",
      "\n",
      " Optimizer iteration 744, batch 353\n",
      "\n",
      " Learning rate 0.0006818132283403235, Model learning rate 0.0006818132242187858\n",
      "354/391 [==========================>...] - ETA: 2s - loss: 1.2926 - acc: 0.5941\n",
      " Optimizer iteration 745, batch 354\n",
      "\n",
      " Learning rate 0.0006810625992644084, Model learning rate 0.0006810625782236457\n",
      "\n",
      " Optimizer iteration 746, batch 355\n",
      "\n",
      " Learning rate 0.0006803115002303499, Model learning rate 0.0006803115247748792\n",
      "356/391 [==========================>...] - ETA: 2s - loss: 1.2924 - acc: 0.5942\n",
      " Optimizer iteration 747, batch 356\n",
      "\n",
      " Learning rate 0.0006795599331876678, Model learning rate 0.0006795599474571645\n",
      "\n",
      " Optimizer iteration 748, batch 357\n",
      "\n",
      " Learning rate 0.0006788079000870966, Model learning rate 0.0006788079044781625\n",
      "358/391 [==========================>...] - ETA: 2s - loss: 1.2922 - acc: 0.5943\n",
      " Optimizer iteration 749, batch 358\n",
      "\n",
      " Learning rate 0.0006780554028805803, Model learning rate 0.0006780553958378732\n",
      "359/391 [==========================>...] - ETA: 2s - loss: 1.2918 - acc: 0.5945\n",
      " Optimizer iteration 750, batch 359\n",
      "\n",
      " Learning rate 0.0006773024435212678, Model learning rate 0.0006773024215362966\n",
      "360/391 [==========================>...] - ETA: 2s - loss: 1.2915 - acc: 0.5945\n",
      " Optimizer iteration 751, batch 360\n",
      "\n",
      " Learning rate 0.0006765490239635075, Model learning rate 0.0006765490397810936\n",
      "361/391 [==========================>...] - ETA: 2s - loss: 1.2916 - acc: 0.5944\n",
      " Optimizer iteration 752, batch 361\n",
      "\n",
      " Learning rate 0.0006757951461628416, Model learning rate 0.0006757951341569424\n",
      "362/391 [==========================>...] - ETA: 2s - loss: 1.2911 - acc: 0.5946\n",
      " Optimizer iteration 753, batch 362\n",
      "\n",
      " Learning rate 0.0006750408120760029, Model learning rate 0.0006750408210791647\n",
      "363/391 [==========================>...] - ETA: 2s - loss: 1.2908 - acc: 0.5946\n",
      " Optimizer iteration 754, batch 363\n",
      "\n",
      " Learning rate 0.0006742860236609076, Model learning rate 0.0006742860423400998\n",
      "364/391 [==========================>...] - ETA: 1s - loss: 1.2907 - acc: 0.5945\n",
      " Optimizer iteration 755, batch 364\n",
      "\n",
      " Learning rate 0.0006735307828766515, Model learning rate 0.0006735307979397476\n",
      "365/391 [===========================>..] - ETA: 1s - loss: 1.2904 - acc: 0.5946\n",
      " Optimizer iteration 756, batch 365\n",
      "\n",
      " Learning rate 0.0006727750916835043, Model learning rate 0.000672775087878108\n",
      "366/391 [===========================>..] - ETA: 1s - loss: 1.2899 - acc: 0.5948\n",
      " Optimizer iteration 757, batch 366\n",
      "\n",
      " Learning rate 0.0006720189520429052, Model learning rate 0.0006720189703628421\n",
      "\n",
      " Optimizer iteration 758, batch 367\n",
      "\n",
      " Learning rate 0.0006712623659174569, Model learning rate 0.0006712623871862888\n",
      "368/391 [===========================>..] - ETA: 1s - loss: 1.2895 - acc: 0.5950\n",
      " Optimizer iteration 759, batch 368\n",
      "\n",
      " Learning rate 0.0006705053352709212, Model learning rate 0.0006705053383484483\n",
      "\n",
      " Optimizer iteration 760, batch 369\n",
      "\n",
      " Learning rate 0.0006697478620682136, Model learning rate 0.0006697478820569813\n",
      "370/391 [===========================>..] - ETA: 1s - loss: 1.2894 - acc: 0.5949\n",
      " Optimizer iteration 761, batch 370\n",
      "\n",
      " Learning rate 0.0006689899482753984, Model learning rate 0.0006689899601042271\n",
      "\n",
      " Optimizer iteration 762, batch 371\n",
      "\n",
      " Learning rate 0.0006682315958596836, Model learning rate 0.0006682315724901855\n",
      "372/391 [===========================>..] - ETA: 1s - loss: 1.2885 - acc: 0.5953\n",
      " Optimizer iteration 763, batch 372\n",
      "\n",
      " Learning rate 0.0006674728067894149, Model learning rate 0.0006674728356301785\n",
      "\n",
      " Optimizer iteration 764, batch 373\n",
      "\n",
      " Learning rate 0.0006667135830340727, Model learning rate 0.0006667135749012232\n",
      "374/391 [===========================>..] - ETA: 1s - loss: 1.2872 - acc: 0.5959\n",
      " Optimizer iteration 765, batch 374\n",
      "\n",
      " Learning rate 0.0006659539265642643, Model learning rate 0.0006659539067186415\n",
      "\n",
      " Optimizer iteration 766, batch 375\n",
      "\n",
      " Learning rate 0.000665193839351721, Model learning rate 0.0006651938310824335\n",
      "376/391 [===========================>..] - ETA: 1s - loss: 1.2872 - acc: 0.5958\n",
      " Optimizer iteration 767, batch 376\n",
      "\n",
      " Learning rate 0.0006644333233692916, Model learning rate 0.000664433347992599\n",
      "\n",
      " Optimizer iteration 768, batch 377\n",
      "\n",
      " Learning rate 0.0006636723805909383, Model learning rate 0.0006636723992414773\n",
      "378/391 [============================>.] - ETA: 0s - loss: 1.2867 - acc: 0.5958\n",
      " Optimizer iteration 769, batch 378\n",
      "\n",
      " Learning rate 0.0006629110129917308, Model learning rate 0.0006629109848290682\n",
      "379/391 [============================>.] - ETA: 0s - loss: 1.2866 - acc: 0.5959\n",
      " Optimizer iteration 770, batch 379\n",
      "\n",
      " Learning rate 0.0006621492225478414, Model learning rate 0.0006621492211706936\n",
      "380/391 [============================>.] - ETA: 0s - loss: 1.2866 - acc: 0.5957\n",
      " Optimizer iteration 771, batch 380\n",
      "\n",
      " Learning rate 0.0006613870112365398, Model learning rate 0.0006613869918510318\n",
      "\n",
      " Optimizer iteration 772, batch 381\n",
      "\n",
      " Learning rate 0.0006606243810361885, Model learning rate 0.0006606243550777435\n",
      "382/391 [============================>.] - ETA: 0s - loss: 1.2869 - acc: 0.5955\n",
      " Optimizer iteration 773, batch 382\n",
      "\n",
      " Learning rate 0.0006598613339262369, Model learning rate 0.0006598613108508289\n",
      "383/391 [============================>.] - ETA: 0s - loss: 1.2871 - acc: 0.5954\n",
      " Optimizer iteration 774, batch 383\n",
      "\n",
      " Learning rate 0.0006590978718872166, Model learning rate 0.0006590978591702878\n",
      "384/391 [============================>.] - ETA: 0s - loss: 1.2867 - acc: 0.5956\n",
      " Optimizer iteration 775, batch 384\n",
      "\n",
      " Learning rate 0.0006583339969007363, Model learning rate 0.0006583340000361204\n",
      "385/391 [============================>.] - ETA: 0s - loss: 1.2863 - acc: 0.5958\n",
      " Optimizer iteration 776, batch 385\n",
      "\n",
      " Learning rate 0.0006575697109494763, Model learning rate 0.0006575697334483266\n",
      "386/391 [============================>.] - ETA: 0s - loss: 1.2860 - acc: 0.5960\n",
      " Optimizer iteration 777, batch 386\n",
      "\n",
      " Learning rate 0.0006568050160171837, Model learning rate 0.0006568050011992455\n",
      "387/391 [============================>.] - ETA: 0s - loss: 1.2861 - acc: 0.5959\n",
      " Optimizer iteration 778, batch 387\n",
      "\n",
      " Learning rate 0.0006560399140886673, Model learning rate 0.0006560399197041988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388/391 [============================>.] - ETA: 0s - loss: 1.2861 - acc: 0.5959\n",
      " Optimizer iteration 779, batch 388\n",
      "\n",
      " Learning rate 0.0006552744071497918, Model learning rate 0.0006552744307555258\n",
      "389/391 [============================>.] - ETA: 0s - loss: 1.2858 - acc: 0.5959\n",
      " Optimizer iteration 780, batch 389\n",
      "\n",
      " Learning rate 0.0006545084971874737, Model learning rate 0.0006545084761455655\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.2854 - acc: 0.5959\n",
      " Optimizer iteration 781, batch 390\n",
      "\n",
      " Learning rate 0.0006537421861896752, Model learning rate 0.0006537421722896397\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 1.2852 - acc: 0.5958 - val_loss: 1.4594 - val_acc: 0.5373\n",
      "\n",
      "Epoch 00002: saving model to /home/ubuntu/Projects/hybrid-ensemble/model/run_200/cifar10_ResNet20v1_model-0002.h5\n",
      "Epoch 3/5\n",
      "\n",
      " Optimizer iteration 782, batch 0\n",
      "\n",
      " Learning rate 0.0006529754761453999, Model learning rate 0.0006529754609800875\n",
      "  1/391 [..............................] - ETA: 15s - loss: 1.1736 - acc: 0.6094\n",
      " Optimizer iteration 783, batch 1\n",
      "\n",
      " Learning rate 0.0006522083690446862, Model learning rate 0.0006522083422169089\n",
      "\n",
      " Optimizer iteration 784, batch 2\n",
      "\n",
      " Learning rate 0.0006514408668786038, Model learning rate 0.0006514408742077649\n",
      "  3/391 [..............................] - ETA: 16s - loss: 1.1745 - acc: 0.6406\n",
      " Optimizer iteration 785, batch 3\n",
      "\n",
      " Learning rate 0.000650672971639248, Model learning rate 0.0006506729987449944\n",
      "  4/391 [..............................] - ETA: 17s - loss: 1.1980 - acc: 0.6348\n",
      " Optimizer iteration 786, batch 4\n",
      "\n",
      " Learning rate 0.0006499046853197338, Model learning rate 0.0006499046576209366\n",
      "  5/391 [..............................] - ETA: 17s - loss: 1.1711 - acc: 0.6422\n",
      " Optimizer iteration 787, batch 5\n",
      "\n",
      " Learning rate 0.0006491360099141913, Model learning rate 0.0006491360254585743\n",
      "\n",
      " Optimizer iteration 788, batch 6\n",
      "\n",
      " Learning rate 0.0006483669474177608, Model learning rate 0.0006483669276349247\n",
      "  7/391 [..............................] - ETA: 17s - loss: 1.1914 - acc: 0.6406\n",
      " Optimizer iteration 789, batch 7\n",
      "\n",
      " Learning rate 0.0006475974998265874, Model learning rate 0.0006475974805653095\n",
      "\n",
      " Optimizer iteration 790, batch 8\n",
      "\n",
      " Learning rate 0.0006468276691378155, Model learning rate 0.0006468276842497289\n",
      "  9/391 [..............................] - ETA: 17s - loss: 1.1726 - acc: 0.6398\n",
      " Optimizer iteration 791, batch 9\n",
      "\n",
      " Learning rate 0.0006460574573495835, Model learning rate 0.0006460574804805219\n",
      "\n",
      " Optimizer iteration 792, batch 10\n",
      "\n",
      " Learning rate 0.0006452868664610196, Model learning rate 0.0006452868692576885\n",
      " 11/391 [..............................] - ETA: 18s - loss: 1.1642 - acc: 0.6413\n",
      " Optimizer iteration 793, batch 11\n",
      "\n",
      " Learning rate 0.0006445158984722358, Model learning rate 0.0006445159087888896\n",
      "\n",
      " Optimizer iteration 794, batch 12\n",
      "\n",
      " Learning rate 0.0006437445553843229, Model learning rate 0.0006437445408664644\n",
      " 13/391 [..............................] - ETA: 17s - loss: 1.1435 - acc: 0.6496\n",
      " Optimizer iteration 795, batch 13\n",
      "\n",
      " Learning rate 0.0006429728391993446, Model learning rate 0.0006429728236980736\n",
      "\n",
      " Optimizer iteration 796, batch 14\n",
      "\n",
      " Learning rate 0.0006422007519203343, Model learning rate 0.0006422007572837174\n",
      " 15/391 [>.............................] - ETA: 17s - loss: 1.1622 - acc: 0.6396\n",
      " Optimizer iteration 797, batch 15\n",
      "\n",
      " Learning rate 0.0006414282955512875, Model learning rate 0.0006414282834157348\n",
      " 16/391 [>.............................] - ETA: 17s - loss: 1.1653 - acc: 0.6372\n",
      " Optimizer iteration 798, batch 16\n",
      "\n",
      " Learning rate 0.0006406554720971582, Model learning rate 0.0006406554603017867\n",
      " 17/391 [>.............................] - ETA: 18s - loss: 1.1646 - acc: 0.6388\n",
      " Optimizer iteration 799, batch 17\n",
      "\n",
      " Learning rate 0.000639882283563853, Model learning rate 0.0006398822879418731\n",
      "\n",
      " Optimizer iteration 800, batch 18\n",
      "\n",
      " Learning rate 0.0006391087319582263, Model learning rate 0.0006391087081283331\n",
      " 19/391 [>.............................] - ETA: 18s - loss: 1.1674 - acc: 0.6410\n",
      " Optimizer iteration 801, batch 19\n",
      "\n",
      " Learning rate 0.0006383348192880747, Model learning rate 0.0006383348372764885\n",
      " 20/391 [>.............................] - ETA: 18s - loss: 1.1684 - acc: 0.6391\n",
      " Optimizer iteration 802, batch 20\n",
      "\n",
      " Learning rate 0.0006375605475621318, Model learning rate 0.0006375605589710176\n",
      " 21/391 [>.............................] - ETA: 19s - loss: 1.1652 - acc: 0.6391\n",
      " Optimizer iteration 803, batch 21\n",
      "\n",
      " Learning rate 0.0006367859187900634, Model learning rate 0.0006367859314195812\n",
      "\n",
      " Optimizer iteration 804, batch 22\n",
      "\n",
      " Learning rate 0.0006360109349824621, Model learning rate 0.0006360109546221793\n",
      " 23/391 [>.............................] - ETA: 19s - loss: 1.1579 - acc: 0.6410\n",
      " Optimizer iteration 805, batch 23\n",
      "\n",
      " Learning rate 0.000635235598150842, Model learning rate 0.000635235570371151\n",
      " 24/391 [>.............................] - ETA: 19s - loss: 1.1584 - acc: 0.6403\n",
      " Optimizer iteration 806, batch 24\n",
      "\n",
      " Learning rate 0.0006344599103076329, Model learning rate 0.0006344598950818181\n",
      " 25/391 [>.............................] - ETA: 20s - loss: 1.1567 - acc: 0.6412\n",
      " Optimizer iteration 807, batch 25\n",
      "\n",
      " Learning rate 0.0006336838734661765, Model learning rate 0.0006336838705465198\n",
      "\n",
      " Optimizer iteration 808, batch 26\n",
      "\n",
      " Learning rate 0.0006329074896407202, Model learning rate 0.0006329074967652559\n",
      " 27/391 [=>............................] - ETA: 20s - loss: 1.1488 - acc: 0.6427\n",
      " Optimizer iteration 809, batch 27\n",
      "\n",
      " Learning rate 0.0006321307608464113, Model learning rate 0.0006321307737380266\n",
      " 28/391 [=>............................] - ETA: 20s - loss: 1.1492 - acc: 0.6434\n",
      " Optimizer iteration 810, batch 28\n",
      "\n",
      " Learning rate 0.0006313536890992934, Model learning rate 0.0006313537014648318\n",
      " 29/391 [=>............................] - ETA: 21s - loss: 1.1478 - acc: 0.6439\n",
      " Optimizer iteration 811, batch 29\n",
      "\n",
      " Learning rate 0.0006305762764162999, Model learning rate 0.0006305762799456716\n",
      " 30/391 [=>............................] - ETA: 21s - loss: 1.1471 - acc: 0.6440\n",
      " Optimizer iteration 812, batch 30\n",
      "\n",
      " Learning rate 0.0006297985248152486, Model learning rate 0.0006297985091805458\n",
      "\n",
      " Optimizer iteration 813, batch 31\n",
      "\n",
      " Learning rate 0.000629020436314838, Model learning rate 0.0006290204473771155\n",
      " 32/391 [=>............................] - ETA: 21s - loss: 1.1511 - acc: 0.6431\n",
      " Optimizer iteration 814, batch 32\n",
      "\n",
      " Learning rate 0.0006282420129346401, Model learning rate 0.0006282420363277197\n",
      " 33/391 [=>............................] - ETA: 21s - loss: 1.1534 - acc: 0.6423\n",
      " Optimizer iteration 815, batch 33\n",
      "\n",
      " Learning rate 0.0006274632566950966, Model learning rate 0.0006274632760323584\n",
      " 34/391 [=>............................] - ETA: 21s - loss: 1.1601 - acc: 0.6413\n",
      " Optimizer iteration 816, batch 34\n",
      "\n",
      " Learning rate 0.0006266841696175131, Model learning rate 0.0006266841664910316\n",
      " 35/391 [=>............................] - ETA: 21s - loss: 1.1551 - acc: 0.6433\n",
      " Optimizer iteration 817, batch 35\n",
      "\n",
      " Learning rate 0.000625904753724054, Model learning rate 0.0006259047659114003\n",
      " 36/391 [=>............................] - ETA: 21s - loss: 1.1602 - acc: 0.6406\n",
      " Optimizer iteration 818, batch 36\n",
      "\n",
      " Learning rate 0.0006251250110377367, Model learning rate 0.0006251250160858035\n",
      " 37/391 [=>............................] - ETA: 21s - loss: 1.1591 - acc: 0.6413\n",
      " Optimizer iteration 819, batch 37\n",
      "\n",
      " Learning rate 0.0006243449435824276, Model learning rate 0.0006243449170142412\n",
      "\n",
      " Optimizer iteration 820, batch 38\n",
      "\n",
      " Learning rate 0.0006235645533828348, Model learning rate 0.0006235645269043744\n",
      " 39/391 [=>............................] - ETA: 21s - loss: 1.1604 - acc: 0.6416\n",
      " Optimizer iteration 821, batch 39\n",
      "\n",
      " Learning rate 0.0006227838424645056, Model learning rate 0.0006227838457562029\n",
      " 40/391 [==>...........................] - ETA: 21s - loss: 1.1613 - acc: 0.6412\n",
      " Optimizer iteration 822, batch 40\n",
      "\n",
      " Learning rate 0.0006220028128538187, Model learning rate 0.000622002815362066\n",
      " 41/391 [==>...........................] - ETA: 21s - loss: 1.1616 - acc: 0.6406\n",
      " Optimizer iteration 823, batch 41\n",
      "\n",
      " Learning rate 0.0006212214665779805, Model learning rate 0.0006212214939296246\n",
      " 42/391 [==>...........................] - ETA: 21s - loss: 1.1616 - acc: 0.6408\n",
      " Optimizer iteration 824, batch 42\n",
      "\n",
      " Learning rate 0.000620439805665019, Model learning rate 0.0006204398232512176\n",
      " 43/391 [==>...........................] - ETA: 21s - loss: 1.1590 - acc: 0.6419\n",
      " Optimizer iteration 825, batch 43\n",
      "\n",
      " Learning rate 0.0006196578321437789, Model learning rate 0.0006196578033268452\n",
      " 44/391 [==>...........................] - ETA: 22s - loss: 1.1568 - acc: 0.6422\n",
      " Optimizer iteration 826, batch 44\n",
      "\n",
      " Learning rate 0.0006188755480439165, Model learning rate 0.0006188755505718291\n",
      " 45/391 [==>...........................] - ETA: 22s - loss: 1.1562 - acc: 0.6422\n",
      " Optimizer iteration 827, batch 45\n",
      "\n",
      " Learning rate 0.0006180929553958942, Model learning rate 0.0006180929485708475\n",
      " 46/391 [==>...........................] - ETA: 22s - loss: 1.1582 - acc: 0.6422\n",
      " Optimizer iteration 828, batch 46\n",
      "\n",
      " Learning rate 0.000617310056230975, Model learning rate 0.0006173100555315614\n",
      "\n",
      " Optimizer iteration 829, batch 47\n",
      "\n",
      " Learning rate 0.0006165268525812178, Model learning rate 0.0006165268714539707\n",
      " 48/391 [==>...........................] - ETA: 22s - loss: 1.1611 - acc: 0.6390\n",
      " Optimizer iteration 830, batch 48\n",
      "\n",
      " Learning rate 0.0006157433464794716, Model learning rate 0.0006157433381304145\n",
      " 49/391 [==>...........................] - ETA: 22s - loss: 1.1625 - acc: 0.6390\n",
      " Optimizer iteration 831, batch 49\n",
      "\n",
      " Learning rate 0.0006149595399593703, Model learning rate 0.0006149595137685537\n",
      " 50/391 [==>...........................] - ETA: 22s - loss: 1.1644 - acc: 0.6388\n",
      " Optimizer iteration 832, batch 50\n",
      "\n",
      " Learning rate 0.0006141754350553279, Model learning rate 0.0006141754565760493\n",
      " 51/391 [==>...........................] - ETA: 22s - loss: 1.1635 - acc: 0.6392\n",
      " Optimizer iteration 833, batch 51\n",
      "\n",
      " Learning rate 0.0006133910338025328, Model learning rate 0.0006133910501375794\n",
      " 52/391 [==>...........................] - ETA: 22s - loss: 1.1644 - acc: 0.6393\n",
      " Optimizer iteration 834, batch 52\n",
      "\n",
      " Learning rate 0.0006126063382369423, Model learning rate 0.000612606352660805\n",
      " 53/391 [===>..........................] - ETA: 22s - loss: 1.1646 - acc: 0.6392\n",
      " Optimizer iteration 835, batch 53\n",
      "\n",
      " Learning rate 0.0006118213503952778, Model learning rate 0.000611821364145726\n",
      " 54/391 [===>..........................] - ETA: 22s - loss: 1.1665 - acc: 0.6379\n",
      " Optimizer iteration 836, batch 54\n",
      "\n",
      " Learning rate 0.0006110360723150194, Model learning rate 0.0006110360845923424\n",
      "\n",
      " Optimizer iteration 837, batch 55\n",
      "\n",
      " Learning rate 0.0006102505060344006, Model learning rate 0.0006102505140006542\n",
      " 56/391 [===>..........................] - ETA: 22s - loss: 1.1662 - acc: 0.6395\n",
      " Optimizer iteration 838, batch 56\n",
      "\n",
      " Learning rate 0.0006094646535924025, Model learning rate 0.0006094646523706615\n",
      " 57/391 [===>..........................] - ETA: 22s - loss: 1.1653 - acc: 0.6404\n",
      " Optimizer iteration 839, batch 57\n",
      "\n",
      " Learning rate 0.0006086785170287495, Model learning rate 0.0006086784997023642\n",
      " 58/391 [===>..........................] - ETA: 22s - loss: 1.1654 - acc: 0.6409\n",
      " Optimizer iteration 840, batch 58\n",
      "\n",
      " Learning rate 0.0006078920983839031, Model learning rate 0.0006078921142034233\n",
      " 59/391 [===>..........................] - ETA: 22s - loss: 1.1650 - acc: 0.6414\n",
      " Optimizer iteration 841, batch 59\n",
      "\n",
      " Learning rate 0.000607105399699057, Model learning rate 0.0006071053794585168\n",
      " 60/391 [===>..........................] - ETA: 22s - loss: 1.1640 - acc: 0.6414\n",
      " Optimizer iteration 842, batch 60\n",
      "\n",
      " Learning rate 0.0006063184230161318, Model learning rate 0.0006063184118829668\n",
      " 61/391 [===>..........................] - ETA: 22s - loss: 1.1661 - acc: 0.6411\n",
      " Optimizer iteration 843, batch 61\n",
      "\n",
      " Learning rate 0.0006055311703777698, Model learning rate 0.0006055311532691121\n",
      " 62/391 [===>..........................] - ETA: 22s - loss: 1.1642 - acc: 0.6421\n",
      " Optimizer iteration 844, batch 62\n",
      "\n",
      " Learning rate 0.0006047436438273293, Model learning rate 0.0006047436618246138\n",
      " 63/391 [===>..........................] - ETA: 22s - loss: 1.1619 - acc: 0.6427\n",
      " Optimizer iteration 845, batch 63\n",
      "\n",
      " Learning rate 0.0006039558454088796, Model learning rate 0.00060395582113415\n",
      " 64/391 [===>..........................] - ETA: 22s - loss: 1.1643 - acc: 0.6417\n",
      " Optimizer iteration 846, batch 64\n",
      "\n",
      " Learning rate 0.0006031677771671962, Model learning rate 0.0006031678058207035\n",
      " 65/391 [===>..........................] - ETA: 22s - loss: 1.1642 - acc: 0.6418\n",
      " Optimizer iteration 847, batch 65\n",
      "\n",
      " Learning rate 0.0006023794411477537, Model learning rate 0.0006023794412612915\n",
      " 66/391 [====>.........................] - ETA: 22s - loss: 1.1645 - acc: 0.6416\n",
      " Optimizer iteration 848, batch 66\n",
      "\n",
      " Learning rate 0.0006015908393967232, Model learning rate 0.0006015908438712358\n",
      " 67/391 [====>.........................] - ETA: 22s - loss: 1.1646 - acc: 0.6411\n",
      " Optimizer iteration 849, batch 67\n",
      "\n",
      " Learning rate 0.0006008019739609646, Model learning rate 0.0006008019554428756\n",
      " 68/391 [====>.........................] - ETA: 21s - loss: 1.1639 - acc: 0.6409\n",
      " Optimizer iteration 850, batch 68\n",
      "\n",
      " Learning rate 0.0006000128468880223, Model learning rate 0.0006000128341838717\n",
      " 69/391 [====>.........................] - ETA: 22s - loss: 1.1635 - acc: 0.6414\n",
      " Optimizer iteration 851, batch 69\n",
      "\n",
      " Learning rate 0.00059922346022612, Model learning rate 0.0005992234800942242\n",
      " 70/391 [====>.........................] - ETA: 22s - loss: 1.1634 - acc: 0.6411\n",
      " Optimizer iteration 852, batch 70\n",
      "\n",
      " Learning rate 0.0005984338160241551, Model learning rate 0.0005984338349662721\n",
      " 71/391 [====>.........................] - ETA: 21s - loss: 1.1637 - acc: 0.6413\n",
      " Optimizer iteration 853, batch 71\n",
      "\n",
      " Learning rate 0.0005976439163316936, Model learning rate 0.0005976438988000154\n",
      "\n",
      " Optimizer iteration 854, batch 72\n",
      "\n",
      " Learning rate 0.0005968537631989645, Model learning rate 0.000596853788010776\n",
      " 73/391 [====>.........................] - ETA: 21s - loss: 1.1613 - acc: 0.6417\n",
      " Optimizer iteration 855, batch 73\n",
      "\n",
      " Learning rate 0.0005960633586768543, Model learning rate 0.0005960633861832321\n",
      " 74/391 [====>.........................] - ETA: 21s - loss: 1.1612 - acc: 0.6416\n",
      " Optimizer iteration 856, batch 74\n",
      "\n",
      " Learning rate 0.0005952727048169024, Model learning rate 0.0005952726933173835\n",
      "\n",
      " Optimizer iteration 857, batch 75\n",
      "\n",
      " Learning rate 0.0005944818036712959, Model learning rate 0.0005944818258285522\n",
      " 76/391 [====>.........................] - ETA: 21s - loss: 1.1599 - acc: 0.6421\n",
      " Optimizer iteration 858, batch 76\n",
      "\n",
      " Learning rate 0.0005936906572928624, Model learning rate 0.0005936906673014164\n",
      " 77/391 [====>.........................] - ETA: 21s - loss: 1.1606 - acc: 0.6420\n",
      " Optimizer iteration 859, batch 77\n",
      "\n",
      " Learning rate 0.000592899267735067, Model learning rate 0.0005928992759436369\n",
      " 78/391 [====>.........................] - ETA: 21s - loss: 1.1593 - acc: 0.6427\n",
      " Optimizer iteration 860, batch 78\n",
      "\n",
      " Learning rate 0.0005921076370520058, Model learning rate 0.0005921076517552137\n",
      " 79/391 [=====>........................] - ETA: 21s - loss: 1.1578 - acc: 0.6440\n",
      " Optimizer iteration 861, batch 79\n",
      "\n",
      " Learning rate 0.0005913157672984006, Model learning rate 0.0005913157947361469\n",
      " 80/391 [=====>........................] - ETA: 21s - loss: 1.1567 - acc: 0.6444\n",
      " Optimizer iteration 862, batch 80\n",
      "\n",
      " Learning rate 0.000590523660529594, Model learning rate 0.0005905236466787755\n",
      " 81/391 [=====>........................] - ETA: 21s - loss: 1.1556 - acc: 0.6449\n",
      " Optimizer iteration 863, batch 81\n",
      "\n",
      " Learning rate 0.0005897313188015433, Model learning rate 0.0005897313239984214\n",
      " 82/391 [=====>........................] - ETA: 21s - loss: 1.1573 - acc: 0.6436\n",
      " Optimizer iteration 864, batch 82\n",
      "\n",
      " Learning rate 0.0005889387441708161, Model learning rate 0.0005889387684874237\n",
      " 83/391 [=====>........................] - ETA: 21s - loss: 1.1604 - acc: 0.6425\n",
      " Optimizer iteration 865, batch 83\n",
      "\n",
      " Learning rate 0.0005881459386945845, Model learning rate 0.0005881459219381213\n",
      " 84/391 [=====>........................] - ETA: 21s - loss: 1.1581 - acc: 0.6435\n",
      " Optimizer iteration 866, batch 84\n",
      "\n",
      " Learning rate 0.0005873529044306193, Model learning rate 0.0005873529007658362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85/391 [=====>........................] - ETA: 21s - loss: 1.1587 - acc: 0.6437\n",
      " Optimizer iteration 867, batch 85\n",
      "\n",
      " Learning rate 0.0005865596434372857, Model learning rate 0.0005865596467629075\n",
      " 86/391 [=====>........................] - ETA: 21s - loss: 1.1567 - acc: 0.6451\n",
      " Optimizer iteration 868, batch 86\n",
      "\n",
      " Learning rate 0.0005857661577735372, Model learning rate 0.0005857661599293351\n",
      " 87/391 [=====>........................] - ETA: 21s - loss: 1.1578 - acc: 0.6448\n",
      " Optimizer iteration 869, batch 87\n",
      "\n",
      " Learning rate 0.0005849724494989103, Model learning rate 0.0005849724402651191\n",
      " 88/391 [=====>........................] - ETA: 21s - loss: 1.1593 - acc: 0.6441\n",
      " Optimizer iteration 870, batch 88\n",
      "\n",
      " Learning rate 0.0005841785206735191, Model learning rate 0.0005841785459779203\n",
      " 89/391 [=====>........................] - ETA: 21s - loss: 1.1582 - acc: 0.6444\n",
      " Optimizer iteration 871, batch 89\n",
      "\n",
      " Learning rate 0.0005833843733580511, Model learning rate 0.0005833843606524169\n",
      " 90/391 [=====>........................] - ETA: 21s - loss: 1.1585 - acc: 0.6438\n",
      " Optimizer iteration 872, batch 90\n",
      "\n",
      " Learning rate 0.0005825900096137599, Model learning rate 0.0005825900007039309\n",
      " 91/391 [=====>........................] - ETA: 20s - loss: 1.1573 - acc: 0.6446\n",
      " Optimizer iteration 873, batch 91\n",
      "\n",
      " Learning rate 0.0005817954315024609, Model learning rate 0.0005817954079248011\n",
      " 92/391 [======>.......................] - ETA: 20s - loss: 1.1569 - acc: 0.6451\n",
      " Optimizer iteration 874, batch 92\n",
      "\n",
      " Learning rate 0.0005810006410865267, Model learning rate 0.0005810006405226886\n",
      " 93/391 [======>.......................] - ETA: 20s - loss: 1.1574 - acc: 0.6448\n",
      " Optimizer iteration 875, batch 93\n",
      "\n",
      " Learning rate 0.0005802056404288802, Model learning rate 0.0005802056402899325\n",
      " 94/391 [======>.......................] - ETA: 20s - loss: 1.1569 - acc: 0.6450\n",
      " Optimizer iteration 876, batch 94\n",
      "\n",
      " Learning rate 0.0005794104315929903, Model learning rate 0.0005794104072265327\n",
      "\n",
      " Optimizer iteration 877, batch 95\n",
      "\n",
      " Learning rate 0.0005786150166428661, Model learning rate 0.0005786149995401502\n",
      " 96/391 [======>.......................] - ETA: 20s - loss: 1.1586 - acc: 0.6453\n",
      " Optimizer iteration 878, batch 96\n",
      "\n",
      " Learning rate 0.0005778193976430518, Model learning rate 0.0005778194172307849\n",
      " 97/391 [======>.......................] - ETA: 20s - loss: 1.1584 - acc: 0.6455\n",
      " Optimizer iteration 879, batch 97\n",
      "\n",
      " Learning rate 0.0005770235766586215, Model learning rate 0.000577023602090776\n",
      " 98/391 [======>.......................] - ETA: 20s - loss: 1.1572 - acc: 0.6460\n",
      " Optimizer iteration 880, batch 98\n",
      "\n",
      " Learning rate 0.0005762275557551727, Model learning rate 0.0005762275541201234\n",
      " 99/391 [======>.......................] - ETA: 20s - loss: 1.1561 - acc: 0.6463\n",
      " Optimizer iteration 881, batch 99\n",
      "\n",
      " Learning rate 0.0005754313369988227, Model learning rate 0.0005754313315264881\n",
      "100/391 [======>.......................] - ETA: 20s - loss: 1.1541 - acc: 0.6470\n",
      " Optimizer iteration 882, batch 100\n",
      "\n",
      " Learning rate 0.0005746349224562021, Model learning rate 0.00057463493430987\n",
      "101/391 [======>.......................] - ETA: 20s - loss: 1.1524 - acc: 0.6480\n",
      " Optimizer iteration 883, batch 101\n",
      "\n",
      " Learning rate 0.0005738383141944493, Model learning rate 0.0005738383042626083\n",
      "102/391 [======>.......................] - ETA: 20s - loss: 1.1537 - acc: 0.6468\n",
      " Optimizer iteration 884, batch 102\n",
      "\n",
      " Learning rate 0.0005730415142812059, Model learning rate 0.0005730414995923638\n",
      "103/391 [======>.......................] - ETA: 20s - loss: 1.1522 - acc: 0.6471\n",
      " Optimizer iteration 885, batch 103\n",
      "\n",
      " Learning rate 0.0005722445247846107, Model learning rate 0.0005722445202991366\n",
      "104/391 [======>.......................] - ETA: 20s - loss: 1.1514 - acc: 0.6472\n",
      " Optimizer iteration 886, batch 104\n",
      "\n",
      " Learning rate 0.0005714473477732947, Model learning rate 0.0005714473663829267\n",
      "105/391 [=======>......................] - ETA: 20s - loss: 1.1505 - acc: 0.6471\n",
      " Optimizer iteration 887, batch 105\n",
      "\n",
      " Learning rate 0.000570649985316376, Model learning rate 0.0005706499796360731\n",
      "106/391 [=======>......................] - ETA: 20s - loss: 1.1491 - acc: 0.6474\n",
      " Optimizer iteration 888, batch 106\n",
      "\n",
      " Learning rate 0.000569852439483453, Model learning rate 0.0005698524182662368\n",
      "107/391 [=======>......................] - ETA: 20s - loss: 1.1487 - acc: 0.6472\n",
      " Optimizer iteration 889, batch 107\n",
      "\n",
      " Learning rate 0.000569054712344601, Model learning rate 0.0005690547404810786\n",
      "108/391 [=======>......................] - ETA: 19s - loss: 1.1490 - acc: 0.6470\n",
      " Optimizer iteration 890, batch 108\n",
      "\n",
      " Learning rate 0.0005682568059703659, Model learning rate 0.0005682568298652768\n",
      "\n",
      " Optimizer iteration 891, batch 109\n",
      "\n",
      " Learning rate 0.0005674587224317579, Model learning rate 0.0005674587446264923\n",
      "110/391 [=======>......................] - ETA: 19s - loss: 1.1499 - acc: 0.6468\n",
      " Optimizer iteration 892, batch 110\n",
      "\n",
      " Learning rate 0.000566660463800248, Model learning rate 0.000566660484764725\n",
      "111/391 [=======>......................] - ETA: 19s - loss: 1.1502 - acc: 0.6468\n",
      " Optimizer iteration 893, batch 111\n",
      "\n",
      " Learning rate 0.0005658620321477612, Model learning rate 0.0005658620502799749\n",
      "112/391 [=======>......................] - ETA: 19s - loss: 1.1499 - acc: 0.6466\n",
      " Optimizer iteration 894, batch 112\n",
      "\n",
      " Learning rate 0.0005650634295466716, Model learning rate 0.0005650634411722422\n",
      "113/391 [=======>......................] - ETA: 19s - loss: 1.1497 - acc: 0.6466\n",
      " Optimizer iteration 895, batch 113\n",
      "\n",
      " Learning rate 0.0005642646580697973, Model learning rate 0.0005642646574415267\n",
      "114/391 [=======>......................] - ETA: 19s - loss: 1.1488 - acc: 0.6468\n",
      " Optimizer iteration 896, batch 114\n",
      "\n",
      " Learning rate 0.0005634657197903944, Model learning rate 0.0005634656990878284\n",
      "115/391 [=======>......................] - ETA: 19s - loss: 1.1491 - acc: 0.6465\n",
      " Optimizer iteration 897, batch 115\n",
      "\n",
      " Learning rate 0.0005626666167821521, Model learning rate 0.0005626666243188083\n",
      "116/391 [=======>......................] - ETA: 19s - loss: 1.1497 - acc: 0.6463\n",
      " Optimizer iteration 898, batch 116\n",
      "\n",
      " Learning rate 0.0005618673511191873, Model learning rate 0.0005618673749268055\n",
      "117/391 [=======>......................] - ETA: 19s - loss: 1.1487 - acc: 0.6466\n",
      " Optimizer iteration 899, batch 117\n",
      "\n",
      " Learning rate 0.0005610679248760384, Model learning rate 0.0005610679509118199\n",
      "118/391 [========>.....................] - ETA: 19s - loss: 1.1482 - acc: 0.6469\n",
      " Optimizer iteration 900, batch 118\n",
      "\n",
      " Learning rate 0.0005602683401276614, Model learning rate 0.0005602683522738516\n",
      "119/391 [========>.....................] - ETA: 19s - loss: 1.1488 - acc: 0.6464\n",
      " Optimizer iteration 901, batch 119\n",
      "\n",
      " Learning rate 0.0005594685989494238, Model learning rate 0.0005594685790129006\n",
      "120/391 [========>.....................] - ETA: 19s - loss: 1.1482 - acc: 0.6465\n",
      " Optimizer iteration 902, batch 120\n",
      "\n",
      " Learning rate 0.0005586687034170981, Model learning rate 0.0005586686893366277\n",
      "121/391 [========>.....................] - ETA: 19s - loss: 1.1472 - acc: 0.6466\n",
      " Optimizer iteration 903, batch 121\n",
      "\n",
      " Learning rate 0.0005578686556068585, Model learning rate 0.000557868683245033\n",
      "122/391 [========>.....................] - ETA: 19s - loss: 1.1474 - acc: 0.6463\n",
      " Optimizer iteration 904, batch 122\n",
      "\n",
      " Learning rate 0.0005570684575952737, Model learning rate 0.0005570684443227947\n",
      "123/391 [========>.....................] - ETA: 19s - loss: 1.1477 - acc: 0.6462\n",
      " Optimizer iteration 905, batch 123\n",
      "\n",
      " Learning rate 0.0005562681114593028, Model learning rate 0.0005562680889852345\n",
      "124/391 [========>.....................] - ETA: 19s - loss: 1.1478 - acc: 0.6459\n",
      " Optimizer iteration 906, batch 124\n",
      "\n",
      " Learning rate 0.0005554676192762891, Model learning rate 0.0005554676172323525\n",
      "125/391 [========>.....................] - ETA: 18s - loss: 1.1475 - acc: 0.6458\n",
      " Optimizer iteration 907, batch 125\n",
      "\n",
      " Learning rate 0.000554666983123955, Model learning rate 0.0005546669708564878\n",
      "126/391 [========>.....................] - ETA: 18s - loss: 1.1473 - acc: 0.6458\n",
      " Optimizer iteration 908, batch 126\n",
      "\n",
      " Learning rate 0.0005538662050803964, Model learning rate 0.0005538662080653012\n",
      "127/391 [========>.....................] - ETA: 18s - loss: 1.1479 - acc: 0.6455\n",
      " Optimizer iteration 909, batch 127\n",
      "\n",
      " Learning rate 0.0005530652872240779, Model learning rate 0.0005530652706511319\n",
      "128/391 [========>.....................] - ETA: 18s - loss: 1.1474 - acc: 0.6454\n",
      " Optimizer iteration 910, batch 128\n",
      "\n",
      " Learning rate 0.0005522642316338268, Model learning rate 0.0005522642168216407\n",
      "\n",
      " Optimizer iteration 911, batch 129\n",
      "\n",
      " Learning rate 0.0005514630403888278, Model learning rate 0.0005514630465768278\n",
      "130/391 [========>.....................] - ETA: 18s - loss: 1.1471 - acc: 0.6456\n",
      " Optimizer iteration 912, batch 130\n",
      "\n",
      " Learning rate 0.0005506617155686176, Model learning rate 0.0005506617017090321\n",
      "131/391 [=========>....................] - ETA: 18s - loss: 1.1472 - acc: 0.6457\n",
      " Optimizer iteration 913, batch 131\n",
      "\n",
      " Learning rate 0.0005498602592530799, Model learning rate 0.0005498602404259145\n",
      "132/391 [=========>....................] - ETA: 18s - loss: 1.1474 - acc: 0.6457\n",
      " Optimizer iteration 914, batch 132\n",
      "\n",
      " Learning rate 0.0005490586735224398, Model learning rate 0.0005490586627274752\n",
      "\n",
      " Optimizer iteration 915, batch 133\n",
      "\n",
      " Learning rate 0.0005482569604572576, Model learning rate 0.000548256968613714\n",
      "134/391 [=========>....................] - ETA: 18s - loss: 1.1472 - acc: 0.6455\n",
      " Optimizer iteration 916, batch 134\n",
      "\n",
      " Learning rate 0.000547455122138425, Model learning rate 0.00054745509987697\n",
      "135/391 [=========>....................] - ETA: 18s - loss: 1.1472 - acc: 0.6452\n",
      " Optimizer iteration 917, batch 135\n",
      "\n",
      " Learning rate 0.000546653160647158, Model learning rate 0.0005466531729325652\n",
      "136/391 [=========>....................] - ETA: 18s - loss: 1.1477 - acc: 0.6448\n",
      " Optimizer iteration 918, batch 136\n",
      "\n",
      " Learning rate 0.0005458510780649931, Model learning rate 0.0005458510713651776\n",
      "137/391 [=========>....................] - ETA: 18s - loss: 1.1475 - acc: 0.6448\n",
      " Optimizer iteration 919, batch 137\n",
      "\n",
      " Learning rate 0.0005450488764737804, Model learning rate 0.0005450488533824682\n",
      "138/391 [=========>....................] - ETA: 18s - loss: 1.1471 - acc: 0.6451\n",
      " Optimizer iteration 920, batch 138\n",
      "\n",
      " Learning rate 0.0005442465579556792, Model learning rate 0.0005442465771920979\n",
      "139/391 [=========>....................] - ETA: 18s - loss: 1.1465 - acc: 0.6452\n",
      " Optimizer iteration 921, batch 139\n",
      "\n",
      " Learning rate 0.0005434441245931524, Model learning rate 0.0005434441263787448\n",
      "140/391 [=========>....................] - ETA: 18s - loss: 1.1462 - acc: 0.6455\n",
      " Optimizer iteration 922, batch 140\n",
      "\n",
      " Learning rate 0.0005426415784689607, Model learning rate 0.00054264155915007\n",
      "141/391 [=========>....................] - ETA: 17s - loss: 1.1462 - acc: 0.6452\n",
      " Optimizer iteration 923, batch 141\n",
      "\n",
      " Learning rate 0.0005418389216661579, Model learning rate 0.0005418389337137341\n",
      "142/391 [=========>....................] - ETA: 17s - loss: 1.1466 - acc: 0.6450\n",
      " Optimizer iteration 924, batch 142\n",
      "\n",
      " Learning rate 0.0005410361562680842, Model learning rate 0.0005410361336544156\n",
      "143/391 [=========>....................] - ETA: 17s - loss: 1.1470 - acc: 0.6448\n",
      " Optimizer iteration 925, batch 143\n",
      "\n",
      " Learning rate 0.000540233284358363, Model learning rate 0.0005402332753874362\n",
      "144/391 [==========>...................] - ETA: 17s - loss: 1.1464 - acc: 0.6449\n",
      " Optimizer iteration 926, batch 144\n",
      "\n",
      " Learning rate 0.000539430308020893, Model learning rate 0.0005394303007051349\n",
      "\n",
      " Optimizer iteration 927, batch 145\n",
      "\n",
      " Learning rate 0.0005386272293398444, Model learning rate 0.0005386272096075118\n",
      "146/391 [==========>...................] - ETA: 17s - loss: 1.1445 - acc: 0.6455\n",
      " Optimizer iteration 928, batch 146\n",
      "\n",
      " Learning rate 0.0005378240503996531, Model learning rate 0.0005378240603022277\n",
      "147/391 [==========>...................] - ETA: 17s - loss: 1.1440 - acc: 0.6458\n",
      " Optimizer iteration 929, batch 147\n",
      "\n",
      " Learning rate 0.000537020773285015, Model learning rate 0.0005370207945816219\n",
      "148/391 [==========>...................] - ETA: 17s - loss: 1.1450 - acc: 0.6453\n",
      " Optimizer iteration 930, batch 148\n",
      "\n",
      " Learning rate 0.0005362174000808813, Model learning rate 0.0005362174124456942\n",
      "149/391 [==========>...................] - ETA: 17s - loss: 1.1454 - acc: 0.6451\n",
      " Optimizer iteration 931, batch 149\n",
      "\n",
      " Learning rate 0.0005354139328724518, Model learning rate 0.0005354139138944447\n",
      "150/391 [==========>...................] - ETA: 17s - loss: 1.1448 - acc: 0.6454\n",
      " Optimizer iteration 932, batch 150\n",
      "\n",
      " Learning rate 0.000534610373745171, Model learning rate 0.0005346103571355343\n",
      "151/391 [==========>...................] - ETA: 17s - loss: 1.1438 - acc: 0.6460\n",
      " Optimizer iteration 933, batch 151\n",
      "\n",
      " Learning rate 0.0005338067247847219, Model learning rate 0.000533806742168963\n",
      "152/391 [==========>...................] - ETA: 17s - loss: 1.1439 - acc: 0.6460\n",
      " Optimizer iteration 934, batch 152\n",
      "\n",
      " Learning rate 0.0005330029880770201, Model learning rate 0.0005330030107870698\n",
      "\n",
      " Optimizer iteration 935, batch 153\n",
      "\n",
      " Learning rate 0.0005321991657082097, Model learning rate 0.0005321991629898548\n",
      "154/391 [==========>...................] - ETA: 17s - loss: 1.1437 - acc: 0.6463\n",
      " Optimizer iteration 936, batch 154\n",
      "\n",
      " Learning rate 0.0005313952597646568, Model learning rate 0.0005313952569849789\n",
      "155/391 [==========>...................] - ETA: 16s - loss: 1.1428 - acc: 0.6466\n",
      " Optimizer iteration 937, batch 155\n",
      "\n",
      " Learning rate 0.0005305912723329441, Model learning rate 0.0005305912927724421\n",
      "156/391 [==========>...................] - ETA: 16s - loss: 1.1426 - acc: 0.6471\n",
      " Optimizer iteration 938, batch 156\n",
      "\n",
      " Learning rate 0.0005297872054998662, Model learning rate 0.0005297872121445835\n",
      "\n",
      " Optimizer iteration 939, batch 157\n",
      "\n",
      " Learning rate 0.0005289830613524241, Model learning rate 0.0005289830733090639\n",
      "158/391 [===========>..................] - ETA: 16s - loss: 1.1419 - acc: 0.6474\n",
      " Optimizer iteration 940, batch 158\n",
      "\n",
      " Learning rate 0.0005281788419778188, Model learning rate 0.0005281788180582225\n",
      "159/391 [===========>..................] - ETA: 16s - loss: 1.1426 - acc: 0.6474\n",
      " Optimizer iteration 941, batch 159\n",
      "\n",
      " Learning rate 0.0005273745494634467, Model learning rate 0.0005273745628073812\n",
      "160/391 [===========>..................] - ETA: 16s - loss: 1.1420 - acc: 0.6476\n",
      " Optimizer iteration 942, batch 160\n",
      "\n",
      " Learning rate 0.0005265701858968943, Model learning rate 0.000526570191141218\n",
      "\n",
      " Optimizer iteration 943, batch 161\n",
      "\n",
      " Learning rate 0.0005257657533659325, Model learning rate 0.0005257657612673938\n",
      "162/391 [===========>..................] - ETA: 16s - loss: 1.1407 - acc: 0.6474\n",
      " Optimizer iteration 944, batch 162\n",
      "\n",
      " Learning rate 0.0005249612539585113, Model learning rate 0.0005249612731859088\n",
      "163/391 [===========>..................] - ETA: 16s - loss: 1.1405 - acc: 0.6473\n",
      " Optimizer iteration 945, batch 163\n",
      "\n",
      " Learning rate 0.0005241566897627535, Model learning rate 0.0005241566686891019\n",
      "164/391 [===========>..................] - ETA: 16s - loss: 1.1399 - acc: 0.6474\n",
      " Optimizer iteration 946, batch 164\n",
      "\n",
      " Learning rate 0.0005233520628669512, Model learning rate 0.0005233520641922951\n",
      "165/391 [===========>..................] - ETA: 16s - loss: 1.1389 - acc: 0.6478\n",
      " Optimizer iteration 947, batch 165\n",
      "\n",
      " Learning rate 0.0005225473753595585, Model learning rate 0.0005225474014878273\n",
      "166/391 [===========>..................] - ETA: 16s - loss: 1.1407 - acc: 0.6473\n",
      " Optimizer iteration 948, batch 166\n",
      "\n",
      " Learning rate 0.0005217426293291868, Model learning rate 0.0005217426223680377\n",
      "167/391 [===========>..................] - ETA: 16s - loss: 1.1404 - acc: 0.6474\n",
      " Optimizer iteration 949, batch 167\n",
      "\n",
      " Learning rate 0.0005209378268645998, Model learning rate 0.0005209378432482481\n",
      "168/391 [===========>..................] - ETA: 16s - loss: 1.1398 - acc: 0.6475\n",
      " Optimizer iteration 950, batch 168\n",
      "\n",
      " Learning rate 0.0005201329700547076, Model learning rate 0.0005201329477131367\n",
      "169/391 [===========>..................] - ETA: 16s - loss: 1.1395 - acc: 0.6475\n",
      " Optimizer iteration 951, batch 169\n",
      "\n",
      " Learning rate 0.0005193280609885611, Model learning rate 0.0005193280521780252\n",
      "170/391 [============>.................] - ETA: 15s - loss: 1.1393 - acc: 0.6477\n",
      " Optimizer iteration 952, batch 170\n",
      "\n",
      " Learning rate 0.000518523101755347, Model learning rate 0.0005185230984352529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/391 [============>.................] - ETA: 15s - loss: 1.1398 - acc: 0.6476\n",
      " Optimizer iteration 953, batch 171\n",
      "\n",
      " Learning rate 0.0005177180944443821, Model learning rate 0.0005177180864848197\n",
      "172/391 [============>.................] - ETA: 15s - loss: 1.1397 - acc: 0.6476\n",
      " Optimizer iteration 954, batch 172\n",
      "\n",
      " Learning rate 0.0005169130411451083, Model learning rate 0.0005169130163267255\n",
      "173/391 [============>.................] - ETA: 15s - loss: 1.1389 - acc: 0.6481\n",
      " Optimizer iteration 955, batch 173\n",
      "\n",
      " Learning rate 0.0005161079439470866, Model learning rate 0.0005161079461686313\n",
      "174/391 [============>.................] - ETA: 15s - loss: 1.1385 - acc: 0.6480\n",
      " Optimizer iteration 956, batch 174\n",
      "\n",
      " Learning rate 0.0005153028049399916, Model learning rate 0.0005153028178028762\n",
      "175/391 [============>.................] - ETA: 15s - loss: 1.1392 - acc: 0.6477\n",
      " Optimizer iteration 957, batch 175\n",
      "\n",
      " Learning rate 0.0005144976262136073, Model learning rate 0.0005144976312294602\n",
      "176/391 [============>.................] - ETA: 15s - loss: 1.1395 - acc: 0.6475\n",
      " Optimizer iteration 958, batch 176\n",
      "\n",
      " Learning rate 0.00051369240985782, Model learning rate 0.0005136923864483833\n",
      "177/391 [============>.................] - ETA: 15s - loss: 1.1396 - acc: 0.6474\n",
      " Optimizer iteration 959, batch 177\n",
      "\n",
      " Learning rate 0.0005128871579626142, Model learning rate 0.0005128871416673064\n",
      "\n",
      " Optimizer iteration 960, batch 178\n",
      "\n",
      " Learning rate 0.0005120818726180662, Model learning rate 0.0005120818968862295\n",
      "179/391 [============>.................] - ETA: 15s - loss: 1.1391 - acc: 0.6480\n",
      " Optimizer iteration 961, batch 179\n",
      "\n",
      " Learning rate 0.0005112765559143394, Model learning rate 0.0005112765356898308\n",
      "180/391 [============>.................] - ETA: 15s - loss: 1.1387 - acc: 0.6481\n",
      " Optimizer iteration 962, batch 180\n",
      "\n",
      " Learning rate 0.0005104712099416785, Model learning rate 0.000510471232701093\n",
      "181/391 [============>.................] - ETA: 15s - loss: 1.1394 - acc: 0.6482\n",
      " Optimizer iteration 963, batch 181\n",
      "\n",
      " Learning rate 0.0005096658367904042, Model learning rate 0.0005096658132970333\n",
      "182/391 [============>.................] - ETA: 15s - loss: 1.1396 - acc: 0.6482\n",
      " Optimizer iteration 964, batch 182\n",
      "\n",
      " Learning rate 0.0005088604385509079, Model learning rate 0.0005088604521006346\n",
      "183/391 [=============>................] - ETA: 15s - loss: 1.1405 - acc: 0.6477\n",
      " Optimizer iteration 965, batch 183\n",
      "\n",
      " Learning rate 0.0005080550173136456, Model learning rate 0.0005080550326965749\n",
      "184/391 [=============>................] - ETA: 14s - loss: 1.1407 - acc: 0.6475\n",
      " Optimizer iteration 966, batch 184\n",
      "\n",
      " Learning rate 0.0005072495751691338, Model learning rate 0.0005072495550848544\n",
      "185/391 [=============>................] - ETA: 14s - loss: 1.1408 - acc: 0.6473\n",
      " Optimizer iteration 967, batch 185\n",
      "\n",
      " Learning rate 0.0005064441142079425, Model learning rate 0.0005064441356807947\n",
      "\n",
      " Optimizer iteration 968, batch 186\n",
      "\n",
      " Learning rate 0.0005056386365206907, Model learning rate 0.0005056386580690742\n",
      "187/391 [=============>................] - ETA: 14s - loss: 1.1427 - acc: 0.6471\n",
      " Optimizer iteration 969, batch 187\n",
      "\n",
      " Learning rate 0.0005048331441980416, Model learning rate 0.0005048331222496927\n",
      "188/391 [=============>................] - ETA: 14s - loss: 1.1421 - acc: 0.6469\n",
      " Optimizer iteration 970, batch 188\n",
      "\n",
      " Learning rate 0.0005040276393306949, Model learning rate 0.0005040276446379721\n",
      "189/391 [=============>................] - ETA: 14s - loss: 1.1422 - acc: 0.6472\n",
      " Optimizer iteration 971, batch 189\n",
      "\n",
      " Learning rate 0.0005032221240093846, Model learning rate 0.0005032221088185906\n",
      "190/391 [=============>................] - ETA: 14s - loss: 1.1423 - acc: 0.6471\n",
      " Optimizer iteration 972, batch 190\n",
      "\n",
      " Learning rate 0.0005024166003248702, Model learning rate 0.0005024165729992092\n",
      "191/391 [=============>................] - ETA: 14s - loss: 1.1420 - acc: 0.6473\n",
      " Optimizer iteration 973, batch 191\n",
      "\n",
      " Learning rate 0.000501611070367934, Model learning rate 0.0005016110953874886\n",
      "192/391 [=============>................] - ETA: 14s - loss: 1.1416 - acc: 0.6471\n",
      " Optimizer iteration 974, batch 192\n",
      "\n",
      " Learning rate 0.0005008055362293743, Model learning rate 0.0005008055595681071\n",
      "193/391 [=============>................] - ETA: 14s - loss: 1.1414 - acc: 0.6472\n",
      " Optimizer iteration 975, batch 193\n",
      "\n",
      " Learning rate 0.0005, Model learning rate 0.0005000000237487257\n",
      "194/391 [=============>................] - ETA: 14s - loss: 1.1410 - acc: 0.6473\n",
      " Optimizer iteration 976, batch 194\n",
      "\n",
      " Learning rate 0.0004991944637706257, Model learning rate 0.0004991944879293442\n",
      "195/391 [=============>................] - ETA: 14s - loss: 1.1406 - acc: 0.6474\n",
      " Optimizer iteration 977, batch 195\n",
      "\n",
      " Learning rate 0.000498388929632066, Model learning rate 0.0004983889521099627\n",
      "196/391 [==============>...............] - ETA: 14s - loss: 1.1399 - acc: 0.6475\n",
      " Optimizer iteration 978, batch 196\n",
      "\n",
      " Learning rate 0.0004975833996751299, Model learning rate 0.0004975834162905812\n",
      "197/391 [==============>...............] - ETA: 14s - loss: 1.1400 - acc: 0.6474\n",
      " Optimizer iteration 979, batch 197\n",
      "\n",
      " Learning rate 0.0004967778759906157, Model learning rate 0.0004967778804711998\n",
      "198/391 [==============>...............] - ETA: 14s - loss: 1.1393 - acc: 0.6478\n",
      " Optimizer iteration 980, batch 198\n",
      "\n",
      " Learning rate 0.0004959723606693051, Model learning rate 0.0004959723446518183\n",
      "199/391 [==============>...............] - ETA: 13s - loss: 1.1388 - acc: 0.6481\n",
      " Optimizer iteration 981, batch 199\n",
      "\n",
      " Learning rate 0.0004951668558019585, Model learning rate 0.0004951668670400977\n",
      "200/391 [==============>...............] - ETA: 13s - loss: 1.1386 - acc: 0.6481\n",
      " Optimizer iteration 982, batch 200\n",
      "\n",
      " Learning rate 0.0004943613634793092, Model learning rate 0.0004943613894283772\n",
      "201/391 [==============>...............] - ETA: 13s - loss: 1.1385 - acc: 0.6481\n",
      " Optimizer iteration 983, batch 201\n",
      "\n",
      " Learning rate 0.0004935558857920576, Model learning rate 0.0004935559118166566\n",
      "202/391 [==============>...............] - ETA: 13s - loss: 1.1378 - acc: 0.6487\n",
      " Optimizer iteration 984, batch 202\n",
      "\n",
      " Learning rate 0.0004927504248308663, Model learning rate 0.000492750434204936\n",
      "203/391 [==============>...............] - ETA: 13s - loss: 1.1383 - acc: 0.6485\n",
      " Optimizer iteration 985, batch 203\n",
      "\n",
      " Learning rate 0.0004919449826863544, Model learning rate 0.0004919449565932155\n",
      "204/391 [==============>...............] - ETA: 13s - loss: 1.1382 - acc: 0.6486\n",
      " Optimizer iteration 986, batch 204\n",
      "\n",
      " Learning rate 0.0004911395614490922, Model learning rate 0.0004911395371891558\n",
      "205/391 [==============>...............] - ETA: 13s - loss: 1.1379 - acc: 0.6486\n",
      " Optimizer iteration 987, batch 205\n",
      "\n",
      " Learning rate 0.0004903341632095958, Model learning rate 0.0004903341759927571\n",
      "206/391 [==============>...............] - ETA: 13s - loss: 1.1377 - acc: 0.6486\n",
      " Optimizer iteration 988, batch 206\n",
      "\n",
      " Learning rate 0.0004895287900583215, Model learning rate 0.0004895288147963583\n",
      "207/391 [==============>...............] - ETA: 13s - loss: 1.1374 - acc: 0.6486\n",
      " Optimizer iteration 989, batch 207\n",
      "\n",
      " Learning rate 0.0004887234440856608, Model learning rate 0.0004887234535999596\n",
      "208/391 [==============>...............] - ETA: 13s - loss: 1.1374 - acc: 0.6489\n",
      " Optimizer iteration 990, batch 208\n",
      "\n",
      " Learning rate 0.000487918127381934, Model learning rate 0.00048791812150739133\n",
      "209/391 [===============>..............] - ETA: 13s - loss: 1.1374 - acc: 0.6490\n",
      " Optimizer iteration 991, batch 209\n",
      "\n",
      " Learning rate 0.0004871128420373859, Model learning rate 0.00048711284762248397\n",
      "210/391 [===============>..............] - ETA: 13s - loss: 1.1378 - acc: 0.6488\n",
      " Optimizer iteration 992, batch 210\n",
      "\n",
      " Learning rate 0.00048630759014218, Model learning rate 0.00048630760284140706\n",
      "211/391 [===============>..............] - ETA: 13s - loss: 1.1377 - acc: 0.6489\n",
      " Optimizer iteration 993, batch 211\n",
      "\n",
      " Learning rate 0.00048550237378639275, Model learning rate 0.0004855023871641606\n",
      "212/391 [===============>..............] - ETA: 13s - loss: 1.1381 - acc: 0.6488\n",
      " Optimizer iteration 994, batch 212\n",
      "\n",
      " Learning rate 0.00048469719506000837, Model learning rate 0.0004846972005907446\n",
      "\n",
      " Optimizer iteration 995, batch 213\n",
      "\n",
      " Learning rate 0.00048389205605291365, Model learning rate 0.0004838920431211591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/391 [===============>..............] - ETA: 12s - loss: 1.1384 - acc: 0.6487\n",
      " Optimizer iteration 996, batch 214\n",
      "\n",
      " Learning rate 0.0004830869588548918, Model learning rate 0.0004830869729630649\n",
      "215/391 [===============>..............] - ETA: 12s - loss: 1.1385 - acc: 0.6487\n",
      " Optimizer iteration 997, batch 215\n",
      "\n",
      " Learning rate 0.0004822819055556179, Model learning rate 0.00048228190280497074\n",
      "216/391 [===============>..............] - ETA: 12s - loss: 1.1387 - acc: 0.6485\n",
      " Optimizer iteration 998, batch 216\n",
      "\n",
      " Learning rate 0.00048147689824465313, Model learning rate 0.0004814768908545375\n",
      "\n",
      " Optimizer iteration 999, batch 217\n",
      "\n",
      " Learning rate 0.00048067193901143887, Model learning rate 0.00048067193711176515\n",
      "218/391 [===============>..............] - ETA: 12s - loss: 1.1386 - acc: 0.6485\n",
      " Optimizer iteration 1000, batch 218\n",
      "\n",
      " Learning rate 0.0004798670299452926, Model learning rate 0.0004798670415766537\n",
      "219/391 [===============>..............] - ETA: 12s - loss: 1.1383 - acc: 0.6483\n",
      " Optimizer iteration 1001, batch 219\n",
      "\n",
      " Learning rate 0.0004790621731354003, Model learning rate 0.00047906217514537275\n",
      "220/391 [===============>..............] - ETA: 12s - loss: 1.1372 - acc: 0.6486\n",
      " Optimizer iteration 1002, batch 220\n",
      "\n",
      " Learning rate 0.00047825737067081327, Model learning rate 0.0004782573669217527\n",
      "221/391 [===============>..............] - ETA: 12s - loss: 1.1373 - acc: 0.6486\n",
      " Optimizer iteration 1003, batch 221\n",
      "\n",
      " Learning rate 0.00047745262464044165, Model learning rate 0.00047745261690579355\n",
      "222/391 [================>.............] - ETA: 12s - loss: 1.1367 - acc: 0.6489\n",
      " Optimizer iteration 1004, batch 222\n",
      "\n",
      " Learning rate 0.0004766479371330488, Model learning rate 0.0004766479250974953\n",
      "223/391 [================>.............] - ETA: 12s - loss: 1.1376 - acc: 0.6486\n",
      " Optimizer iteration 1005, batch 223\n",
      "\n",
      " Learning rate 0.00047584331023724653, Model learning rate 0.00047584332060068846\n",
      "224/391 [================>.............] - ETA: 12s - loss: 1.1369 - acc: 0.6491\n",
      " Optimizer iteration 1006, batch 224\n",
      "\n",
      " Learning rate 0.0004750387460414889, Model learning rate 0.00047503874520771205\n",
      "\n",
      " Optimizer iteration 1007, batch 225\n",
      "\n",
      " Learning rate 0.00047423424663406747, Model learning rate 0.000474234257126227\n",
      "226/391 [================>.............] - ETA: 12s - loss: 1.1359 - acc: 0.6493\n",
      " Optimizer iteration 1008, batch 226\n",
      "\n",
      " Learning rate 0.0004734298141031057, Model learning rate 0.0004734298272524029\n",
      "227/391 [================>.............] - ETA: 11s - loss: 1.1351 - acc: 0.6496\n",
      " Optimizer iteration 1009, batch 227\n",
      "\n",
      " Learning rate 0.00047262545053655344, Model learning rate 0.0004726254555862397\n",
      "228/391 [================>.............] - ETA: 11s - loss: 1.1342 - acc: 0.6499\n",
      " Optimizer iteration 1010, batch 228\n",
      "\n",
      " Learning rate 0.0004718211580221812, Model learning rate 0.00047182117123156786\n",
      "229/391 [================>.............] - ETA: 11s - loss: 1.1347 - acc: 0.6499\n",
      " Optimizer iteration 1011, batch 229\n",
      "\n",
      " Learning rate 0.00047101693864757605, Model learning rate 0.00047101694508455694\n",
      "230/391 [================>.............] - ETA: 11s - loss: 1.1347 - acc: 0.6498\n",
      " Optimizer iteration 1012, batch 230\n",
      "\n",
      " Learning rate 0.00047021279450013383, Model learning rate 0.0004702128062490374\n",
      "231/391 [================>.............] - ETA: 11s - loss: 1.1346 - acc: 0.6499\n",
      " Optimizer iteration 1013, batch 231\n",
      "\n",
      " Learning rate 0.000469408727667056, Model learning rate 0.00046940872562117875\n",
      "232/391 [================>.............] - ETA: 11s - loss: 1.1331 - acc: 0.6504\n",
      " Optimizer iteration 1014, batch 232\n",
      "\n",
      " Learning rate 0.0004686047402353433, Model learning rate 0.0004686047323048115\n",
      "233/391 [================>.............] - ETA: 11s - loss: 1.1325 - acc: 0.6505\n",
      " Optimizer iteration 1015, batch 233\n",
      "\n",
      " Learning rate 0.00046780083429179026, Model learning rate 0.0004678008262999356\n",
      "234/391 [================>.............] - ETA: 11s - loss: 1.1323 - acc: 0.6504\n",
      " Optimizer iteration 1016, batch 234\n",
      "\n",
      " Learning rate 0.00046699701192297994, Model learning rate 0.00046699700760655105\n",
      "235/391 [=================>............] - ETA: 11s - loss: 1.1325 - acc: 0.6504\n",
      " Optimizer iteration 1017, batch 235\n",
      "\n",
      " Learning rate 0.00046619327521527825, Model learning rate 0.0004661932762246579\n",
      "236/391 [=================>............] - ETA: 11s - loss: 1.1329 - acc: 0.6502\n",
      " Optimizer iteration 1018, batch 236\n",
      "\n",
      " Learning rate 0.00046538962625482905, Model learning rate 0.0004653896321542561\n",
      "\n",
      " Optimizer iteration 1019, batch 237\n",
      "\n",
      " Learning rate 0.0004645860671275483, Model learning rate 0.0004645860753953457\n",
      "238/391 [=================>............] - ETA: 11s - loss: 1.1331 - acc: 0.6500\n",
      " Optimizer iteration 1020, batch 238\n",
      "\n",
      " Learning rate 0.00046378259991911887, Model learning rate 0.00046378260594792664\n",
      "239/391 [=================>............] - ETA: 11s - loss: 1.1326 - acc: 0.6502\n",
      " Optimizer iteration 1021, batch 239\n",
      "\n",
      " Learning rate 0.0004629792267149849, Model learning rate 0.00046297922381199896\n",
      "240/391 [=================>............] - ETA: 11s - loss: 1.1323 - acc: 0.6504\n",
      " Optimizer iteration 1022, batch 240\n",
      "\n",
      " Learning rate 0.00046217594960034714, Model learning rate 0.0004621759580913931\n",
      "241/391 [=================>............] - ETA: 10s - loss: 1.1325 - acc: 0.6504\n",
      " Optimizer iteration 1023, batch 241\n",
      "\n",
      " Learning rate 0.0004613727706601557, Model learning rate 0.00046137277968227863\n",
      "\n",
      " Optimizer iteration 1024, batch 242\n",
      "\n",
      " Learning rate 0.00046056969197910707, Model learning rate 0.0004605696885846555\n",
      "243/391 [=================>............] - ETA: 10s - loss: 1.1324 - acc: 0.6503\n",
      " Optimizer iteration 1025, batch 243\n",
      "\n",
      " Learning rate 0.00045976671564163706, Model learning rate 0.00045976671390235424\n",
      "244/391 [=================>............] - ETA: 10s - loss: 1.1325 - acc: 0.6503\n",
      " Optimizer iteration 1026, batch 244\n",
      "\n",
      " Learning rate 0.0004589638437319157, Model learning rate 0.0004589638556353748\n",
      "245/391 [=================>............] - ETA: 10s - loss: 1.1321 - acc: 0.6506\n",
      " Optimizer iteration 1027, batch 245\n",
      "\n",
      " Learning rate 0.00045816107833384235, Model learning rate 0.0004581610846798867\n",
      "246/391 [=================>............] - ETA: 10s - loss: 1.1321 - acc: 0.6505\n",
      " Optimizer iteration 1028, batch 246\n",
      "\n",
      " Learning rate 0.00045735842153103934, Model learning rate 0.00045735843013972044\n",
      "247/391 [=================>............] - ETA: 10s - loss: 1.1323 - acc: 0.6504\n",
      " Optimizer iteration 1029, batch 247\n",
      "\n",
      " Learning rate 0.0004565558754068477, Model learning rate 0.00045655586291104555\n",
      "248/391 [==================>...........] - ETA: 10s - loss: 1.1324 - acc: 0.6504\n",
      " Optimizer iteration 1030, batch 248\n",
      "\n",
      " Learning rate 0.00045575344204432084, Model learning rate 0.00045575344120152295\n",
      "\n",
      " Optimizer iteration 1031, batch 249\n",
      "\n",
      " Learning rate 0.00045495112352621957, Model learning rate 0.00045495113590732217\n",
      "250/391 [==================>...........] - ETA: 10s - loss: 1.1325 - acc: 0.6505\n",
      " Optimizer iteration 1032, batch 250\n",
      "\n",
      " Learning rate 0.0004541489219350069, Model learning rate 0.00045414891792461276\n",
      "251/391 [==================>...........] - ETA: 10s - loss: 1.1327 - acc: 0.6504\n",
      " Optimizer iteration 1033, batch 251\n",
      "\n",
      " Learning rate 0.0004533468393528421, Model learning rate 0.00045334684546105564\n",
      "252/391 [==================>...........] - ETA: 10s - loss: 1.1329 - acc: 0.6503\n",
      " Optimizer iteration 1034, batch 252\n",
      "\n",
      " Learning rate 0.0004525448778615752, Model learning rate 0.00045254488941282034\n",
      "\n",
      " Optimizer iteration 1035, batch 253\n",
      "\n",
      " Learning rate 0.00045174303954274245, Model learning rate 0.00045174304977990687\n",
      "254/391 [==================>...........] - ETA: 10s - loss: 1.1317 - acc: 0.6507\n",
      " Optimizer iteration 1036, batch 254\n",
      "\n",
      " Learning rate 0.0004509413264775604, Model learning rate 0.0004509413265623152\n",
      "\n",
      " Optimizer iteration 1037, batch 255\n",
      "\n",
      " Learning rate 0.0004501397407469201, Model learning rate 0.00045013974886387587\n",
      "256/391 [==================>...........] - ETA: 9s - loss: 1.1301 - acc: 0.6513 \n",
      " Optimizer iteration 1038, batch 256\n",
      "\n",
      " Learning rate 0.0004493382844313826, Model learning rate 0.00044933828758075833\n",
      "257/391 [==================>...........] - ETA: 9s - loss: 1.1301 - acc: 0.6513\n",
      " Optimizer iteration 1039, batch 257\n",
      "\n",
      " Learning rate 0.0004485369596111724, Model learning rate 0.0004485369718167931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258/391 [==================>...........] - ETA: 9s - loss: 1.1304 - acc: 0.6511\n",
      " Optimizer iteration 1040, batch 258\n",
      "\n",
      " Learning rate 0.00044773576836617336, Model learning rate 0.00044773577246814966\n",
      "\n",
      " Optimizer iteration 1041, batch 259\n",
      "\n",
      " Learning rate 0.0004469347127759222, Model learning rate 0.0004469347186386585\n",
      "260/391 [==================>...........] - ETA: 9s - loss: 1.1304 - acc: 0.6511\n",
      " Optimizer iteration 1042, batch 260\n",
      "\n",
      " Learning rate 0.0004461337949196036, Model learning rate 0.0004461337812244892\n",
      "\n",
      " Optimizer iteration 1043, batch 261\n",
      "\n",
      " Learning rate 0.0004453330168760451, Model learning rate 0.00044533301843330264\n",
      "262/391 [===================>..........] - ETA: 9s - loss: 1.1301 - acc: 0.6514\n",
      " Optimizer iteration 1044, batch 262\n",
      "\n",
      " Learning rate 0.00044453238072371116, Model learning rate 0.0004445323720574379\n",
      "263/391 [===================>..........] - ETA: 9s - loss: 1.1300 - acc: 0.6515\n",
      " Optimizer iteration 1045, batch 263\n",
      "\n",
      " Learning rate 0.0004437318885406973, Model learning rate 0.0004437319003045559\n",
      "264/391 [===================>..........] - ETA: 9s - loss: 1.1297 - acc: 0.6517\n",
      " Optimizer iteration 1046, batch 264\n",
      "\n",
      " Learning rate 0.0004429315424047263, Model learning rate 0.0004429315449669957\n",
      "265/391 [===================>..........] - ETA: 9s - loss: 1.1293 - acc: 0.6517\n",
      " Optimizer iteration 1047, batch 265\n",
      "\n",
      " Learning rate 0.0004421313443931416, Model learning rate 0.0004421313351485878\n",
      "\n",
      " Optimizer iteration 1048, batch 266\n",
      "\n",
      " Learning rate 0.00044133129658290195, Model learning rate 0.00044133129995316267\n",
      "267/391 [===================>..........] - ETA: 9s - loss: 1.1285 - acc: 0.6519\n",
      " Optimizer iteration 1049, batch 267\n",
      "\n",
      " Learning rate 0.00044053140105057635, Model learning rate 0.0004405314102768898\n",
      "268/391 [===================>..........] - ETA: 9s - loss: 1.1282 - acc: 0.6521\n",
      " Optimizer iteration 1050, batch 268\n",
      "\n",
      " Learning rate 0.00043973165987233853, Model learning rate 0.0004397316661197692\n",
      "269/391 [===================>..........] - ETA: 8s - loss: 1.1280 - acc: 0.6521\n",
      " Optimizer iteration 1051, batch 269\n",
      "\n",
      " Learning rate 0.0004389320751239617, Model learning rate 0.0004389320674818009\n",
      "270/391 [===================>..........] - ETA: 8s - loss: 1.1281 - acc: 0.6520\n",
      " Optimizer iteration 1052, batch 270\n",
      "\n",
      " Learning rate 0.00043813264888081284, Model learning rate 0.00043813264346681535\n",
      "271/391 [===================>..........] - ETA: 8s - loss: 1.1285 - acc: 0.6518\n",
      " Optimizer iteration 1053, batch 271\n",
      "\n",
      " Learning rate 0.00043733338321784784, Model learning rate 0.00043733339407481253\n",
      "272/391 [===================>..........] - ETA: 8s - loss: 1.1282 - acc: 0.6519\n",
      " Optimizer iteration 1054, batch 272\n",
      "\n",
      " Learning rate 0.0004365342802096057, Model learning rate 0.000436534290201962\n",
      "\n",
      " Optimizer iteration 1055, batch 273\n",
      "\n",
      " Learning rate 0.00043573534193020277, Model learning rate 0.00043573533184826374\n",
      "274/391 [====================>.........] - ETA: 8s - loss: 1.1276 - acc: 0.6521\n",
      " Optimizer iteration 1056, batch 274\n",
      "\n",
      " Learning rate 0.0004349365704533284, Model learning rate 0.0004349365772213787\n",
      "275/391 [====================>.........] - ETA: 8s - loss: 1.1279 - acc: 0.6518\n",
      " Optimizer iteration 1057, batch 275\n",
      "\n",
      " Learning rate 0.0004341379678522389, Model learning rate 0.0004341379681136459\n",
      "276/391 [====================>.........] - ETA: 8s - loss: 1.1277 - acc: 0.6518\n",
      " Optimizer iteration 1058, batch 276\n",
      "\n",
      " Learning rate 0.00043333953619975206, Model learning rate 0.0004333395336288959\n",
      "277/391 [====================>.........] - ETA: 8s - loss: 1.1272 - acc: 0.6521\n",
      " Optimizer iteration 1059, batch 277\n",
      "\n",
      " Learning rate 0.00043254127756824214, Model learning rate 0.0004325412737671286\n",
      "\n",
      " Optimizer iteration 1060, batch 278\n",
      "\n",
      " Learning rate 0.0004317431940296343, Model learning rate 0.00043174318852834404\n",
      "279/391 [====================>.........] - ETA: 8s - loss: 1.1266 - acc: 0.6522\n",
      " Optimizer iteration 1061, batch 279\n",
      "\n",
      " Learning rate 0.000430945287655399, Model learning rate 0.0004309452779125422\n",
      "280/391 [====================>.........] - ETA: 8s - loss: 1.1262 - acc: 0.6522\n",
      " Optimizer iteration 1062, batch 280\n",
      "\n",
      " Learning rate 0.00043014756051654705, Model learning rate 0.0004301475710235536\n",
      "281/391 [====================>.........] - ETA: 8s - loss: 1.1256 - acc: 0.6525\n",
      " Optimizer iteration 1063, batch 281\n",
      "\n",
      " Learning rate 0.00042935001468362405, Model learning rate 0.0004293500096537173\n",
      "282/391 [====================>.........] - ETA: 7s - loss: 1.1254 - acc: 0.6524\n",
      " Optimizer iteration 1064, batch 282\n",
      "\n",
      " Learning rate 0.00042855265222670517, Model learning rate 0.00042855265201069415\n",
      "283/391 [====================>.........] - ETA: 7s - loss: 1.1249 - acc: 0.6526\n",
      " Optimizer iteration 1065, batch 283\n",
      "\n",
      " Learning rate 0.0004277554752153895, Model learning rate 0.00042775546899065375\n",
      "284/391 [====================>.........] - ETA: 7s - loss: 1.1240 - acc: 0.6531\n",
      " Optimizer iteration 1066, batch 284\n",
      "\n",
      " Learning rate 0.00042695848571879425, Model learning rate 0.00042695848969742656\n",
      "285/391 [====================>.........] - ETA: 7s - loss: 1.1239 - acc: 0.6531\n",
      " Optimizer iteration 1067, batch 285\n",
      "\n",
      " Learning rate 0.0004261616858055508, Model learning rate 0.0004261616850271821\n",
      "286/391 [====================>.........] - ETA: 7s - loss: 1.1238 - acc: 0.6533\n",
      " Optimizer iteration 1068, batch 286\n",
      "\n",
      " Learning rate 0.000425365077543798, Model learning rate 0.00042536508408375084\n",
      "287/391 [=====================>........] - ETA: 7s - loss: 1.1234 - acc: 0.6535\n",
      " Optimizer iteration 1069, batch 287\n",
      "\n",
      " Learning rate 0.00042456866300117724, Model learning rate 0.0004245686577633023\n",
      "288/391 [=====================>........] - ETA: 7s - loss: 1.1231 - acc: 0.6538\n",
      " Optimizer iteration 1070, batch 288\n",
      "\n",
      " Learning rate 0.0004237724442448273, Model learning rate 0.000423772435169667\n",
      "289/391 [=====================>........] - ETA: 7s - loss: 1.1228 - acc: 0.6540\n",
      " Optimizer iteration 1071, batch 289\n",
      "\n",
      " Learning rate 0.00042297642334137875, Model learning rate 0.0004229764163028449\n",
      "290/391 [=====================>........] - ETA: 7s - loss: 1.1221 - acc: 0.6542\n",
      " Optimizer iteration 1072, batch 290\n",
      "\n",
      " Learning rate 0.00042218060235694826, Model learning rate 0.00042218060116283596\n",
      "291/391 [=====================>........] - ETA: 7s - loss: 1.1224 - acc: 0.6541\n",
      " Optimizer iteration 1073, batch 291\n",
      "\n",
      " Learning rate 0.000421384983357134, Model learning rate 0.0004213849897496402\n",
      "292/391 [=====================>........] - ETA: 7s - loss: 1.1228 - acc: 0.6539\n",
      " Optimizer iteration 1074, batch 292\n",
      "\n",
      " Learning rate 0.0004205895684070098, Model learning rate 0.0004205895820632577\n",
      "293/391 [=====================>........] - ETA: 7s - loss: 1.1228 - acc: 0.6538\n",
      " Optimizer iteration 1075, batch 293\n",
      "\n",
      " Learning rate 0.0004197943595711198, Model learning rate 0.0004197943489998579\n",
      "\n",
      " Optimizer iteration 1076, batch 294\n",
      "\n",
      " Learning rate 0.0004189993589134735, Model learning rate 0.00041899934876710176\n",
      "295/391 [=====================>........] - ETA: 7s - loss: 1.1234 - acc: 0.6534\n",
      " Optimizer iteration 1077, batch 295\n",
      "\n",
      " Learning rate 0.0004182045684975391, Model learning rate 0.0004182045813649893\n",
      "296/391 [=====================>........] - ETA: 6s - loss: 1.1235 - acc: 0.6534\n",
      " Optimizer iteration 1078, batch 296\n",
      "\n",
      " Learning rate 0.0004174099903862403, Model learning rate 0.00041740998858585954\n",
      "297/391 [=====================>........] - ETA: 6s - loss: 1.1234 - acc: 0.6534\n",
      " Optimizer iteration 1079, batch 297\n",
      "\n",
      " Learning rate 0.0004166156266419489, Model learning rate 0.00041661562863737345\n",
      "298/391 [=====================>........] - ETA: 6s - loss: 1.1230 - acc: 0.6537\n",
      " Optimizer iteration 1080, batch 298\n",
      "\n",
      " Learning rate 0.0004158214793264807, Model learning rate 0.00041582147241570055\n",
      "299/391 [=====================>........] - ETA: 6s - loss: 1.1223 - acc: 0.6540\n",
      " Optimizer iteration 1081, batch 299\n",
      "\n",
      " Learning rate 0.00041502755050108975, Model learning rate 0.0004150275490246713\n",
      "300/391 [======================>.......] - ETA: 6s - loss: 1.1226 - acc: 0.6538\n",
      " Optimizer iteration 1082, batch 300\n",
      "\n",
      " Learning rate 0.00041423384222646285, Model learning rate 0.0004142338293604553\n",
      "301/391 [======================>.......] - ETA: 6s - loss: 1.1224 - acc: 0.6538\n",
      " Optimizer iteration 1083, batch 301\n",
      "\n",
      " Learning rate 0.00041344035656271436, Model learning rate 0.0004134403425268829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302/391 [======================>.......] - ETA: 6s - loss: 1.1224 - acc: 0.6538\n",
      " Optimizer iteration 1084, batch 302\n",
      "\n",
      " Learning rate 0.0004126470955693806, Model learning rate 0.00041264708852395415\n",
      "\n",
      " Optimizer iteration 1085, batch 303\n",
      "\n",
      " Learning rate 0.0004118540613054155, Model learning rate 0.0004118540673516691\n",
      "304/391 [======================>.......] - ETA: 6s - loss: 1.1221 - acc: 0.6542\n",
      " Optimizer iteration 1086, batch 304\n",
      "\n",
      " Learning rate 0.00041106125582918385, Model learning rate 0.0004110612499061972\n",
      "305/391 [======================>.......] - ETA: 6s - loss: 1.1219 - acc: 0.6543\n",
      " Optimizer iteration 1087, batch 305\n",
      "\n",
      " Learning rate 0.0004102686811984568, Model learning rate 0.0004102686943951994\n",
      "306/391 [======================>.......] - ETA: 6s - loss: 1.1221 - acc: 0.6543\n",
      " Optimizer iteration 1088, batch 306\n",
      "\n",
      " Learning rate 0.00040947633947040614, Model learning rate 0.00040947634261101484\n",
      "\n",
      " Optimizer iteration 1089, batch 307\n",
      "\n",
      " Learning rate 0.00040868423270159945, Model learning rate 0.0004086842236574739\n",
      "308/391 [======================>.......] - ETA: 6s - loss: 1.1213 - acc: 0.6547\n",
      " Optimizer iteration 1090, batch 308\n",
      "\n",
      " Learning rate 0.0004078923629479943, Model learning rate 0.0004078923666384071\n",
      "309/391 [======================>.......] - ETA: 6s - loss: 1.1212 - acc: 0.6546\n",
      " Optimizer iteration 1091, batch 309\n",
      "\n",
      " Learning rate 0.00040710073226493307, Model learning rate 0.00040710074244998395\n",
      "\n",
      " Optimizer iteration 1092, batch 310\n",
      "\n",
      " Learning rate 0.0004063093427071376, Model learning rate 0.00040630935109220445\n",
      "311/391 [======================>.......] - ETA: 5s - loss: 1.1218 - acc: 0.6542\n",
      " Optimizer iteration 1093, batch 311\n",
      "\n",
      " Learning rate 0.00040551819632870433, Model learning rate 0.0004055181925650686\n",
      "312/391 [======================>.......] - ETA: 5s - loss: 1.1215 - acc: 0.6542\n",
      " Optimizer iteration 1094, batch 312\n",
      "\n",
      " Learning rate 0.0004047272951830976, Model learning rate 0.00040472729597240686\n",
      "313/391 [=======================>......] - ETA: 5s - loss: 1.1216 - acc: 0.6543\n",
      " Optimizer iteration 1095, batch 313\n",
      "\n",
      " Learning rate 0.00040393664132314577, Model learning rate 0.0004039366322103888\n",
      "\n",
      " Optimizer iteration 1096, batch 314\n",
      "\n",
      " Learning rate 0.0004031462368010357, Model learning rate 0.0004031462303828448\n",
      "315/391 [=======================>......] - ETA: 5s - loss: 1.1211 - acc: 0.6545\n",
      " Optimizer iteration 1097, batch 315\n",
      "\n",
      " Learning rate 0.0004023560836683065, Model learning rate 0.00040235609048977494\n",
      "316/391 [=======================>......] - ETA: 5s - loss: 1.1212 - acc: 0.6546\n",
      " Optimizer iteration 1098, batch 316\n",
      "\n",
      " Learning rate 0.000401566183975845, Model learning rate 0.00040156618342734873\n",
      "317/391 [=======================>......] - ETA: 5s - loss: 1.1212 - acc: 0.6545\n",
      " Optimizer iteration 1099, batch 317\n",
      "\n",
      " Learning rate 0.00040077653977388015, Model learning rate 0.00040077653829939663\n",
      "\n",
      " Optimizer iteration 1100, batch 318\n",
      "\n",
      " Learning rate 0.0003999871531119779, Model learning rate 0.00039998715510591865\n",
      "319/391 [=======================>......] - ETA: 5s - loss: 1.1217 - acc: 0.6544\n",
      " Optimizer iteration 1101, batch 319\n",
      "\n",
      " Learning rate 0.00039919802603903553, Model learning rate 0.00039919803384691477\n",
      "320/391 [=======================>......] - ETA: 5s - loss: 1.1217 - acc: 0.6544\n",
      " Optimizer iteration 1102, batch 320\n",
      "\n",
      " Learning rate 0.0003984091606032768, Model learning rate 0.000398409174522385\n",
      "321/391 [=======================>......] - ETA: 5s - loss: 1.1219 - acc: 0.6544\n",
      " Optimizer iteration 1103, batch 321\n",
      "\n",
      " Learning rate 0.0003976205588522461, Model learning rate 0.0003976205480284989\n",
      "\n",
      " Optimizer iteration 1104, batch 322\n",
      "\n",
      " Learning rate 0.0003968322228328041, Model learning rate 0.00039683221257291734\n",
      "323/391 [=======================>......] - ETA: 4s - loss: 1.1213 - acc: 0.6546\n",
      " Optimizer iteration 1105, batch 323\n",
      "\n",
      " Learning rate 0.0003960441545911204, Model learning rate 0.00039604416815564036\n",
      "324/391 [=======================>......] - ETA: 4s - loss: 1.1208 - acc: 0.6547\n",
      " Optimizer iteration 1106, batch 324\n",
      "\n",
      " Learning rate 0.00039525635617267075, Model learning rate 0.00039525635656900704\n",
      "325/391 [=======================>......] - ETA: 4s - loss: 1.1205 - acc: 0.6548\n",
      " Optimizer iteration 1107, batch 325\n",
      "\n",
      " Learning rate 0.00039446882962223027, Model learning rate 0.0003944688360206783\n",
      "326/391 [========================>.....] - ETA: 4s - loss: 1.1202 - acc: 0.6549\n",
      " Optimizer iteration 1108, batch 326\n",
      "\n",
      " Learning rate 0.0003936815769838682, Model learning rate 0.00039368157740682364\n",
      "327/391 [========================>.....] - ETA: 4s - loss: 1.1205 - acc: 0.6548\n",
      " Optimizer iteration 1109, batch 327\n",
      "\n",
      " Learning rate 0.0003928946003009431, Model learning rate 0.00039289460983127356\n",
      "328/391 [========================>.....] - ETA: 4s - loss: 1.1204 - acc: 0.6550\n",
      " Optimizer iteration 1110, batch 328\n",
      "\n",
      " Learning rate 0.000392107901616097, Model learning rate 0.0003921079041901976\n",
      "329/391 [========================>.....] - ETA: 4s - loss: 1.1203 - acc: 0.6549\n",
      " Optimizer iteration 1111, batch 329\n",
      "\n",
      " Learning rate 0.00039132148297125053, Model learning rate 0.0003913214895874262\n",
      "330/391 [========================>.....] - ETA: 4s - loss: 1.1200 - acc: 0.6549\n",
      " Optimizer iteration 1112, batch 330\n",
      "\n",
      " Learning rate 0.0003905353464075975, Model learning rate 0.0003905353369191289\n",
      "331/391 [========================>.....] - ETA: 4s - loss: 1.1199 - acc: 0.6550\n",
      " Optimizer iteration 1113, batch 331\n",
      "\n",
      " Learning rate 0.0003897494939655995, Model learning rate 0.00038974950439296663\n",
      "332/391 [========================>.....] - ETA: 4s - loss: 1.1198 - acc: 0.6550\n",
      " Optimizer iteration 1114, batch 332\n",
      "\n",
      " Learning rate 0.00038896392768498074, Model learning rate 0.00038896393380127847\n",
      "333/391 [========================>.....] - ETA: 4s - loss: 1.1199 - acc: 0.6549\n",
      " Optimizer iteration 1115, batch 333\n",
      "\n",
      " Learning rate 0.00038817864960472237, Model learning rate 0.0003881786542478949\n",
      "\n",
      " Optimizer iteration 1116, batch 334\n",
      "\n",
      " Learning rate 0.00038739366176305785, Model learning rate 0.00038739366573281586\n",
      "335/391 [========================>.....] - ETA: 4s - loss: 1.1192 - acc: 0.6551\n",
      " Optimizer iteration 1117, batch 335\n",
      "\n",
      " Learning rate 0.00038660896619746734, Model learning rate 0.0003866089682560414\n",
      "336/391 [========================>.....] - ETA: 4s - loss: 1.1190 - acc: 0.6553\n",
      " Optimizer iteration 1118, batch 336\n",
      "\n",
      " Learning rate 0.0003858245649446721, Model learning rate 0.0003858245618175715\n",
      "337/391 [========================>.....] - ETA: 3s - loss: 1.1189 - acc: 0.6554\n",
      " Optimizer iteration 1119, batch 337\n",
      "\n",
      " Learning rate 0.00038504046004062974, Model learning rate 0.0003850404464174062\n",
      "\n",
      " Optimizer iteration 1120, batch 338\n",
      "\n",
      " Learning rate 0.0003842566535205286, Model learning rate 0.0003842566511593759\n",
      "339/391 [=========================>....] - ETA: 3s - loss: 1.1186 - acc: 0.6556\n",
      " Optimizer iteration 1121, batch 339\n",
      "\n",
      " Learning rate 0.00038347314741878227, Model learning rate 0.0003834731469396502\n",
      "340/391 [=========================>....] - ETA: 3s - loss: 1.1179 - acc: 0.6557\n",
      " Optimizer iteration 1122, batch 340\n",
      "\n",
      " Learning rate 0.000382689943769025, Model learning rate 0.000382689933758229\n",
      "341/391 [=========================>....] - ETA: 3s - loss: 1.1181 - acc: 0.6556\n",
      " Optimizer iteration 1123, batch 341\n",
      "\n",
      " Learning rate 0.00038190704460410585, Model learning rate 0.0003819070407189429\n",
      "\n",
      " Optimizer iteration 1124, batch 342\n",
      "\n",
      " Learning rate 0.00038112445195608334, Model learning rate 0.0003811244387179613\n",
      "343/391 [=========================>....] - ETA: 3s - loss: 1.1178 - acc: 0.6554\n",
      " Optimizer iteration 1125, batch 343\n",
      "\n",
      " Learning rate 0.00038034216785622126, Model learning rate 0.00038034215685911477\n",
      "\n",
      " Optimizer iteration 1126, batch 344\n",
      "\n",
      " Learning rate 0.00037956019433498124, Model learning rate 0.00037956019514240324\n",
      "345/391 [=========================>....] - ETA: 3s - loss: 1.1182 - acc: 0.6553\n",
      " Optimizer iteration 1127, batch 345\n",
      "\n",
      " Learning rate 0.0003787785334220196, Model learning rate 0.0003787785244639963\n",
      "\n",
      " Optimizer iteration 1128, batch 346\n",
      "\n",
      " Learning rate 0.00037799718714618124, Model learning rate 0.00037799717392772436\n",
      "347/391 [=========================>....] - ETA: 3s - loss: 1.1173 - acc: 0.6556\n",
      " Optimizer iteration 1129, batch 347\n",
      "\n",
      " Learning rate 0.00037721615753549446, Model learning rate 0.00037721614353358746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/391 [=========================>....] - ETA: 3s - loss: 1.1172 - acc: 0.6557\n",
      " Optimizer iteration 1130, batch 348\n",
      "\n",
      " Learning rate 0.0003764354466171652, Model learning rate 0.0003764354332815856\n",
      "349/391 [=========================>....] - ETA: 3s - loss: 1.1170 - acc: 0.6556\n",
      " Optimizer iteration 1131, batch 349\n",
      "\n",
      " Learning rate 0.0003756550564175727, Model learning rate 0.0003756550431717187\n",
      "\n",
      " Optimizer iteration 1132, batch 350\n",
      "\n",
      " Learning rate 0.00037487498896226335, Model learning rate 0.00037487500230781734\n",
      "351/391 [=========================>....] - ETA: 2s - loss: 1.1163 - acc: 0.6559\n",
      " Optimizer iteration 1133, batch 351\n",
      "\n",
      " Learning rate 0.00037409524627594605, Model learning rate 0.00037409525248222053\n",
      "352/391 [==========================>...] - ETA: 2s - loss: 1.1165 - acc: 0.6559\n",
      " Optimizer iteration 1134, batch 352\n",
      "\n",
      " Learning rate 0.0003733158303824868, Model learning rate 0.00037331582279875875\n",
      "353/391 [==========================>...] - ETA: 2s - loss: 1.1157 - acc: 0.6562\n",
      " Optimizer iteration 1135, batch 353\n",
      "\n",
      " Learning rate 0.0003725367433049033, Model learning rate 0.00037253674236126244\n",
      "354/391 [==========================>...] - ETA: 2s - loss: 1.1153 - acc: 0.6564\n",
      " Optimizer iteration 1136, batch 354\n",
      "\n",
      " Learning rate 0.0003717579870653601, Model learning rate 0.00037175798206590116\n",
      "355/391 [==========================>...] - ETA: 2s - loss: 1.1153 - acc: 0.6564\n",
      " Optimizer iteration 1137, batch 355\n",
      "\n",
      " Learning rate 0.0003709795636851622, Model learning rate 0.00037097957101650536\n",
      "\n",
      " Optimizer iteration 1138, batch 356\n",
      "\n",
      " Learning rate 0.00037020147518475137, Model learning rate 0.0003702014801092446\n",
      "357/391 [==========================>...] - ETA: 2s - loss: 1.1154 - acc: 0.6562\n",
      " Optimizer iteration 1139, batch 357\n",
      "\n",
      " Learning rate 0.00036942372358370024, Model learning rate 0.00036942370934411883\n",
      "358/391 [==========================>...] - ETA: 2s - loss: 1.1153 - acc: 0.6562\n",
      " Optimizer iteration 1140, batch 358\n",
      "\n",
      " Learning rate 0.0003686463109007065, Model learning rate 0.000368646316928789\n",
      "359/391 [==========================>...] - ETA: 2s - loss: 1.1152 - acc: 0.6562\n",
      " Optimizer iteration 1141, batch 359\n",
      "\n",
      " Learning rate 0.0003678692391535886, Model learning rate 0.00036786924465559423\n",
      "\n",
      " Optimizer iteration 1142, batch 360\n",
      "\n",
      " Learning rate 0.00036709251035927997, Model learning rate 0.0003670925216283649\n",
      "361/391 [==========================>...] - ETA: 2s - loss: 1.1143 - acc: 0.6564\n",
      " Optimizer iteration 1143, batch 361\n",
      "\n",
      " Learning rate 0.0003663161265338235, Model learning rate 0.00036631611874327064\n",
      "\n",
      " Optimizer iteration 1144, batch 362\n",
      "\n",
      " Learning rate 0.00036554008969236717, Model learning rate 0.0003655400942079723\n",
      "363/391 [==========================>...] - ETA: 2s - loss: 1.1147 - acc: 0.6561\n",
      " Optimizer iteration 1145, batch 363\n",
      "\n",
      " Learning rate 0.0003647644018491582, Model learning rate 0.00036476438981480896\n",
      "\n",
      " Optimizer iteration 1146, batch 364\n",
      "\n",
      " Learning rate 0.00036398906501753785, Model learning rate 0.0003639890637714416\n",
      "365/391 [===========================>..] - ETA: 1s - loss: 1.1146 - acc: 0.6562\n",
      " Optimizer iteration 1147, batch 365\n",
      "\n",
      " Learning rate 0.0003632140812099368, Model learning rate 0.0003632140869740397\n",
      "366/391 [===========================>..] - ETA: 1s - loss: 1.1143 - acc: 0.6563\n",
      " Optimizer iteration 1148, batch 366\n",
      "\n",
      " Learning rate 0.00036243945243786836, Model learning rate 0.00036243945942260325\n",
      "367/391 [===========================>..] - ETA: 1s - loss: 1.1143 - acc: 0.6564\n",
      " Optimizer iteration 1149, batch 367\n",
      "\n",
      " Learning rate 0.00036166518071192546, Model learning rate 0.0003616651811171323\n",
      "368/391 [===========================>..] - ETA: 1s - loss: 1.1144 - acc: 0.6563\n",
      " Optimizer iteration 1150, batch 368\n",
      "\n",
      " Learning rate 0.0003608912680417737, Model learning rate 0.0003608912811614573\n",
      "369/391 [===========================>..] - ETA: 1s - loss: 1.1143 - acc: 0.6563\n",
      " Optimizer iteration 1151, batch 369\n",
      "\n",
      " Learning rate 0.0003601177164361469, Model learning rate 0.0003601177304517478\n",
      "\n",
      " Optimizer iteration 1152, batch 370\n",
      "\n",
      " Learning rate 0.00035934452790284177, Model learning rate 0.00035934452898800373\n",
      "371/391 [===========================>..] - ETA: 1s - loss: 1.1145 - acc: 0.6560\n",
      " Optimizer iteration 1153, batch 371\n",
      "\n",
      " Learning rate 0.00035857170444871254, Model learning rate 0.0003585717058740556\n",
      "372/391 [===========================>..] - ETA: 1s - loss: 1.1149 - acc: 0.6558\n",
      " Optimizer iteration 1154, batch 372\n",
      "\n",
      " Learning rate 0.0003577992480796658, Model learning rate 0.00035779926110990345\n",
      "373/391 [===========================>..] - ETA: 1s - loss: 1.1147 - acc: 0.6559\n",
      " Optimizer iteration 1155, batch 373\n",
      "\n",
      " Learning rate 0.00035702716080065545, Model learning rate 0.00035702716559171677\n",
      "\n",
      " Optimizer iteration 1156, batch 374\n",
      "\n",
      " Learning rate 0.00035625544461567727, Model learning rate 0.000356255448423326\n",
      "375/391 [===========================>..] - ETA: 1s - loss: 1.1144 - acc: 0.6561\n",
      " Optimizer iteration 1157, batch 375\n",
      "\n",
      " Learning rate 0.0003554841015277641, Model learning rate 0.0003554841096047312\n",
      "\n",
      " Optimizer iteration 1158, batch 376\n",
      "\n",
      " Learning rate 0.00035471313353898054, Model learning rate 0.00035471312003210187\n",
      "377/391 [===========================>..] - ETA: 1s - loss: 1.1141 - acc: 0.6563\n",
      " Optimizer iteration 1159, batch 377\n",
      "\n",
      " Learning rate 0.00035394254265041657, Model learning rate 0.00035394253791309893\n",
      "\n",
      " Optimizer iteration 1160, batch 378\n",
      "\n",
      " Learning rate 0.0003531723308621847, Model learning rate 0.00035317233414389193\n",
      "379/391 [============================>.] - ETA: 0s - loss: 1.1138 - acc: 0.6565\n",
      " Optimizer iteration 1161, batch 379\n",
      "\n",
      " Learning rate 0.0003524025001734126, Model learning rate 0.00035240250872448087\n",
      "\n",
      " Optimizer iteration 1162, batch 380\n",
      "\n",
      " Learning rate 0.0003516330525822391, Model learning rate 0.00035163306165486574\n",
      "381/391 [============================>.] - ETA: 0s - loss: 1.1138 - acc: 0.6565\n",
      " Optimizer iteration 1163, batch 381\n",
      "\n",
      " Learning rate 0.00035086399008580884, Model learning rate 0.00035086399293504655\n",
      "382/391 [============================>.] - ETA: 0s - loss: 1.1136 - acc: 0.6566\n",
      " Optimizer iteration 1164, batch 382\n",
      "\n",
      " Learning rate 0.00035009531468026647, Model learning rate 0.0003500953025650233\n",
      "383/391 [============================>.] - ETA: 0s - loss: 1.1136 - acc: 0.6567\n",
      " Optimizer iteration 1165, batch 383\n",
      "\n",
      " Learning rate 0.00034932702836075214, Model learning rate 0.00034932701964862645\n",
      "384/391 [============================>.] - ETA: 0s - loss: 1.1138 - acc: 0.6565\n",
      " Optimizer iteration 1166, batch 384\n",
      "\n",
      " Learning rate 0.0003485591331213962, Model learning rate 0.000348559144185856\n",
      "385/391 [============================>.] - ETA: 0s - loss: 1.1137 - acc: 0.6566\n",
      " Optimizer iteration 1167, batch 385\n",
      "\n",
      " Learning rate 0.00034779163095531384, Model learning rate 0.000347791617969051\n",
      "386/391 [============================>.] - ETA: 0s - loss: 1.1135 - acc: 0.6567\n",
      " Optimizer iteration 1168, batch 386\n",
      "\n",
      " Learning rate 0.00034702452385460014, Model learning rate 0.0003470245283097029\n",
      "387/391 [============================>.] - ETA: 0s - loss: 1.1131 - acc: 0.6569\n",
      " Optimizer iteration 1169, batch 387\n",
      "\n",
      " Learning rate 0.00034625781381032484, Model learning rate 0.0003462578170001507\n",
      "\n",
      " Optimizer iteration 1170, batch 388\n",
      "\n",
      " Learning rate 0.00034549150281252633, Model learning rate 0.0003454915131442249\n",
      "389/391 [============================>.] - ETA: 0s - loss: 1.1123 - acc: 0.6573\n",
      " Optimizer iteration 1171, batch 389\n",
      "\n",
      " Learning rate 0.00034472559285020826, Model learning rate 0.000344725587638095\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.1121 - acc: 0.6574\n",
      " Optimizer iteration 1172, batch 390\n",
      "\n",
      " Learning rate 0.0003439600859113329, Model learning rate 0.000343960098689422\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 1.1115 - acc: 0.6576 - val_loss: 1.4236 - val_acc: 0.5738\n",
      "\n",
      "Epoch 00003: saving model to /home/ubuntu/Projects/hybrid-ensemble/model/run_200/cifar10_ResNet20v1_model-0003.h5\n",
      "Epoch 4/5\n",
      "\n",
      " Optimizer iteration 1173, batch 0\n",
      "\n",
      " Learning rate 0.00034319498398281635, Model learning rate 0.00034319498809054494\n",
      "  1/391 [..............................] - ETA: 15s - loss: 0.8878 - acc: 0.7578\n",
      " Optimizer iteration 1174, batch 1\n",
      "\n",
      " Learning rate 0.00034243028905052387, Model learning rate 0.00034243028494529426\n",
      "\n",
      " Optimizer iteration 1175, batch 2\n",
      "\n",
      " Learning rate 0.00034166600309926387, Model learning rate 0.00034166598925367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/391 [..............................] - ETA: 16s - loss: 1.0563 - acc: 0.6979\n",
      " Optimizer iteration 1176, batch 3\n",
      "\n",
      " Learning rate 0.0003409021281127835, Model learning rate 0.00034090213011950254\n",
      "  4/391 [..............................] - ETA: 17s - loss: 1.0520 - acc: 0.6875\n",
      " Optimizer iteration 1177, batch 4\n",
      "\n",
      " Learning rate 0.00034013866607376307, Model learning rate 0.0003401386784389615\n",
      "\n",
      " Optimizer iteration 1178, batch 5\n",
      "\n",
      " Learning rate 0.00033937561896381146, Model learning rate 0.0003393756051082164\n",
      "  6/391 [..............................] - ETA: 17s - loss: 1.0165 - acc: 0.6901\n",
      " Optimizer iteration 1179, batch 6\n",
      "\n",
      " Learning rate 0.0003386129887634601, Model learning rate 0.0003386129974387586\n",
      "  7/391 [..............................] - ETA: 18s - loss: 1.0468 - acc: 0.6763\n",
      " Optimizer iteration 1180, batch 7\n",
      "\n",
      " Learning rate 0.0003378507774521587, Model learning rate 0.00033785076811909676\n",
      "  8/391 [..............................] - ETA: 18s - loss: 1.0429 - acc: 0.6787\n",
      " Optimizer iteration 1181, batch 8\n",
      "\n",
      " Learning rate 0.0003370889870082692, Model learning rate 0.00033708897535689175\n",
      "\n",
      " Optimizer iteration 1182, batch 9\n",
      "\n",
      " Learning rate 0.0003363276194090617, Model learning rate 0.0003363276191521436\n",
      " 10/391 [..............................] - ETA: 17s - loss: 1.0264 - acc: 0.6875\n",
      " Optimizer iteration 1183, batch 10\n",
      "\n",
      " Learning rate 0.00033556667663070836, Model learning rate 0.00033556667040102184\n",
      " 11/391 [..............................] - ETA: 17s - loss: 1.0189 - acc: 0.6932\n",
      " Optimizer iteration 1184, batch 11\n",
      "\n",
      " Learning rate 0.0003348061606482791, Model learning rate 0.00033480615820735693\n",
      "\n",
      " Optimizer iteration 1185, batch 12\n",
      "\n",
      " Learning rate 0.0003340460734357359, Model learning rate 0.00033404608257114887\n",
      " 13/391 [..............................] - ETA: 17s - loss: 1.0351 - acc: 0.6857\n",
      " Optimizer iteration 1186, batch 13\n",
      "\n",
      " Learning rate 0.0003332864169659275, Model learning rate 0.0003332864143885672\n",
      "\n",
      " Optimizer iteration 1187, batch 14\n",
      "\n",
      " Learning rate 0.0003325271932105851, Model learning rate 0.0003325271827634424\n",
      " 15/391 [>.............................] - ETA: 17s - loss: 1.0253 - acc: 0.6906\n",
      " Optimizer iteration 1188, batch 15\n",
      "\n",
      " Learning rate 0.0003317684041403165, Model learning rate 0.0003317684167996049\n",
      " 16/391 [>.............................] - ETA: 17s - loss: 1.0199 - acc: 0.6973\n",
      " Optimizer iteration 1189, batch 16\n",
      "\n",
      " Learning rate 0.00033101005172460156, Model learning rate 0.0003310100582893938\n",
      " 17/391 [>.............................] - ETA: 17s - loss: 1.0315 - acc: 0.6921\n",
      " Optimizer iteration 1190, batch 17\n",
      "\n",
      " Learning rate 0.00033025213793178644, Model learning rate 0.0003302521363366395\n",
      " 18/391 [>.............................] - ETA: 17s - loss: 1.0230 - acc: 0.6966\n",
      " Optimizer iteration 1191, batch 18\n",
      "\n",
      " Learning rate 0.00032949466472907896, Model learning rate 0.0003294946509413421\n",
      " 19/391 [>.............................] - ETA: 17s - loss: 1.0300 - acc: 0.6933\n",
      " Optimizer iteration 1192, batch 19\n",
      "\n",
      " Learning rate 0.0003287376340825432, Model learning rate 0.000328737631207332\n",
      " 20/391 [>.............................] - ETA: 18s - loss: 1.0268 - acc: 0.6926\n",
      " Optimizer iteration 1193, batch 20\n",
      "\n",
      " Learning rate 0.0003279810479570948, Model learning rate 0.00032798104803077877\n",
      " 21/391 [>.............................] - ETA: 19s - loss: 1.0347 - acc: 0.6916\n",
      " Optimizer iteration 1194, batch 21\n",
      "\n",
      " Learning rate 0.0003272249083164957, Model learning rate 0.00032722490141168237\n",
      " 22/391 [>.............................] - ETA: 19s - loss: 1.0300 - acc: 0.6914\n",
      " Optimizer iteration 1195, batch 22\n",
      "\n",
      " Learning rate 0.00032646921712334856, Model learning rate 0.0003264692204538733\n",
      " 23/391 [>.............................] - ETA: 19s - loss: 1.0301 - acc: 0.6916\n",
      " Optimizer iteration 1196, batch 23\n",
      "\n",
      " Learning rate 0.0003257139763390925, Model learning rate 0.00032571397605352104\n",
      " 24/391 [>.............................] - ETA: 19s - loss: 1.0281 - acc: 0.6924\n",
      " Optimizer iteration 1197, batch 24\n",
      "\n",
      " Learning rate 0.0003249591879239972, Model learning rate 0.0003249591973144561\n",
      " 25/391 [>.............................] - ETA: 20s - loss: 1.0244 - acc: 0.6931\n",
      " Optimizer iteration 1198, batch 25\n",
      "\n",
      " Learning rate 0.00032420485383715845, Model learning rate 0.000324204855132848\n",
      "\n",
      " Optimizer iteration 1199, batch 26\n",
      "\n",
      " Learning rate 0.00032345097603649265, Model learning rate 0.00032345097861252725\n",
      " 27/391 [=>............................] - ETA: 20s - loss: 1.0288 - acc: 0.6907\n",
      " Optimizer iteration 1200, batch 27\n",
      "\n",
      " Learning rate 0.00032269755647873217, Model learning rate 0.0003226975677534938\n",
      " 28/391 [=>............................] - ETA: 20s - loss: 1.0283 - acc: 0.6897\n",
      " Optimizer iteration 1201, batch 28\n",
      "\n",
      " Learning rate 0.00032194459711941967, Model learning rate 0.00032194459345191717\n",
      " 29/391 [=>............................] - ETA: 21s - loss: 1.0288 - acc: 0.6907\n",
      " Optimizer iteration 1202, batch 29\n",
      "\n",
      " Learning rate 0.00032119209991290346, Model learning rate 0.0003211921139154583\n",
      "\n",
      " Optimizer iteration 1203, batch 30\n",
      "\n",
      " Learning rate 0.0003204400668123322, Model learning rate 0.0003204400709364563\n",
      " 31/391 [=>............................] - ETA: 20s - loss: 1.0362 - acc: 0.6867\n",
      " Optimizer iteration 1204, batch 31\n",
      "\n",
      " Learning rate 0.00031968849976965014, Model learning rate 0.00031968849361874163\n",
      " 32/391 [=>............................] - ETA: 21s - loss: 1.0385 - acc: 0.6858\n",
      " Optimizer iteration 1205, batch 32\n",
      "\n",
      " Learning rate 0.00031893740073559167, Model learning rate 0.0003189374110661447\n",
      " 33/391 [=>............................] - ETA: 21s - loss: 1.0410 - acc: 0.6849\n",
      " Optimizer iteration 1206, batch 33\n",
      "\n",
      " Learning rate 0.00031818677165967646, Model learning rate 0.00031818676507100463\n",
      "\n",
      " Optimizer iteration 1207, batch 34\n",
      "\n",
      " Learning rate 0.0003174366144902048, Model learning rate 0.0003174366138409823\n",
      " 35/391 [=>............................] - ETA: 21s - loss: 1.0418 - acc: 0.6835\n",
      " Optimizer iteration 1208, batch 35\n",
      "\n",
      " Learning rate 0.00031668693117425126, Model learning rate 0.0003166869282722473\n",
      " 36/391 [=>............................] - ETA: 21s - loss: 1.0411 - acc: 0.6847\n",
      " Optimizer iteration 1209, batch 36\n",
      "\n",
      " Learning rate 0.00031593772365766105, Model learning rate 0.0003159377374686301\n",
      " 37/391 [=>............................] - ETA: 21s - loss: 1.0411 - acc: 0.6845\n",
      " Optimizer iteration 1210, batch 37\n",
      "\n",
      " Learning rate 0.0003151889938850445, Model learning rate 0.0003151889832224697\n",
      "\n",
      " Optimizer iteration 1211, batch 38\n",
      "\n",
      " Learning rate 0.00031444074379977186, Model learning rate 0.0003144407528452575\n",
      " 39/391 [=>............................] - ETA: 21s - loss: 1.0416 - acc: 0.6843\n",
      " Optimizer iteration 1212, batch 39\n",
      "\n",
      " Learning rate 0.00031369297534396826, Model learning rate 0.00031369298812933266\n",
      " 40/391 [==>...........................] - ETA: 22s - loss: 1.0414 - acc: 0.6850\n",
      " Optimizer iteration 1213, batch 40\n",
      "\n",
      " Learning rate 0.0003129456904585084, Model learning rate 0.0003129456890746951\n",
      " 41/391 [==>...........................] - ETA: 22s - loss: 1.0412 - acc: 0.6846\n",
      " Optimizer iteration 1214, batch 41\n",
      "\n",
      " Learning rate 0.00031219889108301236, Model learning rate 0.0003121988847851753\n",
      " 42/391 [==>...........................] - ETA: 21s - loss: 1.0408 - acc: 0.6845\n",
      " Optimizer iteration 1215, batch 42\n",
      "\n",
      " Learning rate 0.0003114525791558398, Model learning rate 0.0003114525752607733\n",
      " 43/391 [==>...........................] - ETA: 21s - loss: 1.0429 - acc: 0.6840\n",
      " Optimizer iteration 1216, batch 43\n",
      "\n",
      " Learning rate 0.0003107067566140853, Model learning rate 0.00031070676050148904\n",
      " 44/391 [==>...........................] - ETA: 22s - loss: 1.0415 - acc: 0.6838\n",
      " Optimizer iteration 1217, batch 44\n",
      "\n",
      " Learning rate 0.0003099614253935731, Model learning rate 0.0003099614114034921\n",
      " 45/391 [==>...........................] - ETA: 22s - loss: 1.0402 - acc: 0.6844\n",
      " Optimizer iteration 1218, batch 45\n",
      "\n",
      " Learning rate 0.0003092165874288525, Model learning rate 0.00030921658617444336\n",
      "\n",
      " Optimizer iteration 1219, batch 46\n",
      "\n",
      " Learning rate 0.0003084722446531918, Model learning rate 0.0003084722557105124\n",
      " 47/391 [==>...........................] - ETA: 21s - loss: 1.0369 - acc: 0.6855\n",
      " Optimizer iteration 1220, batch 47\n",
      "\n",
      " Learning rate 0.00030772839899857464, Model learning rate 0.00030772839090786874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48/391 [==>...........................] - ETA: 22s - loss: 1.0365 - acc: 0.6865\n",
      " Optimizer iteration 1221, batch 48\n",
      "\n",
      " Learning rate 0.00030698505239569424, Model learning rate 0.0003069850499741733\n",
      " 49/391 [==>...........................] - ETA: 22s - loss: 1.0368 - acc: 0.6873\n",
      " Optimizer iteration 1222, batch 49\n",
      "\n",
      " Learning rate 0.0003062422067739485, Model learning rate 0.00030624220380559564\n",
      "\n",
      " Optimizer iteration 1223, batch 50\n",
      "\n",
      " Learning rate 0.00030549986406143496, Model learning rate 0.00030549985240213573\n",
      " 51/391 [==>...........................] - ETA: 22s - loss: 1.0393 - acc: 0.6869\n",
      " Optimizer iteration 1224, batch 51\n",
      "\n",
      " Learning rate 0.0003047580261849456, Model learning rate 0.00030475802486762404\n",
      " 52/391 [==>...........................] - ETA: 22s - loss: 1.0405 - acc: 0.6863\n",
      " Optimizer iteration 1225, batch 52\n",
      "\n",
      " Learning rate 0.0003040166950699625, Model learning rate 0.0003040166920982301\n",
      " 53/391 [===>..........................] - ETA: 22s - loss: 1.0398 - acc: 0.6860\n",
      " Optimizer iteration 1226, batch 53\n",
      "\n",
      " Learning rate 0.0003032758726406521, Model learning rate 0.0003032758831977844\n",
      "\n",
      " Optimizer iteration 1227, batch 54\n",
      "\n",
      " Learning rate 0.0003025355608198606, Model learning rate 0.0003025355690624565\n",
      " 55/391 [===>..........................] - ETA: 21s - loss: 1.0402 - acc: 0.6865\n",
      " Optimizer iteration 1228, batch 55\n",
      "\n",
      " Learning rate 0.0003017957615291088, Model learning rate 0.0003017957496922463\n",
      " 56/391 [===>..........................] - ETA: 22s - loss: 1.0396 - acc: 0.6876\n",
      " Optimizer iteration 1229, batch 56\n",
      "\n",
      " Learning rate 0.0003010564766885878, Model learning rate 0.0003010564832948148\n",
      " 57/391 [===>..........................] - ETA: 22s - loss: 1.0370 - acc: 0.6885\n",
      " Optimizer iteration 1230, batch 57\n",
      "\n",
      " Learning rate 0.0003003177082171523, Model learning rate 0.0003003177116625011\n",
      "\n",
      " Optimizer iteration 1231, batch 58\n",
      "\n",
      " Learning rate 0.00029957945803231754, Model learning rate 0.0002995794638991356\n",
      " 59/391 [===>..........................] - ETA: 21s - loss: 1.0365 - acc: 0.6880\n",
      " Optimizer iteration 1232, batch 59\n",
      "\n",
      " Learning rate 0.00029884172805025343, Model learning rate 0.0002988417400047183\n",
      " 60/391 [===>..........................] - ETA: 21s - loss: 1.0371 - acc: 0.6876\n",
      " Optimizer iteration 1233, batch 60\n",
      "\n",
      " Learning rate 0.0002981045201857796, Model learning rate 0.0002981045108754188\n",
      " 61/391 [===>..........................] - ETA: 22s - loss: 1.0360 - acc: 0.6878\n",
      " Optimizer iteration 1234, batch 61\n",
      "\n",
      " Learning rate 0.0002973678363523604, Model learning rate 0.00029736783471889794\n",
      "\n",
      " Optimizer iteration 1235, batch 62\n",
      "\n",
      " Learning rate 0.0002966316784621, Model learning rate 0.0002966316824313253\n",
      " 63/391 [===>..........................] - ETA: 21s - loss: 1.0358 - acc: 0.6890\n",
      " Optimizer iteration 1236, batch 63\n",
      "\n",
      " Learning rate 0.00029589604842573757, Model learning rate 0.0002958960540127009\n",
      " 64/391 [===>..........................] - ETA: 21s - loss: 1.0364 - acc: 0.6892\n",
      " Optimizer iteration 1237, batch 64\n",
      "\n",
      " Learning rate 0.00029516094815264216, Model learning rate 0.00029516094946302474\n",
      " 65/391 [===>..........................] - ETA: 21s - loss: 1.0348 - acc: 0.6894\n",
      " Optimizer iteration 1238, batch 65\n",
      "\n",
      " Learning rate 0.00029442637955080786, Model learning rate 0.0002944263687822968\n",
      "\n",
      " Optimizer iteration 1239, batch 66\n",
      "\n",
      " Learning rate 0.0002936923445268488, Model learning rate 0.0002936923410743475\n",
      " 67/391 [====>.........................] - ETA: 21s - loss: 1.0336 - acc: 0.6888\n",
      " Optimizer iteration 1240, batch 67\n",
      "\n",
      " Learning rate 0.0002929588449859941, Model learning rate 0.00029295883723534644\n",
      "\n",
      " Optimizer iteration 1241, batch 68\n",
      "\n",
      " Learning rate 0.00029222588283208274, Model learning rate 0.00029222588636912405\n",
      " 69/391 [====>.........................] - ETA: 21s - loss: 1.0323 - acc: 0.6887\n",
      " Optimizer iteration 1242, batch 69\n",
      "\n",
      " Learning rate 0.00029149345996755936, Model learning rate 0.0002914934593718499\n",
      " 70/391 [====>.........................] - ETA: 21s - loss: 1.0343 - acc: 0.6883\n",
      " Optimizer iteration 1243, batch 70\n",
      "\n",
      " Learning rate 0.00029076157829346883, Model learning rate 0.0002907615853473544\n",
      " 71/391 [====>.........................] - ETA: 21s - loss: 1.0311 - acc: 0.6897\n",
      " Optimizer iteration 1244, batch 71\n",
      "\n",
      " Learning rate 0.00029003023970945057, Model learning rate 0.00029003023519180715\n",
      "\n",
      " Optimizer iteration 1245, batch 72\n",
      "\n",
      " Learning rate 0.00028929944611373555, Model learning rate 0.00028929943800903857\n",
      " 73/391 [====>.........................] - ETA: 21s - loss: 1.0306 - acc: 0.6902\n",
      " Optimizer iteration 1246, batch 73\n",
      "\n",
      " Learning rate 0.0002885691994031393, Model learning rate 0.00028856919379904866\n",
      "\n",
      " Optimizer iteration 1247, batch 74\n",
      "\n",
      " Learning rate 0.0002878395014730579, Model learning rate 0.00028783950256183743\n",
      " 75/391 [====>.........................] - ETA: 21s - loss: 1.0299 - acc: 0.6909\n",
      " Optimizer iteration 1248, batch 75\n",
      "\n",
      " Learning rate 0.00028711035421746366, Model learning rate 0.0002871103642974049\n",
      " 76/391 [====>.........................] - ETA: 21s - loss: 1.0292 - acc: 0.6909\n",
      " Optimizer iteration 1249, batch 76\n",
      "\n",
      " Learning rate 0.0002863817595288993, Model learning rate 0.00028638174990192056\n",
      " 77/391 [====>.........................] - ETA: 21s - loss: 1.0283 - acc: 0.6913\n",
      " Optimizer iteration 1250, batch 77\n",
      "\n",
      " Learning rate 0.00028565371929847286, Model learning rate 0.00028565371758304536\n",
      "\n",
      " Optimizer iteration 1251, batch 78\n",
      "\n",
      " Learning rate 0.00028492623541585404, Model learning rate 0.00028492623823694885\n",
      " 79/391 [=====>........................] - ETA: 21s - loss: 1.0289 - acc: 0.6909\n",
      " Optimizer iteration 1252, batch 79\n",
      "\n",
      " Learning rate 0.000284199309769268, Model learning rate 0.000284199311863631\n",
      " 80/391 [=====>........................] - ETA: 21s - loss: 1.0293 - acc: 0.6909\n",
      " Optimizer iteration 1253, batch 80\n",
      "\n",
      " Learning rate 0.00028347294424549077, Model learning rate 0.00028347293846309185\n",
      " 81/391 [=====>........................] - ETA: 21s - loss: 1.0293 - acc: 0.6916\n",
      " Optimizer iteration 1254, batch 81\n",
      "\n",
      " Learning rate 0.00028274714072984506, Model learning rate 0.0002827471471391618\n",
      "\n",
      " Optimizer iteration 1255, batch 82\n",
      "\n",
      " Learning rate 0.0002820219011061949, Model learning rate 0.0002820219087880105\n",
      " 83/391 [=====>........................] - ETA: 21s - loss: 1.0291 - acc: 0.6915\n",
      " Optimizer iteration 1256, batch 83\n",
      "\n",
      " Learning rate 0.0002812972272569402, Model learning rate 0.0002812972234096378\n",
      "\n",
      " Optimizer iteration 1257, batch 84\n",
      "\n",
      " Learning rate 0.00028057312106301253, Model learning rate 0.0002805731201078743\n",
      " 85/391 [=====>........................] - ETA: 21s - loss: 1.0276 - acc: 0.6919\n",
      " Optimizer iteration 1258, batch 85\n",
      "\n",
      " Learning rate 0.00027984958440387044, Model learning rate 0.0002798495988827199\n",
      " 86/391 [=====>........................] - ETA: 20s - loss: 1.0281 - acc: 0.6920\n",
      " Optimizer iteration 1259, batch 86\n",
      "\n",
      " Learning rate 0.0002791266191574936, Model learning rate 0.00027912663063034415\n",
      " 87/391 [=====>........................] - ETA: 20s - loss: 1.0285 - acc: 0.6924\n",
      " Optimizer iteration 1260, batch 87\n",
      "\n",
      " Learning rate 0.0002784042272003794, Model learning rate 0.0002784042153507471\n",
      " 88/391 [=====>........................] - ETA: 20s - loss: 1.0283 - acc: 0.6928\n",
      " Optimizer iteration 1261, batch 88\n",
      "\n",
      " Learning rate 0.00027768241040753637, Model learning rate 0.00027768241125158966\n",
      " 89/391 [=====>........................] - ETA: 20s - loss: 1.0285 - acc: 0.6927\n",
      " Optimizer iteration 1262, batch 89\n",
      "\n",
      " Learning rate 0.0002769611706524805, Model learning rate 0.0002769611601252109\n",
      " 90/391 [=====>........................] - ETA: 20s - loss: 1.0298 - acc: 0.6920\n",
      " Optimizer iteration 1263, batch 90\n",
      "\n",
      " Learning rate 0.0002762405098072303, Model learning rate 0.0002762405201792717\n",
      " 91/391 [=====>........................] - ETA: 20s - loss: 1.0294 - acc: 0.6921\n",
      " Optimizer iteration 1264, batch 91\n",
      "\n",
      " Learning rate 0.00027552042974230117, Model learning rate 0.0002755204332061112\n",
      "\n",
      " Optimizer iteration 1265, batch 92\n",
      "\n",
      " Learning rate 0.0002748009323267016, Model learning rate 0.0002748009283095598\n",
      " 93/391 [======>.......................] - ETA: 20s - loss: 1.0263 - acc: 0.6929\n",
      " Optimizer iteration 1266, batch 93\n",
      "\n",
      " Learning rate 0.00027408201942792756, Model learning rate 0.0002740820054896176\n",
      " 94/391 [======>.......................] - ETA: 20s - loss: 1.0259 - acc: 0.6932\n",
      " Optimizer iteration 1267, batch 94\n",
      "\n",
      " Learning rate 0.00027336369291195773, Model learning rate 0.00027336369385011494\n",
      " 95/391 [======>.......................] - ETA: 20s - loss: 1.0262 - acc: 0.6927\n",
      " Optimizer iteration 1268, batch 95\n",
      "\n",
      " Learning rate 0.00027264595464324875, Model learning rate 0.00027264596428722143\n",
      " 96/391 [======>.......................] - ETA: 20s - loss: 1.0269 - acc: 0.6922\n",
      " Optimizer iteration 1269, batch 96\n",
      "\n",
      " Learning rate 0.000271928806484731, Model learning rate 0.00027192881680093706\n",
      " 97/391 [======>.......................] - ETA: 20s - loss: 1.0267 - acc: 0.6922\n",
      " Optimizer iteration 1270, batch 97\n",
      "\n",
      " Learning rate 0.0002712122502978024, Model learning rate 0.0002712122513912618\n",
      "\n",
      " Optimizer iteration 1271, batch 98\n",
      "\n",
      " Learning rate 0.00027049628794232505, Model learning rate 0.00027049629716202617\n",
      " 99/391 [======>.......................] - ETA: 20s - loss: 1.0248 - acc: 0.6922\n",
      " Optimizer iteration 1272, batch 99\n",
      "\n",
      " Learning rate 0.00026978092127661945, Model learning rate 0.00026978092500939965\n",
      "100/391 [======>.......................] - ETA: 20s - loss: 1.0257 - acc: 0.6916\n",
      " Optimizer iteration 1273, batch 100\n",
      "\n",
      " Learning rate 0.0002690661521574596, Model learning rate 0.00026906616403721273\n",
      "101/391 [======>.......................] - ETA: 20s - loss: 1.0246 - acc: 0.6917\n",
      " Optimizer iteration 1274, batch 101\n",
      "\n",
      " Learning rate 0.00026835198244006924, Model learning rate 0.00026835198514163494\n",
      "\n",
      " Optimizer iteration 1275, batch 102\n",
      "\n",
      " Learning rate 0.00026763841397811573, Model learning rate 0.00026763841742649674\n",
      "103/391 [======>.......................] - ETA: 20s - loss: 1.0269 - acc: 0.6906\n",
      " Optimizer iteration 1276, batch 103\n",
      "\n",
      " Learning rate 0.0002669254486237062, Model learning rate 0.00026692546089179814\n",
      "\n",
      " Optimizer iteration 1277, batch 104\n",
      "\n",
      " Learning rate 0.0002662130882273825, Model learning rate 0.00026621308643370867\n",
      "105/391 [=======>......................] - ETA: 19s - loss: 1.0256 - acc: 0.6911\n",
      " Optimizer iteration 1278, batch 105\n",
      "\n",
      " Learning rate 0.0002655013346381158, Model learning rate 0.0002655013231560588\n",
      "\n",
      " Optimizer iteration 1279, batch 106\n",
      "\n",
      " Learning rate 0.00026479018970330227, Model learning rate 0.00026479020016267896\n",
      "107/391 [=======>......................] - ETA: 20s - loss: 1.0237 - acc: 0.6918\n",
      " Optimizer iteration 1280, batch 107\n",
      "\n",
      " Learning rate 0.000264079655268759, Model learning rate 0.00026407965924590826\n",
      "\n",
      " Optimizer iteration 1281, batch 108\n",
      "\n",
      " Learning rate 0.00026336973317871756, Model learning rate 0.00026336972950957716\n",
      "109/391 [=======>......................] - ETA: 19s - loss: 1.0227 - acc: 0.6922\n",
      " Optimizer iteration 1282, batch 109\n",
      "\n",
      " Learning rate 0.000262660425275821, Model learning rate 0.00026266041095368564\n",
      "\n",
      " Optimizer iteration 1283, batch 110\n",
      "\n",
      " Learning rate 0.0002619517334011177, Model learning rate 0.0002619517326820642\n",
      "111/391 [=======>......................] - ETA: 19s - loss: 1.0251 - acc: 0.6917\n",
      " Optimizer iteration 1284, batch 111\n",
      "\n",
      " Learning rate 0.0002612436593940568, Model learning rate 0.0002612436655908823\n",
      "112/391 [=======>......................] - ETA: 19s - loss: 1.0257 - acc: 0.6913\n",
      " Optimizer iteration 1285, batch 112\n",
      "\n",
      " Learning rate 0.0002605362050924848, Model learning rate 0.00026053620968014\n",
      "\n",
      " Optimizer iteration 1286, batch 113\n",
      "\n",
      " Learning rate 0.00025982937233263846, Model learning rate 0.0002598293649498373\n",
      "114/391 [=======>......................] - ETA: 19s - loss: 1.0259 - acc: 0.6913\n",
      " Optimizer iteration 1287, batch 114\n",
      "\n",
      " Learning rate 0.0002591231629491423, Model learning rate 0.0002591231605038047\n",
      "115/391 [=======>......................] - ETA: 19s - loss: 1.0253 - acc: 0.6913\n",
      " Optimizer iteration 1288, batch 115\n",
      "\n",
      " Learning rate 0.0002584175787750024, Model learning rate 0.00025841756723821163\n",
      "116/391 [=======>......................] - ETA: 19s - loss: 1.0260 - acc: 0.6913\n",
      " Optimizer iteration 1289, batch 116\n",
      "\n",
      " Learning rate 0.00025771262164160213, Model learning rate 0.00025771261425688863\n",
      "117/391 [=======>......................] - ETA: 19s - loss: 1.0269 - acc: 0.6905\n",
      " Optimizer iteration 1290, batch 117\n",
      "\n",
      " Learning rate 0.00025700829337869696, Model learning rate 0.00025700830155983567\n",
      "\n",
      " Optimizer iteration 1291, batch 118\n",
      "\n",
      " Learning rate 0.0002563045958144109, Model learning rate 0.0002563046000432223\n",
      "119/391 [========>.....................] - ETA: 19s - loss: 1.0276 - acc: 0.6903\n",
      " Optimizer iteration 1292, batch 119\n",
      "\n",
      " Learning rate 0.0002556015307752301, Model learning rate 0.000255601538810879\n",
      "\n",
      " Optimizer iteration 1293, batch 120\n",
      "\n",
      " Learning rate 0.00025489910008599966, Model learning rate 0.00025489908875897527\n",
      "121/391 [========>.....................] - ETA: 18s - loss: 1.0291 - acc: 0.6896\n",
      " Optimizer iteration 1294, batch 121\n",
      "\n",
      " Learning rate 0.0002541973055699178, Model learning rate 0.00025419730809517205\n",
      "\n",
      " Optimizer iteration 1295, batch 122\n",
      "\n",
      " Learning rate 0.0002534961490485313, Model learning rate 0.0002534961386118084\n",
      "123/391 [========>.....................] - ETA: 18s - loss: 1.0286 - acc: 0.6898\n",
      " Optimizer iteration 1296, batch 123\n",
      "\n",
      " Learning rate 0.00025279563234173176, Model learning rate 0.0002527956385165453\n",
      "\n",
      " Optimizer iteration 1297, batch 124\n",
      "\n",
      " Learning rate 0.00025209575726774914, Model learning rate 0.00025209574960172176\n",
      "125/391 [========>.....................] - ETA: 18s - loss: 1.0288 - acc: 0.6898\n",
      " Optimizer iteration 1298, batch 125\n",
      "\n",
      " Learning rate 0.0002513965256431488, Model learning rate 0.00025139653007499874\n",
      "\n",
      " Optimizer iteration 1299, batch 126\n",
      "\n",
      " Learning rate 0.0002506979392828258, Model learning rate 0.00025069795083254576\n",
      "127/391 [========>.....................] - ETA: 18s - loss: 1.0281 - acc: 0.6898\n",
      " Optimizer iteration 1300, batch 127\n",
      "\n",
      " Learning rate 0.0002500000000000001, Model learning rate 0.0002500000118743628\n",
      "128/391 [========>.....................] - ETA: 18s - loss: 1.0277 - acc: 0.6901\n",
      " Optimizer iteration 1301, batch 128\n",
      "\n",
      " Learning rate 0.0002493027096062121, Model learning rate 0.00024930271320044994\n",
      "\n",
      " Optimizer iteration 1302, batch 129\n",
      "\n",
      " Learning rate 0.00024860606991131855, Model learning rate 0.00024860608391463757\n",
      "130/391 [========>.....................] - ETA: 18s - loss: 1.0256 - acc: 0.6907\n",
      " Optimizer iteration 1303, batch 130\n",
      "\n",
      " Learning rate 0.00024791008272348654, Model learning rate 0.00024791009491309524\n",
      "131/391 [=========>....................] - ETA: 18s - loss: 1.0255 - acc: 0.6911\n",
      " Optimizer iteration 1304, batch 131\n",
      "\n",
      " Learning rate 0.0002472147498491902, Model learning rate 0.00024721474619582295\n",
      "\n",
      " Optimizer iteration 1305, batch 132\n",
      "\n",
      " Learning rate 0.00024652007309320496, Model learning rate 0.0002465200668666512\n",
      "133/391 [=========>....................] - ETA: 18s - loss: 1.0250 - acc: 0.6910\n",
      " Optimizer iteration 1306, batch 133\n",
      "\n",
      " Learning rate 0.00024582605425860313, Model learning rate 0.0002458260569255799\n",
      "134/391 [=========>....................] - ETA: 18s - loss: 1.0243 - acc: 0.6915\n",
      " Optimizer iteration 1307, batch 134\n",
      "\n",
      " Learning rate 0.00024513269514674985, Model learning rate 0.0002451326872687787\n",
      "135/391 [=========>....................] - ETA: 18s - loss: 1.0243 - acc: 0.6911\n",
      " Optimizer iteration 1308, batch 135\n",
      "\n",
      " Learning rate 0.0002444399975572974, Model learning rate 0.00024443998700007796\n",
      "136/391 [=========>....................] - ETA: 18s - loss: 1.0231 - acc: 0.6917\n",
      " Optimizer iteration 1309, batch 136\n",
      "\n",
      " Learning rate 0.00024374796328818134, Model learning rate 0.00024374795611947775\n",
      "137/391 [=========>....................] - ETA: 17s - loss: 1.0222 - acc: 0.6923\n",
      " Optimizer iteration 1310, batch 137\n",
      "\n",
      " Learning rate 0.00024305659413561572, Model learning rate 0.00024305659462697804\n",
      "138/391 [=========>....................] - ETA: 17s - loss: 1.0223 - acc: 0.6924\n",
      " Optimizer iteration 1311, batch 138\n",
      "\n",
      " Learning rate 0.0002423658918940878, Model learning rate 0.0002423658879706636\n",
      "139/391 [=========>....................] - ETA: 17s - loss: 1.0218 - acc: 0.6929\n",
      " Optimizer iteration 1312, batch 139\n",
      "\n",
      " Learning rate 0.0002416758583563538, Model learning rate 0.0002416758652543649\n",
      "140/391 [=========>....................] - ETA: 17s - loss: 1.0209 - acc: 0.6935\n",
      " Optimizer iteration 1313, batch 140\n",
      "\n",
      " Learning rate 0.00024098649531343496, Model learning rate 0.00024098649737425148\n",
      "\n",
      " Optimizer iteration 1314, batch 141\n",
      "\n",
      " Learning rate 0.00024029780455461138, Model learning rate 0.00024029779888223857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/391 [=========>....................] - ETA: 17s - loss: 1.0209 - acc: 0.6931\n",
      " Optimizer iteration 1315, batch 142\n",
      "\n",
      " Learning rate 0.00023960978786741877, Model learning rate 0.00023960978433024138\n",
      "143/391 [=========>....................] - ETA: 17s - loss: 1.0202 - acc: 0.6933\n",
      " Optimizer iteration 1316, batch 143\n",
      "\n",
      " Learning rate 0.00023892244703764342, Model learning rate 0.00023892245371825993\n",
      "\n",
      " Optimizer iteration 1317, batch 144\n",
      "\n",
      " Learning rate 0.00023823578384931632, Model learning rate 0.00023823577794246376\n",
      "145/391 [==========>...................] - ETA: 17s - loss: 1.0210 - acc: 0.6923\n",
      " Optimizer iteration 1318, batch 145\n",
      "\n",
      " Learning rate 0.0002375498000847107, Model learning rate 0.00023754980065859854\n",
      "\n",
      " Optimizer iteration 1319, batch 146\n",
      "\n",
      " Learning rate 0.00023686449752433614, Model learning rate 0.00023686449276283383\n",
      "147/391 [==========>...................] - ETA: 17s - loss: 1.0216 - acc: 0.6925\n",
      " Optimizer iteration 1320, batch 147\n",
      "\n",
      " Learning rate 0.00023617987794693357, Model learning rate 0.0002361798833590001\n",
      "148/391 [==========>...................] - ETA: 17s - loss: 1.0227 - acc: 0.6920\n",
      " Optimizer iteration 1321, batch 148\n",
      "\n",
      " Learning rate 0.00023549594312947188, Model learning rate 0.00023549594334326684\n",
      "\n",
      " Optimizer iteration 1322, batch 149\n",
      "\n",
      " Learning rate 0.00023481269484714208, Model learning rate 0.00023481270181946456\n",
      "150/391 [==========>...................] - ETA: 17s - loss: 1.0227 - acc: 0.6916\n",
      " Optimizer iteration 1323, batch 150\n",
      "\n",
      " Learning rate 0.00023413013487335333, Model learning rate 0.0002341301296837628\n",
      "151/391 [==========>...................] - ETA: 17s - loss: 1.0224 - acc: 0.6916\n",
      " Optimizer iteration 1324, batch 151\n",
      "\n",
      " Learning rate 0.0002334482649797287, Model learning rate 0.0002334482705919072\n",
      "152/391 [==========>...................] - ETA: 17s - loss: 1.0225 - acc: 0.6918\n",
      " Optimizer iteration 1325, batch 152\n",
      "\n",
      " Learning rate 0.00023276708693609945, Model learning rate 0.00023276708088815212\n",
      "\n",
      " Optimizer iteration 1326, batch 153\n",
      "\n",
      " Learning rate 0.00023208660251050156, Model learning rate 0.00023208660422824323\n",
      "154/391 [==========>...................] - ETA: 16s - loss: 1.0225 - acc: 0.6913\n",
      " Optimizer iteration 1327, batch 154\n",
      "\n",
      " Learning rate 0.00023140681346917104, Model learning rate 0.00023140681150835007\n",
      "155/391 [==========>...................] - ETA: 16s - loss: 1.0232 - acc: 0.6910\n",
      " Optimizer iteration 1328, batch 155\n",
      "\n",
      " Learning rate 0.00023072772157653766, Model learning rate 0.00023072771728038788\n",
      "\n",
      " Optimizer iteration 1329, batch 156\n",
      "\n",
      " Learning rate 0.00023004932859522305, Model learning rate 0.00023004932154435664\n",
      "157/391 [===========>..................] - ETA: 16s - loss: 1.0257 - acc: 0.6904\n",
      " Optimizer iteration 1330, batch 157\n",
      "\n",
      " Learning rate 0.00022937163628603436, Model learning rate 0.0002293716388521716\n",
      "158/391 [===========>..................] - ETA: 16s - loss: 1.0259 - acc: 0.6905\n",
      " Optimizer iteration 1331, batch 158\n",
      "\n",
      " Learning rate 0.00022869464640795973, Model learning rate 0.0002286946401000023\n",
      "159/391 [===========>..................] - ETA: 16s - loss: 1.0256 - acc: 0.6907\n",
      " Optimizer iteration 1332, batch 159\n",
      "\n",
      " Learning rate 0.00022801836071816473, Model learning rate 0.00022801835439167917\n",
      "\n",
      " Optimizer iteration 1333, batch 160\n",
      "\n",
      " Learning rate 0.00022734278097198669, Model learning rate 0.00022734278172720224\n",
      "161/391 [===========>..................] - ETA: 16s - loss: 1.0270 - acc: 0.6902\n",
      " Optimizer iteration 1334, batch 161\n",
      "\n",
      " Learning rate 0.0002266679089229306, Model learning rate 0.00022666790755465627\n",
      "162/391 [===========>..................] - ETA: 16s - loss: 1.0265 - acc: 0.6904\n",
      " Optimizer iteration 1335, batch 162\n",
      "\n",
      " Learning rate 0.00022599374632266512, Model learning rate 0.0002259937464259565\n",
      "163/391 [===========>..................] - ETA: 16s - loss: 1.0254 - acc: 0.6910\n",
      " Optimizer iteration 1336, batch 163\n",
      "\n",
      " Learning rate 0.00022532029492101674, Model learning rate 0.0002253202983411029\n",
      "164/391 [===========>..................] - ETA: 16s - loss: 1.0253 - acc: 0.6913\n",
      " Optimizer iteration 1337, batch 164\n",
      "\n",
      " Learning rate 0.0002246475564659666, Model learning rate 0.0002246475633000955\n",
      "165/391 [===========>..................] - ETA: 16s - loss: 1.0251 - acc: 0.6913\n",
      " Optimizer iteration 1338, batch 165\n",
      "\n",
      " Learning rate 0.00022397553270364545, Model learning rate 0.00022397552675101906\n",
      "166/391 [===========>..................] - ETA: 15s - loss: 1.0250 - acc: 0.6913\n",
      " Optimizer iteration 1339, batch 166\n",
      "\n",
      " Learning rate 0.000223304225378328, Model learning rate 0.00022330423234961927\n",
      "167/391 [===========>..................] - ETA: 16s - loss: 1.0244 - acc: 0.6917\n",
      " Optimizer iteration 1340, batch 167\n",
      "\n",
      " Learning rate 0.00022263363623243056, Model learning rate 0.00022263363644015044\n",
      "\n",
      " Optimizer iteration 1341, batch 168\n",
      "\n",
      " Learning rate 0.00022196376700650496, Model learning rate 0.00022196376812644303\n",
      "169/391 [===========>..................] - ETA: 15s - loss: 1.0250 - acc: 0.6915\n",
      " Optimizer iteration 1342, batch 169\n",
      "\n",
      " Learning rate 0.00022129461943923406, Model learning rate 0.0002212946128565818\n",
      "170/391 [============>.................] - ETA: 15s - loss: 1.0241 - acc: 0.6919\n",
      " Optimizer iteration 1343, batch 170\n",
      "\n",
      " Learning rate 0.0002206261952674284, Model learning rate 0.00022062619973439723\n",
      "171/391 [============>.................] - ETA: 15s - loss: 1.0247 - acc: 0.6919\n",
      " Optimizer iteration 1344, batch 171\n",
      "\n",
      " Learning rate 0.00021995849622602015, Model learning rate 0.00021995849965605885\n",
      "172/391 [============>.................] - ETA: 15s - loss: 1.0244 - acc: 0.6920\n",
      " Optimizer iteration 1345, batch 172\n",
      "\n",
      " Learning rate 0.00021929152404805957, Model learning rate 0.00021929152717348188\n",
      "173/391 [============>.................] - ETA: 15s - loss: 1.0245 - acc: 0.6918\n",
      " Optimizer iteration 1346, batch 173\n",
      "\n",
      " Learning rate 0.0002186252804647107, Model learning rate 0.00021862528228666633\n",
      "\n",
      " Optimizer iteration 1347, batch 174\n",
      "\n",
      " Learning rate 0.0002179597672052458, Model learning rate 0.0002179597649956122\n",
      "175/391 [============>.................] - ETA: 15s - loss: 1.0251 - acc: 0.6917\n",
      " Optimizer iteration 1348, batch 175\n",
      "\n",
      " Learning rate 0.00021729498599704216, Model learning rate 0.00021729498985223472\n",
      "176/391 [============>.................] - ETA: 15s - loss: 1.0248 - acc: 0.6919\n",
      " Optimizer iteration 1349, batch 176\n",
      "\n",
      " Learning rate 0.00021663093856557708, Model learning rate 0.00021663094230461866\n",
      "177/391 [============>.................] - ETA: 15s - loss: 1.0249 - acc: 0.6918\n",
      " Optimizer iteration 1350, batch 177\n",
      "\n",
      " Learning rate 0.00021596762663442215, Model learning rate 0.000215967622352764\n",
      "178/391 [============>.................] - ETA: 15s - loss: 1.0264 - acc: 0.6911\n",
      " Optimizer iteration 1351, batch 178\n",
      "\n",
      " Learning rate 0.00021530505192524118, Model learning rate 0.00021530505910050124\n",
      "179/391 [============>.................] - ETA: 15s - loss: 1.0262 - acc: 0.6912\n",
      " Optimizer iteration 1352, batch 179\n",
      "\n",
      " Learning rate 0.0002146432161577842, Model learning rate 0.00021464320889208466\n",
      "180/391 [============>.................] - ETA: 15s - loss: 1.0258 - acc: 0.6912\n",
      " Optimizer iteration 1353, batch 180\n",
      "\n",
      " Learning rate 0.00021398212104988275, Model learning rate 0.00021398211538325995\n",
      "181/391 [============>.................] - ETA: 15s - loss: 1.0260 - acc: 0.6912\n",
      " Optimizer iteration 1354, batch 181\n",
      "\n",
      " Learning rate 0.0002133217683174466, Model learning rate 0.0002133217640221119\n",
      "182/391 [============>.................] - ETA: 14s - loss: 1.0254 - acc: 0.6915\n",
      " Optimizer iteration 1355, batch 182\n",
      "\n",
      " Learning rate 0.00021266215967445824, Model learning rate 0.00021266215480864048\n",
      "183/391 [=============>................] - ETA: 14s - loss: 1.0250 - acc: 0.6919\n",
      " Optimizer iteration 1356, batch 183\n",
      "\n",
      " Learning rate 0.0002120032968329687, Model learning rate 0.00021200330229476094\n",
      "\n",
      " Optimizer iteration 1357, batch 184\n",
      "\n",
      " Learning rate 0.0002113451815030939, Model learning rate 0.00021134517737664282\n",
      "185/391 [=============>................] - ETA: 14s - loss: 1.0250 - acc: 0.6916\n",
      " Optimizer iteration 1358, batch 185\n",
      "\n",
      " Learning rate 0.00021068781539300874, Model learning rate 0.00021068780915811658\n",
      "186/391 [=============>................] - ETA: 14s - loss: 1.0248 - acc: 0.6916\n",
      " Optimizer iteration 1359, batch 186\n",
      "\n",
      " Learning rate 0.0002100312002089441, Model learning rate 0.0002100311976391822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/391 [=============>................] - ETA: 14s - loss: 1.0247 - acc: 0.6916\n",
      " Optimizer iteration 1360, batch 187\n",
      "\n",
      " Learning rate 0.00020937533765518184, Model learning rate 0.00020937534281983972\n",
      "\n",
      " Optimizer iteration 1361, batch 188\n",
      "\n",
      " Learning rate 0.0002087202294340494, Model learning rate 0.00020872023014817387\n",
      "189/391 [=============>................] - ETA: 14s - loss: 1.0248 - acc: 0.6915\n",
      " Optimizer iteration 1362, batch 189\n",
      "\n",
      " Learning rate 0.00020806587724591725, Model learning rate 0.0002080658741760999\n",
      "\n",
      " Optimizer iteration 1363, batch 190\n",
      "\n",
      " Learning rate 0.00020741228278919343, Model learning rate 0.00020741228945553303\n",
      "191/391 [=============>................] - ETA: 14s - loss: 1.0247 - acc: 0.6915\n",
      " Optimizer iteration 1364, batch 191\n",
      "\n",
      " Learning rate 0.00020675944776031875, Model learning rate 0.0002067594468826428\n",
      "\n",
      " Optimizer iteration 1365, batch 192\n",
      "\n",
      " Learning rate 0.00020610737385376348, Model learning rate 0.0002061073755612597\n",
      "193/391 [=============>................] - ETA: 14s - loss: 1.0253 - acc: 0.6915\n",
      " Optimizer iteration 1366, batch 193\n",
      "\n",
      " Learning rate 0.0002054560627620219, Model learning rate 0.00020545606093946844\n",
      "\n",
      " Optimizer iteration 1367, batch 194\n",
      "\n",
      " Learning rate 0.00020480551617560832, Model learning rate 0.0002048055175691843\n",
      "195/391 [=============>................] - ETA: 14s - loss: 1.0249 - acc: 0.6915\n",
      " Optimizer iteration 1368, batch 195\n",
      "\n",
      " Learning rate 0.0002041557357830534, Model learning rate 0.00020415573089849204\n",
      "\n",
      " Optimizer iteration 1369, batch 196\n",
      "\n",
      " Learning rate 0.00020350672327089814, Model learning rate 0.0002035067300312221\n",
      "197/391 [==============>...............] - ETA: 13s - loss: 1.0250 - acc: 0.6913\n",
      " Optimizer iteration 1370, batch 197\n",
      "\n",
      " Learning rate 0.00020285848032369137, Model learning rate 0.00020285848586354405\n",
      "\n",
      " Optimizer iteration 1371, batch 198\n",
      "\n",
      " Learning rate 0.00020221100862398374, Model learning rate 0.0002022110129473731\n",
      "199/391 [==============>...............] - ETA: 13s - loss: 1.0245 - acc: 0.6915\n",
      " Optimizer iteration 1372, batch 199\n",
      "\n",
      " Learning rate 0.00020156430985232465, Model learning rate 0.00020156431128270924\n",
      "\n",
      " Optimizer iteration 1373, batch 200\n",
      "\n",
      " Learning rate 0.00020091838568725683, Model learning rate 0.0002009183808695525\n",
      "201/391 [==============>...............] - ETA: 13s - loss: 1.0241 - acc: 0.6916\n",
      " Optimizer iteration 1374, batch 201\n",
      "\n",
      " Learning rate 0.0002002732378053131, Model learning rate 0.00020027323625981808\n",
      "\n",
      " Optimizer iteration 1375, batch 202\n",
      "\n",
      " Learning rate 0.00019962886788101047, Model learning rate 0.00019962886290159076\n",
      "203/391 [==============>...............] - ETA: 13s - loss: 1.0234 - acc: 0.6922\n",
      " Optimizer iteration 1376, batch 203\n",
      "\n",
      " Learning rate 0.00019898527758684787, Model learning rate 0.00019898527534678578\n",
      "\n",
      " Optimizer iteration 1377, batch 204\n",
      "\n",
      " Learning rate 0.00019834246859329964, Model learning rate 0.00019834247359540313\n",
      "205/391 [==============>...............] - ETA: 13s - loss: 1.0231 - acc: 0.6920\n",
      " Optimizer iteration 1378, batch 205\n",
      "\n",
      " Learning rate 0.00019770044256881258, Model learning rate 0.0001977004430955276\n",
      "\n",
      " Optimizer iteration 1379, batch 206\n",
      "\n",
      " Learning rate 0.00019705920117980147, Model learning rate 0.00019705919839907438\n",
      "207/391 [==============>...............] - ETA: 13s - loss: 1.0226 - acc: 0.6921\n",
      " Optimizer iteration 1380, batch 207\n",
      "\n",
      " Learning rate 0.00019641874609064441, Model learning rate 0.0001964187395060435\n",
      "208/391 [==============>...............] - ETA: 13s - loss: 1.0226 - acc: 0.6922\n",
      " Optimizer iteration 1381, batch 208\n",
      "\n",
      " Learning rate 0.00019577907896367846, Model learning rate 0.00019577908096835017\n",
      "209/391 [===============>..............] - ETA: 13s - loss: 1.0225 - acc: 0.6920\n",
      " Optimizer iteration 1382, batch 209\n",
      "\n",
      " Learning rate 0.00019514020145919536, Model learning rate 0.00019514020823407918\n",
      "\n",
      " Optimizer iteration 1383, batch 210\n",
      "\n",
      " Learning rate 0.00019450211523543793, Model learning rate 0.00019450212130323052\n",
      "211/391 [===============>..............] - ETA: 12s - loss: 1.0218 - acc: 0.6924\n",
      " Optimizer iteration 1384, batch 211\n",
      "\n",
      " Learning rate 0.0001938648219485944, Model learning rate 0.0001938648201758042\n",
      "212/391 [===============>..............] - ETA: 12s - loss: 1.0221 - acc: 0.6922\n",
      " Optimizer iteration 1385, batch 212\n",
      "\n",
      " Learning rate 0.0001932283232527956, Model learning rate 0.00019322831940371543\n",
      "\n",
      " Optimizer iteration 1386, batch 213\n",
      "\n",
      " Learning rate 0.00019259262080010937, Model learning rate 0.00019259261898696423\n",
      "214/391 [===============>..............] - ETA: 12s - loss: 1.0220 - acc: 0.6923\n",
      " Optimizer iteration 1387, batch 214\n",
      "\n",
      " Learning rate 0.00019195771624053743, Model learning rate 0.00019195771892555058\n",
      "215/391 [===============>..............] - ETA: 12s - loss: 1.0217 - acc: 0.6922\n",
      " Optimizer iteration 1388, batch 215\n",
      "\n",
      " Learning rate 0.0001913236112220101, Model learning rate 0.00019132360466755927\n",
      "216/391 [===============>..............] - ETA: 12s - loss: 1.0213 - acc: 0.6924\n",
      " Optimizer iteration 1389, batch 216\n",
      "\n",
      " Learning rate 0.00019069030739038222, Model learning rate 0.00019069030531682074\n",
      "\n",
      " Optimizer iteration 1390, batch 217\n",
      "\n",
      " Learning rate 0.00019005780638942983, Model learning rate 0.00019005780632141978\n",
      "218/391 [===============>..............] - ETA: 12s - loss: 1.0214 - acc: 0.6926\n",
      " Optimizer iteration 1391, batch 218\n",
      "\n",
      " Learning rate 0.00018942610986084484, Model learning rate 0.00018942610768135637\n",
      "219/391 [===============>..............] - ETA: 12s - loss: 1.0213 - acc: 0.6927\n",
      " Optimizer iteration 1392, batch 219\n",
      "\n",
      " Learning rate 0.0001887952194442309, Model learning rate 0.00018879522394854575\n",
      "220/391 [===============>..............] - ETA: 12s - loss: 1.0213 - acc: 0.6928\n",
      " Optimizer iteration 1393, batch 220\n",
      "\n",
      " Learning rate 0.00018816513677709934, Model learning rate 0.0001881651405710727\n",
      "\n",
      " Optimizer iteration 1394, batch 221\n",
      "\n",
      " Learning rate 0.00018753586349486552, Model learning rate 0.0001875358575489372\n",
      "222/391 [================>.............] - ETA: 12s - loss: 1.0203 - acc: 0.6930\n",
      " Optimizer iteration 1395, batch 222\n",
      "\n",
      " Learning rate 0.00018690740123084316, Model learning rate 0.00018690740398596972\n",
      "223/391 [================>.............] - ETA: 12s - loss: 1.0202 - acc: 0.6928\n",
      " Optimizer iteration 1396, batch 223\n",
      "\n",
      " Learning rate 0.00018627975161624165, Model learning rate 0.0001862797507783398\n",
      "224/391 [================>.............] - ETA: 12s - loss: 1.0197 - acc: 0.6929\n",
      " Optimizer iteration 1397, batch 224\n",
      "\n",
      " Learning rate 0.00018565291628016062, Model learning rate 0.00018565291247796267\n",
      "225/391 [================>.............] - ETA: 11s - loss: 1.0192 - acc: 0.6931\n",
      " Optimizer iteration 1398, batch 225\n",
      "\n",
      " Learning rate 0.00018502689684958662, Model learning rate 0.00018502690363675356\n",
      "\n",
      " Optimizer iteration 1399, batch 226\n",
      "\n",
      " Learning rate 0.00018440169494938802, Model learning rate 0.000184401695150882\n",
      "227/391 [================>.............] - ETA: 11s - loss: 1.0192 - acc: 0.6931\n",
      " Optimizer iteration 1400, batch 227\n",
      "\n",
      " Learning rate 0.0001837773122023114, Model learning rate 0.00018377731612417847\n",
      "228/391 [================>.............] - ETA: 11s - loss: 1.0183 - acc: 0.6935\n",
      " Optimizer iteration 1401, batch 228\n",
      "\n",
      " Learning rate 0.00018315375022897736, Model learning rate 0.00018315375200472772\n",
      "\n",
      " Optimizer iteration 1402, batch 229\n",
      "\n",
      " Learning rate 0.0001825310106478762, Model learning rate 0.000182531017344445\n",
      "230/391 [================>.............] - ETA: 11s - loss: 1.0175 - acc: 0.6936\n",
      " Optimizer iteration 1403, batch 230\n",
      "\n",
      " Learning rate 0.00018190909507536323, Model learning rate 0.00018190909759141505\n",
      "231/391 [================>.............] - ETA: 11s - loss: 1.0181 - acc: 0.6936\n",
      " Optimizer iteration 1404, batch 231\n",
      "\n",
      " Learning rate 0.00018128800512565513, Model learning rate 0.00018128800729755312\n",
      "232/391 [================>.............] - ETA: 11s - loss: 1.0174 - acc: 0.6937\n",
      " Optimizer iteration 1405, batch 232\n",
      "\n",
      " Learning rate 0.00018066774241082612, Model learning rate 0.0001806677464628592\n",
      "233/391 [================>.............] - ETA: 11s - loss: 1.0172 - acc: 0.6937\n",
      " Optimizer iteration 1406, batch 233\n",
      "\n",
      " Learning rate 0.0001800483085408025, Model learning rate 0.00018004831508733332\n",
      "\n",
      " Optimizer iteration 1407, batch 234\n",
      "\n",
      " Learning rate 0.00017942970512335998, Model learning rate 0.00017942969861906022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/391 [=================>............] - ETA: 11s - loss: 1.0177 - acc: 0.6935\n",
      " Optimizer iteration 1408, batch 235\n",
      "\n",
      " Learning rate 0.00017881193376411818, Model learning rate 0.0001788119407137856\n",
      "236/391 [=================>............] - ETA: 11s - loss: 1.0178 - acc: 0.6936\n",
      " Optimizer iteration 1409, batch 236\n",
      "\n",
      " Learning rate 0.00017819499606653772, Model learning rate 0.00017819499771576375\n",
      "237/391 [=================>............] - ETA: 11s - loss: 1.0176 - acc: 0.6939\n",
      " Optimizer iteration 1410, batch 237\n",
      "\n",
      " Learning rate 0.00017757889363191482, Model learning rate 0.00017757889872882515\n",
      "\n",
      " Optimizer iteration 1411, batch 238\n",
      "\n",
      " Learning rate 0.00017696362805937776, Model learning rate 0.00017696362920105457\n",
      "239/391 [=================>............] - ETA: 10s - loss: 1.0178 - acc: 0.6939\n",
      " Optimizer iteration 1412, batch 239\n",
      "\n",
      " Learning rate 0.00017634920094588308, Model learning rate 0.00017634920368436724\n",
      "240/391 [=================>............] - ETA: 10s - loss: 1.0180 - acc: 0.6940\n",
      " Optimizer iteration 1413, batch 240\n",
      "\n",
      " Learning rate 0.00017573561388621101, Model learning rate 0.00017573560762684792\n",
      "241/391 [=================>............] - ETA: 10s - loss: 1.0177 - acc: 0.6942\n",
      " Optimizer iteration 1414, batch 241\n",
      "\n",
      " Learning rate 0.00017512286847296105, Model learning rate 0.00017512287013232708\n",
      "242/391 [=================>............] - ETA: 10s - loss: 1.0183 - acc: 0.6940\n",
      " Optimizer iteration 1415, batch 242\n",
      "\n",
      " Learning rate 0.0001745109662965481, Model learning rate 0.00017451096209697425\n",
      "243/391 [=================>............] - ETA: 10s - loss: 1.0184 - acc: 0.6940\n",
      " Optimizer iteration 1416, batch 243\n",
      "\n",
      " Learning rate 0.0001738999089451991, Model learning rate 0.0001738999126246199\n",
      "\n",
      " Optimizer iteration 1417, batch 244\n",
      "\n",
      " Learning rate 0.00017328969800494727, Model learning rate 0.00017328969261143357\n",
      "245/391 [=================>............] - ETA: 10s - loss: 1.0186 - acc: 0.6938\n",
      " Optimizer iteration 1418, batch 245\n",
      "\n",
      " Learning rate 0.0001726803350596297, Model learning rate 0.0001726803311612457\n",
      "246/391 [=================>............] - ETA: 10s - loss: 1.0188 - acc: 0.6936\n",
      " Optimizer iteration 1419, batch 246\n",
      "\n",
      " Learning rate 0.00017207182169088204, Model learning rate 0.00017207182827405632\n",
      "247/391 [=================>............] - ETA: 10s - loss: 1.0189 - acc: 0.6935\n",
      " Optimizer iteration 1420, batch 247\n",
      "\n",
      " Learning rate 0.00017146415947813472, Model learning rate 0.00017146415484603494\n",
      "248/391 [==================>...........] - ETA: 10s - loss: 1.0190 - acc: 0.6935\n",
      " Optimizer iteration 1421, batch 248\n",
      "\n",
      " Learning rate 0.00017085734999860937, Model learning rate 0.00017085735453292727\n",
      "\n",
      " Optimizer iteration 1422, batch 249\n",
      "\n",
      " Learning rate 0.00017025139482731384, Model learning rate 0.00017025139823090285\n",
      "250/391 [==================>...........] - ETA: 10s - loss: 1.0183 - acc: 0.6938\n",
      " Optimizer iteration 1423, batch 250\n",
      "\n",
      " Learning rate 0.00016964629553703893, Model learning rate 0.0001696463004918769\n",
      "251/391 [==================>...........] - ETA: 10s - loss: 1.0178 - acc: 0.6941\n",
      " Optimizer iteration 1424, batch 251\n",
      "\n",
      " Learning rate 0.000169042053698354, Model learning rate 0.0001690420467639342\n",
      "252/391 [==================>...........] - ETA: 10s - loss: 1.0174 - acc: 0.6942\n",
      " Optimizer iteration 1425, batch 252\n",
      "\n",
      " Learning rate 0.00016843867087960252, Model learning rate 0.0001684386661509052\n",
      "253/391 [==================>...........] - ETA: 9s - loss: 1.0171 - acc: 0.6943 \n",
      " Optimizer iteration 1426, batch 253\n",
      "\n",
      " Learning rate 0.00016783614864689827, Model learning rate 0.00016783614410087466\n",
      "\n",
      " Optimizer iteration 1427, batch 254\n",
      "\n",
      " Learning rate 0.00016723448856412188, Model learning rate 0.00016723449516575783\n",
      "255/391 [==================>...........] - ETA: 9s - loss: 1.0165 - acc: 0.6945\n",
      " Optimizer iteration 1428, batch 255\n",
      "\n",
      " Learning rate 0.00016663369219291558, Model learning rate 0.00016663369024172425\n",
      "256/391 [==================>...........] - ETA: 9s - loss: 1.0162 - acc: 0.6946\n",
      " Optimizer iteration 1429, batch 256\n",
      "\n",
      " Learning rate 0.00016603376109268042, Model learning rate 0.00016603375843260437\n",
      "\n",
      " Optimizer iteration 1430, batch 257\n",
      "\n",
      " Learning rate 0.00016543469682057105, Model learning rate 0.0001654346997383982\n",
      "258/391 [==================>...........] - ETA: 9s - loss: 1.0155 - acc: 0.6947\n",
      " Optimizer iteration 1431, batch 258\n",
      "\n",
      " Learning rate 0.00016483650093149227, Model learning rate 0.0001648364996071905\n",
      "259/391 [==================>...........] - ETA: 9s - loss: 1.0155 - acc: 0.6946\n",
      " Optimizer iteration 1432, batch 259\n",
      "\n",
      " Learning rate 0.00016423917497809532, Model learning rate 0.0001642391725908965\n",
      "260/391 [==================>...........] - ETA: 9s - loss: 1.0150 - acc: 0.6949\n",
      " Optimizer iteration 1433, batch 260\n",
      "\n",
      " Learning rate 0.00016364272051077333, Model learning rate 0.0001636427186895162\n",
      "261/391 [===================>..........] - ETA: 9s - loss: 1.0146 - acc: 0.6949\n",
      " Optimizer iteration 1434, batch 261\n",
      "\n",
      " Learning rate 0.00016304713907765712, Model learning rate 0.0001630471379030496\n",
      "\n",
      " Optimizer iteration 1435, batch 262\n",
      "\n",
      " Learning rate 0.00016245243222461197, Model learning rate 0.0001624524302314967\n",
      "263/391 [===================>..........] - ETA: 9s - loss: 1.0148 - acc: 0.6949\n",
      " Optimizer iteration 1436, batch 263\n",
      "\n",
      " Learning rate 0.00016185860149523285, Model learning rate 0.0001618585956748575\n",
      "264/391 [===================>..........] - ETA: 9s - loss: 1.0143 - acc: 0.6951\n",
      " Optimizer iteration 1437, batch 264\n",
      "\n",
      " Learning rate 0.00016126564843084052, Model learning rate 0.00016126564878504723\n",
      "\n",
      " Optimizer iteration 1438, batch 265\n",
      "\n",
      " Learning rate 0.00016067357457047837, Model learning rate 0.00016067357501015067\n",
      "266/391 [===================>..........] - ETA: 9s - loss: 1.0139 - acc: 0.6952\n",
      " Optimizer iteration 1439, batch 266\n",
      "\n",
      " Learning rate 0.00016008238145090692, Model learning rate 0.0001600823743501678\n",
      "267/391 [===================>..........] - ETA: 8s - loss: 1.0134 - acc: 0.6953\n",
      " Optimizer iteration 1440, batch 267\n",
      "\n",
      " Learning rate 0.00015949207060660136, Model learning rate 0.0001594920759089291\n",
      "268/391 [===================>..........] - ETA: 8s - loss: 1.0131 - acc: 0.6953\n",
      " Optimizer iteration 1441, batch 268\n",
      "\n",
      " Learning rate 0.00015890264356974688, Model learning rate 0.0001589026505826041\n",
      "269/391 [===================>..........] - ETA: 8s - loss: 1.0128 - acc: 0.6954\n",
      " Optimizer iteration 1442, batch 269\n",
      "\n",
      " Learning rate 0.00015831410187023388, Model learning rate 0.0001583140983711928\n",
      "270/391 [===================>..........] - ETA: 8s - loss: 1.0123 - acc: 0.6956\n",
      " Optimizer iteration 1443, batch 270\n",
      "\n",
      " Learning rate 0.00015772644703565563, Model learning rate 0.00015772644837852567\n",
      "271/391 [===================>..........] - ETA: 8s - loss: 1.0116 - acc: 0.6959\n",
      " Optimizer iteration 1444, batch 271\n",
      "\n",
      " Learning rate 0.00015713968059130346, Model learning rate 0.00015713968605268747\n",
      "272/391 [===================>..........] - ETA: 8s - loss: 1.0108 - acc: 0.6961\n",
      " Optimizer iteration 1445, batch 272\n",
      "\n",
      " Learning rate 0.00015655380406016234, Model learning rate 0.00015655379684176296\n",
      "\n",
      " Optimizer iteration 1446, batch 273\n",
      "\n",
      " Learning rate 0.000155968818962908, Model learning rate 0.00015596882440149784\n",
      "274/391 [====================>.........] - ETA: 8s - loss: 1.0100 - acc: 0.6965\n",
      " Optimizer iteration 1447, batch 274\n",
      "\n",
      " Learning rate 0.00015538472681790185, Model learning rate 0.00015538472507614642\n",
      "275/391 [====================>.........] - ETA: 8s - loss: 1.0097 - acc: 0.6965\n",
      " Optimizer iteration 1448, batch 275\n",
      "\n",
      " Learning rate 0.00015480152914118783, Model learning rate 0.00015480152796953917\n",
      "276/391 [====================>.........] - ETA: 8s - loss: 1.0095 - acc: 0.6965\n",
      " Optimizer iteration 1449, batch 276\n",
      "\n",
      " Learning rate 0.00015421922744648846, Model learning rate 0.00015421923308167607\n",
      "\n",
      " Optimizer iteration 1450, batch 277\n",
      "\n",
      " Learning rate 0.00015363782324520031, Model learning rate 0.0001536378258606419\n",
      "278/391 [====================>.........] - ETA: 8s - loss: 1.0098 - acc: 0.6966\n",
      " Optimizer iteration 1451, batch 278\n",
      "\n",
      " Learning rate 0.00015305731804639066, Model learning rate 0.00015305732085835189\n",
      "279/391 [====================>.........] - ETA: 8s - loss: 1.0100 - acc: 0.6967\n",
      " Optimizer iteration 1452, batch 279\n",
      "\n",
      " Learning rate 0.00015247771335679373, Model learning rate 0.00015247771807480603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/391 [====================>.........] - ETA: 8s - loss: 1.0097 - acc: 0.6967\n",
      " Optimizer iteration 1453, batch 280\n",
      "\n",
      " Learning rate 0.00015189901068080535, Model learning rate 0.00015189901751000434\n",
      "\n",
      " Optimizer iteration 1454, batch 281\n",
      "\n",
      " Learning rate 0.00015132121152048117, Model learning rate 0.00015132120461203158\n",
      "282/391 [====================>.........] - ETA: 7s - loss: 1.0099 - acc: 0.6969\n",
      " Optimizer iteration 1455, batch 282\n",
      "\n",
      " Learning rate 0.00015074431737553158, Model learning rate 0.00015074432303663343\n",
      "283/391 [====================>.........] - ETA: 7s - loss: 1.0096 - acc: 0.6971\n",
      " Optimizer iteration 1456, batch 283\n",
      "\n",
      " Learning rate 0.00015016832974331724, Model learning rate 0.00015016832912806422\n",
      "284/391 [====================>.........] - ETA: 7s - loss: 1.0092 - acc: 0.6973\n",
      " Optimizer iteration 1457, batch 284\n",
      "\n",
      " Learning rate 0.00014959325011884683, Model learning rate 0.00014959325199015439\n",
      "285/391 [====================>.........] - ETA: 7s - loss: 1.0094 - acc: 0.6971\n",
      " Optimizer iteration 1458, batch 285\n",
      "\n",
      " Learning rate 0.00014901907999477165, Model learning rate 0.00014901907707098871\n",
      "286/391 [====================>.........] - ETA: 7s - loss: 1.0096 - acc: 0.6970\n",
      " Optimizer iteration 1459, batch 286\n",
      "\n",
      " Learning rate 0.00014844582086138232, Model learning rate 0.00014844581892248243\n",
      "287/391 [=====================>........] - ETA: 7s - loss: 1.0095 - acc: 0.6970\n",
      " Optimizer iteration 1460, batch 287\n",
      "\n",
      " Learning rate 0.0001478734742066054, Model learning rate 0.00014787347754463553\n",
      "288/391 [=====================>........] - ETA: 7s - loss: 1.0093 - acc: 0.6971\n",
      " Optimizer iteration 1461, batch 288\n",
      "\n",
      " Learning rate 0.00014730204151599846, Model learning rate 0.0001473020383855328\n",
      "\n",
      " Optimizer iteration 1462, batch 289\n",
      "\n",
      " Learning rate 0.00014673152427274738, Model learning rate 0.00014673153054900467\n",
      "290/391 [=====================>........] - ETA: 7s - loss: 1.0094 - acc: 0.6970\n",
      " Optimizer iteration 1463, batch 290\n",
      "\n",
      " Learning rate 0.00014616192395766186, Model learning rate 0.0001461619249312207\n",
      "291/391 [=====================>........] - ETA: 7s - loss: 1.0086 - acc: 0.6973\n",
      " Optimizer iteration 1464, batch 291\n",
      "\n",
      " Learning rate 0.000145593242049171, Model learning rate 0.00014559323608409613\n",
      "292/391 [=====================>........] - ETA: 7s - loss: 1.0083 - acc: 0.6974\n",
      " Optimizer iteration 1465, batch 292\n",
      "\n",
      " Learning rate 0.00014502548002332088, Model learning rate 0.00014502547855954617\n",
      "293/391 [=====================>........] - ETA: 7s - loss: 1.0082 - acc: 0.6976\n",
      " Optimizer iteration 1466, batch 293\n",
      "\n",
      " Learning rate 0.0001444586393537699, Model learning rate 0.0001444586378056556\n",
      "\n",
      " Optimizer iteration 1467, batch 294\n",
      "\n",
      " Learning rate 0.00014389272151178452, Model learning rate 0.00014389272837433964\n",
      "295/391 [=====================>........] - ETA: 6s - loss: 1.0077 - acc: 0.6977\n",
      " Optimizer iteration 1468, batch 295\n",
      "\n",
      " Learning rate 0.00014332772796623656, Model learning rate 0.00014332772116176784\n",
      "296/391 [=====================>........] - ETA: 6s - loss: 1.0074 - acc: 0.6978\n",
      " Optimizer iteration 1469, batch 296\n",
      "\n",
      " Learning rate 0.00014276366018359842, Model learning rate 0.00014276365982368588\n",
      "\n",
      " Optimizer iteration 1470, batch 297\n",
      "\n",
      " Learning rate 0.00014220051962793951, Model learning rate 0.00014220051525626332\n",
      "298/391 [=====================>........] - ETA: 6s - loss: 1.0073 - acc: 0.6977\n",
      " Optimizer iteration 1471, batch 298\n",
      "\n",
      " Learning rate 0.000141638307760923, Model learning rate 0.00014163830201141536\n",
      "299/391 [=====================>........] - ETA: 6s - loss: 1.0071 - acc: 0.6977\n",
      " Optimizer iteration 1472, batch 299\n",
      "\n",
      " Learning rate 0.00014107702604180118, Model learning rate 0.00014107702008914202\n",
      "300/391 [======================>.......] - ETA: 6s - loss: 1.0067 - acc: 0.6978\n",
      " Optimizer iteration 1473, batch 300\n",
      "\n",
      " Learning rate 0.0001405166759274123, Model learning rate 0.0001405166694894433\n",
      "\n",
      " Optimizer iteration 1474, batch 301\n",
      "\n",
      " Learning rate 0.0001399572588721769, Model learning rate 0.00013995726476423442\n",
      "302/391 [======================>.......] - ETA: 6s - loss: 1.0062 - acc: 0.6978\n",
      " Optimizer iteration 1475, batch 302\n",
      "\n",
      " Learning rate 0.0001393987763280928, Model learning rate 0.00013939877680968493\n",
      "303/391 [======================>.......] - ETA: 6s - loss: 1.0058 - acc: 0.6978\n",
      " Optimizer iteration 1476, batch 303\n",
      "\n",
      " Learning rate 0.00013884122974473307, Model learning rate 0.00013884123472962528\n",
      "304/391 [======================>.......] - ETA: 6s - loss: 1.0057 - acc: 0.6979\n",
      " Optimizer iteration 1477, batch 304\n",
      "\n",
      " Learning rate 0.0001382846205692413, Model learning rate 0.00013828462397214025\n",
      "\n",
      " Optimizer iteration 1478, batch 305\n",
      "\n",
      " Learning rate 0.00013772895024632753, Model learning rate 0.00013772894453722984\n",
      "306/391 [======================>.......] - ETA: 6s - loss: 1.0061 - acc: 0.6977\n",
      " Optimizer iteration 1479, batch 306\n",
      "\n",
      " Learning rate 0.00013717422021826569, Model learning rate 0.0001371742255287245\n",
      "307/391 [======================>.......] - ETA: 6s - loss: 1.0059 - acc: 0.6978\n",
      " Optimizer iteration 1480, batch 307\n",
      "\n",
      " Learning rate 0.0001366204319248885, Model learning rate 0.00013662043784279376\n",
      "308/391 [======================>.......] - ETA: 6s - loss: 1.0058 - acc: 0.6977\n",
      " Optimizer iteration 1481, batch 308\n",
      "\n",
      " Learning rate 0.00013606758680358445, Model learning rate 0.00013606758147943765\n",
      "\n",
      " Optimizer iteration 1482, batch 309\n",
      "\n",
      " Learning rate 0.00013551568628929433, Model learning rate 0.0001355156855424866\n",
      "310/391 [======================>.......] - ETA: 5s - loss: 1.0055 - acc: 0.6981\n",
      " Optimizer iteration 1483, batch 310\n",
      "\n",
      " Learning rate 0.0001349647318145067, Model learning rate 0.0001349647354800254\n",
      "311/391 [======================>.......] - ETA: 5s - loss: 1.0052 - acc: 0.6982\n",
      " Optimizer iteration 1484, batch 311\n",
      "\n",
      " Learning rate 0.00013441472480925492, Model learning rate 0.00013441473129205406\n",
      "312/391 [======================>.......] - ETA: 5s - loss: 1.0053 - acc: 0.6982\n",
      " Optimizer iteration 1485, batch 312\n",
      "\n",
      " Learning rate 0.0001338656667011134, Model learning rate 0.00013386567297857255\n",
      "\n",
      " Optimizer iteration 1486, batch 313\n",
      "\n",
      " Learning rate 0.00013331755891519265, Model learning rate 0.00013331756053958088\n",
      "314/391 [=======================>......] - ETA: 5s - loss: 1.0049 - acc: 0.6982\n",
      " Optimizer iteration 1487, batch 314\n",
      "\n",
      " Learning rate 0.00013277040287413755, Model learning rate 0.0001327704085269943\n",
      "315/391 [=======================>......] - ETA: 5s - loss: 1.0045 - acc: 0.6983\n",
      " Optimizer iteration 1488, batch 315\n",
      "\n",
      " Learning rate 0.00013222419999812246, Model learning rate 0.00013222420238889754\n",
      "316/391 [=======================>......] - ETA: 5s - loss: 1.0045 - acc: 0.6982\n",
      " Optimizer iteration 1489, batch 316\n",
      "\n",
      " Learning rate 0.0001316789517048473, Model learning rate 0.00013167895667720586\n",
      "317/391 [=======================>......] - ETA: 5s - loss: 1.0044 - acc: 0.6983\n",
      " Optimizer iteration 1490, batch 317\n",
      "\n",
      " Learning rate 0.00013113465940953496, Model learning rate 0.00013113465684000403\n",
      "\n",
      " Optimizer iteration 1491, batch 318\n",
      "\n",
      " Learning rate 0.00013059132452492651, Model learning rate 0.00013059131742920727\n",
      "319/391 [=======================>......] - ETA: 5s - loss: 1.0044 - acc: 0.6983\n",
      " Optimizer iteration 1492, batch 319\n",
      "\n",
      " Learning rate 0.0001300489484612779, Model learning rate 0.0001300489529967308\n",
      "320/391 [=======================>......] - ETA: 5s - loss: 1.0044 - acc: 0.6982\n",
      " Optimizer iteration 1493, batch 320\n",
      "\n",
      " Learning rate 0.00012950753262635711, Model learning rate 0.0001295075344387442\n",
      "321/391 [=======================>......] - ETA: 5s - loss: 1.0042 - acc: 0.6983\n",
      " Optimizer iteration 1494, batch 321\n",
      "\n",
      " Learning rate 0.00012896707842543898, Model learning rate 0.00012896707630716264\n",
      "\n",
      " Optimizer iteration 1495, batch 322\n",
      "\n",
      " Learning rate 0.00012842758726130281, Model learning rate 0.0001284275931539014\n",
      "323/391 [=======================>......] - ETA: 4s - loss: 1.0041 - acc: 0.6984\n",
      " Optimizer iteration 1496, batch 323\n",
      "\n",
      " Learning rate 0.0001278890605342285, Model learning rate 0.00012788905587513\n",
      "324/391 [=======================>......] - ETA: 4s - loss: 1.0040 - acc: 0.6985\n",
      " Optimizer iteration 1497, batch 324\n",
      "\n",
      " Learning rate 0.0001273514996419921, Model learning rate 0.0001273514935746789\n",
      "325/391 [=======================>......] - ETA: 4s - loss: 1.0042 - acc: 0.6987\n",
      " Optimizer iteration 1498, batch 325\n",
      "\n",
      " Learning rate 0.00012681490597986312, Model learning rate 0.0001268149062525481\n",
      "\n",
      " Optimizer iteration 1499, batch 326\n",
      "\n",
      " Learning rate 0.00012627928094060065, Model learning rate 0.00012627927935682237\n",
      "327/391 [========================>.....] - ETA: 4s - loss: 1.0047 - acc: 0.6986\n",
      " Optimizer iteration 1500, batch 327\n",
      "\n",
      " Learning rate 0.0001257446259144494, Model learning rate 0.00012574462743941694\n",
      "328/391 [========================>.....] - ETA: 4s - loss: 1.0046 - acc: 0.6986\n",
      " Optimizer iteration 1501, batch 328\n",
      "\n",
      " Learning rate 0.00012521094228913683, Model learning rate 0.0001252109359484166\n",
      "\n",
      " Optimizer iteration 1502, batch 329\n",
      "\n",
      " Learning rate 0.00012467823144986844, Model learning rate 0.00012467823398765177\n",
      "330/391 [========================>.....] - ETA: 4s - loss: 1.0042 - acc: 0.6985\n",
      " Optimizer iteration 1503, batch 330\n",
      "\n",
      " Learning rate 0.0001241464947793251, Model learning rate 0.000124146492453292\n",
      "331/391 [========================>.....] - ETA: 4s - loss: 1.0044 - acc: 0.6985\n",
      " Optimizer iteration 1504, batch 331\n",
      "\n",
      " Learning rate 0.00012361573365765938, Model learning rate 0.0001236157404491678\n",
      "332/391 [========================>.....] - ETA: 4s - loss: 1.0044 - acc: 0.6984\n",
      " Optimizer iteration 1505, batch 332\n",
      "\n",
      " Learning rate 0.00012308594946249164, Model learning rate 0.00012308594887144864\n",
      "333/391 [========================>.....] - ETA: 4s - loss: 1.0043 - acc: 0.6985\n",
      " Optimizer iteration 1506, batch 333\n",
      "\n",
      " Learning rate 0.0001225571435689062, Model learning rate 0.000122557146823965\n",
      "\n",
      " Optimizer iteration 1507, batch 334\n",
      "\n",
      " Learning rate 0.00012202931734944878, Model learning rate 0.00012202931975480169\n",
      "335/391 [========================>.....] - ETA: 4s - loss: 1.0045 - acc: 0.6984\n",
      " Optimizer iteration 1508, batch 335\n",
      "\n",
      " Learning rate 0.00012150247217412185, Model learning rate 0.00012150247493991628\n",
      "336/391 [========================>.....] - ETA: 4s - loss: 1.0049 - acc: 0.6983\n",
      " Optimizer iteration 1509, batch 336\n",
      "\n",
      " Learning rate 0.00012097660941038147, Model learning rate 0.00012097661237930879\n",
      "337/391 [========================>.....] - ETA: 3s - loss: 1.0047 - acc: 0.6983\n",
      " Optimizer iteration 1510, batch 337\n",
      "\n",
      " Learning rate 0.00012045173042313429, Model learning rate 0.00012045173207297921\n",
      "\n",
      " Optimizer iteration 1511, batch 338\n",
      "\n",
      " Learning rate 0.00011992783657473289, Model learning rate 0.00011992783402092755\n",
      "339/391 [=========================>....] - ETA: 3s - loss: 1.0045 - acc: 0.6981\n",
      " Optimizer iteration 1512, batch 339\n",
      "\n",
      " Learning rate 0.00011940492922497337, Model learning rate 0.00011940493277506903\n",
      "340/391 [=========================>....] - ETA: 3s - loss: 1.0042 - acc: 0.6982\n",
      " Optimizer iteration 1513, batch 340\n",
      "\n",
      " Learning rate 0.00011888300973109112, Model learning rate 0.00011888300650753081\n",
      "341/391 [=========================>....] - ETA: 3s - loss: 1.0040 - acc: 0.6983\n",
      " Optimizer iteration 1514, batch 341\n",
      "\n",
      " Learning rate 0.00011836207944775728, Model learning rate 0.00011836207704618573\n",
      "342/391 [=========================>....] - ETA: 3s - loss: 1.0042 - acc: 0.6981\n",
      " Optimizer iteration 1515, batch 342\n",
      "\n",
      " Learning rate 0.0001178421397270758, Model learning rate 0.00011784213711507618\n",
      "343/391 [=========================>....] - ETA: 3s - loss: 1.0041 - acc: 0.6981\n",
      " Optimizer iteration 1516, batch 343\n",
      "\n",
      " Learning rate 0.00011732319191857954, Model learning rate 0.00011732319399015978\n",
      "344/391 [=========================>....] - ETA: 3s - loss: 1.0039 - acc: 0.6981\n",
      " Optimizer iteration 1517, batch 344\n",
      "\n",
      " Learning rate 0.0001168052373692266, Model learning rate 0.0001168052403954789\n",
      "345/391 [=========================>....] - ETA: 3s - loss: 1.0043 - acc: 0.6979\n",
      " Optimizer iteration 1518, batch 345\n",
      "\n",
      " Learning rate 0.00011628827742339687, Model learning rate 0.00011628827633103356\n",
      "\n",
      " Optimizer iteration 1519, batch 346\n",
      "\n",
      " Learning rate 0.0001157723134228893, Model learning rate 0.00011577231634873897\n",
      "347/391 [=========================>....] - ETA: 3s - loss: 1.0045 - acc: 0.6976\n",
      " Optimizer iteration 1520, batch 347\n",
      "\n",
      " Learning rate 0.00011525734670691701, Model learning rate 0.00011525734589667991\n",
      "348/391 [=========================>....] - ETA: 3s - loss: 1.0039 - acc: 0.6977\n",
      " Optimizer iteration 1521, batch 348\n",
      "\n",
      " Learning rate 0.00011474337861210544, Model learning rate 0.0001147433795267716\n",
      "349/391 [=========================>....] - ETA: 3s - loss: 1.0037 - acc: 0.6977\n",
      " Optimizer iteration 1522, batch 349\n",
      "\n",
      " Learning rate 0.00011423041047248728, Model learning rate 0.00011423040996305645\n",
      "350/391 [=========================>....] - ETA: 2s - loss: 1.0036 - acc: 0.6978\n",
      " Optimizer iteration 1523, batch 350\n",
      "\n",
      " Learning rate 0.00011371844361950045, Model learning rate 0.00011371844448149204\n",
      "\n",
      " Optimizer iteration 1524, batch 351\n",
      "\n",
      " Learning rate 0.00011320747938198356, Model learning rate 0.00011320747580612078\n",
      "352/391 [==========================>...] - ETA: 2s - loss: 1.0033 - acc: 0.6979\n",
      " Optimizer iteration 1525, batch 352\n",
      "\n",
      " Learning rate 0.00011269751908617276, Model learning rate 0.0001126975184888579\n",
      "353/391 [==========================>...] - ETA: 2s - loss: 1.0030 - acc: 0.6980\n",
      " Optimizer iteration 1526, batch 353\n",
      "\n",
      " Learning rate 0.00011218856405569883, Model learning rate 0.00011218856525374576\n",
      "\n",
      " Optimizer iteration 1527, batch 354\n",
      "\n",
      " Learning rate 0.00011168061561158321, Model learning rate 0.00011168061610078439\n",
      "355/391 [==========================>...] - ETA: 2s - loss: 1.0027 - acc: 0.6981\n",
      " Optimizer iteration 1528, batch 355\n",
      "\n",
      " Learning rate 0.00011117367507223452, Model learning rate 0.00011117367830593139\n",
      "356/391 [==========================>...] - ETA: 2s - loss: 1.0026 - acc: 0.6982\n",
      " Optimizer iteration 1529, batch 356\n",
      "\n",
      " Learning rate 0.0001106677437534453, Model learning rate 0.00011066774459322914\n",
      "357/391 [==========================>...] - ETA: 2s - loss: 1.0025 - acc: 0.6982\n",
      " Optimizer iteration 1530, batch 357\n",
      "\n",
      " Learning rate 0.00011016282296838886, Model learning rate 0.00011016282223863527\n",
      "\n",
      " Optimizer iteration 1531, batch 358\n",
      "\n",
      " Learning rate 0.0001096589140276153, Model learning rate 0.00010965891124214977\n",
      "359/391 [==========================>...] - ETA: 2s - loss: 1.0021 - acc: 0.6985\n",
      " Optimizer iteration 1532, batch 359\n",
      "\n",
      " Learning rate 0.00010915601823904875, Model learning rate 0.00010915601887973025\n",
      "360/391 [==========================>...] - ETA: 2s - loss: 1.0023 - acc: 0.6985\n",
      " Optimizer iteration 1533, batch 360\n",
      "\n",
      " Learning rate 0.00010865413690798321, Model learning rate 0.00010865413787541911\n",
      "361/391 [==========================>...] - ETA: 2s - loss: 1.0023 - acc: 0.6984\n",
      " Optimizer iteration 1534, batch 361\n",
      "\n",
      " Learning rate 0.00010815327133708014, Model learning rate 0.00010815326822921634\n",
      "\n",
      " Optimizer iteration 1535, batch 362\n",
      "\n",
      " Learning rate 0.00010765342282636414, Model learning rate 0.00010765342449303716\n",
      "363/391 [==========================>...] - ETA: 2s - loss: 1.0016 - acc: 0.6987\n",
      " Optimizer iteration 1536, batch 363\n",
      "\n",
      " Learning rate 0.00010715459267321997, Model learning rate 0.00010715459211496636\n",
      "364/391 [==========================>...] - ETA: 1s - loss: 1.0018 - acc: 0.6987\n",
      " Optimizer iteration 1537, batch 364\n",
      "\n",
      " Learning rate 0.00010665678217238933, Model learning rate 0.00010665678564691916\n",
      "365/391 [===========================>..] - ETA: 1s - loss: 1.0017 - acc: 0.6988\n",
      " Optimizer iteration 1538, batch 365\n",
      "\n",
      " Learning rate 0.0001061599926159676, Model learning rate 0.00010615999053698033\n",
      "366/391 [===========================>..] - ETA: 1s - loss: 1.0021 - acc: 0.6987\n",
      " Optimizer iteration 1539, batch 366\n",
      "\n",
      " Learning rate 0.0001056642252933997, Model learning rate 0.00010566422861302271\n",
      "367/391 [===========================>..] - ETA: 1s - loss: 1.0020 - acc: 0.6986\n",
      " Optimizer iteration 1540, batch 367\n",
      "\n",
      " Learning rate 0.00010516948149147753, Model learning rate 0.00010516947804717347\n",
      "368/391 [===========================>..] - ETA: 1s - loss: 1.0018 - acc: 0.6988\n",
      " Optimizer iteration 1541, batch 368\n",
      "\n",
      " Learning rate 0.00010467576249433663, Model learning rate 0.00010467576066730544\n",
      "369/391 [===========================>..] - ETA: 1s - loss: 1.0018 - acc: 0.6988\n",
      " Optimizer iteration 1542, batch 369\n",
      "\n",
      " Learning rate 0.00010418306958345213, Model learning rate 0.00010418306919746101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/391 [===========================>..] - ETA: 1s - loss: 1.0017 - acc: 0.6988\n",
      " Optimizer iteration 1543, batch 370\n",
      "\n",
      " Learning rate 0.00010369140403763638, Model learning rate 0.00010369140363764018\n",
      "371/391 [===========================>..] - ETA: 1s - loss: 1.0015 - acc: 0.6990\n",
      " Optimizer iteration 1544, batch 371\n",
      "\n",
      " Learning rate 0.00010320076713303467, Model learning rate 0.00010320076398784295\n",
      "372/391 [===========================>..] - ETA: 1s - loss: 1.0016 - acc: 0.6990\n",
      " Optimizer iteration 1545, batch 372\n",
      "\n",
      " Learning rate 0.00010271116014312292, Model learning rate 0.00010271115752402693\n",
      "373/391 [===========================>..] - ETA: 1s - loss: 1.0015 - acc: 0.6991\n",
      " Optimizer iteration 1546, batch 373\n",
      "\n",
      " Learning rate 0.00010222258433870341, Model learning rate 0.00010222258424619213\n",
      "374/391 [===========================>..] - ETA: 1s - loss: 1.0013 - acc: 0.6991\n",
      " Optimizer iteration 1547, batch 374\n",
      "\n",
      " Learning rate 0.00010173504098790188, Model learning rate 0.00010173504415433854\n",
      "375/391 [===========================>..] - ETA: 1s - loss: 1.0015 - acc: 0.6990\n",
      " Optimizer iteration 1548, batch 375\n",
      "\n",
      " Learning rate 0.00010124853135616475, Model learning rate 0.00010124852997250855\n",
      "376/391 [===========================>..] - ETA: 1s - loss: 1.0014 - acc: 0.6990\n",
      " Optimizer iteration 1549, batch 376\n",
      "\n",
      " Learning rate 0.00010076305670625507, Model learning rate 0.00010076305625261739\n",
      "\n",
      " Optimizer iteration 1550, batch 377\n",
      "\n",
      " Learning rate 0.00010027861829824952, Model learning rate 0.00010027861571870744\n",
      "378/391 [============================>.] - ETA: 0s - loss: 1.0016 - acc: 0.6990\n",
      " Optimizer iteration 1551, batch 378\n",
      "\n",
      " Learning rate 9.9795217389535e-05, Model learning rate 9.979521564673632e-05\n",
      "379/391 [============================>.] - ETA: 0s - loss: 1.0014 - acc: 0.6991\n",
      " Optimizer iteration 1552, batch 379\n",
      "\n",
      " Learning rate 9.931285523480604e-05, Model learning rate 9.931285603670403e-05\n",
      "380/391 [============================>.] - ETA: 0s - loss: 1.0011 - acc: 0.6993\n",
      " Optimizer iteration 1553, batch 380\n",
      "\n",
      " Learning rate 9.883153308606035e-05, Model learning rate 9.883152961265296e-05\n",
      "\n",
      " Optimizer iteration 1554, batch 381\n",
      "\n",
      " Learning rate 9.835125219259695e-05, Model learning rate 9.835125092649832e-05\n",
      "382/391 [============================>.] - ETA: 0s - loss: 1.0008 - acc: 0.6993\n",
      " Optimizer iteration 1555, batch 382\n",
      "\n",
      " Learning rate 9.787201380101158e-05, Model learning rate 9.787201270228252e-05\n",
      "\n",
      " Optimizer iteration 1556, batch 383\n",
      "\n",
      " Learning rate 9.739381915519457e-05, Model learning rate 9.739382221596316e-05\n",
      "384/391 [============================>.] - ETA: 0s - loss: 1.0003 - acc: 0.6995\n",
      " Optimizer iteration 1557, batch 384\n",
      "\n",
      " Learning rate 9.691666949632683e-05, Model learning rate 9.691667219158262e-05\n",
      "385/391 [============================>.] - ETA: 0s - loss: 1.0004 - acc: 0.6995\n",
      " Optimizer iteration 1558, batch 385\n",
      "\n",
      " Learning rate 9.644056606287727e-05, Model learning rate 9.644056262914091e-05\n",
      "386/391 [============================>.] - ETA: 0s - loss: 1.0002 - acc: 0.6995\n",
      " Optimizer iteration 1559, batch 386\n",
      "\n",
      " Learning rate 9.596551009059884e-05, Model learning rate 9.596550808055326e-05\n",
      "\n",
      " Optimizer iteration 1560, batch 387\n",
      "\n",
      " Learning rate 9.549150281252633e-05, Model learning rate 9.549150126986206e-05\n",
      "388/391 [============================>.] - ETA: 0s - loss: 1.0000 - acc: 0.6997\n",
      " Optimizer iteration 1561, batch 388\n",
      "\n",
      " Learning rate 9.501854545897203e-05, Model learning rate 9.501854219706729e-05\n",
      "389/391 [============================>.] - ETA: 0s - loss: 0.9999 - acc: 0.6997\n",
      " Optimizer iteration 1562, batch 389\n",
      "\n",
      " Learning rate 9.454663925752316e-05, Model learning rate 9.454663813812658e-05\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.9997 - acc: 0.6999\n",
      " Optimizer iteration 1563, batch 390\n",
      "\n",
      " Learning rate 9.407578543303913e-05, Model learning rate 9.407578181708232e-05\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.9997 - acc: 0.6999 - val_loss: 0.9859 - val_acc: 0.7009\n",
      "\n",
      "Epoch 00004: saving model to /home/ubuntu/Projects/hybrid-ensemble/model/run_200/cifar10_ResNet20v1_model-0004.h5\n",
      "Epoch 5/5\n",
      "\n",
      " Optimizer iteration 1564, batch 0\n",
      "\n",
      " Learning rate 9.360598520764712e-05, Model learning rate 9.360598778584972e-05\n",
      "  1/391 [..............................] - ETA: 16s - loss: 0.9489 - acc: 0.7266\n",
      " Optimizer iteration 1565, batch 1\n",
      "\n",
      " Learning rate 9.313723980074018e-05, Model learning rate 9.313724149251357e-05\n",
      "\n",
      " Optimizer iteration 1566, batch 2\n",
      "\n",
      " Learning rate 9.266955042897358e-05, Model learning rate 9.266955021303147e-05\n",
      "  3/391 [..............................] - ETA: 16s - loss: 0.9657 - acc: 0.7292\n",
      " Optimizer iteration 1567, batch 3\n",
      "\n",
      " Learning rate 9.220291830626082e-05, Model learning rate 9.220292122336105e-05\n",
      "\n",
      " Optimizer iteration 1568, batch 4\n",
      "\n",
      " Learning rate 9.173734464377204e-05, Model learning rate 9.173734724754468e-05\n",
      "  5/391 [..............................] - ETA: 16s - loss: 0.9500 - acc: 0.7234\n",
      " Optimizer iteration 1569, batch 5\n",
      "\n",
      " Learning rate 9.127283064992997e-05, Model learning rate 9.127282828558236e-05\n",
      "  6/391 [..............................] - ETA: 17s - loss: 0.9321 - acc: 0.7344\n",
      " Optimizer iteration 1570, batch 6\n",
      "\n",
      " Learning rate 9.080937753040646e-05, Model learning rate 9.080937888938934e-05\n",
      "  7/391 [..............................] - ETA: 17s - loss: 0.9312 - acc: 0.7288\n",
      " Optimizer iteration 1571, batch 7\n",
      "\n",
      " Learning rate 9.034698648812046e-05, Model learning rate 9.034698450705037e-05\n",
      "  8/391 [..............................] - ETA: 17s - loss: 0.9302 - acc: 0.7256\n",
      " Optimizer iteration 1572, batch 8\n",
      "\n",
      " Learning rate 8.988565872323362e-05, Model learning rate 8.988565969048068e-05\n",
      "  9/391 [..............................] - ETA: 18s - loss: 0.9200 - acc: 0.7318\n",
      " Optimizer iteration 1573, batch 9\n",
      "\n",
      " Learning rate 8.942539543314798e-05, Model learning rate 8.942539716372266e-05\n",
      " 10/391 [..............................] - ETA: 18s - loss: 0.9244 - acc: 0.7305\n",
      " Optimizer iteration 1574, batch 10\n",
      "\n",
      " Learning rate 8.896619781250309e-05, Model learning rate 8.896619692677632e-05\n",
      " 11/391 [..............................] - ETA: 18s - loss: 0.9283 - acc: 0.7287\n",
      " Optimizer iteration 1575, batch 11\n",
      "\n",
      " Learning rate 8.850806705317183e-05, Model learning rate 8.850806625559926e-05\n",
      "\n",
      " Optimizer iteration 1576, batch 12\n",
      "\n",
      " Learning rate 8.805100434425845e-05, Model learning rate 8.805100515019149e-05\n",
      " 13/391 [..............................] - ETA: 18s - loss: 0.9226 - acc: 0.7332\n",
      " Optimizer iteration 1577, batch 13\n",
      "\n",
      " Learning rate 8.75950108720951e-05, Model learning rate 8.7595013610553e-05\n",
      " 14/391 [>.............................] - ETA: 18s - loss: 0.9166 - acc: 0.7360\n",
      " Optimizer iteration 1578, batch 14\n",
      "\n",
      " Learning rate 8.714008782023796e-05, Model learning rate 8.714008436072618e-05\n",
      " 15/391 [>.............................] - ETA: 18s - loss: 0.9164 - acc: 0.7365\n",
      " Optimizer iteration 1579, batch 15\n",
      "\n",
      " Learning rate 8.668623636946566e-05, Model learning rate 8.668623922858387e-05\n",
      "\n",
      " Optimizer iteration 1580, batch 16\n",
      "\n",
      " Learning rate 8.623345769777513e-05, Model learning rate 8.623345638625324e-05\n",
      " 17/391 [>.............................] - ETA: 18s - loss: 0.9250 - acc: 0.7348\n",
      " Optimizer iteration 1581, batch 17\n",
      "\n",
      " Learning rate 8.578175298037872e-05, Model learning rate 8.57817503856495e-05\n",
      " 18/391 [>.............................] - ETA: 18s - loss: 0.9366 - acc: 0.7274\n",
      " Optimizer iteration 1582, batch 18\n",
      "\n",
      " Learning rate 8.533112338970156e-05, Model learning rate 8.533112122677267e-05\n",
      "\n",
      " Optimizer iteration 1583, batch 19\n",
      "\n",
      " Learning rate 8.488157009537794e-05, Model learning rate 8.488156890962273e-05\n",
      " 20/391 [>.............................] - ETA: 19s - loss: 0.9378 - acc: 0.7281\n",
      " Optimizer iteration 1584, batch 20\n",
      "\n",
      " Learning rate 8.443309426424861e-05, Model learning rate 8.443309343419969e-05\n",
      " 21/391 [>.............................] - ETA: 19s - loss: 0.9301 - acc: 0.7329\n",
      " Optimizer iteration 1585, batch 21\n",
      "\n",
      " Learning rate 8.398569706035791e-05, Model learning rate 8.398569480050355e-05\n",
      " 22/391 [>.............................] - ETA: 19s - loss: 0.9297 - acc: 0.7330\n",
      " Optimizer iteration 1586, batch 22\n",
      "\n",
      " Learning rate 8.353937964495028e-05, Model learning rate 8.353938028449193e-05\n",
      "\n",
      " Optimizer iteration 1587, batch 23\n",
      "\n",
      " Learning rate 8.309414317646769e-05, Model learning rate 8.30941426102072e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24/391 [>.............................] - ETA: 20s - loss: 0.9338 - acc: 0.7314\n",
      " Optimizer iteration 1588, batch 24\n",
      "\n",
      " Learning rate 8.264998881054659e-05, Model learning rate 8.264998905360699e-05\n",
      " 25/391 [>.............................] - ETA: 20s - loss: 0.9334 - acc: 0.7309\n",
      " Optimizer iteration 1589, batch 25\n",
      "\n",
      " Learning rate 8.220691770001421e-05, Model learning rate 8.220691961469129e-05\n",
      "\n",
      " Optimizer iteration 1590, batch 26\n",
      "\n",
      " Learning rate 8.176493099488664e-05, Model learning rate 8.17649342934601e-05\n",
      " 27/391 [=>............................] - ETA: 20s - loss: 0.9394 - acc: 0.7295\n",
      " Optimizer iteration 1591, batch 27\n",
      "\n",
      " Learning rate 8.132402984236531e-05, Model learning rate 8.132403308991343e-05\n",
      " 28/391 [=>............................] - ETA: 21s - loss: 0.9408 - acc: 0.7291\n",
      " Optimizer iteration 1592, batch 28\n",
      "\n",
      " Learning rate 8.088421538683377e-05, Model learning rate 8.088421600405127e-05\n",
      " 29/391 [=>............................] - ETA: 21s - loss: 0.9383 - acc: 0.7303\n",
      " Optimizer iteration 1593, batch 29\n",
      "\n",
      " Learning rate 8.04454887698553e-05, Model learning rate 8.044549031183124e-05\n",
      " 30/391 [=>............................] - ETA: 21s - loss: 0.9402 - acc: 0.7286\n",
      " Optimizer iteration 1594, batch 30\n",
      "\n",
      " Learning rate 8.000785113016939e-05, Model learning rate 8.000784873729572e-05\n",
      "\n",
      " Optimizer iteration 1595, batch 31\n",
      "\n",
      " Learning rate 7.957130360368897e-05, Model learning rate 7.957130583235994e-05\n",
      " 32/391 [=>............................] - ETA: 21s - loss: 0.9398 - acc: 0.7278\n",
      " Optimizer iteration 1596, batch 32\n",
      "\n",
      " Learning rate 7.913584732349787e-05, Model learning rate 7.913584704510868e-05\n",
      " 33/391 [=>............................] - ETA: 22s - loss: 0.9396 - acc: 0.7270\n",
      " Optimizer iteration 1597, batch 33\n",
      "\n",
      " Learning rate 7.870148341984713e-05, Model learning rate 7.870148692745715e-05\n",
      "\n",
      " Optimizer iteration 1598, batch 34\n",
      "\n",
      " Learning rate 7.826821302015275e-05, Model learning rate 7.826821092749014e-05\n",
      " 35/391 [=>............................] - ETA: 21s - loss: 0.9443 - acc: 0.7254\n",
      " Optimizer iteration 1599, batch 35\n",
      "\n",
      " Learning rate 7.783603724899258e-05, Model learning rate 7.783604087308049e-05\n",
      " 36/391 [=>............................] - ETA: 22s - loss: 0.9517 - acc: 0.7240\n",
      " Optimizer iteration 1600, batch 36\n",
      "\n",
      " Learning rate 7.74049572281027e-05, Model learning rate 7.740495493635535e-05\n",
      " 37/391 [=>............................] - ETA: 21s - loss: 0.9502 - acc: 0.7249\n",
      " Optimizer iteration 1601, batch 37\n",
      "\n",
      " Learning rate 7.697497407637566e-05, Model learning rate 7.697497494518757e-05\n",
      " 38/391 [=>............................] - ETA: 21s - loss: 0.9523 - acc: 0.7253\n",
      " Optimizer iteration 1602, batch 38\n",
      "\n",
      " Learning rate 7.654608890985709e-05, Model learning rate 7.654608634766191e-05\n",
      " 39/391 [=>............................] - ETA: 21s - loss: 0.9501 - acc: 0.7256\n",
      " Optimizer iteration 1603, batch 39\n",
      "\n",
      " Learning rate 7.611830284174221e-05, Model learning rate 7.611830369569361e-05\n",
      " 40/391 [==>...........................] - ETA: 22s - loss: 0.9469 - acc: 0.7262\n",
      " Optimizer iteration 1604, batch 40\n",
      "\n",
      " Learning rate 7.569161698237404e-05, Model learning rate 7.569161971332505e-05\n",
      " 41/391 [==>...........................] - ETA: 22s - loss: 0.9507 - acc: 0.7258\n",
      " Optimizer iteration 1605, batch 41\n",
      "\n",
      " Learning rate 7.526603243923958e-05, Model learning rate 7.526603440055624e-05\n",
      "\n",
      " Optimizer iteration 1606, batch 42\n",
      "\n",
      " Learning rate 7.484155031696727e-05, Model learning rate 7.484154775738716e-05\n",
      " 43/391 [==>...........................] - ETA: 21s - loss: 0.9522 - acc: 0.7238\n",
      " Optimizer iteration 1607, batch 43\n",
      "\n",
      " Learning rate 7.441817171732457e-05, Model learning rate 7.441817433573306e-05\n",
      " 44/391 [==>...........................] - ETA: 22s - loss: 0.9529 - acc: 0.7237\n",
      " Optimizer iteration 1608, batch 44\n",
      "\n",
      " Learning rate 7.399589773921412e-05, Model learning rate 7.399589958367869e-05\n",
      " 45/391 [==>...........................] - ETA: 22s - loss: 0.9542 - acc: 0.7236\n",
      " Optimizer iteration 1609, batch 45\n",
      "\n",
      " Learning rate 7.357472947867188e-05, Model learning rate 7.357473077718168e-05\n",
      "\n",
      " Optimizer iteration 1610, batch 46\n",
      "\n",
      " Learning rate 7.315466802886401e-05, Model learning rate 7.315466791624203e-05\n",
      " 47/391 [==>...........................] - ETA: 21s - loss: 0.9550 - acc: 0.7229\n",
      " Optimizer iteration 1611, batch 47\n",
      "\n",
      " Learning rate 7.273571448008304e-05, Model learning rate 7.273571100085974e-05\n",
      " 48/391 [==>...........................] - ETA: 22s - loss: 0.9555 - acc: 0.7233\n",
      " Optimizer iteration 1612, batch 48\n",
      "\n",
      " Learning rate 7.23178699197467e-05, Model learning rate 7.231786730699241e-05\n",
      " 49/391 [==>...........................] - ETA: 22s - loss: 0.9541 - acc: 0.7232\n",
      " Optimizer iteration 1613, batch 49\n",
      "\n",
      " Learning rate 7.190113543239407e-05, Model learning rate 7.190113683464006e-05\n",
      " 50/391 [==>...........................] - ETA: 22s - loss: 0.9540 - acc: 0.7234\n",
      " Optimizer iteration 1614, batch 50\n",
      "\n",
      " Learning rate 7.148551209968279e-05, Model learning rate 7.148551230784506e-05\n",
      "\n",
      " Optimizer iteration 1615, batch 51\n",
      "\n",
      " Learning rate 7.107100100038672e-05, Model learning rate 7.107100100256503e-05\n",
      " 52/391 [==>...........................] - ETA: 22s - loss: 0.9571 - acc: 0.7231\n",
      " Optimizer iteration 1616, batch 52\n",
      "\n",
      " Learning rate 7.06576032103926e-05, Model learning rate 7.065760291879997e-05\n",
      " 53/391 [===>..........................] - ETA: 22s - loss: 0.9572 - acc: 0.7224\n",
      " Optimizer iteration 1617, batch 53\n",
      "\n",
      " Learning rate 7.024531980269744e-05, Model learning rate 7.024531805654988e-05\n",
      " 54/391 [===>..........................] - ETA: 22s - loss: 0.9583 - acc: 0.7211\n",
      " Optimizer iteration 1618, batch 54\n",
      "\n",
      " Learning rate 6.983415184740616e-05, Model learning rate 6.983415369177237e-05\n",
      "\n",
      " Optimizer iteration 1619, batch 55\n",
      "\n",
      " Learning rate 6.942410041172836e-05, Model learning rate 6.942410254850984e-05\n",
      " 56/391 [===>..........................] - ETA: 22s - loss: 0.9541 - acc: 0.7224\n",
      " Optimizer iteration 1620, batch 56\n",
      "\n",
      " Learning rate 6.901516655997537e-05, Model learning rate 6.901516462676227e-05\n",
      " 57/391 [===>..........................] - ETA: 22s - loss: 0.9550 - acc: 0.7219\n",
      " Optimizer iteration 1621, batch 57\n",
      "\n",
      " Learning rate 6.860735135355811e-05, Model learning rate 6.86073544784449e-05\n",
      " 58/391 [===>..........................] - ETA: 22s - loss: 0.9555 - acc: 0.7221\n",
      " Optimizer iteration 1622, batch 58\n",
      "\n",
      " Learning rate 6.820065585098378e-05, Model learning rate 6.820065755164251e-05\n",
      " 59/391 [===>..........................] - ETA: 21s - loss: 0.9543 - acc: 0.7222\n",
      " Optimizer iteration 1623, batch 59\n",
      "\n",
      " Learning rate 6.779508110785331e-05, Model learning rate 6.77950811223127e-05\n",
      " 60/391 [===>..........................] - ETA: 22s - loss: 0.9538 - acc: 0.7217\n",
      " Optimizer iteration 1624, batch 60\n",
      "\n",
      " Learning rate 6.739062817685892e-05, Model learning rate 6.739062519045547e-05\n",
      " 61/391 [===>..........................] - ETA: 22s - loss: 0.9524 - acc: 0.7222\n",
      " Optimizer iteration 1625, batch 61\n",
      "\n",
      " Learning rate 6.698729810778065e-05, Model learning rate 6.698729703202844e-05\n",
      "\n",
      " Optimizer iteration 1626, batch 62\n",
      "\n",
      " Learning rate 6.658509194748463e-05, Model learning rate 6.658508937107399e-05\n",
      " 63/391 [===>..........................] - ETA: 21s - loss: 0.9556 - acc: 0.7215\n",
      " Optimizer iteration 1627, batch 63\n",
      "\n",
      " Learning rate 6.618401073991936e-05, Model learning rate 6.618400948354974e-05\n",
      " 64/391 [===>..........................] - ETA: 21s - loss: 0.9536 - acc: 0.7224\n",
      " Optimizer iteration 1628, batch 64\n",
      "\n",
      " Learning rate 6.57840555261136e-05, Model learning rate 6.57840573694557e-05\n",
      " 65/391 [===>..........................] - ETA: 22s - loss: 0.9545 - acc: 0.7224\n",
      " Optimizer iteration 1629, batch 65\n",
      "\n",
      " Learning rate 6.538522734417357e-05, Model learning rate 6.538522575283423e-05\n",
      " 66/391 [====>.........................] - ETA: 21s - loss: 0.9544 - acc: 0.7222\n",
      " Optimizer iteration 1630, batch 66\n",
      "\n",
      " Learning rate 6.498752722928042e-05, Model learning rate 6.498752918560058e-05\n",
      "\n",
      " Optimizer iteration 1631, batch 67\n",
      "\n",
      " Learning rate 6.459095621368682e-05, Model learning rate 6.459095311583951e-05\n",
      " 68/391 [====>.........................] - ETA: 21s - loss: 0.9543 - acc: 0.7230\n",
      " Optimizer iteration 1632, batch 68\n",
      "\n",
      " Learning rate 6.419551532671541e-05, Model learning rate 6.419551209546626e-05\n",
      " 69/391 [====>.........................] - ETA: 21s - loss: 0.9534 - acc: 0.7233\n",
      " Optimizer iteration 1633, batch 69\n",
      "\n",
      " Learning rate 6.380120559475506e-05, Model learning rate 6.380120612448081e-05\n",
      " 70/391 [====>.........................] - ETA: 21s - loss: 0.9518 - acc: 0.7237\n",
      " Optimizer iteration 1634, batch 70\n",
      "\n",
      " Learning rate 6.340802804125873e-05, Model learning rate 6.340802792692557e-05\n",
      " 71/391 [====>.........................] - ETA: 21s - loss: 0.9536 - acc: 0.7227\n",
      " Optimizer iteration 1635, batch 71\n",
      "\n",
      " Learning rate 6.301598368674105e-05, Model learning rate 6.301598477875814e-05\n",
      " 72/391 [====>.........................] - ETA: 21s - loss: 0.9575 - acc: 0.7216\n",
      " Optimizer iteration 1636, batch 72\n",
      "\n",
      " Learning rate 6.262507354877494e-05, Model learning rate 6.262507667997852e-05\n",
      " 73/391 [====>.........................] - ETA: 21s - loss: 0.9590 - acc: 0.7222\n",
      " Optimizer iteration 1637, batch 73\n",
      "\n",
      " Learning rate 6.223529864198984e-05, Model learning rate 6.22352963546291e-05\n",
      "\n",
      " Optimizer iteration 1638, batch 74\n",
      "\n",
      " Learning rate 6.184665997806832e-05, Model learning rate 6.18466583546251e-05\n",
      " 75/391 [====>.........................] - ETA: 21s - loss: 0.9615 - acc: 0.7208\n",
      " Optimizer iteration 1639, batch 75\n",
      "\n",
      " Learning rate 6.145915856574363e-05, Model learning rate 6.145915540400892e-05\n",
      " 76/391 [====>.........................] - ETA: 21s - loss: 0.9604 - acc: 0.7214\n",
      " Optimizer iteration 1640, batch 76\n",
      "\n",
      " Learning rate 6.10727954107977e-05, Model learning rate 6.107279477873817e-05\n",
      " 77/391 [====>.........................] - ETA: 21s - loss: 0.9607 - acc: 0.7210\n",
      " Optimizer iteration 1641, batch 77\n",
      "\n",
      " Learning rate 6.0687571516057803e-05, Model learning rate 6.0687572840834036e-05\n",
      " 78/391 [====>.........................] - ETA: 21s - loss: 0.9604 - acc: 0.7212\n",
      " Optimizer iteration 1642, batch 78\n",
      "\n",
      " Learning rate 6.030348788139406e-05, Model learning rate 6.030348959029652e-05\n",
      "\n",
      " Optimizer iteration 1643, batch 79\n",
      "\n",
      " Learning rate 5.9920545503717226e-05, Model learning rate 5.992054502712563e-05\n",
      " 80/391 [=====>........................] - ETA: 21s - loss: 0.9563 - acc: 0.7229\n",
      " Optimizer iteration 1644, batch 80\n",
      "\n",
      " Learning rate 5.9538745376975736e-05, Model learning rate 5.9538746427278966e-05\n",
      " 81/391 [=====>........................] - ETA: 21s - loss: 0.9568 - acc: 0.7224\n",
      " Optimizer iteration 1645, batch 81\n",
      "\n",
      " Learning rate 5.9158088492153036e-05, Model learning rate 5.915809015277773e-05\n",
      " 82/391 [=====>........................] - ETA: 21s - loss: 0.9563 - acc: 0.7226\n",
      " Optimizer iteration 1646, batch 82\n",
      "\n",
      " Learning rate 5.8778575837265754e-05, Model learning rate 5.8778576203621924e-05\n",
      "\n",
      " Optimizer iteration 1647, batch 83\n",
      "\n",
      " Learning rate 5.84002083973601e-05, Model learning rate 5.840020821779035e-05\n",
      " 84/391 [=====>........................] - ETA: 21s - loss: 0.9557 - acc: 0.7222\n",
      " Optimizer iteration 1648, batch 84\n",
      "\n",
      " Learning rate 5.8022987154510154e-05, Model learning rate 5.802298619528301e-05\n",
      "\n",
      " Optimizer iteration 1649, batch 85\n",
      "\n",
      " Learning rate 5.7646913087814725e-05, Model learning rate 5.764691377407871e-05\n",
      " 86/391 [=====>........................] - ETA: 20s - loss: 0.9571 - acc: 0.7215\n",
      " Optimizer iteration 1650, batch 86\n",
      "\n",
      " Learning rate 5.72719871733951e-05, Model learning rate 5.727198731619865e-05\n",
      "\n",
      " Optimizer iteration 1651, batch 87\n",
      "\n",
      " Learning rate 5.6898210384392636e-05, Model learning rate 5.689821045962162e-05\n",
      " 88/391 [=====>........................] - ETA: 21s - loss: 0.9567 - acc: 0.7204\n",
      " Optimizer iteration 1652, batch 88\n",
      "\n",
      " Learning rate 5.6525583690966056e-05, Model learning rate 5.652558320434764e-05\n",
      " 89/391 [=====>........................] - ETA: 20s - loss: 0.9578 - acc: 0.7198\n",
      " Optimizer iteration 1653, batch 89\n",
      "\n",
      " Learning rate 5.6154108060288756e-05, Model learning rate 5.6154109188355505e-05\n",
      " 90/391 [=====>........................] - ETA: 20s - loss: 0.9595 - acc: 0.7191\n",
      " Optimizer iteration 1654, batch 90\n",
      "\n",
      " Learning rate 5.578378445654664e-05, Model learning rate 5.578378477366641e-05\n",
      "\n",
      " Optimizer iteration 1655, batch 91\n",
      "\n",
      " Learning rate 5.541461384093549e-05, Model learning rate 5.5414613598259166e-05\n",
      " 92/391 [======>.......................] - ETA: 20s - loss: 0.9581 - acc: 0.7196\n",
      " Optimizer iteration 1656, batch 92\n",
      "\n",
      " Learning rate 5.5046597171658106e-05, Model learning rate 5.504659566213377e-05\n",
      "\n",
      " Optimizer iteration 1657, batch 93\n",
      "\n",
      " Learning rate 5.467973540392274e-05, Model learning rate 5.4679734603269026e-05\n",
      " 94/391 [======>.......................] - ETA: 20s - loss: 0.9568 - acc: 0.7202\n",
      " Optimizer iteration 1658, batch 94\n",
      "\n",
      " Learning rate 5.4314029489939464e-05, Model learning rate 5.431403042166494e-05\n",
      "\n",
      " Optimizer iteration 1659, batch 95\n",
      "\n",
      " Learning rate 5.394948037891867e-05, Model learning rate 5.39494794793427e-05\n",
      " 96/391 [======>.......................] - ETA: 20s - loss: 0.9570 - acc: 0.7202\n",
      " Optimizer iteration 1660, batch 96\n",
      "\n",
      " Learning rate 5.358608901706802e-05, Model learning rate 5.358608905225992e-05\n",
      " 97/391 [======>.......................] - ETA: 20s - loss: 0.9569 - acc: 0.7198\n",
      " Optimizer iteration 1661, batch 97\n",
      "\n",
      " Learning rate 5.3223856347590084e-05, Model learning rate 5.32238555024378e-05\n",
      "\n",
      " Optimizer iteration 1662, batch 98\n",
      "\n",
      " Learning rate 5.286278331068017e-05, Model learning rate 5.286278246785514e-05\n",
      " 99/391 [======>.......................] - ETA: 20s - loss: 0.9590 - acc: 0.7190\n",
      " Optimizer iteration 1663, batch 99\n",
      "\n",
      " Learning rate 5.250287084352373e-05, Model learning rate 5.250286994851194e-05\n",
      "100/391 [======>.......................] - ETA: 20s - loss: 0.9582 - acc: 0.7193\n",
      " Optimizer iteration 1664, batch 100\n",
      "\n",
      " Learning rate 5.214411988029355e-05, Model learning rate 5.2144121582387015e-05\n",
      "\n",
      " Optimizer iteration 1665, batch 101\n",
      "\n",
      " Learning rate 5.1786531352148115e-05, Model learning rate 5.178653009352274e-05\n",
      "102/391 [======>.......................] - ETA: 20s - loss: 0.9579 - acc: 0.7193\n",
      " Optimizer iteration 1666, batch 102\n",
      "\n",
      " Learning rate 5.1430106187228485e-05, Model learning rate 5.1430106395855546e-05\n",
      "\n",
      " Optimizer iteration 1667, batch 103\n",
      "\n",
      " Learning rate 5.107484531065604e-05, Model learning rate 5.107484685140662e-05\n",
      "104/391 [======>.......................] - ETA: 20s - loss: 0.9567 - acc: 0.7200\n",
      " Optimizer iteration 1668, batch 104\n",
      "\n",
      " Learning rate 5.072074964453055e-05, Model learning rate 5.072075146017596e-05\n",
      "105/391 [=======>......................] - ETA: 19s - loss: 0.9557 - acc: 0.7204\n",
      " Optimizer iteration 1669, batch 105\n",
      "\n",
      " Learning rate 5.0367820107926957e-05, Model learning rate 5.036782022216357e-05\n",
      "106/391 [=======>......................] - ETA: 19s - loss: 0.9567 - acc: 0.7199\n",
      " Optimizer iteration 1670, batch 106\n",
      "\n",
      " Learning rate 5.0016057616893987e-05, Model learning rate 5.001605677534826e-05\n",
      "\n",
      " Optimizer iteration 1671, batch 107\n",
      "\n",
      " Learning rate 4.966546308445074e-05, Model learning rate 4.966546475770883e-05\n",
      "108/391 [=======>......................] - ETA: 19s - loss: 0.9591 - acc: 0.7195\n",
      " Optimizer iteration 1672, batch 108\n",
      "\n",
      " Learning rate 4.9316037420584935e-05, Model learning rate 4.9316036893287674e-05\n",
      "\n",
      " Optimizer iteration 1673, batch 109\n",
      "\n",
      " Learning rate 4.896778153225062e-05, Model learning rate 4.89677804580424e-05\n",
      "110/391 [=======>......................] - ETA: 19s - loss: 0.9604 - acc: 0.7186\n",
      " Optimizer iteration 1674, batch 110\n",
      "\n",
      " Learning rate 4.862069632336558e-05, Model learning rate 4.8620695451973006e-05\n",
      "111/391 [=======>......................] - ETA: 19s - loss: 0.9590 - acc: 0.7188\n",
      " Optimizer iteration 1675, batch 111\n",
      "\n",
      " Learning rate 4.827478269480895e-05, Model learning rate 4.82747818750795e-05\n",
      "112/391 [=======>......................] - ETA: 19s - loss: 0.9598 - acc: 0.7185\n",
      " Optimizer iteration 1676, batch 112\n",
      "\n",
      " Learning rate 4.793004154441877e-05, Model learning rate 4.793003972736187e-05\n",
      "\n",
      " Optimizer iteration 1677, batch 113\n",
      "\n",
      " Learning rate 4.758647376699032e-05, Model learning rate 4.758647264679894e-05\n",
      "114/391 [=======>......................] - ETA: 19s - loss: 0.9575 - acc: 0.7191\n",
      " Optimizer iteration 1678, batch 114\n",
      "\n",
      " Learning rate 4.7244080254272795e-05, Model learning rate 4.7244080633390695e-05\n",
      "115/391 [=======>......................] - ETA: 19s - loss: 0.9574 - acc: 0.7193\n",
      " Optimizer iteration 1679, batch 115\n",
      "\n",
      " Learning rate 4.690286189496795e-05, Model learning rate 4.690286368713714e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/391 [=======>......................] - ETA: 19s - loss: 0.9576 - acc: 0.7190\n",
      " Optimizer iteration 1680, batch 116\n",
      "\n",
      " Learning rate 4.65628195747273e-05, Model learning rate 4.656281817005947e-05\n",
      "\n",
      " Optimizer iteration 1681, batch 117\n",
      "\n",
      " Learning rate 4.6223954176149606e-05, Model learning rate 4.622395499609411e-05\n",
      "118/391 [========>.....................] - ETA: 19s - loss: 0.9557 - acc: 0.7193\n",
      " Optimizer iteration 1682, batch 118\n",
      "\n",
      " Learning rate 4.588626657877898e-05, Model learning rate 4.5886266889283434e-05\n",
      "119/391 [========>.....................] - ETA: 19s - loss: 0.9556 - acc: 0.7193\n",
      " Optimizer iteration 1683, batch 119\n",
      "\n",
      " Learning rate 4.5549757659102795e-05, Model learning rate 4.554975748760626e-05\n",
      "120/391 [========>.....................] - ETA: 19s - loss: 0.9561 - acc: 0.7191\n",
      " Optimizer iteration 1684, batch 120\n",
      "\n",
      " Learning rate 4.521442829054856e-05, Model learning rate 4.521442679106258e-05\n",
      "121/391 [========>.....................] - ETA: 19s - loss: 0.9558 - acc: 0.7192\n",
      " Optimizer iteration 1685, batch 121\n",
      "\n",
      " Learning rate 4.488027934348271e-05, Model learning rate 4.4880278437631205e-05\n",
      "122/391 [========>.....................] - ETA: 18s - loss: 0.9550 - acc: 0.7196\n",
      " Optimizer iteration 1686, batch 122\n",
      "\n",
      " Learning rate 4.4547311685207536e-05, Model learning rate 4.4547312427312136e-05\n",
      "123/391 [========>.....................] - ETA: 18s - loss: 0.9538 - acc: 0.7197\n",
      " Optimizer iteration 1687, batch 123\n",
      "\n",
      " Learning rate 4.4215526179959165e-05, Model learning rate 4.4215525122126564e-05\n",
      "124/391 [========>.....................] - ETA: 18s - loss: 0.9531 - acc: 0.7201\n",
      " Optimizer iteration 1688, batch 124\n",
      "\n",
      " Learning rate 4.388492368890568e-05, Model learning rate 4.3884923798032105e-05\n",
      "125/391 [========>.....................] - ETA: 18s - loss: 0.9521 - acc: 0.7204\n",
      " Optimizer iteration 1689, batch 125\n",
      "\n",
      " Learning rate 4.3555505070144276e-05, Model learning rate 4.355550481704995e-05\n",
      "126/391 [========>.....................] - ETA: 18s - loss: 0.9512 - acc: 0.7205\n",
      " Optimizer iteration 1690, batch 126\n",
      "\n",
      " Learning rate 4.322727117869951e-05, Model learning rate 4.322727181715891e-05\n",
      "127/391 [========>.....................] - ETA: 18s - loss: 0.9518 - acc: 0.7207\n",
      " Optimizer iteration 1691, batch 127\n",
      "\n",
      " Learning rate 4.2900222866521014e-05, Model learning rate 4.290022116038017e-05\n",
      "128/391 [========>.....................] - ETA: 18s - loss: 0.9514 - acc: 0.7209\n",
      " Optimizer iteration 1692, batch 128\n",
      "\n",
      " Learning rate 4.257436098248091e-05, Model learning rate 4.257436012267135e-05\n",
      "\n",
      " Optimizer iteration 1693, batch 129\n",
      "\n",
      " Learning rate 4.224968637237198e-05, Model learning rate 4.2249685066053644e-05\n",
      "130/391 [========>.....................] - ETA: 18s - loss: 0.9503 - acc: 0.7210\n",
      " Optimizer iteration 1694, batch 130\n",
      "\n",
      " Learning rate 4.192619987890556e-05, Model learning rate 4.1926199628505856e-05\n",
      "\n",
      " Optimizer iteration 1695, batch 131\n",
      "\n",
      " Learning rate 4.16039023417088e-05, Model learning rate 4.160390381002799e-05\n",
      "132/391 [=========>....................] - ETA: 18s - loss: 0.9511 - acc: 0.7205\n",
      " Optimizer iteration 1696, batch 132\n",
      "\n",
      " Learning rate 4.128279459732326e-05, Model learning rate 4.128279397264123e-05\n",
      "\n",
      " Optimizer iteration 1697, batch 133\n",
      "\n",
      " Learning rate 4.096287747920202e-05, Model learning rate 4.09628773923032e-05\n",
      "134/391 [=========>....................] - ETA: 18s - loss: 0.9495 - acc: 0.7210\n",
      " Optimizer iteration 1698, batch 134\n",
      "\n",
      " Learning rate 4.0644151817707866e-05, Model learning rate 4.0644150431035087e-05\n",
      "\n",
      " Optimizer iteration 1699, batch 135\n",
      "\n",
      " Learning rate 4.0326618440111316e-05, Model learning rate 4.03266167268157e-05\n",
      "136/391 [=========>....................] - ETA: 18s - loss: 0.9497 - acc: 0.7210\n",
      " Optimizer iteration 1700, batch 136\n",
      "\n",
      " Learning rate 4.001027817058789e-05, Model learning rate 4.001027991762385e-05\n",
      "\n",
      " Optimizer iteration 1701, batch 137\n",
      "\n",
      " Learning rate 3.9695131830216616e-05, Model learning rate 3.9695132727501914e-05\n",
      "138/391 [=========>....................] - ETA: 17s - loss: 0.9495 - acc: 0.7207\n",
      " Optimizer iteration 1702, batch 138\n",
      "\n",
      " Learning rate 3.938118023697762e-05, Model learning rate 3.9381178794428706e-05\n",
      "139/391 [=========>....................] - ETA: 17s - loss: 0.9502 - acc: 0.7206\n",
      " Optimizer iteration 1703, batch 139\n",
      "\n",
      " Learning rate 3.90684242057498e-05, Model learning rate 3.906842539436184e-05\n",
      "140/391 [=========>....................] - ETA: 17s - loss: 0.9509 - acc: 0.7203\n",
      " Optimizer iteration 1704, batch 140\n",
      "\n",
      " Learning rate 3.8756864548308845e-05, Model learning rate 3.87568652513437e-05\n",
      "141/391 [=========>....................] - ETA: 17s - loss: 0.9506 - acc: 0.7203\n",
      " Optimizer iteration 1705, batch 141\n",
      "\n",
      " Learning rate 3.844650207332562e-05, Model learning rate 3.844650200335309e-05\n",
      "\n",
      " Optimizer iteration 1706, batch 142\n",
      "\n",
      " Learning rate 3.8137337586363063e-05, Model learning rate 3.813733928836882e-05\n",
      "143/391 [=========>....................] - ETA: 17s - loss: 0.9507 - acc: 0.7198\n",
      " Optimizer iteration 1707, batch 143\n",
      "\n",
      " Learning rate 3.782937188987523e-05, Model learning rate 3.7829373468412086e-05\n",
      "144/391 [==========>...................] - ETA: 17s - loss: 0.9510 - acc: 0.7197\n",
      " Optimizer iteration 1708, batch 144\n",
      "\n",
      " Learning rate 3.7522605783204264e-05, Model learning rate 3.7522604543482885e-05\n",
      "145/391 [==========>...................] - ETA: 17s - loss: 0.9511 - acc: 0.7196\n",
      " Optimizer iteration 1709, batch 145\n",
      "\n",
      " Learning rate 3.7217040062578756e-05, Model learning rate 3.721703978953883e-05\n",
      "146/391 [==========>...................] - ETA: 17s - loss: 0.9499 - acc: 0.7200\n",
      " Optimizer iteration 1710, batch 146\n",
      "\n",
      " Learning rate 3.691267552111183e-05, Model learning rate 3.6912675568601117e-05\n",
      "\n",
      " Optimizer iteration 1711, batch 147\n",
      "\n",
      " Learning rate 3.660951294879855e-05, Model learning rate 3.660951188066974e-05\n",
      "148/391 [==========>...................] - ETA: 17s - loss: 0.9491 - acc: 0.7205\n",
      " Optimizer iteration 1712, batch 148\n",
      "\n",
      " Learning rate 3.6307553132514546e-05, Model learning rate 3.6307552363723516e-05\n",
      "\n",
      " Optimizer iteration 1713, batch 149\n",
      "\n",
      " Learning rate 3.6006796856013493e-05, Model learning rate 3.600679701776244e-05\n",
      "150/391 [==========>...................] - ETA: 17s - loss: 0.9495 - acc: 0.7203\n",
      " Optimizer iteration 1714, batch 150\n",
      "\n",
      " Learning rate 3.5707244899925164e-05, Model learning rate 3.5707245842786506e-05\n",
      "\n",
      " Optimizer iteration 1715, batch 151\n",
      "\n",
      " Learning rate 3.54088980417534e-05, Model learning rate 3.540889883879572e-05\n",
      "152/391 [==========>...................] - ETA: 17s - loss: 0.9493 - acc: 0.7199\n",
      " Optimizer iteration 1716, batch 152\n",
      "\n",
      " Learning rate 3.5111757055874326e-05, Model learning rate 3.5111756005790085e-05\n",
      "\n",
      " Optimizer iteration 1717, batch 153\n",
      "\n",
      " Learning rate 3.4815822713533954e-05, Model learning rate 3.48158209817484e-05\n",
      "154/391 [==========>...................] - ETA: 16s - loss: 0.9502 - acc: 0.7198\n",
      " Optimizer iteration 1718, batch 154\n",
      "\n",
      " Learning rate 3.4521095782846624e-05, Model learning rate 3.452109740464948e-05\n",
      "155/391 [==========>...................] - ETA: 16s - loss: 0.9503 - acc: 0.7198\n",
      " Optimizer iteration 1719, batch 155\n",
      "\n",
      " Learning rate 3.422757702879259e-05, Model learning rate 3.422757799853571e-05\n",
      "156/391 [==========>...................] - ETA: 16s - loss: 0.9499 - acc: 0.7201\n",
      " Optimizer iteration 1720, batch 156\n",
      "\n",
      " Learning rate 3.393526721321616e-05, Model learning rate 3.393526640138589e-05\n",
      "157/391 [===========>..................] - ETA: 16s - loss: 0.9494 - acc: 0.7202\n",
      " Optimizer iteration 1721, batch 157\n",
      "\n",
      " Learning rate 3.3644167094823985e-05, Model learning rate 3.364416625117883e-05\n",
      "\n",
      " Optimizer iteration 1722, batch 158\n",
      "\n",
      " Learning rate 3.335427742918262e-05, Model learning rate 3.3354277547914535e-05\n",
      "159/391 [===========>..................] - ETA: 16s - loss: 0.9491 - acc: 0.7205\n",
      " Optimizer iteration 1723, batch 159\n",
      "\n",
      " Learning rate 3.3065598968717137e-05, Model learning rate 3.3065600291593e-05\n",
      "160/391 [===========>..................] - ETA: 16s - loss: 0.9486 - acc: 0.7209\n",
      " Optimizer iteration 1724, batch 160\n",
      "\n",
      " Learning rate 3.277813246270872e-05, Model learning rate 3.277813084423542e-05\n",
      "161/391 [===========>..................] - ETA: 16s - loss: 0.9493 - acc: 0.7205\n",
      " Optimizer iteration 1725, batch 161\n",
      "\n",
      " Learning rate 3.249187865729264e-05, Model learning rate 3.2491880119778216e-05\n",
      "\n",
      " Optimizer iteration 1726, batch 162\n",
      "\n",
      " Learning rate 3.220683829545678e-05, Model learning rate 3.2206837204284966e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/391 [===========>..................] - ETA: 16s - loss: 0.9479 - acc: 0.7213\n",
      " Optimizer iteration 1727, batch 163\n",
      "\n",
      " Learning rate 3.192301211703952e-05, Model learning rate 3.192301301169209e-05\n",
      "164/391 [===========>..................] - ETA: 16s - loss: 0.9474 - acc: 0.7217\n",
      " Optimizer iteration 1728, batch 164\n",
      "\n",
      " Learning rate 3.164040085872755e-05, Model learning rate 3.164040026604198e-05\n",
      "165/391 [===========>..................] - ETA: 16s - loss: 0.9476 - acc: 0.7217\n",
      " Optimizer iteration 1729, batch 165\n",
      "\n",
      " Learning rate 3.1359005254054274e-05, Model learning rate 3.135900624329224e-05\n",
      "\n",
      " Optimizer iteration 1730, batch 166\n",
      "\n",
      " Learning rate 3.107882603339785e-05, Model learning rate 3.1078827305464074e-05\n",
      "167/391 [===========>..................] - ETA: 15s - loss: 0.9468 - acc: 0.7221\n",
      " Optimizer iteration 1731, batch 167\n",
      "\n",
      " Learning rate 3.079986392397899e-05, Model learning rate 3.0799863452557474e-05\n",
      "168/391 [===========>..................] - ETA: 15s - loss: 0.9464 - acc: 0.7226\n",
      " Optimizer iteration 1732, batch 168\n",
      "\n",
      " Learning rate 3.052211964985974e-05, Model learning rate 3.052211832255125e-05\n",
      "169/391 [===========>..................] - ETA: 15s - loss: 0.9457 - acc: 0.7225\n",
      " Optimizer iteration 1733, batch 169\n",
      "\n",
      " Learning rate 3.024559393194076e-05, Model learning rate 3.0245593734434806e-05\n",
      "170/391 [============>.................] - ETA: 15s - loss: 0.9445 - acc: 0.7230\n",
      " Optimizer iteration 1734, batch 170\n",
      "\n",
      " Learning rate 2.9970287487960158e-05, Model learning rate 2.9970287869218737e-05\n",
      "\n",
      " Optimizer iteration 1735, batch 171\n",
      "\n",
      " Learning rate 2.9696201032491433e-05, Model learning rate 2.9696200726903044e-05\n",
      "172/391 [============>.................] - ETA: 15s - loss: 0.9442 - acc: 0.7229\n",
      " Optimizer iteration 1736, batch 172\n",
      "\n",
      " Learning rate 2.942333527694113e-05, Model learning rate 2.9423335945466533e-05\n",
      "173/391 [============>.................] - ETA: 15s - loss: 0.9441 - acc: 0.7228\n",
      " Optimizer iteration 1737, batch 173\n",
      "\n",
      " Learning rate 2.9151690929547726e-05, Model learning rate 2.9151691705919802e-05\n",
      "\n",
      " Optimizer iteration 1738, batch 174\n",
      "\n",
      " Learning rate 2.8881268695379436e-05, Model learning rate 2.888126800826285e-05\n",
      "175/391 [============>.................] - ETA: 15s - loss: 0.9448 - acc: 0.7226\n",
      " Optimizer iteration 1739, batch 175\n",
      "\n",
      " Learning rate 2.8612069276332253e-05, Model learning rate 2.8612068490474485e-05\n",
      "176/391 [============>.................] - ETA: 15s - loss: 0.9448 - acc: 0.7227\n",
      " Optimizer iteration 1740, batch 176\n",
      "\n",
      " Learning rate 2.834409337112842e-05, Model learning rate 2.8344093152554706e-05\n",
      "177/391 [============>.................] - ETA: 15s - loss: 0.9454 - acc: 0.7223\n",
      " Optimizer iteration 1741, batch 177\n",
      "\n",
      " Learning rate 2.807734167531456e-05, Model learning rate 2.8077341994503513e-05\n",
      "\n",
      " Optimizer iteration 1742, batch 178\n",
      "\n",
      " Learning rate 2.78118148812595e-05, Model learning rate 2.7811815016320907e-05\n",
      "179/391 [============>.................] - ETA: 15s - loss: 0.9462 - acc: 0.7220\n",
      " Optimizer iteration 1743, batch 179\n",
      "\n",
      " Learning rate 2.7547513678153e-05, Model learning rate 2.754751403699629e-05\n",
      "180/391 [============>.................] - ETA: 15s - loss: 0.9471 - acc: 0.7215\n",
      " Optimizer iteration 1744, batch 180\n",
      "\n",
      " Learning rate 2.7284438752003758e-05, Model learning rate 2.7284439056529664e-05\n",
      "181/391 [============>.................] - ETA: 15s - loss: 0.9471 - acc: 0.7216\n",
      " Optimizer iteration 1745, batch 181\n",
      "\n",
      " Learning rate 2.70225907856374e-05, Model learning rate 2.7022590074921027e-05\n",
      "\n",
      " Optimizer iteration 1746, batch 182\n",
      "\n",
      " Learning rate 2.6761970458695107e-05, Model learning rate 2.6761970730149187e-05\n",
      "183/391 [=============>................] - ETA: 14s - loss: 0.9473 - acc: 0.7215\n",
      " Optimizer iteration 1747, batch 183\n",
      "\n",
      " Learning rate 2.65025784476316e-05, Model learning rate 2.650257920322474e-05\n",
      "184/391 [=============>................] - ETA: 14s - loss: 0.9478 - acc: 0.7212\n",
      " Optimizer iteration 1748, batch 184\n",
      "\n",
      " Learning rate 2.6244415425713264e-05, Model learning rate 2.6244415494147688e-05\n",
      "185/391 [=============>................] - ETA: 14s - loss: 0.9482 - acc: 0.7211\n",
      " Optimizer iteration 1749, batch 185\n",
      "\n",
      " Learning rate 2.598748206301682e-05, Model learning rate 2.5987481421907432e-05\n",
      "\n",
      " Optimizer iteration 1750, batch 186\n",
      "\n",
      " Learning rate 2.573177902642726e-05, Model learning rate 2.5731778805493377e-05\n",
      "187/391 [=============>................] - ETA: 14s - loss: 0.9481 - acc: 0.7210\n",
      " Optimizer iteration 1751, batch 187\n",
      "\n",
      " Learning rate 2.547730697963607e-05, Model learning rate 2.5477307644905522e-05\n",
      "188/391 [=============>................] - ETA: 14s - loss: 0.9479 - acc: 0.7210\n",
      " Optimizer iteration 1752, batch 188\n",
      "\n",
      " Learning rate 2.522406658313997e-05, Model learning rate 2.5224066121154465e-05\n",
      "189/391 [=============>................] - ETA: 14s - loss: 0.9471 - acc: 0.7213\n",
      " Optimizer iteration 1753, batch 189\n",
      "\n",
      " Learning rate 2.4972058494238337e-05, Model learning rate 2.497205787221901e-05\n",
      "\n",
      " Optimizer iteration 1754, batch 190\n",
      "\n",
      " Learning rate 2.4721283367032387e-05, Model learning rate 2.472128289809916e-05\n",
      "191/391 [=============>................] - ETA: 14s - loss: 0.9473 - acc: 0.7208\n",
      " Optimizer iteration 1755, batch 191\n",
      "\n",
      " Learning rate 2.4471741852423235e-05, Model learning rate 2.4471741198794916e-05\n",
      "192/391 [=============>................] - ETA: 14s - loss: 0.9465 - acc: 0.7210\n",
      " Optimizer iteration 1756, batch 192\n",
      "\n",
      " Learning rate 2.422343459810966e-05, Model learning rate 2.422343459329568e-05\n",
      "193/391 [=============>................] - ETA: 14s - loss: 0.9464 - acc: 0.7210\n",
      " Optimizer iteration 1757, batch 193\n",
      "\n",
      " Learning rate 2.3976362248587293e-05, Model learning rate 2.397636308160145e-05\n",
      "\n",
      " Optimizer iteration 1758, batch 194\n",
      "\n",
      " Learning rate 2.3730525445146145e-05, Model learning rate 2.3730524844722822e-05\n",
      "195/391 [=============>................] - ETA: 13s - loss: 0.9461 - acc: 0.7214\n",
      " Optimizer iteration 1759, batch 195\n",
      "\n",
      " Learning rate 2.348592482586942e-05, Model learning rate 2.348592533962801e-05\n",
      "196/391 [==============>...............] - ETA: 13s - loss: 0.9465 - acc: 0.7215\n",
      " Optimizer iteration 1760, batch 196\n",
      "\n",
      " Learning rate 2.324256102563188e-05, Model learning rate 2.3242560928338207e-05\n",
      "197/391 [==============>...............] - ETA: 13s - loss: 0.9468 - acc: 0.7211\n",
      " Optimizer iteration 1761, batch 197\n",
      "\n",
      " Learning rate 2.3000434676097803e-05, Model learning rate 2.300043524883222e-05\n",
      "198/391 [==============>...............] - ETA: 13s - loss: 0.9463 - acc: 0.7216\n",
      " Optimizer iteration 1762, batch 198\n",
      "\n",
      " Learning rate 2.275954640571981e-05, Model learning rate 2.275954648212064e-05\n",
      "\n",
      " Optimizer iteration 1763, batch 199\n",
      "\n",
      " Learning rate 2.2519896839737097e-05, Model learning rate 2.2519896447192878e-05\n",
      "200/391 [==============>...............] - ETA: 13s - loss: 0.9463 - acc: 0.7215\n",
      " Optimizer iteration 1764, batch 200\n",
      "\n",
      " Learning rate 2.2281486600173206e-05, Model learning rate 2.2281486963038333e-05\n",
      "201/391 [==============>...............] - ETA: 13s - loss: 0.9464 - acc: 0.7216\n",
      " Optimizer iteration 1765, batch 201\n",
      "\n",
      " Learning rate 2.2044316305835478e-05, Model learning rate 2.2044316210667603e-05\n",
      "\n",
      " Optimizer iteration 1766, batch 202\n",
      "\n",
      " Learning rate 2.180838657231282e-05, Model learning rate 2.180838600907009e-05\n",
      "203/391 [==============>...............] - ETA: 13s - loss: 0.9467 - acc: 0.7217\n",
      " Optimizer iteration 1767, batch 203\n",
      "\n",
      " Learning rate 2.1573698011973954e-05, Model learning rate 2.15736981772352e-05\n",
      "204/391 [==============>...............] - ETA: 13s - loss: 0.9469 - acc: 0.7214\n",
      " Optimizer iteration 1768, batch 204\n",
      "\n",
      " Learning rate 2.134025123396638e-05, Model learning rate 2.134025089617353e-05\n",
      "205/391 [==============>...............] - ETA: 13s - loss: 0.9467 - acc: 0.7215\n",
      " Optimizer iteration 1769, batch 205\n",
      "\n",
      " Learning rate 2.1108046844214192e-05, Model learning rate 2.110804598487448e-05\n",
      "\n",
      " Optimizer iteration 1770, batch 206\n",
      "\n",
      " Learning rate 2.087708544541689e-05, Model learning rate 2.0877085262327455e-05\n",
      "207/391 [==============>...............] - ETA: 13s - loss: 0.9469 - acc: 0.7217\n",
      " Optimizer iteration 1771, batch 207\n",
      "\n",
      " Learning rate 2.0647367637047887e-05, Model learning rate 2.0647366909543052e-05\n",
      "208/391 [==============>...............] - ETA: 13s - loss: 0.9466 - acc: 0.7219\n",
      " Optimizer iteration 1772, batch 208\n",
      "\n",
      " Learning rate 2.041889401535252e-05, Model learning rate 2.041889456450008e-05\n",
      "209/391 [===============>..............] - ETA: 13s - loss: 0.9465 - acc: 0.7219\n",
      " Optimizer iteration 1773, batch 209\n",
      "\n",
      " Learning rate 2.019166517334703e-05, Model learning rate 2.0191664589219727e-05\n",
      "\n",
      " Optimizer iteration 1774, batch 210\n",
      "\n",
      " Learning rate 1.9965681700816584e-05, Model learning rate 1.9965682440670207e-05\n",
      "211/391 [===============>..............] - ETA: 12s - loss: 0.9462 - acc: 0.7219\n",
      " Optimizer iteration 1775, batch 211\n",
      "\n",
      " Learning rate 1.974094418431388e-05, Model learning rate 1.9740944480872713e-05\n",
      "212/391 [===============>..............] - ETA: 12s - loss: 0.9462 - acc: 0.7220\n",
      " Optimizer iteration 1776, batch 212\n",
      "\n",
      " Learning rate 1.9517453207157864e-05, Model learning rate 1.9517452528816648e-05\n",
      "213/391 [===============>..............] - ETA: 12s - loss: 0.9458 - acc: 0.7221\n",
      " Optimizer iteration 1777, batch 213\n",
      "\n",
      " Learning rate 1.929520934943191e-05, Model learning rate 1.929521022248082e-05\n",
      "\n",
      " Optimizer iteration 1778, batch 214\n",
      "\n",
      " Learning rate 1.9074213187982415e-05, Model learning rate 1.907421392388642e-05\n",
      "215/391 [===============>..............] - ETA: 12s - loss: 0.9457 - acc: 0.7223\n",
      " Optimizer iteration 1779, batch 215\n",
      "\n",
      " Learning rate 1.885446529641732e-05, Model learning rate 1.885446545202285e-05\n",
      "216/391 [===============>..............] - ETA: 12s - loss: 0.9455 - acc: 0.7225\n",
      " Optimizer iteration 1780, batch 216\n",
      "\n",
      " Learning rate 1.8635966245104663e-05, Model learning rate 1.863596662587952e-05\n",
      "217/391 [===============>..............] - ETA: 12s - loss: 0.9459 - acc: 0.7224\n",
      " Optimizer iteration 1781, batch 217\n",
      "\n",
      " Learning rate 1.841871660117095e-05, Model learning rate 1.8418717445456423e-05\n",
      "\n",
      " Optimizer iteration 1782, batch 218\n",
      "\n",
      " Learning rate 1.820271692849984e-05, Model learning rate 1.820271609176416e-05\n",
      "219/391 [===============>..............] - ETA: 12s - loss: 0.9451 - acc: 0.7231\n",
      " Optimizer iteration 1783, batch 219\n",
      "\n",
      " Learning rate 1.7987967787730542e-05, Model learning rate 1.798796802177094e-05\n",
      "220/391 [===============>..............] - ETA: 12s - loss: 0.9451 - acc: 0.7229\n",
      " Optimizer iteration 1784, batch 220\n",
      "\n",
      " Learning rate 1.7774469736256683e-05, Model learning rate 1.7774469597497955e-05\n",
      "221/391 [===============>..............] - ETA: 12s - loss: 0.9447 - acc: 0.7230\n",
      " Optimizer iteration 1785, batch 221\n",
      "\n",
      " Learning rate 1.7562223328224324e-05, Model learning rate 1.756222263793461e-05\n",
      "\n",
      " Optimizer iteration 1786, batch 222\n",
      "\n",
      " Learning rate 1.735122911453091e-05, Model learning rate 1.735122896207031e-05\n",
      "223/391 [================>.............] - ETA: 12s - loss: 0.9445 - acc: 0.7231\n",
      " Optimizer iteration 1787, batch 223\n",
      "\n",
      " Learning rate 1.7141487642823806e-05, Model learning rate 1.7141486750915647e-05\n",
      "224/391 [================>.............] - ETA: 12s - loss: 0.9452 - acc: 0.7228\n",
      " Optimizer iteration 1788, batch 224\n",
      "\n",
      " Learning rate 1.693299945749882e-05, Model learning rate 1.693299964244943e-05\n",
      "225/391 [================>.............] - ETA: 11s - loss: 0.9452 - acc: 0.7227\n",
      " Optimizer iteration 1789, batch 225\n",
      "\n",
      " Learning rate 1.6725765099698698e-05, Model learning rate 1.672576581768226e-05\n",
      "\n",
      " Optimizer iteration 1790, batch 226\n",
      "\n",
      " Learning rate 1.651978510731189e-05, Model learning rate 1.651978527661413e-05\n",
      "227/391 [================>.............] - ETA: 11s - loss: 0.9447 - acc: 0.7228\n",
      " Optimizer iteration 1791, batch 227\n",
      "\n",
      " Learning rate 1.6315060014971016e-05, Model learning rate 1.6315059838234447e-05\n",
      "228/391 [================>.............] - ETA: 11s - loss: 0.9448 - acc: 0.7229\n",
      " Optimizer iteration 1792, batch 228\n",
      "\n",
      " Learning rate 1.6111590354051464e-05, Model learning rate 1.611158950254321e-05\n",
      "229/391 [================>.............] - ETA: 11s - loss: 0.9448 - acc: 0.7227\n",
      " Optimizer iteration 1793, batch 229\n",
      "\n",
      " Learning rate 1.5909376652670282e-05, Model learning rate 1.5909376088529825e-05\n",
      "\n",
      " Optimizer iteration 1794, batch 230\n",
      "\n",
      " Learning rate 1.5708419435684463e-05, Model learning rate 1.570841959619429e-05\n",
      "231/391 [================>.............] - ETA: 11s - loss: 0.9440 - acc: 0.7229\n",
      " Optimizer iteration 1795, batch 231\n",
      "\n",
      " Learning rate 1.5508719224689714e-05, Model learning rate 1.5508720025536604e-05\n",
      "232/391 [================>.............] - ETA: 11s - loss: 0.9445 - acc: 0.7228\n",
      " Optimizer iteration 1796, batch 232\n",
      "\n",
      " Learning rate 1.5310276538019196e-05, Model learning rate 1.531027737655677e-05\n",
      "233/391 [================>.............] - ETA: 11s - loss: 0.9444 - acc: 0.7226\n",
      " Optimizer iteration 1797, batch 233\n",
      "\n",
      " Learning rate 1.5113091890741948e-05, Model learning rate 1.5113091649254784e-05\n",
      "234/391 [================>.............] - ETA: 11s - loss: 0.9447 - acc: 0.7224\n",
      " Optimizer iteration 1798, batch 234\n",
      "\n",
      " Learning rate 1.4917165794661846e-05, Model learning rate 1.4917165572114754e-05\n",
      "\n",
      " Optimizer iteration 1799, batch 235\n",
      "\n",
      " Learning rate 1.4722498758316161e-05, Model learning rate 1.472249914513668e-05\n",
      "236/391 [=================>............] - ETA: 11s - loss: 0.9443 - acc: 0.7226\n",
      " Optimizer iteration 1800, batch 236\n",
      "\n",
      " Learning rate 1.4529091286973995e-05, Model learning rate 1.452909145882586e-05\n",
      "237/391 [=================>............] - ETA: 11s - loss: 0.9440 - acc: 0.7227\n",
      " Optimizer iteration 1801, batch 237\n",
      "\n",
      " Learning rate 1.4336943882635345e-05, Model learning rate 1.4336944332171697e-05\n",
      "\n",
      " Optimizer iteration 1802, batch 238\n",
      "\n",
      " Learning rate 1.414605704402966e-05, Model learning rate 1.414605685567949e-05\n",
      "239/391 [=================>............] - ETA: 10s - loss: 0.9444 - acc: 0.7228\n",
      " Optimizer iteration 1803, batch 239\n",
      "\n",
      " Learning rate 1.3956431266614278e-05, Model learning rate 1.3956430848338641e-05\n",
      "240/391 [=================>............] - ETA: 10s - loss: 0.9444 - acc: 0.7227\n",
      " Optimizer iteration 1804, batch 240\n",
      "\n",
      " Learning rate 1.3768067042573662e-05, Model learning rate 1.3768067219643854e-05\n",
      "241/391 [=================>............] - ETA: 10s - loss: 0.9437 - acc: 0.7230\n",
      " Optimizer iteration 1805, batch 241\n",
      "\n",
      " Learning rate 1.3580964860817779e-05, Model learning rate 1.3580965060100425e-05\n",
      "242/391 [=================>............] - ETA: 10s - loss: 0.9436 - acc: 0.7228\n",
      " Optimizer iteration 1806, batch 242\n",
      "\n",
      " Learning rate 1.3395125206980774e-05, Model learning rate 1.3395125279203057e-05\n",
      "\n",
      " Optimizer iteration 1807, batch 243\n",
      "\n",
      " Learning rate 1.3210548563419855e-05, Model learning rate 1.3210548786446452e-05\n",
      "244/391 [=================>............] - ETA: 10s - loss: 0.9435 - acc: 0.7231\n",
      " Optimizer iteration 1808, batch 244\n",
      "\n",
      " Learning rate 1.3027235409214189e-05, Model learning rate 1.302723558183061e-05\n",
      "245/391 [=================>............] - ETA: 10s - loss: 0.9427 - acc: 0.7234\n",
      " Optimizer iteration 1809, batch 245\n",
      "\n",
      " Learning rate 1.2845186220163286e-05, Model learning rate 1.2845186574850231e-05\n",
      "\n",
      " Optimizer iteration 1810, batch 246\n",
      "\n",
      " Learning rate 1.2664401468786114e-05, Model learning rate 1.2664401765505318e-05\n",
      "247/391 [=================>............] - ETA: 10s - loss: 0.9427 - acc: 0.7232\n",
      " Optimizer iteration 1811, batch 247\n",
      "\n",
      " Learning rate 1.2484881624319489e-05, Model learning rate 1.248488206329057e-05\n",
      "248/391 [==================>...........] - ETA: 10s - loss: 0.9429 - acc: 0.7231\n",
      " Optimizer iteration 1812, batch 248\n",
      "\n",
      " Learning rate 1.2306627152717408e-05, Model learning rate 1.2306627468205988e-05\n",
      "249/391 [==================>...........] - ETA: 10s - loss: 0.9426 - acc: 0.7232\n",
      " Optimizer iteration 1813, batch 249\n",
      "\n",
      " Learning rate 1.2129638516649278e-05, Model learning rate 1.2129638889746275e-05\n",
      "\n",
      " Optimizer iteration 1814, batch 250\n",
      "\n",
      " Learning rate 1.1953916175499068e-05, Model learning rate 1.1953916327911429e-05\n",
      "251/391 [==================>...........] - ETA: 10s - loss: 0.9421 - acc: 0.7234\n",
      " Optimizer iteration 1815, batch 251\n",
      "\n",
      " Learning rate 1.1779460585363943e-05, Model learning rate 1.1779460692196153e-05\n",
      "252/391 [==================>...........] - ETA: 10s - loss: 0.9423 - acc: 0.7232\n",
      " Optimizer iteration 1816, batch 252\n",
      "\n",
      " Learning rate 1.1606272199053247e-05, Model learning rate 1.1606271982600447e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253/391 [==================>...........] - ETA: 9s - loss: 0.9426 - acc: 0.7231 \n",
      " Optimizer iteration 1817, batch 253\n",
      "\n",
      " Learning rate 1.1434351466087178e-05, Model learning rate 1.1434351108619012e-05\n",
      "254/391 [==================>...........] - ETA: 9s - loss: 0.9426 - acc: 0.7232\n",
      " Optimizer iteration 1818, batch 254\n",
      "\n",
      " Learning rate 1.1263698832695512e-05, Model learning rate 1.126369897974655e-05\n",
      "255/391 [==================>...........] - ETA: 9s - loss: 0.9432 - acc: 0.7229\n",
      " Optimizer iteration 1819, batch 255\n",
      "\n",
      " Learning rate 1.1094314741816935e-05, Model learning rate 1.109431468648836e-05\n",
      "256/391 [==================>...........] - ETA: 9s - loss: 0.9436 - acc: 0.7227\n",
      " Optimizer iteration 1820, batch 256\n",
      "\n",
      " Learning rate 1.0926199633097156e-05, Model learning rate 1.0926200047833845e-05\n",
      "257/391 [==================>...........] - ETA: 9s - loss: 0.9431 - acc: 0.7230\n",
      " Optimizer iteration 1821, batch 257\n",
      "\n",
      " Learning rate 1.0759353942888573e-05, Model learning rate 1.0759354154288303e-05\n",
      "\n",
      " Optimizer iteration 1822, batch 258\n",
      "\n",
      " Learning rate 1.0593778104248441e-05, Model learning rate 1.0593777915346436e-05\n",
      "259/391 [==================>...........] - ETA: 9s - loss: 0.9427 - acc: 0.7231\n",
      " Optimizer iteration 1823, batch 259\n",
      "\n",
      " Learning rate 1.0429472546938157e-05, Model learning rate 1.0429472240502946e-05\n",
      "260/391 [==================>...........] - ETA: 9s - loss: 0.9429 - acc: 0.7230\n",
      " Optimizer iteration 1824, batch 260\n",
      "\n",
      " Learning rate 1.0266437697422026e-05, Model learning rate 1.0266438039252535e-05\n",
      "261/391 [===================>..........] - ETA: 9s - loss: 0.9427 - acc: 0.7229\n",
      " Optimizer iteration 1825, batch 261\n",
      "\n",
      " Learning rate 1.0104673978866164e-05, Model learning rate 1.01046744021005e-05\n",
      "\n",
      " Optimizer iteration 1826, batch 262\n",
      "\n",
      " Learning rate 9.94418181113732e-06, Model learning rate 9.944182238541543e-06\n",
      "263/391 [===================>..........] - ETA: 9s - loss: 0.9434 - acc: 0.7225\n",
      " Optimizer iteration 1827, batch 263\n",
      "\n",
      " Learning rate 9.784961610802113e-06, Model learning rate 9.784961548575666e-06\n",
      "264/391 [===================>..........] - ETA: 9s - loss: 0.9431 - acc: 0.7225\n",
      " Optimizer iteration 1828, batch 264\n",
      "\n",
      " Learning rate 9.627013791125295e-06, Model learning rate 9.62701415119227e-06\n",
      "265/391 [===================>..........] - ETA: 9s - loss: 0.9434 - acc: 0.7223\n",
      " Optimizer iteration 1829, batch 265\n",
      "\n",
      " Learning rate 9.470338762069431e-06, Model learning rate 9.470339136896655e-06\n",
      "\n",
      " Optimizer iteration 1830, batch 266\n",
      "\n",
      " Learning rate 9.314936930293284e-06, Model learning rate 9.31493650568882e-06\n",
      "267/391 [===================>..........] - ETA: 8s - loss: 0.9439 - acc: 0.7222\n",
      " Optimizer iteration 1831, batch 267\n",
      "\n",
      " Learning rate 9.16080869915109e-06, Model learning rate 9.16080898605287e-06\n",
      "268/391 [===================>..........] - ETA: 8s - loss: 0.9437 - acc: 0.7222\n",
      " Optimizer iteration 1832, batch 268\n",
      "\n",
      " Learning rate 9.007954468691294e-06, Model learning rate 9.007954758999404e-06\n",
      "269/391 [===================>..........] - ETA: 8s - loss: 0.9439 - acc: 0.7221\n",
      " Optimizer iteration 1833, batch 269\n",
      "\n",
      " Learning rate 8.856374635655695e-06, Model learning rate 8.85637473402312e-06\n",
      "270/391 [===================>..........] - ETA: 8s - loss: 0.9445 - acc: 0.7221\n",
      " Optimizer iteration 1834, batch 270\n",
      "\n",
      " Learning rate 8.706069593478139e-06, Model learning rate 8.706069820618723e-06\n",
      "\n",
      " Optimizer iteration 1835, batch 271\n",
      "\n",
      " Learning rate 8.557039732283945e-06, Model learning rate 8.55704001878621e-06\n",
      "272/391 [===================>..........] - ETA: 8s - loss: 0.9440 - acc: 0.7221\n",
      " Optimizer iteration 1836, batch 272\n",
      "\n",
      " Learning rate 8.409285438888358e-06, Model learning rate 8.409285328525584e-06\n",
      "\n",
      " Optimizer iteration 1837, batch 273\n",
      "\n",
      " Learning rate 8.262807096795999e-06, Model learning rate 8.262806659331545e-06\n",
      "274/391 [====================>.........] - ETA: 8s - loss: 0.9442 - acc: 0.7218\n",
      " Optimizer iteration 1838, batch 274\n",
      "\n",
      " Learning rate 8.117605086199687e-06, Model learning rate 8.117604920698795e-06\n",
      "\n",
      " Optimizer iteration 1839, batch 275\n",
      "\n",
      " Learning rate 7.973679783979337e-06, Model learning rate 7.973680112627335e-06\n",
      "276/391 [====================>.........] - ETA: 8s - loss: 0.9445 - acc: 0.7215\n",
      " Optimizer iteration 1840, batch 276\n",
      "\n",
      " Learning rate 7.83103156370113e-06, Model learning rate 7.831031325622462e-06\n",
      "\n",
      " Optimizer iteration 1841, batch 277\n",
      "\n",
      " Learning rate 7.689660795616559e-06, Model learning rate 7.68966037867358e-06\n",
      "278/391 [====================>.........] - ETA: 8s - loss: 0.9448 - acc: 0.7213\n",
      " Optimizer iteration 1842, batch 278\n",
      "\n",
      " Learning rate 7.549567846661387e-06, Model learning rate 7.549567726528039e-06\n",
      "279/391 [====================>.........] - ETA: 8s - loss: 0.9454 - acc: 0.7211\n",
      " Optimizer iteration 1843, batch 279\n",
      "\n",
      " Learning rate 7.410753080454746e-06, Model learning rate 7.41075291443849e-06\n",
      "280/391 [====================>.........] - ETA: 8s - loss: 0.9451 - acc: 0.7212\n",
      " Optimizer iteration 1844, batch 280\n",
      "\n",
      " Learning rate 7.2732168572981485e-06, Model learning rate 7.273216851899633e-06\n",
      "\n",
      " Optimizer iteration 1845, batch 281\n",
      "\n",
      " Learning rate 7.136959534174592e-06, Model learning rate 7.136959538911469e-06\n",
      "282/391 [====================>.........] - ETA: 7s - loss: 0.9453 - acc: 0.7213\n",
      " Optimizer iteration 1846, batch 282\n",
      "\n",
      " Learning rate 7.001981464747565e-06, Model learning rate 7.001981430221349e-06\n",
      "\n",
      " Optimizer iteration 1847, batch 283\n",
      "\n",
      " Learning rate 6.868282999360265e-06, Model learning rate 6.868282980576623e-06\n",
      "284/391 [====================>.........] - ETA: 7s - loss: 0.9450 - acc: 0.7215\n",
      " Optimizer iteration 1848, batch 284\n",
      "\n",
      " Learning rate 6.735864485034493e-06, Model learning rate 6.735864644724643e-06\n",
      "285/391 [====================>.........] - ETA: 7s - loss: 0.9445 - acc: 0.7217\n",
      " Optimizer iteration 1849, batch 285\n",
      "\n",
      " Learning rate 6.604726265470096e-06, Model learning rate 6.604726422665408e-06\n",
      "286/391 [====================>.........] - ETA: 7s - loss: 0.9440 - acc: 0.7219\n",
      " Optimizer iteration 1850, batch 286\n",
      "\n",
      " Learning rate 6.474868681043577e-06, Model learning rate 6.474868769146269e-06\n",
      "\n",
      " Optimizer iteration 1851, batch 287\n",
      "\n",
      " Learning rate 6.346292068807602e-06, Model learning rate 6.346292138914578e-06\n",
      "288/391 [=====================>........] - ETA: 7s - loss: 0.9438 - acc: 0.7219\n",
      " Optimizer iteration 1852, batch 288\n",
      "\n",
      " Learning rate 6.2189967624899925e-06, Model learning rate 6.218996986717684e-06\n",
      "289/391 [=====================>........] - ETA: 7s - loss: 0.9436 - acc: 0.7219\n",
      " Optimizer iteration 1853, batch 289\n",
      "\n",
      " Learning rate 6.092983092492843e-06, Model learning rate 6.092983312555589e-06\n",
      "290/391 [=====================>........] - ETA: 7s - loss: 0.9435 - acc: 0.7220\n",
      " Optimizer iteration 1854, batch 290\n",
      "\n",
      " Learning rate 5.968251385891743e-06, Model learning rate 5.968251571175642e-06\n",
      "\n",
      " Optimizer iteration 1855, batch 291\n",
      "\n",
      " Learning rate 5.844801966434832e-06, Model learning rate 5.844801762577845e-06\n",
      "292/391 [=====================>........] - ETA: 7s - loss: 0.9435 - acc: 0.7219\n",
      " Optimizer iteration 1856, batch 292\n",
      "\n",
      " Learning rate 5.722635154541967e-06, Model learning rate 5.722635251004249e-06\n",
      "\n",
      " Optimizer iteration 1857, batch 293\n",
      "\n",
      " Learning rate 5.601751267304056e-06, Model learning rate 5.601751126960153e-06\n",
      "294/391 [=====================>........] - ETA: 7s - loss: 0.9436 - acc: 0.7219\n",
      " Optimizer iteration 1858, batch 294\n",
      "\n",
      " Learning rate 5.482150618481952e-06, Model learning rate 5.482150754687609e-06\n",
      "295/391 [=====================>........] - ETA: 6s - loss: 0.9436 - acc: 0.7218\n",
      " Optimizer iteration 1859, batch 295\n",
      "\n",
      " Learning rate 5.363833518505834e-06, Model learning rate 5.363833679439267e-06\n",
      "296/391 [=====================>........] - ETA: 6s - loss: 0.9441 - acc: 0.7218\n",
      " Optimizer iteration 1860, batch 296\n",
      "\n",
      " Learning rate 5.2468002744744395e-06, Model learning rate 5.246800355962478e-06\n",
      "\n",
      " Optimizer iteration 1861, batch 297\n",
      "\n",
      " Learning rate 5.131051190154113e-06, Model learning rate 5.1310512390045915e-06\n",
      "298/391 [=====================>........] - ETA: 6s - loss: 0.9436 - acc: 0.7218\n",
      " Optimizer iteration 1862, batch 298\n",
      "\n",
      " Learning rate 5.016586565978087e-06, Model learning rate 5.01658678331296e-06\n",
      "299/391 [=====================>........] - ETA: 6s - loss: 0.9437 - acc: 0.7219\n",
      " Optimizer iteration 1863, batch 299\n",
      "\n",
      " Learning rate 4.9034066990457095e-06, Model learning rate 4.903406534140231e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/391 [======================>.......] - ETA: 6s - loss: 0.9436 - acc: 0.7219\n",
      " Optimizer iteration 1864, batch 300\n",
      "\n",
      " Learning rate 4.791511883121713e-06, Model learning rate 4.791511855728459e-06\n",
      "\n",
      " Optimizer iteration 1865, batch 301\n",
      "\n",
      " Learning rate 4.680902408635335e-06, Model learning rate 4.680902293330291e-06\n",
      "302/391 [======================>.......] - ETA: 6s - loss: 0.9432 - acc: 0.7220\n",
      " Optimizer iteration 1866, batch 302\n",
      "\n",
      " Learning rate 4.571578562679757e-06, Model learning rate 4.571578756440431e-06\n",
      "303/391 [======================>.......] - ETA: 6s - loss: 0.9427 - acc: 0.7223\n",
      " Optimizer iteration 1867, batch 303\n",
      "\n",
      " Learning rate 4.463540629010998e-06, Model learning rate 4.4635407903115265e-06\n",
      "304/391 [======================>.......] - ETA: 6s - loss: 0.9432 - acc: 0.7221\n",
      " Optimizer iteration 1868, batch 304\n",
      "\n",
      " Learning rate 4.356788888047747e-06, Model learning rate 4.356788849690929e-06\n",
      "305/391 [======================>.......] - ETA: 6s - loss: 0.9426 - acc: 0.7224\n",
      " Optimizer iteration 1869, batch 305\n",
      "\n",
      " Learning rate 4.2513236168700845e-06, Model learning rate 4.25132384407334e-06\n",
      "306/391 [======================>.......] - ETA: 6s - loss: 0.9425 - acc: 0.7223\n",
      " Optimizer iteration 1870, batch 306\n",
      "\n",
      " Learning rate 4.147145089218984e-06, Model learning rate 4.1471448639640585e-06\n",
      "307/391 [======================>.......] - ETA: 6s - loss: 0.9425 - acc: 0.7224\n",
      " Optimizer iteration 1871, batch 307\n",
      "\n",
      " Learning rate 4.04425357549576e-06, Model learning rate 4.044253728352487e-06\n",
      "308/391 [======================>.......] - ETA: 6s - loss: 0.9423 - acc: 0.7224\n",
      " Optimizer iteration 1872, batch 308\n",
      "\n",
      " Learning rate 3.942649342761117e-06, Model learning rate 3.942649527743924e-06\n",
      "309/391 [======================>.......] - ETA: 5s - loss: 0.9420 - acc: 0.7226\n",
      " Optimizer iteration 1873, batch 309\n",
      "\n",
      " Learning rate 3.842332654734437e-06, Model learning rate 3.842332716885721e-06\n",
      "\n",
      " Optimizer iteration 1874, batch 310\n",
      "\n",
      " Learning rate 3.7433037717933828e-06, Model learning rate 3.7433037505252287e-06\n",
      "311/391 [======================>.......] - ETA: 5s - loss: 0.9426 - acc: 0.7225\n",
      " Optimizer iteration 1875, batch 311\n",
      "\n",
      " Learning rate 3.645562950973014e-06, Model learning rate 3.645562856036122e-06\n",
      "312/391 [======================>.......] - ETA: 5s - loss: 0.9425 - acc: 0.7224\n",
      " Optimizer iteration 1876, batch 312\n",
      "\n",
      " Learning rate 3.5491104459650646e-06, Model learning rate 3.549110488165752e-06\n",
      "313/391 [=======================>......] - ETA: 5s - loss: 0.9425 - acc: 0.7225\n",
      " Optimizer iteration 1877, batch 313\n",
      "\n",
      " Learning rate 3.453946507117445e-06, Model learning rate 3.4539464195404435e-06\n",
      "\n",
      " Optimizer iteration 1878, batch 314\n",
      "\n",
      " Learning rate 3.3600713814335158e-06, Model learning rate 3.3600713322812226e-06\n",
      "315/391 [=======================>......] - ETA: 5s - loss: 0.9419 - acc: 0.7227\n",
      " Optimizer iteration 1879, batch 315\n",
      "\n",
      " Learning rate 3.2674853125714276e-06, Model learning rate 3.2674852263880894e-06\n",
      "316/391 [=======================>......] - ETA: 5s - loss: 0.9417 - acc: 0.7228\n",
      " Optimizer iteration 1880, batch 316\n",
      "\n",
      " Learning rate 3.1761885408435056e-06, Model learning rate 3.1761885566083947e-06\n",
      "317/391 [=======================>......] - ETA: 5s - loss: 0.9417 - acc: 0.7228\n",
      " Optimizer iteration 1881, batch 317\n",
      "\n",
      " Learning rate 3.0861813032156404e-06, Model learning rate 3.0861813229421386e-06\n",
      "\n",
      " Optimizer iteration 1882, batch 318\n",
      "\n",
      " Learning rate 2.997463833306735e-06, Model learning rate 2.9974637527629966e-06\n",
      "319/391 [=======================>......] - ETA: 5s - loss: 0.9415 - acc: 0.7227\n",
      " Optimizer iteration 1883, batch 319\n",
      "\n",
      " Learning rate 2.9100363613879243e-06, Model learning rate 2.9100363008183194e-06\n",
      "320/391 [=======================>......] - ETA: 5s - loss: 0.9414 - acc: 0.7227\n",
      " Optimizer iteration 1884, batch 320\n",
      "\n",
      " Learning rate 2.823899114382078e-06, Model learning rate 2.8238991944817826e-06\n",
      "321/391 [=======================>......] - ETA: 5s - loss: 0.9413 - acc: 0.7228\n",
      " Optimizer iteration 1885, batch 321\n",
      "\n",
      " Learning rate 2.739052315863355e-06, Model learning rate 2.7390522063797107e-06\n",
      "\n",
      " Optimizer iteration 1886, batch 322\n",
      "\n",
      " Learning rate 2.655496186056261e-06, Model learning rate 2.6554962460068054e-06\n",
      "323/391 [=======================>......] - ETA: 4s - loss: 0.9409 - acc: 0.7229\n",
      " Optimizer iteration 1887, batch 323\n",
      "\n",
      " Learning rate 2.573230941835536e-06, Model learning rate 2.573230858615716e-06\n",
      "324/391 [=======================>......] - ETA: 4s - loss: 0.9411 - acc: 0.7229\n",
      " Optimizer iteration 1888, batch 324\n",
      "\n",
      " Learning rate 2.492256796725212e-06, Model learning rate 2.4922567263274686e-06\n",
      "325/391 [=======================>......] - ETA: 4s - loss: 0.9412 - acc: 0.7227\n",
      " Optimizer iteration 1889, batch 325\n",
      "\n",
      " Learning rate 2.4125739608981124e-06, Model learning rate 2.4125738491420634e-06\n",
      "\n",
      " Optimizer iteration 1890, batch 326\n",
      "\n",
      " Learning rate 2.334182641175686e-06, Model learning rate 2.334182681806851e-06\n",
      "327/391 [========================>.....] - ETA: 4s - loss: 0.9409 - acc: 0.7228\n",
      " Optimizer iteration 1891, batch 327\n",
      "\n",
      " Learning rate 2.2570830410268973e-06, Model learning rate 2.2570829969481565e-06\n",
      "328/391 [========================>.....] - ETA: 4s - loss: 0.9410 - acc: 0.7227\n",
      " Optimizer iteration 1892, batch 328\n",
      "\n",
      " Learning rate 2.181275360568169e-06, Model learning rate 2.1812752493133303e-06\n",
      "329/391 [========================>.....] - ETA: 4s - loss: 0.9412 - acc: 0.7227\n",
      " Optimizer iteration 1893, batch 329\n",
      "\n",
      " Learning rate 2.106759796562496e-06, Model learning rate 2.1067598936497234e-06\n",
      "\n",
      " Optimizer iteration 1894, batch 330\n",
      "\n",
      " Learning rate 2.0335365424192786e-06, Model learning rate 2.033536475209985e-06\n",
      "331/391 [========================>.....] - ETA: 4s - loss: 0.9407 - acc: 0.7230\n",
      " Optimizer iteration 1895, batch 331\n",
      "\n",
      " Learning rate 1.9616057881935434e-06, Model learning rate 1.9616056761151413e-06\n",
      "332/391 [========================>.....] - ETA: 4s - loss: 0.9406 - acc: 0.7230\n",
      " Optimizer iteration 1896, batch 332\n",
      "\n",
      " Learning rate 1.890967720585668e-06, Model learning rate 1.8909677237388678e-06\n",
      "333/391 [========================>.....] - ETA: 4s - loss: 0.9402 - acc: 0.7231\n",
      " Optimizer iteration 1897, batch 333\n",
      "\n",
      " Learning rate 1.8216225229406026e-06, Model learning rate 1.8216225043943268e-06\n",
      "334/391 [========================>.....] - ETA: 4s - loss: 0.9408 - acc: 0.7230\n",
      " Optimizer iteration 1898, batch 334\n",
      "\n",
      " Learning rate 1.753570375247815e-06, Model learning rate 1.7535703591420315e-06\n",
      "335/391 [========================>.....] - ETA: 4s - loss: 0.9412 - acc: 0.7229\n",
      " Optimizer iteration 1899, batch 335\n",
      "\n",
      " Learning rate 1.6868114541404577e-06, Model learning rate 1.6868114016688196e-06\n",
      "336/391 [========================>.....] - ETA: 3s - loss: 0.9415 - acc: 0.7226\n",
      " Optimizer iteration 1900, batch 336\n",
      "\n",
      " Learning rate 1.6213459328950354e-06, Model learning rate 1.6213459730352042e-06\n",
      "337/391 [========================>.....] - ETA: 3s - loss: 0.9416 - acc: 0.7226\n",
      " Optimizer iteration 1901, batch 337\n",
      "\n",
      " Learning rate 1.5571739814309594e-06, Model learning rate 1.5571739595543477e-06\n",
      "338/391 [========================>.....] - ETA: 3s - loss: 0.9421 - acc: 0.7223\n",
      " Optimizer iteration 1902, batch 338\n",
      "\n",
      " Learning rate 1.494295766310161e-06, Model learning rate 1.4942958159736008e-06\n",
      "339/391 [=========================>....] - ETA: 3s - loss: 0.9421 - acc: 0.7223\n",
      " Optimizer iteration 1903, batch 339\n",
      "\n",
      " Learning rate 1.4327114507365346e-06, Model learning rate 1.432711428606126e-06\n",
      "340/391 [=========================>....] - ETA: 3s - loss: 0.9420 - acc: 0.7223\n",
      " Optimizer iteration 1904, batch 340\n",
      "\n",
      " Learning rate 1.372421194555773e-06, Model learning rate 1.3724211385124363e-06\n",
      "341/391 [=========================>....] - ETA: 3s - loss: 0.9425 - acc: 0.7220\n",
      " Optimizer iteration 1905, batch 341\n",
      "\n",
      " Learning rate 1.3134251542544773e-06, Model learning rate 1.3134251730662072e-06\n",
      "342/391 [=========================>....] - ETA: 3s - loss: 0.9423 - acc: 0.7221\n",
      " Optimizer iteration 1906, batch 342\n",
      "\n",
      " Learning rate 1.2557234829601582e-06, Model learning rate 1.2557235322674387e-06\n",
      "343/391 [=========================>....] - ETA: 3s - loss: 0.9422 - acc: 0.7221\n",
      " Optimizer iteration 1907, batch 343\n",
      "\n",
      " Learning rate 1.1993163304409027e-06, Model learning rate 1.1993163298029685e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344/391 [=========================>....] - ETA: 3s - loss: 0.9420 - acc: 0.7222\n",
      " Optimizer iteration 1908, batch 344\n",
      "\n",
      " Learning rate 1.1442038431044854e-06, Model learning rate 1.1442037930464721e-06\n",
      "345/391 [=========================>....] - ETA: 3s - loss: 0.9422 - acc: 0.7223\n",
      " Optimizer iteration 1909, batch 345\n",
      "\n",
      " Learning rate 1.0903861639985914e-06, Model learning rate 1.090386149371625e-06\n",
      "346/391 [=========================>....] - ETA: 3s - loss: 0.9425 - acc: 0.7222\n",
      " Optimizer iteration 1910, batch 346\n",
      "\n",
      " Learning rate 1.0378634328099267e-06, Model learning rate 1.037863398778427e-06\n",
      "347/391 [=========================>....] - ETA: 3s - loss: 0.9424 - acc: 0.7222\n",
      " Optimizer iteration 1911, batch 347\n",
      "\n",
      " Learning rate 9.866357858642206e-07, Model learning rate 9.866357686405536e-07\n",
      "348/391 [=========================>....] - ETA: 3s - loss: 0.9422 - acc: 0.7223\n",
      " Optimizer iteration 1912, batch 348\n",
      "\n",
      " Learning rate 9.367033561257233e-07, Model learning rate 9.367033726448426e-07\n",
      "349/391 [=========================>....] - ETA: 3s - loss: 0.9422 - acc: 0.7223\n",
      " Optimizer iteration 1913, batch 349\n",
      "\n",
      " Learning rate 8.880662731968747e-07, Model learning rate 8.880662676347129e-07\n",
      "\n",
      " Optimizer iteration 1914, batch 350\n",
      "\n",
      " Learning rate 8.4072466331786e-07, Model learning rate 8.407246809838398e-07\n",
      "351/391 [=========================>....] - ETA: 2s - loss: 0.9416 - acc: 0.7224\n",
      " Optimizer iteration 1915, batch 351\n",
      "\n",
      " Learning rate 7.946786493666647e-07, Model learning rate 7.946786695356423e-07\n",
      "352/391 [==========================>...] - ETA: 2s - loss: 0.9414 - acc: 0.7225\n",
      " Optimizer iteration 1916, batch 352\n",
      "\n",
      " Learning rate 7.499283508581311e-07, Model learning rate 7.49928346976958e-07\n",
      "353/391 [==========================>...] - ETA: 2s - loss: 0.9417 - acc: 0.7224\n",
      " Optimizer iteration 1917, batch 353\n",
      "\n",
      " Learning rate 7.064738839442364e-07, Model learning rate 7.064738838380435e-07\n",
      "\n",
      " Optimizer iteration 1918, batch 354\n",
      "\n",
      " Learning rate 6.643153614134811e-07, Model learning rate 6.643153369623178e-07\n",
      "355/391 [==========================>...] - ETA: 2s - loss: 0.9420 - acc: 0.7221\n",
      " Optimizer iteration 1919, batch 355\n",
      "\n",
      " Learning rate 6.234528926907234e-07, Model learning rate 6.234528768800374e-07\n",
      "356/391 [==========================>...] - ETA: 2s - loss: 0.9417 - acc: 0.7222\n",
      " Optimizer iteration 1920, batch 356\n",
      "\n",
      " Learning rate 5.838865838366791e-07, Model learning rate 5.838865604346211e-07\n",
      "357/391 [==========================>...] - ETA: 2s - loss: 0.9417 - acc: 0.7222\n",
      " Optimizer iteration 1921, batch 357\n",
      "\n",
      " Learning rate 5.456165375480882e-07, Model learning rate 5.456165581563255e-07\n",
      "\n",
      " Optimizer iteration 1922, batch 358\n",
      "\n",
      " Learning rate 5.08642853156882e-07, Model learning rate 5.086428700451506e-07\n",
      "359/391 [==========================>...] - ETA: 2s - loss: 0.9421 - acc: 0.7219\n",
      " Optimizer iteration 1923, batch 359\n",
      "\n",
      " Learning rate 4.729656266304061e-07, Model learning rate 4.7296563820964366e-07\n",
      "360/391 [==========================>...] - ETA: 2s - loss: 0.9421 - acc: 0.7219\n",
      " Optimizer iteration 1924, batch 360\n",
      "\n",
      " Learning rate 4.3858495057080837e-07, Model learning rate 4.3858494791493285e-07\n",
      "361/391 [==========================>...] - ETA: 2s - loss: 0.9420 - acc: 0.7220\n",
      " Optimizer iteration 1925, batch 361\n",
      "\n",
      " Learning rate 4.055009142152066e-07, Model learning rate 4.0550091284785594e-07\n",
      "\n",
      " Optimizer iteration 1926, batch 362\n",
      "\n",
      " Learning rate 3.737136034349109e-07, Model learning rate 3.737135898518318e-07\n",
      "363/391 [==========================>...] - ETA: 2s - loss: 0.9418 - acc: 0.7220\n",
      " Optimizer iteration 1927, batch 363\n",
      "\n",
      " Learning rate 3.432231007358122e-07, Model learning rate 3.432230926136981e-07\n",
      "364/391 [==========================>...] - ETA: 1s - loss: 0.9421 - acc: 0.7218\n",
      " Optimizer iteration 1928, batch 364\n",
      "\n",
      " Learning rate 3.1402948525766086e-07, Model learning rate 3.1402947797687375e-07\n",
      "365/391 [===========================>..] - ETA: 1s - loss: 0.9423 - acc: 0.7217\n",
      " Optimizer iteration 1929, batch 365\n",
      "\n",
      " Learning rate 2.861328327741219e-07, Model learning rate 2.86132831206487e-07\n",
      "366/391 [===========================>..] - ETA: 1s - loss: 0.9421 - acc: 0.7219\n",
      " Optimizer iteration 1930, batch 366\n",
      "\n",
      " Learning rate 2.595332156925534e-07, Model learning rate 2.595332091459568e-07\n",
      "367/391 [===========================>..] - ETA: 1s - loss: 0.9422 - acc: 0.7219\n",
      " Optimizer iteration 1931, batch 367\n",
      "\n",
      " Learning rate 2.3423070305367278e-07, Model learning rate 2.3423069706041133e-07\n",
      "368/391 [===========================>..] - ETA: 1s - loss: 0.9422 - acc: 0.7218\n",
      " Optimizer iteration 1932, batch 368\n",
      "\n",
      " Learning rate 2.1022536053166842e-07, Model learning rate 2.1022536600412423e-07\n",
      "369/391 [===========================>..] - ETA: 1s - loss: 0.9426 - acc: 0.7218\n",
      " Optimizer iteration 1933, batch 369\n",
      "\n",
      " Learning rate 1.8751725043375523e-07, Model learning rate 1.875172443988049e-07\n",
      "\n",
      " Optimizer iteration 1934, batch 370\n",
      "\n",
      " Learning rate 1.6610643170000827e-07, Model learning rate 1.6610643172043638e-07\n",
      "371/391 [===========================>..] - ETA: 1s - loss: 0.9428 - acc: 0.7218\n",
      " Optimizer iteration 1935, batch 371\n",
      "\n",
      " Learning rate 1.4599295990352924e-07, Model learning rate 1.4599295639072807e-07\n",
      "372/391 [===========================>..] - ETA: 1s - loss: 0.9428 - acc: 0.7218\n",
      " Optimizer iteration 1936, batch 372\n",
      "\n",
      " Learning rate 1.271768872498913e-07, Model learning rate 1.2717688946395356e-07\n",
      "373/391 [===========================>..] - ETA: 1s - loss: 0.9428 - acc: 0.7217\n",
      " Optimizer iteration 1937, batch 373\n",
      "\n",
      " Learning rate 1.096582625772502e-07, Model learning rate 1.0965825936182227e-07\n",
      "374/391 [===========================>..] - ETA: 1s - loss: 0.9428 - acc: 0.7217\n",
      " Optimizer iteration 1938, batch 374\n",
      "\n",
      " Learning rate 9.343713135623322e-08, Model learning rate 9.343713003318044e-08\n",
      "\n",
      " Optimizer iteration 1939, batch 375\n",
      "\n",
      " Learning rate 7.851353568971708e-08, Model learning rate 7.851353700516484e-08\n",
      "376/391 [===========================>..] - ETA: 1s - loss: 0.9426 - acc: 0.7218\n",
      " Optimizer iteration 1940, batch 376\n",
      "\n",
      " Learning rate 6.488751431266149e-08, Model learning rate 6.488751580491225e-08\n",
      "377/391 [===========================>..] - ETA: 1s - loss: 0.9429 - acc: 0.7216\n",
      " Optimizer iteration 1941, batch 377\n",
      "\n",
      " Learning rate 5.2559102592164565e-08, Model learning rate 5.2559101959559484e-08\n",
      "\n",
      " Optimizer iteration 1942, batch 378\n",
      "\n",
      " Learning rate 4.15283325274074e-08, Model learning rate 4.152833099624331e-08\n",
      "379/391 [============================>.] - ETA: 0s - loss: 0.9436 - acc: 0.7214\n",
      " Optimizer iteration 1943, batch 379\n",
      "\n",
      " Learning rate 3.179523274932094e-08, Model learning rate 3.179523133667317e-08\n",
      "380/391 [============================>.] - ETA: 0s - loss: 0.9441 - acc: 0.7212\n",
      " Optimizer iteration 1944, batch 380\n",
      "\n",
      " Learning rate 2.3359828520641556e-08, Model learning rate 2.3359827849844805e-08\n",
      "381/391 [============================>.] - ETA: 0s - loss: 0.9441 - acc: 0.7213\n",
      " Optimizer iteration 1945, batch 381\n",
      "\n",
      " Learning rate 1.622214173602199e-08, Model learning rate 1.6222141852040295e-08\n",
      "\n",
      " Optimizer iteration 1946, batch 382\n",
      "\n",
      " Learning rate 1.0382190921753854e-08, Model learning rate 1.0382191106828031e-08\n",
      "383/391 [============================>.] - ETA: 0s - loss: 0.9439 - acc: 0.7213\n",
      " Optimizer iteration 1947, batch 383\n",
      "\n",
      " Learning rate 5.839991235656594e-09, Model learning rate 5.83999115733036e-09\n",
      "384/391 [============================>.] - ETA: 0s - loss: 0.9441 - acc: 0.7213\n",
      " Optimizer iteration 1948, batch 384\n",
      "\n",
      " Learning rate 2.5955544673550437e-09, Model learning rate 2.5955544380451556e-09\n",
      "385/391 [============================>.] - ETA: 0s - loss: 0.9439 - acc: 0.7213\n",
      " Optimizer iteration 1949, batch 385\n",
      "\n",
      " Learning rate 6.48889037890843e-10, Model learning rate 6.488890536004988e-10\n",
      "386/391 [============================>.] - ETA: 0s - loss: 0.9437 - acc: 0.7213\n",
      " Optimizer iteration 1950, batch 386\n",
      "\n",
      " Learning rate 0.001, Model learning rate 0.0010000000474974513\n",
      "\n",
      " Optimizer iteration 1951, batch 387\n",
      "\n",
      " Learning rate 0.0009999993511109622, Model learning rate 0.0009999993490055203\n",
      "388/391 [============================>.] - ETA: 0s - loss: 0.9438 - acc: 0.7215\n",
      " Optimizer iteration 1952, batch 388\n",
      "\n",
      " Learning rate 0.0009999974044455327, Model learning rate 0.0009999973699450493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389/391 [============================>.] - ETA: 0s - loss: 0.9439 - acc: 0.7214\n",
      " Optimizer iteration 1953, batch 389\n",
      "\n",
      " Learning rate 0.0009999941600087644, Model learning rate 0.0009999941103160381\n",
      "\n",
      " Optimizer iteration 1954, batch 390\n",
      "\n",
      " Learning rate 0.0009999896178090784, Model learning rate 0.0009999895701184869\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.9442 - acc: 0.7213 - val_loss: 1.0999 - val_acc: 0.6607\n",
      "\n",
      "Epoch 00005: saving model to /home/ubuntu/Projects/hybrid-ensemble/model/run_200/cifar10_ResNet20v1_model-0005.h5\n",
      "Saving epoch training log...\n",
      "Saving batch training log...\n",
      "Writing index file and predict files...\n",
      "Deleting unwanted model files...\n",
      "Saving target file...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Trains a ResNet on the CIFAR10 dataset.\n",
    "\n",
    "ResNet v1\n",
    "[a] Deep Residual Learning for Image Recognition\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "\n",
    "ResNet v2\n",
    "[b] Identity Mappings in Deep Residual Networks\n",
    "https://arxiv.org/pdf/1603.05027.pdf\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import Callback\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "dirpath = os.getcwd()\n",
    "\n",
    "seed = 22\n",
    "resample = True\n",
    "save_dir = '/home/ubuntu/Projects/hybrid-ensemble/model/run_200'\n",
    "datafile = '/home/ubuntu/Projects/hybrid-ensemble/data/cifar10_balance/DS3'\n",
    "top_k = 1\n",
    "\n",
    "# Set random seed\n",
    "if seed is not None:\n",
    "    import tensorflow as tf\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 128  # orig paper trained all networks with batch_size=128\n",
    "epochs = 5 # orig paper epochs = 200\n",
    "data_augmentation = True\n",
    "num_classes = 10\n",
    "initial_lr = 1e-3\n",
    "snapshot_window_size = int(math.ceil(epochs/top_k))\n",
    "\n",
    "# Subtracting pixel mean improves accuracy\n",
    "subtract_pixel_mean = True\n",
    "\n",
    "# Model parameter\n",
    "# ----------------------------------------------------------------------------\n",
    "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
    "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
    "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
    "# ----------------------------------------------------------------------------\n",
    "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
    "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
    "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
    "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
    "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
    "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
    "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
    "# ---------------------------------------------------------------------------\n",
    "n = 3\n",
    "\n",
    "# Model version\n",
    "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
    "version = 1\n",
    "\n",
    "# Computed depth from supplied model parameter n\n",
    "if version == 1:\n",
    "    depth = n * 6 + 2\n",
    "elif version == 2:\n",
    "    depth = n * 9 + 2\n",
    "\n",
    "# Model name, depth and version\n",
    "model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "# Load the CIFAR10 data.\n",
    "print('Loading data...')\n",
    "with open(datafile, 'rb') as f:\n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test) = pickle.load(f)\n",
    "\n",
    "# Resample the training data set from training+validating data set with the same class distribution with the loaded ones\n",
    "if resample:\n",
    "    print('Resampling training and validating data sets...')\n",
    "    x_tv = np.concatenate((x_train, x_valid), axis=0)\n",
    "    y_tv = np.concatenate((y_train, y_valid), axis=0)\n",
    "    index_dict = defaultdict(list)\n",
    "    for i in range(len(y_tv)):\n",
    "        index_dict[y_tv[i][0]].append(i)\n",
    "    valid_index_dict = defaultdict(list)\n",
    "    for i in range(len(y_valid)):\n",
    "        valid_index_dict[y_valid[i][0]].append(i)\n",
    "    valid_index = []\n",
    "    for c in valid_index_dict.keys():\n",
    "        valid_index.extend(np.random.choice(index_dict[c], size=len(valid_index_dict[c]), replace=False))\n",
    "    train_index = np.setdiff1d(range(len(y_tv)), valid_index)\n",
    "\n",
    "    x_train, y_train = x_tv[train_index], y_tv[train_index]\n",
    "    x_valid, y_valid = x_tv[valid_index], y_tv[valid_index]\n",
    "    \n",
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "x_valid = x_valid.astype('float32') / 255\n",
    "\n",
    "# If subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "    x_valid -= x_train_mean\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)\n",
    "print(x_valid.shape[0], 'valid samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_valid = keras.utils.to_categorical(y_valid, num_classes)\n",
    "\n",
    "def next_run_dir(path):\n",
    "    \"\"\"\n",
    "    Naive (slow) version of next_path\n",
    "    \"\"\"\n",
    "    i = 1\n",
    "    while os.path.exists('{}_{}'.format(path, i)):\n",
    "        i += 1\n",
    "    return '{}_{}'.format(path, i)\n",
    "\n",
    "def cyclic_cosine_anneal_schedule(initial_lr=1e-3, update_window_size=40):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with cosine annealing schedule.\n",
    "    '''\n",
    "    def lr_schedule(epoch):\n",
    "        \"\"\"Learning Rate Schedule\n",
    "\n",
    "        Learning rate is scheduled to be updated per epoch with a cosine function per epoch. \n",
    "        Learning rate is raised to initial_lr every snapshot_window_size.\n",
    "\n",
    "        # Arguments\n",
    "            epoch (int): The number of epochs\n",
    "\n",
    "        # Returns\n",
    "            lr (float32): learning rate\n",
    "        \"\"\"\n",
    "        lr = initial_lr / 2 * (math.cos(math.pi * ((epoch % update_window_size) / update_window_size)) + 1)\n",
    "        print('Learning rate: ', lr)\n",
    "        return lr\n",
    "    \n",
    "    return LearningRateScheduler(lr_schedule)\n",
    "\n",
    "class MyCallback(Callback):\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        iteration = self.model.optimizer.iterations   \n",
    "        update_window_size = self.params.update_window_size\n",
    "        lr = initial_lr / 2 * (math.cos(math.pi * ((K.eval(iteration) % update_window_size) / update_window_size)) + 1)\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        print('\\n batch {}, lr {}, iterations {}'.format(batch, K.eval(lr), K.eval(iteration)))\n",
    "        \n",
    "def cyclic_cosine_anneal_schedule_itr(batch_logs, initial_lr=1e-3, update_window_size=15600):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with cosine annealing schedule per iteration.\n",
    "    '''\n",
    "    def lr_schedule(batch, logs):\n",
    "        \"\"\"Learning Rate Schedule\n",
    "\n",
    "        Learning rate is scheduled to be updated per epoch with a cosine function per iteration. \n",
    "        Learning rate is raised to initial_lr every snapshot_window_size.\n",
    "\n",
    "        # Arguments\n",
    "            epoch (int): The number of epochs\n",
    "\n",
    "        # Returns\n",
    "            lr (float32): learning rate\n",
    "        \"\"\"\n",
    "        iteration = model.optimizer.iterations\n",
    "        print('\\n Optimizer iteration {}, batch {}'.format(K.eval(iteration), batch))\n",
    "        lr = initial_lr / 2 * (math.cos(math.pi * ((K.eval(iteration) % update_window_size) / update_window_size)) + 1)\n",
    "        K.set_value(model.optimizer.lr, lr)\n",
    "        print('\\n Learning rate {}, Model learning rate {}'.format(lr, K.eval(model.optimizer.lr)))\n",
    "    \n",
    "    def batch_log(batch, logs):\n",
    "        batch_logs['iteration'].append(K.eval(model.optimizer.iterations))\n",
    "        batch_logs['lr'].append(K.eval(model.optimizer.lr))\n",
    "        batch_logs['loss'].append(logs['loss'])\n",
    "        batch_logs['acc'].append(logs['acc'])\n",
    "        \n",
    "    \n",
    "    return LambdaCallback(on_batch_begin=lr_schedule, on_batch_end=batch_log)\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            activation-bn-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet_v2(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "print('Building model...')\n",
    "if version == 2:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "else:\n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=initial_lr),\n",
    "              metrics=['accuracy'])\n",
    "# model.summary()\n",
    "print(model_type)\n",
    "\n",
    "# Prepare model model saving directory.\n",
    "if not save_dir:\n",
    "    save_dir = next_run_dir('{}/../model/run'.format(dirpath))\n",
    "os.makedirs(save_dir)\n",
    "model_name = 'cifar10_%s_model-{epoch:04d}.h5' % model_type\n",
    "# model_name = 'cifar10_%s_model-{epoch:04d}-{val_acc:.5f}.h5' % model_type\n",
    "# model_name = 'cifar10_{}_model.h5'.format(model_type)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "print('Preparing callbacks...')\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=False,\n",
    "                             mode='max')\n",
    "# Learning rate updater\n",
    "batch_num = int(x_train.shape[0]/batch_size)\n",
    "update_window_size = int(math.ceil(epochs*batch_num/top_k))\n",
    "batch_logs = {'iteration':[], 'lr':[], 'loss':[], 'acc':[]}\n",
    "lr_scheduler = cyclic_cosine_anneal_schedule_itr(initial_lr=initial_lr, \n",
    "                                                 update_window_size=update_window_size, batch_logs=batch_logs)\n",
    "\n",
    "# Training log writer\n",
    "csvlog = CSVLogger('callback_training_log.csv', separator=',', append=False)\n",
    "\n",
    "\n",
    "callbacks = [checkpoint, lr_scheduler, csvlog]\n",
    "\n",
    "# callbacks = [MyCallback()]\n",
    "        \n",
    "# Run training, with or without data augmentation.\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    print('Training...')\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_valid, y_valid),\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    print('Training...')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False)\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    steps_per_epoch = math.ceil(len(x_train) / batch_size)\n",
    "    history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        validation_data=(x_valid, y_valid),\n",
    "                        epochs=epochs, verbose=1, workers=4,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "# Score trained model.\n",
    "# scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "# print('Test loss:', scores[0])\n",
    "# print('Test accuracy:', scores[1])\n",
    "\n",
    "# Save training log\n",
    "print('Saving epoch training log...')\n",
    "train_error = history.history['loss']\n",
    "valid_accuracy = history.history['val_acc']\n",
    "logfile = '{}/training_log.csv'.format(save_dir)\n",
    "f = open(logfile, 'w')\n",
    "f.write('current_epoch,total_epochs,train_loss,validation_accuracy\\n')\n",
    "for i in range(len(train_error)):\n",
    "    f.write('{},{},{},{}\\n'.format(i+1, epochs, train_error[i], valid_accuracy[i]))\n",
    "f.close()\n",
    "\n",
    "print('Saving batch training log...')\n",
    "f = open('batch_training_log.csv', 'w')\n",
    "f.write('current_iteration,total_iteration,learning_rate,train_loss,train_accuracy\\n')\n",
    "total_iteration = int(math.ceil(x_train.shape[0]/batch_size)*epochs)\n",
    "for i in range(len(batch_logs['iteration'])):\n",
    "    f.write('{},{},{},{},{}\\n'.format(batch_logs['iteration'][i], \n",
    "                                   total_iteration, \n",
    "                                   batch_logs['lr'][i], \n",
    "                                   batch_logs['loss'][i], \n",
    "                                   batch_logs['acc'][i]))\n",
    "f.close()\n",
    "\n",
    "import csv\n",
    "with open('batch_training_log.csv', 'w') as f:  \n",
    "    w = csv.DictWriter(f, batch_logs.keys())\n",
    "    w.writeheader()\n",
    "    w.writerow(batch_logs)\n",
    "\n",
    "# Save index for combination\n",
    "print('Writing index file and predict files...')\n",
    "indexfile = '{}/index.csv'.format(save_dir)\n",
    "f = open(indexfile, 'w')\n",
    "window_size = int(epochs/top_k)\n",
    "top_x = []\n",
    "for i in range(0, top_k):\n",
    "    top_x.append(np.argmax(valid_accuracy[i*snapshot_window_size:(i+1)*snapshot_window_size]) + i*snapshot_window_size)\n",
    "top_v = [valid_accuracy[i] for i in top_x]\n",
    "for x,v in zip(top_x, top_v):\n",
    "    name = 'cifar10_{}_model-{:04d}.h5'.format(model_type, x+1)\n",
    "    weight = v\n",
    "    f.write('{},{}\\n'.format(name, weight))\n",
    "    # predicting\n",
    "    filepath = os.path.join(save_dir, name)\n",
    "    model.load_weights(filepath)\n",
    "    predicts = model.predict(x_test)\n",
    "    # Save predicts\n",
    "    predictfile = '{}/prediction_{:04d}.csv'.format(save_dir, x+1)\n",
    "    f1 = open(predictfile,'w')\n",
    "    header = '0,1,2,3,4,5,6,7,8,9\\n'\n",
    "    f1.write(header)\n",
    "    np.savetxt(f1, predicts, delimiter=\",\")\n",
    "    f1.close()\n",
    "f.close()\n",
    "\n",
    "# Delete unwanted model files\n",
    "print('Deleting unwanted model files...')\n",
    "no_top_x = range(len(valid_accuracy))\n",
    "no_top_x = list(set(no_top_x) - set(top_x))\n",
    "for no_top_x_index in no_top_x:\n",
    "    remove_filename = '{}/cifar10_{}_model-{:04d}.h5'.format(save_dir, model_type, no_top_x_index+1)\n",
    "    os.remove(remove_filename)\n",
    "\n",
    "# Save targets\n",
    "print('Saving target file...')\n",
    "targetfile = '{}/target.csv'.format(save_dir)\n",
    "f2 = open(targetfile,'w')\n",
    "header = '0,1,2,3,4,5,6,7,8,9\\n'\n",
    "f2.write(header)\n",
    "np.savetxt(f2, y_test, delimiter=\",\")\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500/4500 [==============================] - 2s 337us/step\n",
      "Test loss: 0.6096683431466421\n",
      "Test accuracy: 0.8351111112170749\n",
      "4500/4500 [==============================] - 1s 142us/step\n",
      "Test loss: 0.5120277782943514\n",
      "Test accuracy: 0.8691111110051473\n",
      "4500/4500 [==============================] - 1s 142us/step\n",
      "Test loss: 0.48847858993212384\n",
      "Test accuracy: 0.8855555554495917\n",
      "4500/4500 [==============================] - 1s 148us/step\n",
      "Test loss: 0.4810516203906801\n",
      "Test accuracy: 0.8931111111111111\n",
      "4500/4500 [==============================] - 1s 148us/step\n",
      "Test loss: 0.48305441366301644\n",
      "Test accuracy: 0.8942222222222223\n",
      "4500/4500 [==============================] - 1s 143us/step\n",
      "Test loss: 0.48185164595974816\n",
      "Test accuracy: 0.9008888888888889\n",
      "4500/4500 [==============================] - 1s 141us/step\n",
      "Test loss: 0.47659654211997987\n",
      "Test accuracy: 0.9022222222222223\n",
      "4500/4500 [==============================] - 1s 142us/step\n",
      "Test loss: 0.49599758842256336\n",
      "Test accuracy: 0.8997777777777778\n",
      "4500/4500 [==============================] - 1s 142us/step\n",
      "Test loss: 0.4874440699285931\n",
      "Test accuracy: 0.9046666666666666\n",
      "4500/4500 [==============================] - 1s 141us/step\n",
      "Test loss: 0.48776051804754467\n",
      "Test accuracy: 0.9044444444444445\n"
     ]
    }
   ],
   "source": [
    "logfile = '../model/run_100/training_log.csv'\n",
    "df = pd.read_csv(logfile, header=0)\n",
    "valid_accuracy = df['validation_accuracy'].values.tolist()\n",
    "top_x = []\n",
    "for i in range(0, 10):\n",
    "    top_x.append(np.argmax(valid_accuracy[i*snapshot_window_size:(i+1)*snapshot_window_size]) + i*snapshot_window_size)\n",
    "\n",
    "\n",
    "if version == 2:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "else:\n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=initial_lr),\n",
    "              metrics=['accuracy'])\n",
    "test_scores = []\n",
    "for k in top_x:\n",
    "    saved_model = '../model/run_100/cifar10_ResNet20v1_model-{:04d}.h5'.format(k+1)\n",
    "    model.load_weights(saved_model)\n",
    "    scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    test_scores.append(scores[1])\n",
    "test_scores_1 = test_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "4500/4500 [==============================] - 2s 411us/step\n",
      "Test loss: 0.623283628516727\n",
      "Test accuracy: 0.8286666666136847\n",
      "4500/4500 [==============================] - 1s 151us/step\n",
      "Test loss: 0.5248715903229184\n",
      "Test accuracy: 0.864\n",
      "4500/4500 [==============================] - 1s 149us/step\n",
      "Test loss: 0.4969917231135898\n",
      "Test accuracy: 0.8771111111111111\n",
      "4500/4500 [==============================] - 1s 152us/step\n",
      "Test loss: 0.4798820976946089\n",
      "Test accuracy: 0.8871111111111111\n",
      "4500/4500 [==============================] - 1s 149us/step\n",
      "Test loss: 0.49182780093616907\n",
      "Test accuracy: 0.8908888888888888\n",
      "4500/4500 [==============================] - 1s 163us/step\n",
      "Test loss: 0.4949813829925325\n",
      "Test accuracy: 0.8993333333333333\n",
      "4500/4500 [==============================] - 1s 151us/step\n",
      "Test loss: 0.500278210149871\n",
      "Test accuracy: 0.9004444444444445\n",
      "4500/4500 [==============================] - 1s 148us/step\n",
      "Test loss: 0.5235393349462085\n",
      "Test accuracy: 0.8928888888888888\n",
      "4500/4500 [==============================] - 1s 148us/step\n",
      "Test loss: 0.4998899468051063\n",
      "Test accuracy: 0.9042222222222223\n",
      "4500/4500 [==============================] - 1s 149us/step\n",
      "Test loss: 0.5158649944994185\n",
      "Test accuracy: 0.9002222222222223\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "if version == 2:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "else:\n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=initial_lr),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "logfile = '../model/run_200/training_log.csv'\n",
    "df = pd.read_csv(logfile, header=0)\n",
    "valid_accuracy = df['validation_accuracy'].values.tolist()\n",
    "version = 1\n",
    "snapshot_window_size = 20\n",
    "top_x = []\n",
    "for i in range(0, 10):\n",
    "    top_x.append(np.argmax(valid_accuracy[i*snapshot_window_size:(i+1)*snapshot_window_size]) + i*snapshot_window_size)\n",
    "\n",
    "\n",
    "if version == 2:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "else:\n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=initial_lr),\n",
    "              metrics=['accuracy'])\n",
    "test_scores = []\n",
    "for k in top_x:\n",
    "    saved_model = '../model/run_200/cifar10_ResNet20v1_model-{:04d}.h5'.format(k+1)\n",
    "    model.load_weights(saved_model)\n",
    "    scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    test_scores.append(scores[1])\n",
    "test_scores_2 = test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:51: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 86.4\n",
      "Test accuracy: 87.71111111111111\n",
      "Test accuracy: 88.71111111111111\n",
      "Test accuracy: 89.08888888888889\n",
      "Test accuracy: 89.93333333333334\n",
      "Test accuracy: 90.04444444444445\n",
      "Test accuracy: 89.28888888888889\n",
      "Test accuracy: 90.42222222222223\n",
      "Test accuracy: 90.02222222222223\n"
     ]
    }
   ],
   "source": [
    "import common_functions as cf\n",
    "logfile = '../model/run_200/training_log.csv'\n",
    "df = pd.read_csv(logfile, header=0)\n",
    "valid_accuracy = df['validation_accuracy'].values.tolist()\n",
    "version = 1\n",
    "snapshot_window_size = 20\n",
    "top_x = []\n",
    "for i in range(1, 10):\n",
    "    top_x.append(np.argmax(valid_accuracy[i*snapshot_window_size:(i+1)*snapshot_window_size]) + i*snapshot_window_size)\n",
    "\n",
    "groundfile = '../model/run_200/target.csv' \n",
    "df_g = pd.read_csv(groundfile,header=0)\n",
    "test_scores = []\n",
    "for k in top_x:\n",
    "    predfile = '../model/run_200/prediction_{:04d}.csv'.format(k+1)  \n",
    "    df_p = pd.read_csv(predfile,header=0)     \n",
    "    # compute confusion matrix\n",
    "    cm = cf.confusion_matrix(df_g, df_p)\n",
    "    tp, total = 0, 0\n",
    "    for i in range(len(cm)):\n",
    "        tp += cm[i,i]\n",
    "        total += np.sum(cm[i])\n",
    "    ea = tp/total*100\n",
    "    print('Test accuracy:', ea)\n",
    "    test_scores.append(ea)\n",
    "test_scores_3 = test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
