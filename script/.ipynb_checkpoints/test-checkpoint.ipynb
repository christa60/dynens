{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Resampling training and validating data sets...\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "4500 test samples\n",
      "y_train shape: (50000, 1)\n",
      "4500 valid samples\n",
      "Building model...\n",
      "ResNet20v1\n",
      "Preparing callbacks...\n",
      "Using real-time data augmentation.\n",
      "Training...\n",
      "Epoch 1/10\n",
      "\n",
      " Optimizer iteration 0, batch 0\n",
      "\n",
      " Learning rate 0.001, Model learning rate 0.0010000000474974513\n",
      "  1/391 [..............................] - ETA: 2:25:18 - loss: 5.0859 - acc: 0.0703\n",
      " Optimizer iteration 1, batch 1\n",
      "\n",
      " Learning rate 0.0009999993511109622, Model learning rate 0.0009999993490055203\n",
      "\n",
      " Optimizer iteration 2, batch 2\n",
      "\n",
      " Learning rate 0.0009999974044455327, Model learning rate 0.0009999973699450493\n",
      "  3/391 [..............................] - ETA: 48:22 - loss: 4.2428 - acc: 0.0729  \n",
      " Optimizer iteration 3, batch 3\n",
      "\n",
      " Learning rate 0.0009999941600087644, Model learning rate 0.0009999941103160381\n",
      "\n",
      " Optimizer iteration 4, batch 4\n",
      "\n",
      " Learning rate 0.0009999896178090784, Model learning rate 0.0009999895701184869\n",
      "  5/391 [..............................] - ETA: 28:59 - loss: 3.7454 - acc: 0.1016\n",
      " Optimizer iteration 5, batch 5\n",
      "\n",
      " Learning rate 0.000999983777858264, Model learning rate 0.0009999837493523955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/callbacks.py:99: UserWarning: Method on_batch_begin() is slow compared to the batch update (9.238642). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/391 [..............................] - ETA: 24:09 - loss: 3.5788 - acc: 0.1159\n",
      " Optimizer iteration 6, batch 6\n",
      "\n",
      " Learning rate 0.0009999766401714793, Model learning rate 0.000999976648017764\n",
      "\n",
      " Optimizer iteration 7, batch 7\n",
      "\n",
      " Learning rate 0.0009999682047672506, Model learning rate 0.0009999681496992707\n",
      "  8/391 [..............................] - ETA: 18:06 - loss: 3.2794 - acc: 0.1484\n",
      " Optimizer iteration 8, batch 8\n",
      "\n",
      " Learning rate 0.0009999584716674725, Model learning rate 0.000999958487227559\n",
      "\n",
      " Optimizer iteration 9, batch 9\n",
      "\n",
      " Learning rate 0.000999947440897408, Model learning rate 0.0009999474277719855\n",
      " 10/391 [..............................] - ETA: 14:28 - loss: 3.1059 - acc: 0.1648\n",
      " Optimizer iteration 10, batch 10\n",
      "\n",
      " Learning rate 0.0009999351124856874, Model learning rate 0.0009999350877478719\n",
      " 11/391 [..............................] - ETA: 13:09 - loss: 3.0336 - acc: 0.1733\n",
      " Optimizer iteration 11, batch 11\n",
      "\n",
      " Learning rate 0.0009999214864643102, Model learning rate 0.0009999214671552181\n",
      " 12/391 [..............................] - ETA: 12:02 - loss: 2.9751 - acc: 0.1777\n",
      " Optimizer iteration 12, batch 12\n",
      "\n",
      " Learning rate 0.0009999065628686437, Model learning rate 0.0009999065659940243\n",
      " 13/391 [..............................] - ETA: 11:07 - loss: 2.9225 - acc: 0.1791\n",
      " Optimizer iteration 13, batch 13\n",
      "\n",
      " Learning rate 0.0009998903417374227, Model learning rate 0.0009998903842642903\n",
      " 14/391 [>.............................] - ETA: 10:19 - loss: 2.8717 - acc: 0.1908\n",
      " Optimizer iteration 14, batch 14\n",
      "\n",
      " Learning rate 0.00099987282311275, Model learning rate 0.0009998728055506945\n",
      "\n",
      " Optimizer iteration 15, batch 15\n",
      "\n",
      " Learning rate 0.0009998540070400965, Model learning rate 0.0009998540626838803\n",
      " 16/391 [>.............................] - ETA: 9:01 - loss: 2.7898 - acc: 0.1978 \n",
      " Optimizer iteration 16, batch 16\n",
      "\n",
      " Learning rate 0.0009998338935683, Model learning rate 0.0009998339228332043\n",
      "\n",
      " Optimizer iteration 17, batch 17\n",
      "\n",
      " Learning rate 0.0009998124827495663, Model learning rate 0.0009998125024139881\n",
      " 18/391 [>.............................] - ETA: 8:00 - loss: 2.7292 - acc: 0.2044\n",
      " Optimizer iteration 18, batch 18\n",
      "\n",
      " Learning rate 0.0009997897746394685, Model learning rate 0.0009997898014262319\n",
      "\n",
      " Optimizer iteration 19, batch 19\n",
      "\n",
      " Learning rate 0.0009997657692969464, Model learning rate 0.0009997658198699355\n",
      " 20/391 [>.............................] - ETA: 7:12 - loss: 2.6759 - acc: 0.2137\n",
      " Optimizer iteration 20, batch 20\n",
      "\n",
      " Learning rate 0.0009997404667843074, Model learning rate 0.0009997404413297772\n",
      " 21/391 [>.............................] - ETA: 6:51 - loss: 2.6483 - acc: 0.2169\n",
      " Optimizer iteration 21, batch 21\n",
      "\n",
      " Learning rate 0.000999713867167226, Model learning rate 0.0009997138986364007\n",
      " 22/391 [>.............................] - ETA: 6:32 - loss: 2.6335 - acc: 0.2191\n",
      " Optimizer iteration 22, batch 22\n",
      "\n",
      " Learning rate 0.0009996859705147423, Model learning rate 0.0009996859589591622\n",
      " 23/391 [>.............................] - ETA: 6:16 - loss: 2.6093 - acc: 0.2221\n",
      " Optimizer iteration 23, batch 23\n",
      "\n",
      " Learning rate 0.0009996567768992641, Model learning rate 0.0009996567387133837\n",
      " 24/391 [>.............................] - ETA: 6:01 - loss: 2.5857 - acc: 0.2246\n",
      " Optimizer iteration 24, batch 24\n",
      "\n",
      " Learning rate 0.000999626286396565, Model learning rate 0.000999626237899065\n",
      " 25/391 [>.............................] - ETA: 5:46 - loss: 2.5660 - acc: 0.2263\n",
      " Optimizer iteration 25, batch 25\n",
      "\n",
      " Learning rate 0.0009995944990857848, Model learning rate 0.0009995944565162063\n",
      "\n",
      " Optimizer iteration 26, batch 26\n",
      "\n",
      " Learning rate 0.0009995614150494292, Model learning rate 0.0009995613945648074\n",
      " 27/391 [=>............................] - ETA: 5:21 - loss: 2.5353 - acc: 0.2312\n",
      " Optimizer iteration 27, batch 27\n",
      "\n",
      " Learning rate 0.0009995270343733697, Model learning rate 0.0009995270520448685\n",
      " 28/391 [=>............................] - ETA: 5:10 - loss: 2.5230 - acc: 0.2335\n",
      " Optimizer iteration 28, batch 28\n",
      "\n",
      " Learning rate 0.000999491357146843, Model learning rate 0.0009994913125410676\n",
      " 29/391 [=>............................] - ETA: 4:59 - loss: 2.5066 - acc: 0.2357\n",
      " Optimizer iteration 29, batch 29\n",
      "\n",
      " Learning rate 0.0009994543834624518, Model learning rate 0.0009994544088840485\n",
      " 30/391 [=>............................] - ETA: 4:49 - loss: 2.4877 - acc: 0.2393\n",
      " Optimizer iteration 30, batch 30\n",
      "\n",
      " Learning rate 0.0009994161134161633, Model learning rate 0.0009994161082431674\n",
      " 31/391 [=>............................] - ETA: 4:40 - loss: 2.4726 - acc: 0.2402\n",
      " Optimizer iteration 31, batch 31\n",
      "\n",
      " Learning rate 0.0009993765471073093, Model learning rate 0.0009993765270337462\n",
      " 32/391 [=>............................] - ETA: 4:31 - loss: 2.4559 - acc: 0.2437\n",
      " Optimizer iteration 32, batch 32\n",
      "\n",
      " Learning rate 0.0009993356846385866, Model learning rate 0.000999335665255785\n",
      " 33/391 [=>............................] - ETA: 4:23 - loss: 2.4408 - acc: 0.2455\n",
      " Optimizer iteration 33, batch 33\n",
      "\n",
      " Learning rate 0.000999293526116056, Model learning rate 0.0009992935229092836\n",
      " 34/391 [=>............................] - ETA: 4:15 - loss: 2.4244 - acc: 0.2495\n",
      " Optimizer iteration 34, batch 34\n",
      "\n",
      " Learning rate 0.000999250071649142, Model learning rate 0.0009992500999942422\n",
      " 35/391 [=>............................] - ETA: 4:08 - loss: 2.4080 - acc: 0.2540\n",
      " Optimizer iteration 35, batch 35\n",
      "\n",
      " Learning rate 0.0009992053213506334, Model learning rate 0.0009992052800953388\n",
      " 36/391 [=>............................] - ETA: 4:01 - loss: 2.3961 - acc: 0.2572\n",
      " Optimizer iteration 36, batch 36\n",
      "\n",
      " Learning rate 0.000999159275336682, Model learning rate 0.0009991592960432172\n",
      " 37/391 [=>............................] - ETA: 3:55 - loss: 2.3876 - acc: 0.2578\n",
      " Optimizer iteration 37, batch 37\n",
      "\n",
      " Learning rate 0.0009991119337268031, Model learning rate 0.0009991119150072336\n",
      " 38/391 [=>............................] - ETA: 3:49 - loss: 2.3805 - acc: 0.2601\n",
      " Optimizer iteration 38, batch 38\n",
      "\n",
      " Learning rate 0.0009990632966438743, Model learning rate 0.00099906325340271\n",
      " 39/391 [=>............................] - ETA: 3:43 - loss: 2.3678 - acc: 0.2634\n",
      " Optimizer iteration 39, batch 39\n",
      "\n",
      " Learning rate 0.0009990133642141358, Model learning rate 0.0009990133112296462\n",
      " 40/391 [==>...........................] - ETA: 3:38 - loss: 2.3559 - acc: 0.2652\n",
      " Optimizer iteration 40, batch 40\n",
      "\n",
      " Learning rate 0.0009989621365671902, Model learning rate 0.0009989620884880424\n",
      " 41/391 [==>...........................] - ETA: 3:33 - loss: 2.3452 - acc: 0.2662\n",
      " Optimizer iteration 41, batch 41\n",
      "\n",
      " Learning rate 0.0009989096138360014, Model learning rate 0.0009989095851778984\n",
      " 42/391 [==>...........................] - ETA: 3:27 - loss: 2.3360 - acc: 0.2673\n",
      " Optimizer iteration 42, batch 42\n",
      "\n",
      " Learning rate 0.0009988557961568955, Model learning rate 0.0009988558012992144\n",
      " 43/391 [==>...........................] - ETA: 3:22 - loss: 2.3254 - acc: 0.2693\n",
      " Optimizer iteration 43, batch 43\n",
      "\n",
      " Learning rate 0.000998800683669559, Model learning rate 0.0009988007368519902\n",
      " 44/391 [==>...........................] - ETA: 3:18 - loss: 2.3175 - acc: 0.2708\n",
      " Optimizer iteration 44, batch 44\n",
      "\n",
      " Learning rate 0.0009987442765170397, Model learning rate 0.0009987442754209042\n",
      " 45/391 [==>...........................] - ETA: 3:14 - loss: 2.3080 - acc: 0.2733\n",
      " Optimizer iteration 45, batch 45\n",
      "\n",
      " Learning rate 0.0009986865748457456, Model learning rate 0.000998686533421278\n",
      " 46/391 [==>...........................] - ETA: 3:10 - loss: 2.2952 - acc: 0.2763\n",
      " Optimizer iteration 46, batch 46\n",
      "\n",
      " Learning rate 0.000998627578805444, Model learning rate 0.0009986276272684336\n",
      "\n",
      " Optimizer iteration 47, batch 47\n",
      "\n",
      " Learning rate 0.0009985672885492634, Model learning rate 0.0009985673241317272\n",
      " 48/391 [==>...........................] - ETA: 3:02 - loss: 2.2784 - acc: 0.2801\n",
      " Optimizer iteration 48, batch 48\n",
      "\n",
      " Learning rate 0.0009985057042336898, Model learning rate 0.0009985057404264808\n",
      " 49/391 [==>...........................] - ETA: 2:58 - loss: 2.2725 - acc: 0.2814\n",
      " Optimizer iteration 49, batch 49\n",
      "\n",
      " Learning rate 0.000998442826018569, Model learning rate 0.0009984428761526942\n",
      " 50/391 [==>...........................] - ETA: 2:54 - loss: 2.2657 - acc: 0.2828\n",
      " Optimizer iteration 50, batch 50\n",
      "\n",
      " Learning rate 0.000998378654067105, Model learning rate 0.0009983786148950458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizer iteration 51, batch 51\n",
      "\n",
      " Learning rate 0.0009983131885458597, Model learning rate 0.000998313189484179\n",
      " 52/391 [==>...........................] - ETA: 2:48 - loss: 2.2517 - acc: 0.2864\n",
      " Optimizer iteration 52, batch 52\n",
      "\n",
      " Learning rate 0.0009982464296247522, Model learning rate 0.0009982464835047722\n",
      " 53/391 [===>..........................] - ETA: 2:45 - loss: 2.2469 - acc: 0.2885\n",
      " Optimizer iteration 53, batch 53\n",
      "\n",
      " Learning rate 0.0009981783774770593, Model learning rate 0.0009981783805415034\n",
      " 54/391 [===>..........................] - ETA: 2:42 - loss: 2.2403 - acc: 0.2902\n",
      " Optimizer iteration 54, batch 54\n",
      "\n",
      " Learning rate 0.0009981090322794144, Model learning rate 0.0009981089970096946\n",
      " 55/391 [===>..........................] - ETA: 2:39 - loss: 2.2302 - acc: 0.2935\n",
      " Optimizer iteration 55, batch 55\n",
      "\n",
      " Learning rate 0.0009980383942118066, Model learning rate 0.0009980384493246675\n",
      " 56/391 [===>..........................] - ETA: 2:36 - loss: 2.2256 - acc: 0.2946\n",
      " Optimizer iteration 56, batch 56\n",
      "\n",
      " Learning rate 0.0009979664634575808, Model learning rate 0.0009979665046557784\n",
      " 57/391 [===>..........................] - ETA: 2:33 - loss: 2.2201 - acc: 0.2951\n",
      " Optimizer iteration 57, batch 57\n",
      "\n",
      " Learning rate 0.0009978932402034376, Model learning rate 0.0009978932794183493\n",
      " 58/391 [===>..........................] - ETA: 2:31 - loss: 2.2141 - acc: 0.2961\n",
      " Optimizer iteration 58, batch 58\n",
      "\n",
      " Learning rate 0.000997818724639432, Model learning rate 0.00099781877361238\n",
      " 59/391 [===>..........................] - ETA: 2:28 - loss: 2.2089 - acc: 0.2959\n",
      " Optimizer iteration 59, batch 59\n",
      "\n",
      " Learning rate 0.0009977429169589732, Model learning rate 0.0009977428708225489\n",
      " 60/391 [===>..........................] - ETA: 2:26 - loss: 2.2029 - acc: 0.2982\n",
      " Optimizer iteration 60, batch 60\n",
      "\n",
      " Learning rate 0.0009976658173588243, Model learning rate 0.0009976658038794994\n",
      " 61/391 [===>..........................] - ETA: 2:23 - loss: 2.1959 - acc: 0.2997\n",
      " Optimizer iteration 61, batch 61\n",
      "\n",
      " Learning rate 0.0009975874260391019, Model learning rate 0.00099758745636791\n",
      " 62/391 [===>..........................] - ETA: 2:21 - loss: 2.1914 - acc: 0.3015\n",
      " Optimizer iteration 62, batch 62\n",
      "\n",
      " Learning rate 0.0009975077432032749, Model learning rate 0.0009975077118724585\n",
      "\n",
      " Optimizer iteration 63, batch 63\n",
      "\n",
      " Learning rate 0.0009974267690581644, Model learning rate 0.0009974268032237887\n",
      " 64/391 [===>..........................] - ETA: 2:16 - loss: 2.1807 - acc: 0.3044\n",
      " Optimizer iteration 64, batch 64\n",
      "\n",
      " Learning rate 0.0009973445038139437, Model learning rate 0.000997344497591257\n",
      " 65/391 [===>..........................] - ETA: 2:14 - loss: 2.1747 - acc: 0.3063\n",
      " Optimizer iteration 65, batch 65\n",
      "\n",
      " Learning rate 0.0009972609476841367, Model learning rate 0.0009972609113901854\n",
      " 66/391 [====>.........................] - ETA: 2:12 - loss: 2.1674 - acc: 0.3079\n",
      " Optimizer iteration 66, batch 66\n",
      "\n",
      " Learning rate 0.000997176100885618, Model learning rate 0.0009971760446205735\n",
      " 67/391 [====>.........................] - ETA: 2:10 - loss: 2.1635 - acc: 0.3088\n",
      " Optimizer iteration 67, batch 67\n",
      "\n",
      " Learning rate 0.000997089963638612, Model learning rate 0.0009970900136977434\n",
      " 68/391 [====>.........................] - ETA: 2:08 - loss: 2.1599 - acc: 0.3092\n",
      " Optimizer iteration 68, batch 68\n",
      "\n",
      " Learning rate 0.0009970025361666932, Model learning rate 0.0009970025857910514\n",
      " 69/391 [====>.........................] - ETA: 2:06 - loss: 2.1562 - acc: 0.3106\n",
      " Optimizer iteration 69, batch 69\n",
      "\n",
      " Learning rate 0.0009969138186967843, Model learning rate 0.0009969137609004974\n",
      "\n",
      " Optimizer iteration 70, batch 70\n",
      "\n",
      " Learning rate 0.0009968238114591566, Model learning rate 0.0009968237718567252\n",
      " 71/391 [====>.........................] - ETA: 2:02 - loss: 2.1472 - acc: 0.3124\n",
      " Optimizer iteration 71, batch 71\n",
      "\n",
      " Learning rate 0.0009967325146874287, Model learning rate 0.000996732502244413\n",
      " 72/391 [====>.........................] - ETA: 2:01 - loss: 2.1435 - acc: 0.3132\n",
      " Optimizer iteration 72, batch 72\n",
      "\n",
      " Learning rate 0.0009966399286185665, Model learning rate 0.0009966399520635605\n",
      " 73/391 [====>.........................] - ETA: 1:59 - loss: 2.1393 - acc: 0.3137\n",
      " Optimizer iteration 73, batch 73\n",
      "\n",
      " Learning rate 0.0009965460534928825, Model learning rate 0.0009965460048988461\n",
      " 74/391 [====>.........................] - ETA: 1:58 - loss: 2.1357 - acc: 0.3149\n",
      " Optimizer iteration 74, batch 74\n",
      "\n",
      " Learning rate 0.0009964508895540349, Model learning rate 0.0009964508935809135\n",
      "\n",
      " Optimizer iteration 75, batch 75\n",
      "\n",
      " Learning rate 0.000996354437049027, Model learning rate 0.000996354385279119\n",
      " 76/391 [====>.........................] - ETA: 1:54 - loss: 2.1329 - acc: 0.3155\n",
      " Optimizer iteration 76, batch 76\n",
      "\n",
      " Learning rate 0.0009962566962282066, Model learning rate 0.0009962567128241062\n",
      " 77/391 [====>.........................] - ETA: 1:53 - loss: 2.1293 - acc: 0.3152\n",
      " Optimizer iteration 77, batch 77\n",
      "\n",
      " Learning rate 0.0009961576673452655, Model learning rate 0.0009961576433852315\n",
      " 78/391 [====>.........................] - ETA: 1:51 - loss: 2.1253 - acc: 0.3165\n",
      " Optimizer iteration 78, batch 78\n",
      "\n",
      " Learning rate 0.000996057350657239, Model learning rate 0.0009960572933778167\n",
      " 79/391 [=====>........................] - ETA: 1:50 - loss: 2.1193 - acc: 0.3182\n",
      " Optimizer iteration 79, batch 79\n",
      "\n",
      " Learning rate 0.0009959557464245042, Model learning rate 0.0009959557792171836\n",
      " 80/391 [=====>........................] - ETA: 1:49 - loss: 2.1142 - acc: 0.3196\n",
      " Optimizer iteration 80, batch 80\n",
      "\n",
      " Learning rate 0.000995852854910781, Model learning rate 0.0009958528680726886\n",
      " 81/391 [=====>........................] - ETA: 1:47 - loss: 2.1116 - acc: 0.3201\n",
      " Optimizer iteration 81, batch 81\n",
      "\n",
      " Learning rate 0.00099574867638313, Model learning rate 0.0009957486763596535\n",
      " 82/391 [=====>........................] - ETA: 1:46 - loss: 2.1069 - acc: 0.3214\n",
      " Optimizer iteration 82, batch 82\n",
      "\n",
      " Learning rate 0.0009956432111119522, Model learning rate 0.0009956432040780783\n",
      " 83/391 [=====>........................] - ETA: 1:44 - loss: 2.1041 - acc: 0.3224\n",
      " Optimizer iteration 83, batch 83\n",
      "\n",
      " Learning rate 0.000995536459370989, Model learning rate 0.000995536451227963\n",
      " 84/391 [=====>........................] - ETA: 1:43 - loss: 2.1000 - acc: 0.3239\n",
      " Optimizer iteration 84, batch 84\n",
      "\n",
      " Learning rate 0.0009954284214373204, Model learning rate 0.0009954284178093076\n",
      " 85/391 [=====>........................] - ETA: 1:42 - loss: 2.0960 - acc: 0.3253\n",
      " Optimizer iteration 85, batch 85\n",
      "\n",
      " Learning rate 0.0009953190975913646, Model learning rate 0.000995319103822112\n",
      " 86/391 [=====>........................] - ETA: 1:41 - loss: 2.0929 - acc: 0.3257\n",
      " Optimizer iteration 86, batch 86\n",
      "\n",
      " Learning rate 0.0009952084881168783, Model learning rate 0.0009952085092663765\n",
      "\n",
      " Optimizer iteration 87, batch 87\n",
      "\n",
      " Learning rate 0.0009950965933009544, Model learning rate 0.0009950966341421008\n",
      " 88/391 [=====>........................] - ETA: 1:38 - loss: 2.0863 - acc: 0.3280\n",
      " Optimizer iteration 88, batch 88\n",
      "\n",
      " Learning rate 0.000994983413434022, Model learning rate 0.0009949833620339632\n",
      " 89/391 [=====>........................] - ETA: 1:37 - loss: 2.0836 - acc: 0.3291\n",
      " Optimizer iteration 89, batch 89\n",
      "\n",
      " Learning rate 0.000994868948809846, Model learning rate 0.0009948689257726073\n",
      "\n",
      " Optimizer iteration 90, batch 90\n",
      "\n",
      " Learning rate 0.0009947531997255255, Model learning rate 0.0009947532089427114\n",
      " 91/391 [=====>........................] - ETA: 1:35 - loss: 2.0776 - acc: 0.3309\n",
      " Optimizer iteration 91, batch 91\n",
      "\n",
      " Learning rate 0.0009946361664814943, Model learning rate 0.0009946362115442753\n",
      " 92/391 [======>.......................] - ETA: 1:34 - loss: 2.0743 - acc: 0.3316\n",
      " Optimizer iteration 92, batch 92\n",
      "\n",
      " Learning rate 0.0009945178493815181, Model learning rate 0.0009945178171619773\n",
      " 93/391 [======>.......................] - ETA: 1:33 - loss: 2.0693 - acc: 0.3331\n",
      " Optimizer iteration 93, batch 93\n",
      "\n",
      " Learning rate 0.000994398248732696, Model learning rate 0.000994398258626461\n",
      " 94/391 [======>.......................] - ETA: 1:32 - loss: 2.0664 - acc: 0.3340\n",
      " Optimizer iteration 94, batch 94\n",
      "\n",
      " Learning rate 0.000994277364845458, Model learning rate 0.0009942774195224047\n",
      "\n",
      " Optimizer iteration 95, batch 95\n",
      "\n",
      " Learning rate 0.0009941551980335653, Model learning rate 0.0009941551834344864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/391 [======>.......................] - ETA: 1:30 - loss: 2.0618 - acc: 0.3349\n",
      " Optimizer iteration 96, batch 96\n",
      "\n",
      " Learning rate 0.0009940317486141082, Model learning rate 0.0009940317831933498\n",
      " 97/391 [======>.......................] - ETA: 1:29 - loss: 2.0576 - acc: 0.3363\n",
      " Optimizer iteration 97, batch 97\n",
      "\n",
      " Learning rate 0.0009939070169075071, Model learning rate 0.0009939069859683514\n",
      " 98/391 [======>.......................] - ETA: 1:28 - loss: 2.0531 - acc: 0.3375\n",
      " Optimizer iteration 98, batch 98\n",
      "\n",
      " Learning rate 0.00099378100323751, Model learning rate 0.0009937810245901346\n",
      " 99/391 [======>.......................] - ETA: 1:26 - loss: 2.0500 - acc: 0.3391\n",
      " Optimizer iteration 99, batch 99\n",
      "\n",
      " Learning rate 0.0009936537079311926, Model learning rate 0.000993653666228056\n",
      "100/391 [======>.......................] - ETA: 1:26 - loss: 2.0462 - acc: 0.3403\n",
      " Optimizer iteration 100, batch 100\n",
      "\n",
      " Learning rate 0.0009935251313189565, Model learning rate 0.000993525143712759\n",
      "\n",
      " Optimizer iteration 101, batch 101\n",
      "\n",
      " Learning rate 0.00099339527373453, Model learning rate 0.0009933952242136002\n",
      "102/391 [======>.......................] - ETA: 1:24 - loss: 2.0403 - acc: 0.3425\n",
      " Optimizer iteration 102, batch 102\n",
      "\n",
      " Learning rate 0.0009932641355149653, Model learning rate 0.000993264140561223\n",
      "\n",
      " Optimizer iteration 103, batch 103\n",
      "\n",
      " Learning rate 0.0009931317170006398, Model learning rate 0.000993131659924984\n",
      "104/391 [======>.......................] - ETA: 1:22 - loss: 2.0353 - acc: 0.3444\n",
      " Optimizer iteration 104, batch 104\n",
      "\n",
      " Learning rate 0.0009929980185352525, Model learning rate 0.0009929980151355267\n",
      "105/391 [=======>......................] - ETA: 1:21 - loss: 2.0319 - acc: 0.3453\n",
      " Optimizer iteration 105, batch 105\n",
      "\n",
      " Learning rate 0.0009928630404658254, Model learning rate 0.0009928630897775292\n",
      "106/391 [=======>......................] - ETA: 1:20 - loss: 2.0280 - acc: 0.3465\n",
      " Optimizer iteration 106, batch 106\n",
      "\n",
      " Learning rate 0.0009927267831427017, Model learning rate 0.00099272676743567\n",
      "\n",
      " Optimizer iteration 107, batch 107\n",
      "\n",
      " Learning rate 0.0009925892469195452, Model learning rate 0.0009925892809405923\n",
      "108/391 [=======>......................] - ETA: 1:19 - loss: 2.0207 - acc: 0.3484\n",
      " Optimizer iteration 108, batch 108\n",
      "\n",
      " Learning rate 0.0009924504321533387, Model learning rate 0.0009924503974616528\n",
      "109/391 [=======>......................] - ETA: 1:18 - loss: 2.0188 - acc: 0.3490\n",
      " Optimizer iteration 109, batch 109\n",
      "\n",
      " Learning rate 0.0009923103392043835, Model learning rate 0.000992310349829495\n",
      "110/391 [=======>......................] - ETA: 1:17 - loss: 2.0163 - acc: 0.3496\n",
      " Optimizer iteration 110, batch 110\n",
      "\n",
      " Learning rate 0.000992168968436299, Model learning rate 0.000992169021628797\n",
      "111/391 [=======>......................] - ETA: 1:16 - loss: 2.0133 - acc: 0.3503\n",
      " Optimizer iteration 111, batch 111\n",
      "\n",
      " Learning rate 0.0009920263202160206, Model learning rate 0.0009920262964442372\n",
      "112/391 [=======>......................] - ETA: 1:16 - loss: 2.0104 - acc: 0.3509\n",
      " Optimizer iteration 112, batch 112\n",
      "\n",
      " Learning rate 0.0009918823949138004, Model learning rate 0.0009918824071064591\n",
      "113/391 [=======>......................] - ETA: 1:15 - loss: 2.0081 - acc: 0.3515\n",
      " Optimizer iteration 113, batch 113\n",
      "\n",
      " Learning rate 0.000991737192903204, Model learning rate 0.000991737237200141\n",
      "114/391 [=======>......................] - ETA: 1:14 - loss: 2.0061 - acc: 0.3516\n",
      " Optimizer iteration 114, batch 114\n",
      "\n",
      " Learning rate 0.0009915907145611115, Model learning rate 0.0009915906703099608\n",
      "115/391 [=======>......................] - ETA: 1:13 - loss: 2.0024 - acc: 0.3531\n",
      " Optimizer iteration 115, batch 115\n",
      "\n",
      " Learning rate 0.0009914429602677162, Model learning rate 0.0009914429392665625\n",
      "116/391 [=======>......................] - ETA: 1:13 - loss: 1.9997 - acc: 0.3538\n",
      " Optimizer iteration 116, batch 116\n",
      "\n",
      " Learning rate 0.0009912939304065219, Model learning rate 0.000991293927654624\n",
      "117/391 [=======>......................] - ETA: 1:12 - loss: 1.9972 - acc: 0.3544\n",
      " Optimizer iteration 117, batch 117\n",
      "\n",
      " Learning rate 0.0009911436253643444, Model learning rate 0.0009911436354741454\n",
      "\n",
      " Optimizer iteration 118, batch 118\n",
      "\n",
      " Learning rate 0.0009909920455313088, Model learning rate 0.0009909920627251267\n",
      "119/391 [========>.....................] - ETA: 1:10 - loss: 1.9916 - acc: 0.3552\n",
      " Optimizer iteration 119, batch 119\n",
      "\n",
      " Learning rate 0.000990839191300849, Model learning rate 0.000990839209407568\n",
      "120/391 [========>.....................] - ETA: 1:10 - loss: 1.9885 - acc: 0.3564\n",
      " Optimizer iteration 120, batch 120\n",
      "\n",
      " Learning rate 0.0009906850630697068, Model learning rate 0.0009906850755214691\n",
      "121/391 [========>.....................] - ETA: 1:09 - loss: 1.9849 - acc: 0.3579\n",
      " Optimizer iteration 121, batch 121\n",
      "\n",
      " Learning rate 0.0009905296612379307, Model learning rate 0.0009905296610668302\n",
      "122/391 [========>.....................] - ETA: 1:08 - loss: 1.9814 - acc: 0.3587\n",
      " Optimizer iteration 122, batch 122\n",
      "\n",
      " Learning rate 0.0009903729862088748, Model learning rate 0.000990372966043651\n",
      "\n",
      " Optimizer iteration 123, batch 123\n",
      "\n",
      " Learning rate 0.0009902150383891979, Model learning rate 0.000990214990451932\n",
      "124/391 [========>.....................] - ETA: 1:07 - loss: 1.9766 - acc: 0.3594\n",
      " Optimizer iteration 124, batch 124\n",
      "\n",
      " Learning rate 0.0009900558181888627, Model learning rate 0.0009900558507069945\n",
      "125/391 [========>.....................] - ETA: 1:07 - loss: 1.9747 - acc: 0.3600\n",
      " Optimizer iteration 125, batch 125\n",
      "\n",
      " Learning rate 0.0009898953260211339, Model learning rate 0.0009898953139781952\n",
      "126/391 [========>.....................] - ETA: 1:06 - loss: 1.9721 - acc: 0.3610\n",
      " Optimizer iteration 126, batch 126\n",
      "\n",
      " Learning rate 0.000989733562302578, Model learning rate 0.0009897336130961776\n",
      "\n",
      " Optimizer iteration 127, batch 127\n",
      "\n",
      " Learning rate 0.0009895705274530619, Model learning rate 0.000989570515230298\n",
      "128/391 [========>.....................] - ETA: 1:05 - loss: 1.9681 - acc: 0.3624\n",
      " Optimizer iteration 128, batch 128\n",
      "\n",
      " Learning rate 0.0009894062218957515, Model learning rate 0.0009894062532112002\n",
      "129/391 [========>.....................] - ETA: 1:04 - loss: 1.9664 - acc: 0.3626\n",
      " Optimizer iteration 129, batch 129\n",
      "\n",
      " Learning rate 0.0009892406460571114, Model learning rate 0.0009892405942082405\n",
      "\n",
      " Optimizer iteration 130, batch 130\n",
      "\n",
      " Learning rate 0.0009890738003669028, Model learning rate 0.0009890737710520625\n",
      "131/391 [=========>....................] - ETA: 1:03 - loss: 1.9626 - acc: 0.3637\n",
      " Optimizer iteration 131, batch 131\n",
      "\n",
      " Learning rate 0.000988905685258183, Model learning rate 0.0009889056673273444\n",
      "132/391 [=========>....................] - ETA: 1:02 - loss: 1.9601 - acc: 0.3646\n",
      " Optimizer iteration 132, batch 132\n",
      "\n",
      " Learning rate 0.0009887363011673045, Model learning rate 0.0009887362830340862\n",
      "133/391 [=========>....................] - ETA: 1:02 - loss: 1.9573 - acc: 0.3652\n",
      " Optimizer iteration 133, batch 133\n",
      "\n",
      " Learning rate 0.0009885656485339128, Model learning rate 0.000988565618172288\n",
      "\n",
      " Optimizer iteration 134, batch 134\n",
      "\n",
      " Learning rate 0.0009883937278009466, Model learning rate 0.0009883936727419496\n",
      "135/391 [=========>....................] - ETA: 1:01 - loss: 1.9525 - acc: 0.3672\n",
      " Optimizer iteration 135, batch 135\n",
      "\n",
      " Learning rate 0.0009882205394146362, Model learning rate 0.000988220563158393\n",
      "136/391 [=========>....................] - ETA: 1:00 - loss: 1.9494 - acc: 0.3680\n",
      " Optimizer iteration 136, batch 136\n",
      "\n",
      " Learning rate 0.000988046083824501, Model learning rate 0.0009880460565909743\n",
      "137/391 [=========>....................] - ETA: 1:00 - loss: 1.9473 - acc: 0.3686\n",
      " Optimizer iteration 137, batch 137\n",
      "\n",
      " Learning rate 0.0009878703614833507, Model learning rate 0.0009878703858703375\n",
      "\n",
      " Optimizer iteration 138, batch 138\n",
      "\n",
      " Learning rate 0.0009876933728472826, Model learning rate 0.0009876933181658387\n",
      "139/391 [=========>....................] - ETA: 59s - loss: 1.9417 - acc: 0.3706 \n",
      " Optimizer iteration 139, batch 139\n",
      "\n",
      " Learning rate 0.0009875151183756806, Model learning rate 0.0009875150863081217\n",
      "140/391 [=========>....................] - ETA: 58s - loss: 1.9396 - acc: 0.3714\n",
      " Optimizer iteration 140, batch 140\n",
      "\n",
      " Learning rate 0.000987335598531214, Model learning rate 0.0009873355738818645\n",
      "141/391 [=========>....................] - ETA: 58s - loss: 1.9385 - acc: 0.3718\n",
      " Optimizer iteration 141, batch 141\n",
      "\n",
      " Learning rate 0.0009871548137798368, Model learning rate 0.0009871547808870673\n",
      "\n",
      " Optimizer iteration 142, batch 142\n",
      "\n",
      " Learning rate 0.0009869727645907857, Model learning rate 0.00098697270732373\n",
      "143/391 [=========>....................] - ETA: 57s - loss: 1.9345 - acc: 0.3726\n",
      " Optimizer iteration 143, batch 143\n",
      "\n",
      " Learning rate 0.0009867894514365802, Model learning rate 0.0009867894696071744\n",
      "144/391 [==========>...................] - ETA: 56s - loss: 1.9325 - acc: 0.3730\n",
      " Optimizer iteration 144, batch 144\n",
      "\n",
      " Learning rate 0.0009866048747930194, Model learning rate 0.0009866048349067569\n",
      "145/391 [==========>...................] - ETA: 56s - loss: 1.9303 - acc: 0.3738\n",
      " Optimizer iteration 145, batch 145\n",
      "\n",
      " Learning rate 0.0009864190351391822, Model learning rate 0.000986419036053121\n",
      "146/391 [==========>...................] - ETA: 55s - loss: 1.9268 - acc: 0.3748\n",
      " Optimizer iteration 146, batch 146\n",
      "\n",
      " Learning rate 0.0009862319329574263, Model learning rate 0.0009862319566309452\n",
      "\n",
      " Optimizer iteration 147, batch 147\n",
      "\n",
      " Learning rate 0.0009860435687333857, Model learning rate 0.0009860435966402292\n",
      "148/391 [==========>...................] - ETA: 54s - loss: 1.9229 - acc: 0.3759\n",
      " Optimizer iteration 148, batch 148\n",
      "\n",
      " Learning rate 0.0009858539429559703, Model learning rate 0.0009858539560809731\n",
      "149/391 [==========>...................] - ETA: 54s - loss: 1.9207 - acc: 0.3764\n",
      " Optimizer iteration 149, batch 149\n",
      "\n",
      " Learning rate 0.0009856630561173648, Model learning rate 0.000985663034953177\n",
      "150/391 [==========>...................] - ETA: 53s - loss: 1.9188 - acc: 0.3766\n",
      " Optimizer iteration 150, batch 150\n",
      "\n",
      " Learning rate 0.000985470908713026, Model learning rate 0.0009854709496721625\n",
      "151/391 [==========>...................] - ETA: 53s - loss: 1.9172 - acc: 0.3772\n",
      " Optimizer iteration 151, batch 151\n",
      "\n",
      " Learning rate 0.000985277501241684, Model learning rate 0.0009852774674072862\n",
      "152/391 [==========>...................] - ETA: 52s - loss: 1.9155 - acc: 0.3777\n",
      " Optimizer iteration 152, batch 152\n",
      "\n",
      " Learning rate 0.0009850828342053382, Model learning rate 0.0009850828209891915\n",
      "153/391 [==========>...................] - ETA: 52s - loss: 1.9145 - acc: 0.3779\n",
      " Optimizer iteration 153, batch 153\n",
      "\n",
      " Learning rate 0.000984886908109258, Model learning rate 0.0009848868940025568\n",
      "154/391 [==========>...................] - ETA: 51s - loss: 1.9126 - acc: 0.3784\n",
      " Optimizer iteration 154, batch 154\n",
      "\n",
      " Learning rate 0.000984689723461981, Model learning rate 0.000984689686447382\n",
      "155/391 [==========>...................] - ETA: 51s - loss: 1.9101 - acc: 0.3793\n",
      " Optimizer iteration 155, batch 155\n",
      "\n",
      " Learning rate 0.0009844912807753104, Model learning rate 0.0009844913147389889\n",
      "156/391 [==========>...................] - ETA: 51s - loss: 1.9076 - acc: 0.3801\n",
      " Optimizer iteration 156, batch 156\n",
      "\n",
      " Learning rate 0.0009842915805643156, Model learning rate 0.0009842915460467339\n",
      "157/391 [===========>..................] - ETA: 50s - loss: 1.9051 - acc: 0.3808\n",
      " Optimizer iteration 157, batch 157\n",
      "\n",
      " Learning rate 0.0009840906233473297, Model learning rate 0.0009840906132012606\n",
      "158/391 [===========>..................] - ETA: 50s - loss: 1.9041 - acc: 0.3811\n",
      " Optimizer iteration 158, batch 158\n",
      "\n",
      " Learning rate 0.0009838884096459487, Model learning rate 0.0009838883997872472\n",
      "159/391 [===========>..................] - ETA: 49s - loss: 1.9021 - acc: 0.3815\n",
      " Optimizer iteration 159, batch 159\n",
      "\n",
      " Learning rate 0.0009836849399850291, Model learning rate 0.0009836849058046937\n",
      "160/391 [===========>..................] - ETA: 49s - loss: 1.9001 - acc: 0.3824\n",
      " Optimizer iteration 160, batch 160\n",
      "\n",
      " Learning rate 0.0009834802148926882, Model learning rate 0.000983480247668922\n",
      "161/391 [===========>..................] - ETA: 49s - loss: 1.8994 - acc: 0.3827\n",
      " Optimizer iteration 161, batch 161\n",
      "\n",
      " Learning rate 0.0009832742349003014, Model learning rate 0.0009832741925492883\n",
      "162/391 [===========>..................] - ETA: 48s - loss: 1.8972 - acc: 0.3837\n",
      " Optimizer iteration 162, batch 162\n",
      "\n",
      " Learning rate 0.0009830670005425012, Model learning rate 0.0009830669732764363\n",
      "163/391 [===========>..................] - ETA: 48s - loss: 1.8946 - acc: 0.3847\n",
      " Optimizer iteration 163, batch 163\n",
      "\n",
      " Learning rate 0.0009828585123571763, Model learning rate 0.0009828584734350443\n",
      "164/391 [===========>..................] - ETA: 47s - loss: 1.8924 - acc: 0.3851\n",
      " Optimizer iteration 164, batch 164\n",
      "\n",
      " Learning rate 0.0009826487708854692, Model learning rate 0.000982648809440434\n",
      "165/391 [===========>..................] - ETA: 47s - loss: 1.8908 - acc: 0.3857\n",
      " Optimizer iteration 165, batch 165\n",
      "\n",
      " Learning rate 0.0009824377766717758, Model learning rate 0.0009824377484619617\n",
      "166/391 [===========>..................] - ETA: 47s - loss: 1.8897 - acc: 0.3858\n",
      " Optimizer iteration 166, batch 166\n",
      "\n",
      " Learning rate 0.0009822255302637435, Model learning rate 0.0009822255233302712\n",
      "167/391 [===========>..................] - ETA: 46s - loss: 1.8891 - acc: 0.3859\n",
      " Optimizer iteration 167, batch 167\n",
      "\n",
      " Learning rate 0.0009820120322122695, Model learning rate 0.0009820120176300406\n",
      "168/391 [===========>..................] - ETA: 46s - loss: 1.8877 - acc: 0.3863\n",
      " Optimizer iteration 168, batch 168\n",
      "\n",
      " Learning rate 0.0009817972830715002, Model learning rate 0.00098179723136127\n",
      "169/391 [===========>..................] - ETA: 46s - loss: 1.8858 - acc: 0.3871\n",
      " Optimizer iteration 169, batch 169\n",
      "\n",
      " Learning rate 0.0009815812833988292, Model learning rate 0.000981581280939281\n",
      "170/391 [============>.................] - ETA: 45s - loss: 1.8845 - acc: 0.3876\n",
      " Optimizer iteration 170, batch 170\n",
      "\n",
      " Learning rate 0.0009813640337548953, Model learning rate 0.000981364049948752\n",
      "171/391 [============>.................] - ETA: 45s - loss: 1.8828 - acc: 0.3881\n",
      " Optimizer iteration 171, batch 171\n",
      "\n",
      " Learning rate 0.0009811455347035827, Model learning rate 0.0009811455383896828\n",
      "172/391 [============>.................] - ETA: 44s - loss: 1.8812 - acc: 0.3889\n",
      " Optimizer iteration 172, batch 172\n",
      "\n",
      " Learning rate 0.0009809257868120176, Model learning rate 0.0009809257462620735\n",
      "173/391 [============>.................] - ETA: 44s - loss: 1.8802 - acc: 0.3889\n",
      " Optimizer iteration 173, batch 173\n",
      "\n",
      " Learning rate 0.0009807047906505682, Model learning rate 0.000980704789981246\n",
      "174/391 [============>.................] - ETA: 44s - loss: 1.8786 - acc: 0.3895\n",
      " Optimizer iteration 174, batch 174\n",
      "\n",
      " Learning rate 0.0009804825467928423, Model learning rate 0.0009804825531318784\n",
      "\n",
      " Optimizer iteration 175, batch 175\n",
      "\n",
      " Learning rate 0.000980259055815686, Model learning rate 0.0009802590357139707\n",
      "176/391 [============>.................] - ETA: 43s - loss: 1.8759 - acc: 0.3903\n",
      " Optimizer iteration 176, batch 176\n",
      "\n",
      " Learning rate 0.0009800343182991835, Model learning rate 0.0009800343541428447\n",
      "177/391 [============>.................] - ETA: 43s - loss: 1.8742 - acc: 0.3906\n",
      " Optimizer iteration 177, batch 177\n",
      "\n",
      " Learning rate 0.000979808334826653, Model learning rate 0.0009798083920031786\n",
      "178/391 [============>.................] - ETA: 42s - loss: 1.8729 - acc: 0.3905\n",
      " Optimizer iteration 178, batch 178\n",
      "\n",
      " Learning rate 0.0009795811059846475, Model learning rate 0.0009795811492949724\n",
      "\n",
      " Optimizer iteration 179, batch 179\n",
      "\n",
      " Learning rate 0.000979352632362952, Model learning rate 0.0009793526260182261\n",
      "180/391 [============>.................] - ETA: 41s - loss: 1.8695 - acc: 0.3919\n",
      " Optimizer iteration 180, batch 180\n",
      "\n",
      " Learning rate 0.0009791229145545831, Model learning rate 0.0009791229385882616\n",
      "181/391 [============>.................] - ETA: 41s - loss: 1.8672 - acc: 0.3926\n",
      " Optimizer iteration 181, batch 181\n",
      "\n",
      " Learning rate 0.0009788919531557858, Model learning rate 0.000978891970589757\n",
      "182/391 [============>.................] - ETA: 41s - loss: 1.8661 - acc: 0.3931\n",
      " Optimizer iteration 182, batch 182\n",
      "\n",
      " Learning rate 0.0009786597487660335, Model learning rate 0.0009786597220227122\n",
      "183/391 [=============>................] - ETA: 41s - loss: 1.8649 - acc: 0.3935\n",
      " Optimizer iteration 183, batch 183\n",
      "\n",
      " Learning rate 0.0009784263019880259, Model learning rate 0.0009784263093024492\n",
      "\n",
      " Optimizer iteration 184, batch 184\n",
      "\n",
      " Learning rate 0.0009781916134276871, Model learning rate 0.0009781916160136461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/391 [=============>................] - ETA: 40s - loss: 1.8611 - acc: 0.3947\n",
      " Optimizer iteration 185, batch 185\n",
      "\n",
      " Learning rate 0.0009779556836941646, Model learning rate 0.000977955642156303\n",
      "\n",
      " Optimizer iteration 186, batch 186\n",
      "\n",
      " Learning rate 0.0009777185133998268, Model learning rate 0.0009777185041457415\n",
      "187/391 [=============>................] - ETA: 39s - loss: 1.8581 - acc: 0.3953\n",
      " Optimizer iteration 187, batch 187\n",
      "\n",
      " Learning rate 0.0009774801031602629, Model learning rate 0.00097748008556664\n",
      "\n",
      " Optimizer iteration 188, batch 188\n",
      "\n",
      " Learning rate 0.00097724045359428, Model learning rate 0.00097724050283432\n",
      "189/391 [=============>................] - ETA: 39s - loss: 1.8562 - acc: 0.3957\n",
      " Optimizer iteration 189, batch 189\n",
      "\n",
      " Learning rate 0.0009769995653239022, Model learning rate 0.0009769995231181383\n",
      "190/391 [=============>................] - ETA: 38s - loss: 1.8544 - acc: 0.3963\n",
      " Optimizer iteration 190, batch 190\n",
      "\n",
      " Learning rate 0.0009767574389743681, Model learning rate 0.0009767574956640601\n",
      "191/391 [=============>................] - ETA: 38s - loss: 1.8526 - acc: 0.3969\n",
      " Optimizer iteration 191, batch 191\n",
      "\n",
      " Learning rate 0.0009765140751741306, Model learning rate 0.00097651407122612\n",
      "192/391 [=============>................] - ETA: 38s - loss: 1.8515 - acc: 0.3974\n",
      " Optimizer iteration 192, batch 192\n",
      "\n",
      " Learning rate 0.000976269474554854, Model learning rate 0.0009762694826349616\n",
      "193/391 [=============>................] - ETA: 37s - loss: 1.8496 - acc: 0.3979\n",
      " Optimizer iteration 193, batch 193\n",
      "\n",
      " Learning rate 0.0009760236377514128, Model learning rate 0.0009760236134752631\n",
      "194/391 [=============>................] - ETA: 37s - loss: 1.8483 - acc: 0.3984\n",
      " Optimizer iteration 194, batch 194\n",
      "\n",
      " Learning rate 0.0009757765654018904, Model learning rate 0.0009757765801623464\n",
      "195/391 [=============>................] - ETA: 37s - loss: 1.8471 - acc: 0.3990\n",
      " Optimizer iteration 195, batch 195\n",
      "\n",
      " Learning rate 0.0009755282581475768, Model learning rate 0.0009755282662808895\n",
      "196/391 [==============>...............] - ETA: 36s - loss: 1.8466 - acc: 0.3993\n",
      " Optimizer iteration 196, batch 196\n",
      "\n",
      " Learning rate 0.0009752787166329675, Model learning rate 0.0009752787300385535\n",
      "197/391 [==============>...............] - ETA: 36s - loss: 1.8447 - acc: 0.4001\n",
      " Optimizer iteration 197, batch 197\n",
      "\n",
      " Learning rate 0.0009750279415057616, Model learning rate 0.0009750279132276773\n",
      "\n",
      " Optimizer iteration 198, batch 198\n",
      "\n",
      " Learning rate 0.0009747759334168601, Model learning rate 0.0009747759322635829\n",
      "199/391 [==============>...............] - ETA: 36s - loss: 1.8419 - acc: 0.4010\n",
      " Optimizer iteration 199, batch 199\n",
      "\n",
      " Learning rate 0.0009745226930203639, Model learning rate 0.0009745226707309484\n",
      "\n",
      " Optimizer iteration 200, batch 200\n",
      "\n",
      " Learning rate 0.0009742682209735727, Model learning rate 0.0009742682450450957\n",
      "201/391 [==============>...............] - ETA: 35s - loss: 1.8398 - acc: 0.4015\n",
      " Optimizer iteration 201, batch 201\n",
      "\n",
      " Learning rate 0.0009740125179369832, Model learning rate 0.0009740125387907028\n",
      "\n",
      " Optimizer iteration 202, batch 202\n",
      "\n",
      " Learning rate 0.0009737555845742868, Model learning rate 0.0009737556101754308\n",
      "203/391 [==============>...............] - ETA: 34s - loss: 1.8375 - acc: 0.4019\n",
      " Optimizer iteration 203, batch 203\n",
      "\n",
      " Learning rate 0.0009734974215523685, Model learning rate 0.0009734974009916186\n",
      "\n",
      " Optimizer iteration 204, batch 204\n",
      "\n",
      " Learning rate 0.0009732380295413049, Model learning rate 0.0009732380276545882\n",
      "205/391 [==============>...............] - ETA: 34s - loss: 1.8352 - acc: 0.4026\n",
      " Optimizer iteration 205, batch 205\n",
      "\n",
      " Learning rate 0.0009729774092143626, Model learning rate 0.0009729774319566786\n",
      "206/391 [==============>...............] - ETA: 34s - loss: 1.8332 - acc: 0.4032\n",
      " Optimizer iteration 206, batch 206\n",
      "\n",
      " Learning rate 0.0009727155612479963, Model learning rate 0.0009727155556902289\n",
      "207/391 [==============>...............] - ETA: 33s - loss: 1.8314 - acc: 0.4038\n",
      " Optimizer iteration 207, batch 207\n",
      "\n",
      " Learning rate 0.000972452486321847, Model learning rate 0.000972452515270561\n",
      "\n",
      " Optimizer iteration 208, batch 208\n",
      "\n",
      " Learning rate 0.0009721881851187406, Model learning rate 0.0009721881942823529\n",
      "209/391 [===============>..............] - ETA: 33s - loss: 1.8289 - acc: 0.4045\n",
      " Optimizer iteration 209, batch 209\n",
      "\n",
      " Learning rate 0.0009719226583246855, Model learning rate 0.0009719226509332657\n",
      "210/391 [===============>..............] - ETA: 32s - loss: 1.8277 - acc: 0.4048\n",
      " Optimizer iteration 210, batch 210\n",
      "\n",
      " Learning rate 0.0009716559066288715, Model learning rate 0.0009716558852232993\n",
      "\n",
      " Optimizer iteration 211, batch 211\n",
      "\n",
      " Learning rate 0.0009713879307236677, Model learning rate 0.0009713879553601146\n",
      "212/391 [===============>..............] - ETA: 32s - loss: 1.8253 - acc: 0.4061\n",
      " Optimizer iteration 212, batch 212\n",
      "\n",
      " Learning rate 0.0009711187313046206, Model learning rate 0.0009711187449283898\n",
      "213/391 [===============>..............] - ETA: 32s - loss: 1.8231 - acc: 0.4067\n",
      " Optimizer iteration 213, batch 213\n",
      "\n",
      " Learning rate 0.0009708483090704523, Model learning rate 0.0009708483121357858\n",
      "214/391 [===============>..............] - ETA: 31s - loss: 1.8226 - acc: 0.4066\n",
      " Optimizer iteration 214, batch 214\n",
      "\n",
      " Learning rate 0.0009705766647230589, Model learning rate 0.0009705766569823027\n",
      "215/391 [===============>..............] - ETA: 31s - loss: 1.8207 - acc: 0.4072\n",
      " Optimizer iteration 215, batch 215\n",
      "\n",
      " Learning rate 0.0009703037989675086, Model learning rate 0.0009703037794679403\n",
      "216/391 [===============>..............] - ETA: 31s - loss: 1.8197 - acc: 0.4075\n",
      " Optimizer iteration 216, batch 216\n",
      "\n",
      " Learning rate 0.0009700297125120399, Model learning rate 0.0009700297378003597\n",
      "217/391 [===============>..............] - ETA: 31s - loss: 1.8183 - acc: 0.4079\n",
      " Optimizer iteration 217, batch 217\n",
      "\n",
      " Learning rate 0.0009697544060680594, Model learning rate 0.000969754415564239\n",
      "218/391 [===============>..............] - ETA: 30s - loss: 1.8168 - acc: 0.4083\n",
      " Optimizer iteration 218, batch 218\n",
      "\n",
      " Learning rate 0.0009694778803501403, Model learning rate 0.0009694778709672391\n",
      "219/391 [===============>..............] - ETA: 30s - loss: 1.8161 - acc: 0.4083\n",
      " Optimizer iteration 219, batch 219\n",
      "\n",
      " Learning rate 0.0009692001360760211, Model learning rate 0.000969200162217021\n",
      "\n",
      " Optimizer iteration 220, batch 220\n",
      "\n",
      " Learning rate 0.0009689211739666023, Model learning rate 0.0009689211728982627\n",
      "221/391 [===============>..............] - ETA: 30s - loss: 1.8130 - acc: 0.4094\n",
      " Optimizer iteration 221, batch 221\n",
      "\n",
      " Learning rate 0.0009686409947459458, Model learning rate 0.0009686410194262862\n",
      "222/391 [================>.............] - ETA: 29s - loss: 1.8123 - acc: 0.4097\n",
      " Optimizer iteration 222, batch 222\n",
      "\n",
      " Learning rate 0.0009683595991412725, Model learning rate 0.0009683595853857696\n",
      "\n",
      " Optimizer iteration 223, batch 223\n",
      "\n",
      " Learning rate 0.0009680769878829606, Model learning rate 0.0009680769871920347\n",
      "224/391 [================>.............] - ETA: 29s - loss: 1.8099 - acc: 0.4106\n",
      " Optimizer iteration 224, batch 224\n",
      "\n",
      " Learning rate 0.0009677931617045432, Model learning rate 0.0009677931666374207\n",
      "225/391 [================>.............] - ETA: 29s - loss: 1.8088 - acc: 0.4110\n",
      " Optimizer iteration 225, batch 225\n",
      "\n",
      " Learning rate 0.0009675081213427075, Model learning rate 0.0009675081237219274\n",
      "226/391 [================>.............] - ETA: 28s - loss: 1.8068 - acc: 0.4117\n",
      " Optimizer iteration 226, batch 226\n",
      "\n",
      " Learning rate 0.0009672218675372913, Model learning rate 0.000967221858445555\n",
      "227/391 [================>.............] - ETA: 28s - loss: 1.8051 - acc: 0.4125\n",
      " Optimizer iteration 227, batch 227\n",
      "\n",
      " Learning rate 0.0009669344010312829, Model learning rate 0.0009669344290159643\n",
      "228/391 [================>.............] - ETA: 28s - loss: 1.8039 - acc: 0.4129\n",
      " Optimizer iteration 228, batch 228\n",
      "\n",
      " Learning rate 0.0009666457225708174, Model learning rate 0.0009666457190178335\n",
      "229/391 [================>.............] - ETA: 28s - loss: 1.8022 - acc: 0.4134\n",
      " Optimizer iteration 229, batch 229\n",
      "\n",
      " Learning rate 0.0009663558329051762, Model learning rate 0.0009663558448664844\n",
      "230/391 [================>.............] - ETA: 27s - loss: 1.8006 - acc: 0.4141\n",
      " Optimizer iteration 230, batch 230\n",
      "\n",
      " Learning rate 0.0009660647327867839, Model learning rate 0.0009660647483542562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231/391 [================>.............] - ETA: 27s - loss: 1.7992 - acc: 0.4149\n",
      " Optimizer iteration 231, batch 231\n",
      "\n",
      " Learning rate 0.0009657724229712075, Model learning rate 0.0009657724294811487\n",
      "\n",
      " Optimizer iteration 232, batch 232\n",
      "\n",
      " Learning rate 0.0009654789042171535, Model learning rate 0.0009654788882471621\n",
      "233/391 [================>.............] - ETA: 27s - loss: 1.7968 - acc: 0.4158\n",
      " Optimizer iteration 233, batch 233\n",
      "\n",
      " Learning rate 0.000965184177286466, Model learning rate 0.0009651841828599572\n",
      "234/391 [================>.............] - ETA: 26s - loss: 1.7959 - acc: 0.4162\n",
      " Optimizer iteration 234, batch 234\n",
      "\n",
      " Learning rate 0.0009648882429441257, Model learning rate 0.0009648882551118731\n",
      "\n",
      " Optimizer iteration 235, batch 235\n",
      "\n",
      " Learning rate 0.0009645911019582466, Model learning rate 0.0009645911050029099\n",
      "236/391 [=================>............] - ETA: 26s - loss: 1.7939 - acc: 0.4169\n",
      " Optimizer iteration 236, batch 236\n",
      "\n",
      " Learning rate 0.0009642927551000749, Model learning rate 0.0009642927325330675\n",
      "237/391 [=================>............] - ETA: 26s - loss: 1.7926 - acc: 0.4174\n",
      " Optimizer iteration 237, batch 237\n",
      "\n",
      " Learning rate 0.0009639932031439866, Model learning rate 0.0009639931959100068\n",
      "\n",
      " Optimizer iteration 238, batch 238\n",
      "\n",
      " Learning rate 0.0009636924468674854, Model learning rate 0.0009636924369260669\n",
      "239/391 [=================>............] - ETA: 25s - loss: 1.7908 - acc: 0.4179\n",
      " Optimizer iteration 239, batch 239\n",
      "\n",
      " Learning rate 0.0009633904870512015, Model learning rate 0.0009633905137889087\n",
      "240/391 [=================>............] - ETA: 25s - loss: 1.7899 - acc: 0.4183\n",
      " Optimizer iteration 240, batch 240\n",
      "\n",
      " Learning rate 0.0009630873244788883, Model learning rate 0.0009630873100832105\n",
      "241/391 [=================>............] - ETA: 25s - loss: 1.7886 - acc: 0.4187\n",
      " Optimizer iteration 241, batch 241\n",
      "\n",
      " Learning rate 0.0009627829599374214, Model learning rate 0.000962782942224294\n",
      "242/391 [=================>............] - ETA: 25s - loss: 1.7876 - acc: 0.4191\n",
      " Optimizer iteration 242, batch 242\n",
      "\n",
      " Learning rate 0.0009624773942167958, Model learning rate 0.0009624774102121592\n",
      "\n",
      " Optimizer iteration 243, batch 243\n",
      "\n",
      " Learning rate 0.0009621706281101248, Model learning rate 0.0009621706558391452\n",
      "244/391 [=================>............] - ETA: 24s - loss: 1.7847 - acc: 0.4202\n",
      " Optimizer iteration 244, batch 244\n",
      "\n",
      " Learning rate 0.0009618626624136369, Model learning rate 0.000961862679105252\n",
      "245/391 [=================>............] - ETA: 24s - loss: 1.7836 - acc: 0.4204\n",
      " Optimizer iteration 245, batch 245\n",
      "\n",
      " Learning rate 0.0009615534979266745, Model learning rate 0.0009615534800104797\n",
      "246/391 [=================>............] - ETA: 24s - loss: 1.7830 - acc: 0.4205\n",
      " Optimizer iteration 246, batch 246\n",
      "\n",
      " Learning rate 0.0009612431354516912, Model learning rate 0.0009612431167624891\n",
      "247/391 [=================>............] - ETA: 23s - loss: 1.7820 - acc: 0.4209\n",
      " Optimizer iteration 247, batch 247\n",
      "\n",
      " Learning rate 0.0009609315757942503, Model learning rate 0.0009609315893612802\n",
      "248/391 [==================>...........] - ETA: 23s - loss: 1.7804 - acc: 0.4214\n",
      " Optimizer iteration 248, batch 248\n",
      "\n",
      " Learning rate 0.0009606188197630223, Model learning rate 0.0009606188395991921\n",
      "249/391 [==================>...........] - ETA: 23s - loss: 1.7792 - acc: 0.4218\n",
      " Optimizer iteration 249, batch 249\n",
      "\n",
      " Learning rate 0.0009603048681697834, Model learning rate 0.0009603048674762249\n",
      "250/391 [==================>...........] - ETA: 23s - loss: 1.7789 - acc: 0.4220\n",
      " Optimizer iteration 250, batch 250\n",
      "\n",
      " Learning rate 0.0009599897218294122, Model learning rate 0.0009599897312000394\n",
      "251/391 [==================>...........] - ETA: 23s - loss: 1.7778 - acc: 0.4222\n",
      " Optimizer iteration 251, batch 251\n",
      "\n",
      " Learning rate 0.0009596733815598888, Model learning rate 0.0009596733725629747\n",
      "252/391 [==================>...........] - ETA: 22s - loss: 1.7771 - acc: 0.4223\n",
      " Optimizer iteration 252, batch 252\n",
      "\n",
      " Learning rate 0.0009593558481822921, Model learning rate 0.0009593558497726917\n",
      "253/391 [==================>...........] - ETA: 22s - loss: 1.7761 - acc: 0.4226\n",
      " Optimizer iteration 253, batch 253\n",
      "\n",
      " Learning rate 0.0009590371225207981, Model learning rate 0.0009590371046215296\n",
      "254/391 [==================>...........] - ETA: 22s - loss: 1.7752 - acc: 0.4228\n",
      " Optimizer iteration 254, batch 254\n",
      "\n",
      " Learning rate 0.0009587172054026768, Model learning rate 0.0009587171953171492\n",
      "255/391 [==================>...........] - ETA: 22s - loss: 1.7739 - acc: 0.4231\n",
      " Optimizer iteration 255, batch 255\n",
      "\n",
      " Learning rate 0.0009583960976582913, Model learning rate 0.0009583961218595505\n",
      "256/391 [==================>...........] - ETA: 22s - loss: 1.7729 - acc: 0.4236\n",
      " Optimizer iteration 256, batch 256\n",
      "\n",
      " Learning rate 0.0009580738001210944, Model learning rate 0.0009580738260410726\n",
      "257/391 [==================>...........] - ETA: 21s - loss: 1.7722 - acc: 0.4240\n",
      " Optimizer iteration 257, batch 257\n",
      "\n",
      " Learning rate 0.000957750313627628, Model learning rate 0.0009577503078617156\n",
      "\n",
      " Optimizer iteration 258, batch 258\n",
      "\n",
      " Learning rate 0.0009574256390175192, Model learning rate 0.0009574256255291402\n",
      "259/391 [==================>...........] - ETA: 21s - loss: 1.7713 - acc: 0.4242\n",
      " Optimizer iteration 259, batch 259\n",
      "\n",
      " Learning rate 0.0009570997771334791, Model learning rate 0.0009570997790433466\n",
      "260/391 [==================>...........] - ETA: 21s - loss: 1.7706 - acc: 0.4245\n",
      " Optimizer iteration 260, batch 260\n",
      "\n",
      " Learning rate 0.0009567727288213005, Model learning rate 0.0009567727101966739\n",
      "261/391 [===================>..........] - ETA: 21s - loss: 1.7699 - acc: 0.4246\n",
      " Optimizer iteration 261, batch 261\n",
      "\n",
      " Learning rate 0.0009564444949298558, Model learning rate 0.0009564444771967828\n",
      "262/391 [===================>..........] - ETA: 20s - loss: 1.7692 - acc: 0.4250\n",
      " Optimizer iteration 262, batch 262\n",
      "\n",
      " Learning rate 0.0009561150763110944, Model learning rate 0.0009561150800436735\n",
      "263/391 [===================>..........] - ETA: 20s - loss: 1.7686 - acc: 0.4252\n",
      " Optimizer iteration 263, batch 263\n",
      "\n",
      " Learning rate 0.0009557844738200408, Model learning rate 0.000955784460529685\n",
      "264/391 [===================>..........] - ETA: 20s - loss: 1.7679 - acc: 0.4255\n",
      " Optimizer iteration 264, batch 264\n",
      "\n",
      " Learning rate 0.0009554526883147925, Model learning rate 0.0009554526768624783\n",
      "265/391 [===================>..........] - ETA: 20s - loss: 1.7676 - acc: 0.4256\n",
      " Optimizer iteration 265, batch 265\n",
      "\n",
      " Learning rate 0.0009551197206565174, Model learning rate 0.0009551197290420532\n",
      "266/391 [===================>..........] - ETA: 20s - loss: 1.7664 - acc: 0.4260\n",
      " Optimizer iteration 266, batch 266\n",
      "\n",
      " Learning rate 0.0009547855717094515, Model learning rate 0.000954785558860749\n",
      "267/391 [===================>..........] - ETA: 19s - loss: 1.7659 - acc: 0.4263\n",
      " Optimizer iteration 267, batch 267\n",
      "\n",
      " Learning rate 0.0009544502423408973, Model learning rate 0.0009544502245262265\n",
      "268/391 [===================>..........] - ETA: 19s - loss: 1.7651 - acc: 0.4265\n",
      " Optimizer iteration 268, batch 268\n",
      "\n",
      " Learning rate 0.0009541137334212211, Model learning rate 0.0009541137260384858\n",
      "269/391 [===================>..........] - ETA: 19s - loss: 1.7649 - acc: 0.4264\n",
      " Optimizer iteration 269, batch 269\n",
      "\n",
      " Learning rate 0.0009537760458238505, Model learning rate 0.0009537760633975267\n",
      "270/391 [===================>..........] - ETA: 19s - loss: 1.7648 - acc: 0.4264\n",
      " Optimizer iteration 270, batch 270\n",
      "\n",
      " Learning rate 0.0009534371804252727, Model learning rate 0.0009534371783956885\n",
      "271/391 [===================>..........] - ETA: 19s - loss: 1.7637 - acc: 0.4269\n",
      " Optimizer iteration 271, batch 271\n",
      "\n",
      " Learning rate 0.0009530971381050319, Model learning rate 0.0009530971292406321\n",
      "272/391 [===================>..........] - ETA: 18s - loss: 1.7626 - acc: 0.4273\n",
      " Optimizer iteration 272, batch 272\n",
      "\n",
      " Learning rate 0.0009527559197457272, Model learning rate 0.0009527559159323573\n",
      "273/391 [===================>..........] - ETA: 18s - loss: 1.7614 - acc: 0.4277\n",
      " Optimizer iteration 273, batch 273\n",
      "\n",
      " Learning rate 0.0009524135262330098, Model learning rate 0.0009524135384708643\n",
      "274/391 [====================>.........] - ETA: 18s - loss: 1.7599 - acc: 0.4280\n",
      " Optimizer iteration 274, batch 274\n",
      "\n",
      " Learning rate 0.0009520699584555812, Model learning rate 0.0009520699386484921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/391 [====================>.........] - ETA: 18s - loss: 1.7590 - acc: 0.4282\n",
      " Optimizer iteration 275, batch 275\n",
      "\n",
      " Learning rate 0.0009517252173051911, Model learning rate 0.0009517252328805625\n",
      "276/391 [====================>.........] - ETA: 18s - loss: 1.7587 - acc: 0.4283\n",
      " Optimizer iteration 276, batch 276\n",
      "\n",
      " Learning rate 0.0009513793036766345, Model learning rate 0.0009513793047517538\n",
      "277/391 [====================>.........] - ETA: 17s - loss: 1.7575 - acc: 0.4286\n",
      " Optimizer iteration 277, batch 277\n",
      "\n",
      " Learning rate 0.0009510322184677493, Model learning rate 0.0009510322124697268\n",
      "278/391 [====================>.........] - ETA: 17s - loss: 1.7565 - acc: 0.4288\n",
      " Optimizer iteration 278, batch 278\n",
      "\n",
      " Learning rate 0.0009506839625794152, Model learning rate 0.0009506839560344815\n",
      "279/391 [====================>.........] - ETA: 17s - loss: 1.7562 - acc: 0.4289\n",
      " Optimizer iteration 279, batch 279\n",
      "\n",
      " Learning rate 0.0009503345369155494, Model learning rate 0.000950334535446018\n",
      "280/391 [====================>.........] - ETA: 17s - loss: 1.7549 - acc: 0.4293\n",
      " Optimizer iteration 280, batch 280\n",
      "\n",
      " Learning rate 0.0009499839423831061, Model learning rate 0.0009499839507043362\n",
      "281/391 [====================>.........] - ETA: 17s - loss: 1.7538 - acc: 0.4296\n",
      " Optimizer iteration 281, batch 281\n",
      "\n",
      " Learning rate 0.0009496321798920731, Model learning rate 0.0009496322018094361\n",
      "282/391 [====================>.........] - ETA: 17s - loss: 1.7532 - acc: 0.4297\n",
      " Optimizer iteration 282, batch 282\n",
      "\n",
      " Learning rate 0.0009492792503554695, Model learning rate 0.0009492792305536568\n",
      "283/391 [====================>.........] - ETA: 16s - loss: 1.7518 - acc: 0.4302\n",
      " Optimizer iteration 283, batch 283\n",
      "\n",
      " Learning rate 0.000948925154689344, Model learning rate 0.0009489251533523202\n",
      "284/391 [====================>.........] - ETA: 16s - loss: 1.7505 - acc: 0.4308\n",
      " Optimizer iteration 284, batch 284\n",
      "\n",
      " Learning rate 0.0009485698938127715, Model learning rate 0.0009485699119977653\n",
      "285/391 [====================>.........] - ETA: 16s - loss: 1.7491 - acc: 0.4313\n",
      " Optimizer iteration 285, batch 285\n",
      "\n",
      " Learning rate 0.0009482134686478518, Model learning rate 0.0009482134482823312\n",
      "286/391 [====================>.........] - ETA: 16s - loss: 1.7482 - acc: 0.4315\n",
      " Optimizer iteration 286, batch 286\n",
      "\n",
      " Learning rate 0.0009478558801197064, Model learning rate 0.0009478558786213398\n",
      "287/391 [=====================>........] - ETA: 16s - loss: 1.7468 - acc: 0.4320\n",
      " Optimizer iteration 287, batch 287\n",
      "\n",
      " Learning rate 0.0009474971291564764, Model learning rate 0.0009474971448071301\n",
      "288/391 [=====================>........] - ETA: 15s - loss: 1.7459 - acc: 0.4323\n",
      " Optimizer iteration 288, batch 288\n",
      "\n",
      " Learning rate 0.0009471372166893198, Model learning rate 0.0009471371886320412\n",
      "289/391 [=====================>........] - ETA: 15s - loss: 1.7446 - acc: 0.4326\n",
      " Optimizer iteration 289, batch 289\n",
      "\n",
      " Learning rate 0.0009467761436524099, Model learning rate 0.000946776126511395\n",
      "290/391 [=====================>........] - ETA: 15s - loss: 1.7442 - acc: 0.4329\n",
      " Optimizer iteration 290, batch 290\n",
      "\n",
      " Learning rate 0.0009464139109829321, Model learning rate 0.0009464139002375305\n",
      "291/391 [=====================>........] - ETA: 15s - loss: 1.7438 - acc: 0.4332\n",
      " Optimizer iteration 291, batch 291\n",
      "\n",
      " Learning rate 0.0009460505196210813, Model learning rate 0.0009460505098104477\n",
      "292/391 [=====================>........] - ETA: 15s - loss: 1.7428 - acc: 0.4336\n",
      " Optimizer iteration 292, batch 292\n",
      "\n",
      " Learning rate 0.0009456859705100606, Model learning rate 0.0009456859552301466\n",
      "293/391 [=====================>........] - ETA: 15s - loss: 1.7422 - acc: 0.4339\n",
      " Optimizer iteration 293, batch 293\n",
      "\n",
      " Learning rate 0.0009453202645960773, Model learning rate 0.0009453202364966273\n",
      "294/391 [=====================>........] - ETA: 14s - loss: 1.7412 - acc: 0.4342\n",
      " Optimizer iteration 294, batch 294\n",
      "\n",
      " Learning rate 0.000944953402828342, Model learning rate 0.0009449534118175507\n",
      "295/391 [=====================>........] - ETA: 14s - loss: 1.7404 - acc: 0.4343\n",
      " Optimizer iteration 295, batch 295\n",
      "\n",
      " Learning rate 0.0009445853861590646, Model learning rate 0.0009445853647775948\n",
      "296/391 [=====================>........] - ETA: 14s - loss: 1.7398 - acc: 0.4349\n",
      " Optimizer iteration 296, batch 296\n",
      "\n",
      " Learning rate 0.0009442162155434534, Model learning rate 0.0009442162117920816\n",
      "297/391 [=====================>........] - ETA: 14s - loss: 1.7386 - acc: 0.4352\n",
      " Optimizer iteration 297, batch 297\n",
      "\n",
      " Learning rate 0.0009438458919397112, Model learning rate 0.0009438458946533501\n",
      "298/391 [=====================>........] - ETA: 14s - loss: 1.7373 - acc: 0.4357\n",
      " Optimizer iteration 298, batch 298\n",
      "\n",
      " Learning rate 0.0009434744163090339, Model learning rate 0.0009434744133614004\n",
      "299/391 [=====================>........] - ETA: 13s - loss: 1.7364 - acc: 0.4360\n",
      " Optimizer iteration 299, batch 299\n",
      "\n",
      " Learning rate 0.0009431017896156073, Model learning rate 0.0009431017679162323\n",
      "300/391 [======================>.......] - ETA: 13s - loss: 1.7355 - acc: 0.4364\n",
      " Optimizer iteration 300, batch 300\n",
      "\n",
      " Learning rate 0.0009427280128266049, Model learning rate 0.000942728016525507\n",
      "301/391 [======================>.......] - ETA: 13s - loss: 1.7344 - acc: 0.4366\n",
      " Optimizer iteration 301, batch 301\n",
      "\n",
      " Learning rate 0.0009423530869121855, Model learning rate 0.0009423531009815633\n",
      "\n",
      " Optimizer iteration 302, batch 302\n",
      "\n",
      " Learning rate 0.0009419770128454898, Model learning rate 0.0009419770212844014\n",
      "303/391 [======================>.......] - ETA: 13s - loss: 1.7337 - acc: 0.4367\n",
      " Optimizer iteration 303, batch 303\n",
      "\n",
      " Learning rate 0.00094159979160264, Model learning rate 0.0009415997774340212\n",
      "304/391 [======================>.......] - ETA: 13s - loss: 1.7333 - acc: 0.4366\n",
      " Optimizer iteration 304, batch 304\n",
      "\n",
      " Learning rate 0.0009412214241627343, Model learning rate 0.0009412214276380837\n",
      "305/391 [======================>.......] - ETA: 12s - loss: 1.7322 - acc: 0.4370\n",
      " Optimizer iteration 305, batch 305\n",
      "\n",
      " Learning rate 0.000940841911507847, Model learning rate 0.0009408419136889279\n",
      "\n",
      " Optimizer iteration 306, batch 306\n",
      "\n",
      " Learning rate 0.0009404612546230243, Model learning rate 0.0009404612355865538\n",
      "307/391 [======================>.......] - ETA: 12s - loss: 1.7309 - acc: 0.4373\n",
      " Optimizer iteration 307, batch 307\n",
      "\n",
      " Learning rate 0.0009400794544962828, Model learning rate 0.0009400794515386224\n",
      "\n",
      " Optimizer iteration 308, batch 308\n",
      "\n",
      " Learning rate 0.0009396965121186058, Model learning rate 0.0009396965033374727\n",
      "309/391 [======================>.......] - ETA: 12s - loss: 1.7289 - acc: 0.4381\n",
      " Optimizer iteration 309, batch 309\n",
      "\n",
      " Learning rate 0.0009393124284839423, Model learning rate 0.0009393124491907656\n",
      "310/391 [======================>.......] - ETA: 12s - loss: 1.7281 - acc: 0.4386\n",
      " Optimizer iteration 310, batch 310\n",
      "\n",
      " Learning rate 0.0009389272045892023, Model learning rate 0.0009389272308908403\n",
      "311/391 [======================>.......] - ETA: 11s - loss: 1.7274 - acc: 0.4386\n",
      " Optimizer iteration 311, batch 311\n",
      "\n",
      " Learning rate 0.0009385408414342564, Model learning rate 0.0009385408484376967\n",
      "\n",
      " Optimizer iteration 312, batch 312\n",
      "\n",
      " Learning rate 0.0009381533400219318, Model learning rate 0.0009381533600389957\n",
      "313/391 [=======================>......] - ETA: 11s - loss: 1.7259 - acc: 0.4390\n",
      " Optimizer iteration 313, batch 313\n",
      "\n",
      " Learning rate 0.0009377647013580102, Model learning rate 0.0009377647074870765\n",
      "314/391 [=======================>......] - ETA: 11s - loss: 1.7251 - acc: 0.4392\n",
      " Optimizer iteration 314, batch 314\n",
      "\n",
      " Learning rate 0.000937374926451225, Model learning rate 0.0009373749489895999\n",
      "\n",
      " Optimizer iteration 315, batch 315\n",
      "\n",
      " Learning rate 0.000936984016313259, Model learning rate 0.0009369840263389051\n",
      "316/391 [=======================>......] - ETA: 11s - loss: 1.7238 - acc: 0.4397\n",
      " Optimizer iteration 316, batch 316\n",
      "\n",
      " Learning rate 0.0009365919719587412, Model learning rate 0.0009365919977426529\n",
      "317/391 [=======================>......] - ETA: 10s - loss: 1.7233 - acc: 0.4399\n",
      " Optimizer iteration 317, batch 317\n",
      "\n",
      " Learning rate 0.000936198794405245, Model learning rate 0.0009361988049931824\n",
      "\n",
      " Optimizer iteration 318, batch 318\n",
      "\n",
      " Learning rate 0.0009358044846732847, Model learning rate 0.0009358045062981546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319/391 [=======================>......] - ETA: 10s - loss: 1.7214 - acc: 0.4407\n",
      " Optimizer iteration 319, batch 319\n",
      "\n",
      " Learning rate 0.0009354090437863132, Model learning rate 0.0009354090434499085\n",
      "320/391 [=======================>......] - ETA: 10s - loss: 1.7205 - acc: 0.4411\n",
      " Optimizer iteration 320, batch 320\n",
      "\n",
      " Learning rate 0.0009350124727707196, Model learning rate 0.000935012474656105\n",
      "321/391 [=======================>......] - ETA: 10s - loss: 1.7204 - acc: 0.4412\n",
      " Optimizer iteration 321, batch 321\n",
      "\n",
      " Learning rate 0.0009346147726558265, Model learning rate 0.0009346147999167442\n",
      "322/391 [=======================>......] - ETA: 10s - loss: 1.7199 - acc: 0.4414\n",
      " Optimizer iteration 322, batch 322\n",
      "\n",
      " Learning rate 0.0009342159444738864, Model learning rate 0.0009342159610241652\n",
      "323/391 [=======================>......] - ETA: 9s - loss: 1.7189 - acc: 0.4418 \n",
      " Optimizer iteration 323, batch 323\n",
      "\n",
      " Learning rate 0.0009338159892600807, Model learning rate 0.0009338160161860287\n",
      "324/391 [=======================>......] - ETA: 9s - loss: 1.7183 - acc: 0.4420\n",
      " Optimizer iteration 324, batch 324\n",
      "\n",
      " Learning rate 0.0009334149080525154, Model learning rate 0.000933414907194674\n",
      "325/391 [=======================>......] - ETA: 9s - loss: 1.7179 - acc: 0.4421\n",
      " Optimizer iteration 325, batch 325\n",
      "\n",
      " Learning rate 0.0009330127018922195, Model learning rate 0.000933012692257762\n",
      "\n",
      " Optimizer iteration 326, batch 326\n",
      "\n",
      " Learning rate 0.0009326093718231412, Model learning rate 0.0009326093713752925\n",
      "327/391 [========================>.....] - ETA: 9s - loss: 1.7164 - acc: 0.4425\n",
      " Optimizer iteration 327, batch 327\n",
      "\n",
      " Learning rate 0.0009322049188921467, Model learning rate 0.0009322049445472658\n",
      "\n",
      " Optimizer iteration 328, batch 328\n",
      "\n",
      " Learning rate 0.0009317993441490162, Model learning rate 0.0009317993535660207\n",
      "329/391 [========================>.....] - ETA: 9s - loss: 1.7150 - acc: 0.4429\n",
      " Optimizer iteration 329, batch 329\n",
      "\n",
      " Learning rate 0.0009313926486464419, Model learning rate 0.0009313926566392183\n",
      "330/391 [========================>.....] - ETA: 8s - loss: 1.7150 - acc: 0.4428\n",
      " Optimizer iteration 330, batch 330\n",
      "\n",
      " Learning rate 0.0009309848334400246, Model learning rate 0.0009309848537668586\n",
      "\n",
      " Optimizer iteration 331, batch 331\n",
      "\n",
      " Learning rate 0.0009305758995882716, Model learning rate 0.0009305758867412806\n",
      "332/391 [========================>.....] - ETA: 8s - loss: 1.7132 - acc: 0.4436\n",
      " Optimizer iteration 332, batch 332\n",
      "\n",
      " Learning rate 0.0009301658481525939, Model learning rate 0.0009301658719778061\n",
      "333/391 [========================>.....] - ETA: 8s - loss: 1.7126 - acc: 0.4439\n",
      " Optimizer iteration 333, batch 333\n",
      "\n",
      " Learning rate 0.0009297546801973025, Model learning rate 0.0009297546930611134\n",
      "\n",
      " Optimizer iteration 334, batch 334\n",
      "\n",
      " Learning rate 0.0009293423967896076, Model learning rate 0.0009293424081988633\n",
      "335/391 [========================>.....] - ETA: 8s - loss: 1.7114 - acc: 0.4443\n",
      " Optimizer iteration 335, batch 335\n",
      "\n",
      " Learning rate 0.0009289289989996133, Model learning rate 0.0009289290173910558\n",
      "336/391 [========================>.....] - ETA: 7s - loss: 1.7107 - acc: 0.4445\n",
      " Optimizer iteration 336, batch 336\n",
      "\n",
      " Learning rate 0.0009285144879003172, Model learning rate 0.0009285144624300301\n",
      "337/391 [========================>.....] - ETA: 7s - loss: 1.7097 - acc: 0.4449\n",
      " Optimizer iteration 337, batch 337\n",
      "\n",
      " Learning rate 0.0009280988645676059, Model learning rate 0.000928098859731108\n",
      "\n",
      " Optimizer iteration 338, batch 338\n",
      "\n",
      " Learning rate 0.0009276821300802534, Model learning rate 0.0009276821510866284\n",
      "339/391 [=========================>....] - ETA: 7s - loss: 1.7084 - acc: 0.4455\n",
      " Optimizer iteration 339, batch 339\n",
      "\n",
      " Learning rate 0.0009272642855199171, Model learning rate 0.0009272642782889307\n",
      "\n",
      " Optimizer iteration 340, batch 340\n",
      "\n",
      " Learning rate 0.0009268453319711362, Model learning rate 0.0009268453577533364\n",
      "341/391 [=========================>....] - ETA: 7s - loss: 1.7069 - acc: 0.4459\n",
      " Optimizer iteration 341, batch 341\n",
      "\n",
      " Learning rate 0.000926425270521328, Model learning rate 0.0009264252730645239\n",
      "342/391 [=========================>....] - ETA: 6s - loss: 1.7062 - acc: 0.4462\n",
      " Optimizer iteration 342, batch 342\n",
      "\n",
      " Learning rate 0.0009260041022607859, Model learning rate 0.0009260040824301541\n",
      "\n",
      " Optimizer iteration 343, batch 343\n",
      "\n",
      " Learning rate 0.0009255818282826756, Model learning rate 0.0009255818440578878\n",
      "344/391 [=========================>....] - ETA: 6s - loss: 1.7046 - acc: 0.4465\n",
      " Optimizer iteration 344, batch 344\n",
      "\n",
      " Learning rate 0.0009251584496830327, Model learning rate 0.0009251584415324032\n",
      "345/391 [=========================>....] - ETA: 6s - loss: 1.7034 - acc: 0.4469\n",
      " Optimizer iteration 345, batch 345\n",
      "\n",
      " Learning rate 0.0009247339675607605, Model learning rate 0.0009247339912690222\n",
      "346/391 [=========================>....] - ETA: 6s - loss: 1.7028 - acc: 0.4471\n",
      " Optimizer iteration 346, batch 346\n",
      "\n",
      " Learning rate 0.000924308383017626, Model learning rate 0.000924308376852423\n",
      "\n",
      " Optimizer iteration 347, batch 347\n",
      "\n",
      " Learning rate 0.0009238816971582578, Model learning rate 0.0009238817146979272\n",
      "348/391 [=========================>....] - ETA: 6s - loss: 1.7016 - acc: 0.4475\n",
      " Optimizer iteration 348, batch 348\n",
      "\n",
      " Learning rate 0.000923453911090143, Model learning rate 0.0009234538883902133\n",
      "349/391 [=========================>....] - ETA: 5s - loss: 1.7009 - acc: 0.4477\n",
      " Optimizer iteration 349, batch 349\n",
      "\n",
      " Learning rate 0.0009230250259236243, Model learning rate 0.0009230250143446028\n",
      "350/391 [=========================>....] - ETA: 5s - loss: 1.7002 - acc: 0.4478\n",
      " Optimizer iteration 350, batch 350\n",
      "\n",
      " Learning rate 0.0009225950427718975, Model learning rate 0.000922595034353435\n",
      "351/391 [=========================>....] - ETA: 5s - loss: 1.6996 - acc: 0.4481\n",
      " Optimizer iteration 351, batch 351\n",
      "\n",
      " Learning rate 0.0009221639627510075, Model learning rate 0.0009221639484167099\n",
      "352/391 [==========================>...] - ETA: 5s - loss: 1.6985 - acc: 0.4486\n",
      " Optimizer iteration 352, batch 352\n",
      "\n",
      " Learning rate 0.0009217317869798471, Model learning rate 0.0009217318147420883\n",
      "353/391 [==========================>...] - ETA: 5s - loss: 1.6978 - acc: 0.4489\n",
      " Optimizer iteration 353, batch 353\n",
      "\n",
      " Learning rate 0.0009212985165801529, Model learning rate 0.0009212985169142485\n",
      "354/391 [==========================>...] - ETA: 5s - loss: 1.6973 - acc: 0.4491\n",
      " Optimizer iteration 354, batch 354\n",
      "\n",
      " Learning rate 0.0009208641526765023, Model learning rate 0.0009208641713485122\n",
      "\n",
      " Optimizer iteration 355, batch 355\n",
      "\n",
      " Learning rate 0.0009204286963963111, Model learning rate 0.0009204287198372185\n",
      "356/391 [==========================>...] - ETA: 4s - loss: 1.6958 - acc: 0.4496\n",
      " Optimizer iteration 356, batch 356\n",
      "\n",
      " Learning rate 0.0009199921488698308, Model learning rate 0.0009199921623803675\n",
      "357/391 [==========================>...] - ETA: 4s - loss: 1.6950 - acc: 0.4498\n",
      " Optimizer iteration 357, batch 357\n",
      "\n",
      " Learning rate 0.0009195545112301446, Model learning rate 0.0009195544989779592\n",
      "358/391 [==========================>...] - ETA: 4s - loss: 1.6943 - acc: 0.4502\n",
      " Optimizer iteration 358, batch 358\n",
      "\n",
      " Learning rate 0.0009191157846131662, Model learning rate 0.0009191157878376544\n",
      "359/391 [==========================>...] - ETA: 4s - loss: 1.6935 - acc: 0.4505\n",
      " Optimizer iteration 359, batch 359\n",
      "\n",
      " Learning rate 0.0009186759701576349, Model learning rate 0.0009186759707517922\n",
      "360/391 [==========================>...] - ETA: 4s - loss: 1.6925 - acc: 0.4507\n",
      " Optimizer iteration 360, batch 360\n",
      "\n",
      " Learning rate 0.0009182350690051134, Model learning rate 0.0009182350477203727\n",
      "361/391 [==========================>...] - ETA: 4s - loss: 1.6919 - acc: 0.4509\n",
      " Optimizer iteration 361, batch 361\n",
      "\n",
      " Learning rate 0.0009177930822999859, Model learning rate 0.0009177930769510567\n",
      "362/391 [==========================>...] - ETA: 4s - loss: 1.6910 - acc: 0.4512\n",
      " Optimizer iteration 362, batch 362\n",
      "\n",
      " Learning rate 0.0009173500111894535, Model learning rate 0.0009173500002361834\n",
      "\n",
      " Optimizer iteration 363, batch 363\n",
      "\n",
      " Learning rate 0.0009169058568235323, Model learning rate 0.0009169058757834136\n",
      "364/391 [==========================>...] - ETA: 3s - loss: 1.6900 - acc: 0.4515\n",
      " Optimizer iteration 364, batch 364\n",
      "\n",
      " Learning rate 0.0009164606203550497, Model learning rate 0.0009164606453850865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/391 [===========================>..] - ETA: 3s - loss: 1.6890 - acc: 0.4519\n",
      " Optimizer iteration 365, batch 365\n",
      "\n",
      " Learning rate 0.0009160143029396422, Model learning rate 0.0009160143090412021\n",
      "366/391 [===========================>..] - ETA: 3s - loss: 1.6880 - acc: 0.4523\n",
      " Optimizer iteration 366, batch 366\n",
      "\n",
      " Learning rate 0.0009155669057357514, Model learning rate 0.0009155669249594212\n",
      "367/391 [===========================>..] - ETA: 3s - loss: 1.6872 - acc: 0.4525\n",
      " Optimizer iteration 367, batch 367\n",
      "\n",
      " Learning rate 0.0009151184299046221, Model learning rate 0.0009151184349320829\n",
      "\n",
      " Optimizer iteration 368, batch 368\n",
      "\n",
      " Learning rate 0.0009146688766102984, Model learning rate 0.0009146688971668482\n",
      "369/391 [===========================>..] - ETA: 3s - loss: 1.6861 - acc: 0.4529\n",
      " Optimizer iteration 369, batch 369\n",
      "\n",
      " Learning rate 0.0009142182470196212, Model learning rate 0.0009142182534560561\n",
      "370/391 [===========================>..] - ETA: 2s - loss: 1.6855 - acc: 0.4530\n",
      " Optimizer iteration 370, batch 370\n",
      "\n",
      " Learning rate 0.000913766542302225, Model learning rate 0.0009137665620073676\n",
      "371/391 [===========================>..] - ETA: 2s - loss: 1.6850 - acc: 0.4531\n",
      " Optimizer iteration 371, batch 371\n",
      "\n",
      " Learning rate 0.0009133137636305345, Model learning rate 0.0009133137646131217\n",
      "\n",
      " Optimizer iteration 372, batch 372\n",
      "\n",
      " Learning rate 0.0009128599121797621, Model learning rate 0.0009128599194809794\n",
      "373/391 [===========================>..] - ETA: 2s - loss: 1.6841 - acc: 0.4533\n",
      " Optimizer iteration 373, batch 373\n",
      "\n",
      " Learning rate 0.000912404989127905, Model learning rate 0.0009124049684032798\n",
      "374/391 [===========================>..] - ETA: 2s - loss: 1.6832 - acc: 0.4536\n",
      " Optimizer iteration 374, batch 374\n",
      "\n",
      " Learning rate 0.0009119489956557415, Model learning rate 0.0009119489695876837\n",
      "375/391 [===========================>..] - ETA: 2s - loss: 1.6826 - acc: 0.4539\n",
      " Optimizer iteration 375, batch 375\n",
      "\n",
      " Learning rate 0.0009114919329468282, Model learning rate 0.0009114919230341911\n",
      "376/391 [===========================>..] - ETA: 2s - loss: 1.6819 - acc: 0.4541\n",
      " Optimizer iteration 376, batch 376\n",
      "\n",
      " Learning rate 0.000911033802187497, Model learning rate 0.0009110338287428021\n",
      "377/391 [===========================>..] - ETA: 1s - loss: 1.6810 - acc: 0.4544\n",
      " Optimizer iteration 377, batch 377\n",
      "\n",
      " Learning rate 0.000910574604566852, Model learning rate 0.0009105746285058558\n",
      "378/391 [============================>.] - ETA: 1s - loss: 1.6805 - acc: 0.4548\n",
      " Optimizer iteration 378, batch 378\n",
      "\n",
      " Learning rate 0.0009101143412767665, Model learning rate 0.0009101143223233521\n",
      "\n",
      " Optimizer iteration 379, batch 379\n",
      "\n",
      " Learning rate 0.0009096530135118797, Model learning rate 0.0009096530266106129\n",
      "380/391 [============================>.] - ETA: 1s - loss: 1.6789 - acc: 0.4552\n",
      " Optimizer iteration 380, batch 380\n",
      "\n",
      " Learning rate 0.0009091906224695935, Model learning rate 0.0009091906249523163\n",
      "381/391 [============================>.] - ETA: 1s - loss: 1.6783 - acc: 0.4553\n",
      " Optimizer iteration 381, batch 381\n",
      "\n",
      " Learning rate 0.00090872716935007, Model learning rate 0.0009087271755561233\n",
      "\n",
      " Optimizer iteration 382, batch 382\n",
      "\n",
      " Learning rate 0.000908262655356228, Model learning rate 0.0009082626784220338\n",
      "383/391 [============================>.] - ETA: 1s - loss: 1.6767 - acc: 0.4560\n",
      " Optimizer iteration 383, batch 383\n",
      "\n",
      " Learning rate 0.0009077970816937393, Model learning rate 0.000907797075342387\n",
      "\n",
      " Optimizer iteration 384, batch 384\n",
      "\n",
      " Learning rate 0.0009073304495710266, Model learning rate 0.0009073304245248437\n",
      "385/391 [============================>.] - ETA: 0s - loss: 1.6750 - acc: 0.4565\n",
      " Optimizer iteration 385, batch 385\n",
      "\n",
      " Learning rate 0.0009068627601992598, Model learning rate 0.0009068627841770649\n",
      "\n",
      " Optimizer iteration 386, batch 386\n",
      "\n",
      " Learning rate 0.0009063940147923528, Model learning rate 0.0009063940378837287\n",
      "387/391 [============================>.] - ETA: 0s - loss: 1.6730 - acc: 0.4571\n",
      " Optimizer iteration 387, batch 387\n",
      "\n",
      " Learning rate 0.000905924214566961, Model learning rate 0.0009059241856448352\n",
      "388/391 [============================>.] - ETA: 0s - loss: 1.6720 - acc: 0.4576\n",
      " Optimizer iteration 388, batch 388\n",
      "\n",
      " Learning rate 0.0009054533607424769, Model learning rate 0.0009054533438757062\n",
      "389/391 [============================>.] - ETA: 0s - loss: 1.6713 - acc: 0.4578\n",
      " Optimizer iteration 389, batch 389\n",
      "\n",
      " Learning rate 0.0009049814545410281, Model learning rate 0.0009049814543686807\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.6705 - acc: 0.4582\n",
      " Optimizer iteration 390, batch 390\n",
      "\n",
      " Learning rate 0.0009045084971874737, Model learning rate 0.0009045085171237588\n",
      "391/391 [==============================] - 55s 140ms/step - loss: 1.6700 - acc: 0.4583 - val_loss: 2.0605 - val_acc: 0.3871\n",
      "\n",
      "Epoch 00001: saving model to /home/ubuntu/Projects/hybrid-ensemble/model/run_200/cifar10_ResNet20v1_model-0001.h5\n",
      "Epoch 2/10\n",
      "\n",
      " Optimizer iteration 391, batch 0\n",
      "\n",
      " Learning rate 0.0009040344899094011, Model learning rate 0.0009040344739332795\n",
      "  1/391 [..............................] - ETA: 17s - loss: 1.2634 - acc: 0.6719\n",
      " Optimizer iteration 392, batch 1\n",
      "\n",
      " Learning rate 0.0009035594339371228, Model learning rate 0.0009035594412125647\n",
      "\n",
      " Optimizer iteration 393, batch 2\n",
      "\n",
      " Learning rate 0.0009030833305036732, Model learning rate 0.0009030833025462925\n",
      "  3/391 [..............................] - ETA: 17s - loss: 1.3352 - acc: 0.5938\n",
      " Optimizer iteration 394, batch 3\n",
      "\n",
      " Learning rate 0.0009026061808448055, Model learning rate 0.0009026061743497849\n",
      "  4/391 [..............................] - ETA: 17s - loss: 1.3705 - acc: 0.5723\n",
      " Optimizer iteration 395, batch 4\n",
      "\n",
      " Learning rate 0.0009021279861989884, Model learning rate 0.0009021279984153807\n",
      "\n",
      " Optimizer iteration 396, batch 5\n",
      "\n",
      " Learning rate 0.0009016487478074031, Model learning rate 0.0009016487747430801\n",
      "  6/391 [..............................] - ETA: 17s - loss: 1.4403 - acc: 0.5417\n",
      " Optimizer iteration 397, batch 6\n",
      "\n",
      " Learning rate 0.0009011684669139397, Model learning rate 0.0009011684451252222\n",
      "  7/391 [..............................] - ETA: 18s - loss: 1.4345 - acc: 0.5480\n",
      " Optimizer iteration 398, batch 7\n",
      "\n",
      " Learning rate 0.0009006871447651941, Model learning rate 0.0009006871259771287\n",
      "\n",
      " Optimizer iteration 399, batch 8\n",
      "\n",
      " Learning rate 0.0009002047826104651, Model learning rate 0.0009002047590911388\n",
      "  9/391 [..............................] - ETA: 18s - loss: 1.4331 - acc: 0.5460\n",
      " Optimizer iteration 400, batch 9\n",
      "\n",
      " Learning rate 0.0008997213817017506, Model learning rate 0.0008997214026749134\n",
      " 10/391 [..............................] - ETA: 18s - loss: 1.4219 - acc: 0.5484\n",
      " Optimizer iteration 401, batch 10\n",
      "\n",
      " Learning rate 0.0008992369432937451, Model learning rate 0.0008992369403131306\n",
      "\n",
      " Optimizer iteration 402, batch 11\n",
      "\n",
      " Learning rate 0.0008987514686438353, Model learning rate 0.0008987514884211123\n",
      " 12/391 [..............................] - ETA: 18s - loss: 1.4004 - acc: 0.5547\n",
      " Optimizer iteration 403, batch 12\n",
      "\n",
      " Learning rate 0.0008982649590120981, Model learning rate 0.0008982649305835366\n",
      " 13/391 [..............................] - ETA: 18s - loss: 1.4001 - acc: 0.5547\n",
      " Optimizer iteration 404, batch 13\n",
      "\n",
      " Learning rate 0.0008977774156612968, Model learning rate 0.0008977774414233863\n",
      " 14/391 [>.............................] - ETA: 18s - loss: 1.3970 - acc: 0.5575\n",
      " Optimizer iteration 405, batch 14\n",
      "\n",
      " Learning rate 0.0008972888398568772, Model learning rate 0.0008972888463176787\n",
      " 15/391 [>.............................] - ETA: 18s - loss: 1.3979 - acc: 0.5531\n",
      " Optimizer iteration 406, batch 15\n",
      "\n",
      " Learning rate 0.0008967992328669654, Model learning rate 0.0008967992616817355\n",
      "\n",
      " Optimizer iteration 407, batch 16\n",
      "\n",
      " Learning rate 0.0008963085959623637, Model learning rate 0.000896308571100235\n",
      " 17/391 [>.............................] - ETA: 18s - loss: 1.4010 - acc: 0.5515\n",
      " Optimizer iteration 408, batch 17\n",
      "\n",
      " Learning rate 0.0008958169304165479, Model learning rate 0.0008958169491961598\n",
      "\n",
      " Optimizer iteration 409, batch 18\n",
      "\n",
      " Learning rate 0.0008953242375056634, Model learning rate 0.0008953242213465273\n",
      " 19/391 [>.............................] - ETA: 18s - loss: 1.4089 - acc: 0.5456\n",
      " Optimizer iteration 410, batch 19\n",
      "\n",
      " Learning rate 0.0008948305185085225, Model learning rate 0.0008948305039666593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20/391 [>.............................] - ETA: 18s - loss: 1.4103 - acc: 0.5457\n",
      " Optimizer iteration 411, batch 20\n",
      "\n",
      " Learning rate 0.0008943357747066003, Model learning rate 0.0008943357970565557\n",
      " 21/391 [>.............................] - ETA: 18s - loss: 1.4057 - acc: 0.5469\n",
      " Optimizer iteration 412, batch 21\n",
      "\n",
      " Learning rate 0.0008938400073840325, Model learning rate 0.0008938399842008948\n",
      " 22/391 [>.............................] - ETA: 19s - loss: 1.3991 - acc: 0.5494\n",
      " Optimizer iteration 413, batch 22\n",
      "\n",
      " Learning rate 0.0008933432178276107, Model learning rate 0.0008933432400226593\n",
      " 23/391 [>.............................] - ETA: 19s - loss: 1.3980 - acc: 0.5486\n",
      " Optimizer iteration 414, batch 23\n",
      "\n",
      " Learning rate 0.0008928454073267801, Model learning rate 0.0008928453898988664\n",
      "\n",
      " Optimizer iteration 415, batch 24\n",
      "\n",
      " Learning rate 0.000892346577173636, Model learning rate 0.000892346550244838\n",
      " 25/391 [>.............................] - ETA: 20s - loss: 1.4053 - acc: 0.5472\n",
      " Optimizer iteration 416, batch 25\n",
      "\n",
      " Learning rate 0.0008918467286629199, Model learning rate 0.0008918467210605741\n",
      " 26/391 [>.............................] - ETA: 20s - loss: 1.4075 - acc: 0.5457\n",
      " Optimizer iteration 417, batch 26\n",
      "\n",
      " Learning rate 0.0008913458630920168, Model learning rate 0.0008913458441384137\n",
      " 27/391 [=>............................] - ETA: 20s - loss: 1.4044 - acc: 0.5469\n",
      " Optimizer iteration 418, batch 27\n",
      "\n",
      " Learning rate 0.0008908439817609514, Model learning rate 0.0008908439776860178\n",
      " 28/391 [=>............................] - ETA: 20s - loss: 1.4032 - acc: 0.5477\n",
      " Optimizer iteration 419, batch 28\n",
      "\n",
      " Learning rate 0.0008903410859723847, Model learning rate 0.0008903410634957254\n",
      " 29/391 [=>............................] - ETA: 21s - loss: 1.4027 - acc: 0.5485\n",
      " Optimizer iteration 420, batch 29\n",
      "\n",
      " Learning rate 0.0008898371770316111, Model learning rate 0.0008898371597751975\n",
      "\n",
      " Optimizer iteration 421, batch 30\n",
      "\n",
      " Learning rate 0.0008893322562465546, Model learning rate 0.0008893322665244341\n",
      " 31/391 [=>............................] - ETA: 21s - loss: 1.3996 - acc: 0.5486\n",
      " Optimizer iteration 422, batch 31\n",
      "\n",
      " Learning rate 0.0008888263249277656, Model learning rate 0.0008888263255357742\n",
      " 32/391 [=>............................] - ETA: 21s - loss: 1.3968 - acc: 0.5493\n",
      " Optimizer iteration 423, batch 32\n",
      "\n",
      " Learning rate 0.0008883193843884169, Model learning rate 0.0008883193950168788\n",
      " 33/391 [=>............................] - ETA: 21s - loss: 1.3981 - acc: 0.5483\n",
      " Optimizer iteration 424, batch 33\n",
      "\n",
      " Learning rate 0.0008878114359443012, Model learning rate 0.000887811416760087\n",
      " 34/391 [=>............................] - ETA: 21s - loss: 1.3969 - acc: 0.5487\n",
      " Optimizer iteration 425, batch 34\n",
      "\n",
      " Learning rate 0.0008873024809138273, Model learning rate 0.0008873025071807206\n",
      " 35/391 [=>............................] - ETA: 21s - loss: 1.3913 - acc: 0.5496\n",
      " Optimizer iteration 426, batch 35\n",
      "\n",
      " Learning rate 0.0008867925206180165, Model learning rate 0.0008867924916557968\n",
      "\n",
      " Optimizer iteration 427, batch 36\n",
      "\n",
      " Learning rate 0.0008862815563804996, Model learning rate 0.0008862815448082983\n",
      " 37/391 [=>............................] - ETA: 22s - loss: 1.3837 - acc: 0.5536\n",
      " Optimizer iteration 428, batch 37\n",
      "\n",
      " Learning rate 0.0008857695895275126, Model learning rate 0.0008857696084305644\n",
      " 38/391 [=>............................] - ETA: 22s - loss: 1.3823 - acc: 0.5549\n",
      " Optimizer iteration 429, batch 38\n",
      "\n",
      " Learning rate 0.0008852566213878947, Model learning rate 0.000885256624314934\n",
      " 39/391 [=>............................] - ETA: 22s - loss: 1.3817 - acc: 0.5551\n",
      " Optimizer iteration 430, batch 39\n",
      "\n",
      " Learning rate 0.000884742653293083, Model learning rate 0.0008847426506690681\n",
      " 40/391 [==>...........................] - ETA: 22s - loss: 1.3828 - acc: 0.5553\n",
      " Optimizer iteration 431, batch 40\n",
      "\n",
      " Learning rate 0.0008842276865771108, Model learning rate 0.0008842276874929667\n",
      " 41/391 [==>...........................] - ETA: 22s - loss: 1.3836 - acc: 0.5549\n",
      " Optimizer iteration 432, batch 41\n",
      "\n",
      " Learning rate 0.0008837117225766032, Model learning rate 0.0008837117347866297\n",
      " 42/391 [==>...........................] - ETA: 22s - loss: 1.3838 - acc: 0.5549\n",
      " Optimizer iteration 433, batch 42\n",
      "\n",
      " Learning rate 0.0008831947626307734, Model learning rate 0.0008831947343423963\n",
      " 43/391 [==>...........................] - ETA: 22s - loss: 1.3808 - acc: 0.5558\n",
      " Optimizer iteration 434, batch 43\n",
      "\n",
      " Learning rate 0.0008826768080814205, Model learning rate 0.0008826768025755882\n",
      " 44/391 [==>...........................] - ETA: 22s - loss: 1.3801 - acc: 0.5545\n",
      " Optimizer iteration 435, batch 44\n",
      "\n",
      " Learning rate 0.0008821578602729241, Model learning rate 0.0008821578812785447\n",
      " 45/391 [==>...........................] - ETA: 22s - loss: 1.3801 - acc: 0.5538\n",
      " Optimizer iteration 436, batch 45\n",
      "\n",
      " Learning rate 0.0008816379205522428, Model learning rate 0.0008816379122436047\n",
      " 46/391 [==>...........................] - ETA: 22s - loss: 1.3751 - acc: 0.5559\n",
      " Optimizer iteration 437, batch 46\n",
      "\n",
      " Learning rate 0.000881116990268909, Model learning rate 0.00088111701188609\n",
      " 47/391 [==>...........................] - ETA: 22s - loss: 1.3704 - acc: 0.5573\n",
      " Optimizer iteration 438, batch 47\n",
      "\n",
      " Learning rate 0.0008805950707750268, Model learning rate 0.000880595063790679\n",
      " 48/391 [==>...........................] - ETA: 22s - loss: 1.3709 - acc: 0.5573\n",
      " Optimizer iteration 439, batch 48\n",
      "\n",
      " Learning rate 0.0008800721634252671, Model learning rate 0.0008800721843726933\n",
      " 49/391 [==>...........................] - ETA: 22s - loss: 1.3703 - acc: 0.5576\n",
      " Optimizer iteration 440, batch 49\n",
      "\n",
      " Learning rate 0.0008795482695768658, Model learning rate 0.0008795482572168112\n",
      "\n",
      " Optimizer iteration 441, batch 50\n",
      "\n",
      " Learning rate 0.0008790233905896185, Model learning rate 0.0008790233987383544\n",
      " 51/391 [==>...........................] - ETA: 22s - loss: 1.3747 - acc: 0.5553\n",
      " Optimizer iteration 442, batch 51\n",
      "\n",
      " Learning rate 0.0008784975278258782, Model learning rate 0.0008784975507296622\n",
      "\n",
      " Optimizer iteration 443, batch 52\n",
      "\n",
      " Learning rate 0.0008779706826505513, Model learning rate 0.0008779706549830735\n",
      " 53/391 [===>..........................] - ETA: 22s - loss: 1.3729 - acc: 0.5568\n",
      " Optimizer iteration 444, batch 53\n",
      "\n",
      " Learning rate 0.0008774428564310938, Model learning rate 0.0008774428279139102\n",
      " 54/391 [===>..........................] - ETA: 22s - loss: 1.3713 - acc: 0.5580\n",
      " Optimizer iteration 445, batch 54\n",
      "\n",
      " Learning rate 0.0008769140505375084, Model learning rate 0.0008769140695221722\n",
      " 55/391 [===>..........................] - ETA: 22s - loss: 1.3712 - acc: 0.5585\n",
      " Optimizer iteration 446, batch 55\n",
      "\n",
      " Learning rate 0.0008763842663423407, Model learning rate 0.0008763842633925378\n",
      "\n",
      " Optimizer iteration 447, batch 56\n",
      "\n",
      " Learning rate 0.0008758535052206749, Model learning rate 0.0008758535259403288\n",
      " 57/391 [===>..........................] - ETA: 22s - loss: 1.3710 - acc: 0.5588\n",
      " Optimizer iteration 448, batch 57\n",
      "\n",
      " Learning rate 0.0008753217685501317, Model learning rate 0.0008753217407502234\n",
      " 58/391 [===>..........................] - ETA: 22s - loss: 1.3730 - acc: 0.5593\n",
      " Optimizer iteration 449, batch 58\n",
      "\n",
      " Learning rate 0.0008747890577108633, Model learning rate 0.0008747890824452043\n",
      " 59/391 [===>..........................] - ETA: 22s - loss: 1.3722 - acc: 0.5587\n",
      " Optimizer iteration 450, batch 59\n",
      "\n",
      " Learning rate 0.0008742553740855505, Model learning rate 0.0008742553764022887\n",
      " 60/391 [===>..........................] - ETA: 22s - loss: 1.3745 - acc: 0.5578\n",
      " Optimizer iteration 451, batch 60\n",
      "\n",
      " Learning rate 0.0008737207190593994, Model learning rate 0.0008737207390367985\n",
      " 61/391 [===>..........................] - ETA: 22s - loss: 1.3709 - acc: 0.5593\n",
      " Optimizer iteration 452, batch 61\n",
      "\n",
      " Learning rate 0.0008731850940201369, Model learning rate 0.0008731851121410728\n",
      " 62/391 [===>..........................] - ETA: 22s - loss: 1.3681 - acc: 0.5602\n",
      " Optimizer iteration 453, batch 62\n",
      "\n",
      " Learning rate 0.000872648500358008, Model learning rate 0.0008726484957151115\n",
      " 63/391 [===>..........................] - ETA: 22s - loss: 1.3677 - acc: 0.5601\n",
      " Optimizer iteration 454, batch 63\n",
      "\n",
      " Learning rate 0.0008721109394657716, Model learning rate 0.0008721109479665756\n",
      " 64/391 [===>..........................] - ETA: 22s - loss: 1.3654 - acc: 0.5610\n",
      " Optimizer iteration 455, batch 64\n",
      "\n",
      " Learning rate 0.0008715724127386971, Model learning rate 0.0008715724106878042\n",
      " 65/391 [===>..........................] - ETA: 22s - loss: 1.3665 - acc: 0.5609\n",
      " Optimizer iteration 456, batch 65\n",
      "\n",
      " Learning rate 0.0008710329215745611, Model learning rate 0.0008710329420864582\n",
      " 66/391 [====>.........................] - ETA: 22s - loss: 1.3646 - acc: 0.5612\n",
      " Optimizer iteration 457, batch 66\n",
      "\n",
      " Learning rate 0.000870492467373643, Model learning rate 0.0008704924839548767\n",
      " 67/391 [====>.........................] - ETA: 22s - loss: 1.3633 - acc: 0.5624\n",
      " Optimizer iteration 458, batch 67\n",
      "\n",
      " Learning rate 0.0008699510515387221, Model learning rate 0.0008699510362930596\n",
      " 68/391 [====>.........................] - ETA: 22s - loss: 1.3635 - acc: 0.5628\n",
      " Optimizer iteration 459, batch 68\n",
      "\n",
      " Learning rate 0.0008694086754750737, Model learning rate 0.0008694086573086679\n",
      " 69/391 [====>.........................] - ETA: 22s - loss: 1.3603 - acc: 0.5639\n",
      " Optimizer iteration 460, batch 69\n",
      "\n",
      " Learning rate 0.0008688653405904651, Model learning rate 0.0008688653470017016\n",
      "\n",
      " Optimizer iteration 461, batch 70\n",
      "\n",
      " Learning rate 0.0008683210482951527, Model learning rate 0.0008683210471644998\n",
      " 71/391 [====>.........................] - ETA: 22s - loss: 1.3580 - acc: 0.5653\n",
      " Optimizer iteration 462, batch 71\n",
      "\n",
      " Learning rate 0.0008677758000018776, Model learning rate 0.0008677758160047233\n",
      " 72/391 [====>.........................] - ETA: 22s - loss: 1.3577 - acc: 0.5655\n",
      " Optimizer iteration 463, batch 72\n",
      "\n",
      " Learning rate 0.0008672295971258625, Model learning rate 0.0008672295953147113\n",
      " 73/391 [====>.........................] - ETA: 22s - loss: 1.3584 - acc: 0.5645\n",
      " Optimizer iteration 464, batch 73\n",
      "\n",
      " Learning rate 0.0008666824410848075, Model learning rate 0.0008666824433021247\n",
      " 74/391 [====>.........................] - ETA: 22s - loss: 1.3567 - acc: 0.5645\n",
      " Optimizer iteration 465, batch 74\n",
      "\n",
      " Learning rate 0.0008661343332988868, Model learning rate 0.0008661343599669635\n",
      "\n",
      " Optimizer iteration 466, batch 75\n",
      "\n",
      " Learning rate 0.0008655852751907451, Model learning rate 0.0008655852871015668\n",
      " 76/391 [====>.........................] - ETA: 21s - loss: 1.3540 - acc: 0.5660\n",
      " Optimizer iteration 467, batch 76\n",
      "\n",
      " Learning rate 0.0008650352681854933, Model learning rate 0.0008650352829135954\n",
      " 77/391 [====>.........................] - ETA: 22s - loss: 1.3543 - acc: 0.5655\n",
      " Optimizer iteration 468, batch 77\n",
      "\n",
      " Learning rate 0.0008644843137107057, Model learning rate 0.0008644842891953886\n",
      " 78/391 [====>.........................] - ETA: 22s - loss: 1.3535 - acc: 0.5656\n",
      " Optimizer iteration 469, batch 78\n",
      "\n",
      " Learning rate 0.0008639324131964155, Model learning rate 0.000863932422362268\n",
      "\n",
      " Optimizer iteration 470, batch 79\n",
      "\n",
      " Learning rate 0.0008633795680751116, Model learning rate 0.0008633795659989119\n",
      " 80/391 [=====>........................] - ETA: 21s - loss: 1.3527 - acc: 0.5663\n",
      " Optimizer iteration 471, batch 80\n",
      "\n",
      " Learning rate 0.0008628257797817344, Model learning rate 0.0008628257783129811\n",
      " 81/391 [=====>........................] - ETA: 22s - loss: 1.3526 - acc: 0.5666\n",
      " Optimizer iteration 472, batch 81\n",
      "\n",
      " Learning rate 0.0008622710497536725, Model learning rate 0.0008622710593044758\n",
      "\n",
      " Optimizer iteration 473, batch 82\n",
      "\n",
      " Learning rate 0.0008617153794307588, Model learning rate 0.0008617153507657349\n",
      " 83/391 [=====>........................] - ETA: 21s - loss: 1.3539 - acc: 0.5661\n",
      " Optimizer iteration 474, batch 83\n",
      "\n",
      " Learning rate 0.000861158770255267, Model learning rate 0.0008611587691120803\n",
      " 84/391 [=====>........................] - ETA: 21s - loss: 1.3524 - acc: 0.5671\n",
      " Optimizer iteration 475, batch 84\n",
      "\n",
      " Learning rate 0.0008606012236719073, Model learning rate 0.0008606011979281902\n",
      " 85/391 [=====>........................] - ETA: 21s - loss: 1.3529 - acc: 0.5670\n",
      " Optimizer iteration 476, batch 85\n",
      "\n",
      " Learning rate 0.0008600427411278233, Model learning rate 0.0008600427536293864\n",
      " 86/391 [=====>........................] - ETA: 21s - loss: 1.3537 - acc: 0.5670\n",
      " Optimizer iteration 477, batch 86\n",
      "\n",
      " Learning rate 0.0008594833240725876, Model learning rate 0.0008594833198003471\n",
      " 87/391 [=====>........................] - ETA: 21s - loss: 1.3525 - acc: 0.5680\n",
      " Optimizer iteration 478, batch 87\n",
      "\n",
      " Learning rate 0.0008589229739581988, Model learning rate 0.0008589229546487331\n",
      " 88/391 [=====>........................] - ETA: 21s - loss: 1.3518 - acc: 0.5684\n",
      " Optimizer iteration 479, batch 88\n",
      "\n",
      " Learning rate 0.0008583616922390771, Model learning rate 0.0008583617163822055\n",
      " 89/391 [=====>........................] - ETA: 21s - loss: 1.3523 - acc: 0.5685\n",
      " Optimizer iteration 480, batch 89\n",
      "\n",
      " Learning rate 0.0008577994803720606, Model learning rate 0.0008577994885854423\n",
      "\n",
      " Optimizer iteration 481, batch 90\n",
      "\n",
      " Learning rate 0.0008572363398164017, Model learning rate 0.0008572363294661045\n",
      " 91/391 [=====>........................] - ETA: 21s - loss: 1.3505 - acc: 0.5691\n",
      " Optimizer iteration 482, batch 91\n",
      "\n",
      " Learning rate 0.0008566722720337634, Model learning rate 0.000856672297231853\n",
      " 92/391 [======>.......................] - ETA: 21s - loss: 1.3499 - acc: 0.5693\n",
      " Optimizer iteration 483, batch 92\n",
      "\n",
      " Learning rate 0.0008561072784882155, Model learning rate 0.000856107275467366\n",
      " 93/391 [======>.......................] - ETA: 21s - loss: 1.3505 - acc: 0.5695\n",
      " Optimizer iteration 484, batch 93\n",
      "\n",
      " Learning rate 0.0008555413606462301, Model learning rate 0.0008555413805879653\n",
      "\n",
      " Optimizer iteration 485, batch 94\n",
      "\n",
      " Learning rate 0.0008549745199766792, Model learning rate 0.000854974496178329\n",
      " 95/391 [======>.......................] - ETA: 21s - loss: 1.3484 - acc: 0.5695\n",
      " Optimizer iteration 486, batch 95\n",
      "\n",
      " Learning rate 0.0008544067579508291, Model learning rate 0.000854406738653779\n",
      " 96/391 [======>.......................] - ETA: 21s - loss: 1.3479 - acc: 0.5692\n",
      " Optimizer iteration 487, batch 96\n",
      "\n",
      " Learning rate 0.0008538380760423383, Model learning rate 0.0008538380498066545\n",
      " 97/391 [======>.......................] - ETA: 21s - loss: 1.3477 - acc: 0.5694\n",
      " Optimizer iteration 488, batch 97\n",
      "\n",
      " Learning rate 0.0008532684757272526, Model learning rate 0.0008532684878446162\n",
      "\n",
      " Optimizer iteration 489, batch 98\n",
      "\n",
      " Learning rate 0.0008526979584840015, Model learning rate 0.0008526979363523424\n",
      " 99/391 [======>.......................] - ETA: 20s - loss: 1.3459 - acc: 0.5701\n",
      " Optimizer iteration 490, batch 99\n",
      "\n",
      " Learning rate 0.0008521265257933948, Model learning rate 0.0008521265117451549\n",
      "100/391 [======>.......................] - ETA: 20s - loss: 1.3470 - acc: 0.5699\n",
      " Optimizer iteration 491, batch 100\n",
      "\n",
      " Learning rate 0.0008515541791386177, Model learning rate 0.0008515541558153927\n",
      "101/391 [======>.......................] - ETA: 20s - loss: 1.3456 - acc: 0.5705\n",
      " Optimizer iteration 492, batch 101\n",
      "\n",
      " Learning rate 0.0008509809200052286, Model learning rate 0.0008509809267707169\n",
      "102/391 [======>.......................] - ETA: 20s - loss: 1.3451 - acc: 0.5706\n",
      " Optimizer iteration 493, batch 102\n",
      "\n",
      " Learning rate 0.0008504067498811532, Model learning rate 0.0008504067664034665\n",
      "103/391 [======>.......................] - ETA: 20s - loss: 1.3448 - acc: 0.5703\n",
      " Optimizer iteration 494, batch 103\n",
      "\n",
      " Learning rate 0.0008498316702566827, Model learning rate 0.0008498316747136414\n",
      "104/391 [======>.......................] - ETA: 20s - loss: 1.3436 - acc: 0.5711\n",
      " Optimizer iteration 495, batch 104\n",
      "\n",
      " Learning rate 0.0008492556826244686, Model learning rate 0.0008492557099089026\n",
      "105/391 [=======>......................] - ETA: 20s - loss: 1.3426 - acc: 0.5720\n",
      " Optimizer iteration 496, batch 105\n",
      "\n",
      " Learning rate 0.0008486787884795188, Model learning rate 0.0008486788137815893\n",
      "106/391 [=======>......................] - ETA: 20s - loss: 1.3428 - acc: 0.5719\n",
      " Optimizer iteration 497, batch 106\n",
      "\n",
      " Learning rate 0.0008481009893191947, Model learning rate 0.0008481009863317013\n",
      "107/391 [=======>......................] - ETA: 20s - loss: 1.3426 - acc: 0.5716\n",
      " Optimizer iteration 498, batch 107\n",
      "\n",
      " Learning rate 0.0008475222866432064, Model learning rate 0.0008475222857668996\n",
      "108/391 [=======>......................] - ETA: 20s - loss: 1.3414 - acc: 0.5722\n",
      " Optimizer iteration 499, batch 108\n",
      "\n",
      " Learning rate 0.0008469426819536092, Model learning rate 0.0008469426538795233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/391 [=======>......................] - ETA: 20s - loss: 1.3416 - acc: 0.5723\n",
      " Optimizer iteration 500, batch 109\n",
      "\n",
      " Learning rate 0.0008463621767547997, Model learning rate 0.0008463621488772333\n",
      "110/391 [=======>......................] - ETA: 20s - loss: 1.3423 - acc: 0.5723\n",
      " Optimizer iteration 501, batch 110\n",
      "\n",
      " Learning rate 0.0008457807725535116, Model learning rate 0.0008457807707600296\n",
      "111/391 [=======>......................] - ETA: 20s - loss: 1.3426 - acc: 0.5720\n",
      " Optimizer iteration 502, batch 111\n",
      "\n",
      " Learning rate 0.0008451984708588121, Model learning rate 0.0008451984613202512\n",
      "\n",
      " Optimizer iteration 503, batch 112\n",
      "\n",
      " Learning rate 0.0008446152731820983, Model learning rate 0.0008446152787655592\n",
      "113/391 [=======>......................] - ETA: 20s - loss: 1.3435 - acc: 0.5709\n",
      " Optimizer iteration 504, batch 113\n",
      "\n",
      " Learning rate 0.0008440311810370921, Model learning rate 0.0008440311648882926\n",
      "\n",
      " Optimizer iteration 505, batch 114\n",
      "\n",
      " Learning rate 0.0008434461959398376, Model learning rate 0.0008434461778961122\n",
      "115/391 [=======>......................] - ETA: 20s - loss: 1.3430 - acc: 0.5710\n",
      " Optimizer iteration 506, batch 115\n",
      "\n",
      " Learning rate 0.0008428603194086966, Model learning rate 0.0008428603177890182\n",
      "\n",
      " Optimizer iteration 507, batch 116\n",
      "\n",
      " Learning rate 0.0008422735529643444, Model learning rate 0.0008422735263593495\n",
      "117/391 [=======>......................] - ETA: 19s - loss: 1.3407 - acc: 0.5720\n",
      " Optimizer iteration 508, batch 117\n",
      "\n",
      " Learning rate 0.0008416858981297663, Model learning rate 0.000841685920022428\n",
      "\n",
      " Optimizer iteration 509, batch 118\n",
      "\n",
      " Learning rate 0.0008410973564302533, Model learning rate 0.000841097382362932\n",
      "119/391 [========>.....................] - ETA: 19s - loss: 1.3390 - acc: 0.5729\n",
      " Optimizer iteration 510, batch 119\n",
      "\n",
      " Learning rate 0.0008405079293933986, Model learning rate 0.0008405079133808613\n",
      "\n",
      " Optimizer iteration 511, batch 120\n",
      "\n",
      " Learning rate 0.000839917618549093, Model learning rate 0.0008399176294915378\n",
      "121/391 [========>.....................] - ETA: 19s - loss: 1.3373 - acc: 0.5737\n",
      " Optimizer iteration 512, batch 121\n",
      "\n",
      " Learning rate 0.0008393264254295217, Model learning rate 0.0008393264142796397\n",
      "\n",
      " Optimizer iteration 513, batch 122\n",
      "\n",
      " Learning rate 0.0008387343515691595, Model learning rate 0.0008387343259528279\n",
      "123/391 [========>.....................] - ETA: 19s - loss: 1.3347 - acc: 0.5748\n",
      " Optimizer iteration 514, batch 123\n",
      "\n",
      " Learning rate 0.0008381413985047672, Model learning rate 0.0008381414227187634\n",
      "124/391 [========>.....................] - ETA: 19s - loss: 1.3337 - acc: 0.5750\n",
      " Optimizer iteration 515, batch 124\n",
      "\n",
      " Learning rate 0.0008375475677753881, Model learning rate 0.0008375475881621242\n",
      "125/391 [========>.....................] - ETA: 19s - loss: 1.3332 - acc: 0.5759\n",
      " Optimizer iteration 516, batch 125\n",
      "\n",
      " Learning rate 0.0008369528609223429, Model learning rate 0.0008369528804905713\n",
      "\n",
      " Optimizer iteration 517, batch 126\n",
      "\n",
      " Learning rate 0.0008363572794892267, Model learning rate 0.0008363572997041047\n",
      "127/391 [========>.....................] - ETA: 19s - loss: 1.3335 - acc: 0.5763\n",
      " Optimizer iteration 518, batch 127\n",
      "\n",
      " Learning rate 0.0008357608250219047, Model learning rate 0.0008357608458027244\n",
      "\n",
      " Optimizer iteration 519, batch 128\n",
      "\n",
      " Learning rate 0.0008351634990685079, Model learning rate 0.0008351635187864304\n",
      "129/391 [========>.....................] - ETA: 19s - loss: 1.3319 - acc: 0.5771\n",
      " Optimizer iteration 520, batch 129\n",
      "\n",
      " Learning rate 0.0008345653031794292, Model learning rate 0.0008345653186552227\n",
      "130/391 [========>.....................] - ETA: 19s - loss: 1.3300 - acc: 0.5777\n",
      " Optimizer iteration 521, batch 130\n",
      "\n",
      " Learning rate 0.0008339662389073197, Model learning rate 0.0008339662454091012\n",
      "\n",
      " Optimizer iteration 522, batch 131\n",
      "\n",
      " Learning rate 0.0008333663078070846, Model learning rate 0.0008333662990480661\n",
      "132/391 [=========>....................] - ETA: 18s - loss: 1.3306 - acc: 0.5772\n",
      " Optimizer iteration 523, batch 132\n",
      "\n",
      " Learning rate 0.0008327655114358782, Model learning rate 0.0008327655377797782\n",
      "133/391 [=========>....................] - ETA: 18s - loss: 1.3306 - acc: 0.5773\n",
      " Optimizer iteration 524, batch 133\n",
      "\n",
      " Learning rate 0.0008321638513531018, Model learning rate 0.0008321638451889157\n",
      "134/391 [=========>....................] - ETA: 18s - loss: 1.3309 - acc: 0.5773\n",
      " Optimizer iteration 525, batch 134\n",
      "\n",
      " Learning rate 0.0008315613291203976, Model learning rate 0.0008315613376908004\n",
      "\n",
      " Optimizer iteration 526, batch 135\n",
      "\n",
      " Learning rate 0.000830957946301646, Model learning rate 0.0008309579570777714\n",
      "136/391 [=========>....................] - ETA: 18s - loss: 1.3322 - acc: 0.5767\n",
      " Optimizer iteration 527, batch 136\n",
      "\n",
      " Learning rate 0.0008303537044629611, Model learning rate 0.0008303537033498287\n",
      "137/391 [=========>....................] - ETA: 18s - loss: 1.3321 - acc: 0.5768\n",
      " Optimizer iteration 528, batch 137\n",
      "\n",
      " Learning rate 0.0008297486051726862, Model learning rate 0.0008297485765069723\n",
      "138/391 [=========>....................] - ETA: 18s - loss: 1.3326 - acc: 0.5763\n",
      " Optimizer iteration 529, batch 138\n",
      "\n",
      " Learning rate 0.0008291426500013908, Model learning rate 0.0008291426347568631\n",
      "\n",
      " Optimizer iteration 530, batch 139\n",
      "\n",
      " Learning rate 0.0008285358405218655, Model learning rate 0.0008285358198918402\n",
      "140/391 [=========>....................] - ETA: 18s - loss: 1.3317 - acc: 0.5767\n",
      " Optimizer iteration 531, batch 140\n",
      "\n",
      " Learning rate 0.0008279281783091181, Model learning rate 0.0008279281901195645\n",
      "141/391 [=========>....................] - ETA: 18s - loss: 1.3314 - acc: 0.5767\n",
      " Optimizer iteration 532, batch 141\n",
      "\n",
      " Learning rate 0.0008273196649403702, Model learning rate 0.0008273196872323751\n",
      "142/391 [=========>....................] - ETA: 18s - loss: 1.3316 - acc: 0.5765\n",
      " Optimizer iteration 533, batch 142\n",
      "\n",
      " Learning rate 0.0008267103019950528, Model learning rate 0.0008267103112302721\n",
      "\n",
      " Optimizer iteration 534, batch 143\n",
      "\n",
      " Learning rate 0.000826100091054801, Model learning rate 0.0008261000621132553\n",
      "144/391 [==========>...................] - ETA: 18s - loss: 1.3304 - acc: 0.5768\n",
      " Optimizer iteration 535, batch 144\n",
      "\n",
      " Learning rate 0.0008254890337034519, Model learning rate 0.0008254890562966466\n",
      "145/391 [==========>...................] - ETA: 18s - loss: 1.3299 - acc: 0.5772\n",
      " Optimizer iteration 536, batch 145\n",
      "\n",
      " Learning rate 0.0008248771315270392, Model learning rate 0.0008248771191574633\n",
      "146/391 [==========>...................] - ETA: 17s - loss: 1.3311 - acc: 0.5767\n",
      " Optimizer iteration 537, batch 146\n",
      "\n",
      " Learning rate 0.0008242643861137891, Model learning rate 0.0008242643671110272\n",
      "\n",
      " Optimizer iteration 538, batch 147\n",
      "\n",
      " Learning rate 0.0008236507990541169, Model learning rate 0.0008236508001573384\n",
      "148/391 [==========>...................] - ETA: 17s - loss: 1.3301 - acc: 0.5769\n",
      " Optimizer iteration 539, batch 148\n",
      "\n",
      " Learning rate 0.0008230363719406223, Model learning rate 0.0008230363600887358\n",
      "149/391 [==========>...................] - ETA: 17s - loss: 1.3297 - acc: 0.5770\n",
      " Optimizer iteration 540, batch 149\n",
      "\n",
      " Learning rate 0.0008224211063680853, Model learning rate 0.0008224211051128805\n",
      "150/391 [==========>...................] - ETA: 17s - loss: 1.3296 - acc: 0.5771\n",
      " Optimizer iteration 541, batch 150\n",
      "\n",
      " Learning rate 0.0008218050039334624, Model learning rate 0.0008218049770221114\n",
      "\n",
      " Optimizer iteration 542, batch 151\n",
      "\n",
      " Learning rate 0.0008211880662358817, Model learning rate 0.0008211880922317505\n",
      "152/391 [==========>...................] - ETA: 17s - loss: 1.3288 - acc: 0.5773\n",
      " Optimizer iteration 543, batch 152\n",
      "\n",
      " Learning rate 0.00082057029487664, Model learning rate 0.0008205702761188149\n",
      "153/391 [==========>...................] - ETA: 17s - loss: 1.3295 - acc: 0.5772\n",
      " Optimizer iteration 544, batch 153\n",
      "\n",
      " Learning rate 0.0008199516914591976, Model learning rate 0.0008199517033062875\n",
      "\n",
      " Optimizer iteration 545, batch 154\n",
      "\n",
      " Learning rate 0.0008193322575891739, Model learning rate 0.0008193322573788464\n",
      "155/391 [==========>...................] - ETA: 17s - loss: 1.3295 - acc: 0.5771\n",
      " Optimizer iteration 546, batch 155\n",
      "\n",
      " Learning rate 0.0008187119948743449, Model learning rate 0.0008187119965441525\n",
      "156/391 [==========>...................] - ETA: 17s - loss: 1.3282 - acc: 0.5777\n",
      " Optimizer iteration 547, batch 156\n",
      "\n",
      " Learning rate 0.000818090904924637, Model learning rate 0.0008180909208022058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/391 [===========>..................] - ETA: 17s - loss: 1.3282 - acc: 0.5778\n",
      " Optimizer iteration 548, batch 157\n",
      "\n",
      " Learning rate 0.0008174689893521239, Model learning rate 0.0008174689719453454\n",
      "158/391 [===========>..................] - ETA: 17s - loss: 1.3274 - acc: 0.5782\n",
      " Optimizer iteration 549, batch 158\n",
      "\n",
      " Learning rate 0.0008168462497710226, Model learning rate 0.0008168462663888931\n",
      "159/391 [===========>..................] - ETA: 17s - loss: 1.3279 - acc: 0.5778\n",
      " Optimizer iteration 550, batch 159\n",
      "\n",
      " Learning rate 0.0008162226877976886, Model learning rate 0.0008162226877175272\n",
      "160/391 [===========>..................] - ETA: 16s - loss: 1.3276 - acc: 0.5778\n",
      " Optimizer iteration 551, batch 160\n",
      "\n",
      " Learning rate 0.0008155983050506122, Model learning rate 0.0008155982941389084\n",
      "161/391 [===========>..................] - ETA: 16s - loss: 1.3269 - acc: 0.5779\n",
      " Optimizer iteration 552, batch 161\n",
      "\n",
      " Learning rate 0.0008149731031504135, Model learning rate 0.0008149730856530368\n",
      "162/391 [===========>..................] - ETA: 16s - loss: 1.3265 - acc: 0.5781\n",
      " Optimizer iteration 553, batch 162\n",
      "\n",
      " Learning rate 0.0008143470837198393, Model learning rate 0.0008143470622599125\n",
      "163/391 [===========>..................] - ETA: 16s - loss: 1.3270 - acc: 0.5781\n",
      " Optimizer iteration 554, batch 163\n",
      "\n",
      " Learning rate 0.0008137202483837583, Model learning rate 0.0008137202239595354\n",
      "\n",
      " Optimizer iteration 555, batch 164\n",
      "\n",
      " Learning rate 0.0008130925987691568, Model learning rate 0.0008130925707519054\n",
      "165/391 [===========>..................] - ETA: 16s - loss: 1.3280 - acc: 0.5777\n",
      " Optimizer iteration 556, batch 165\n",
      "\n",
      " Learning rate 0.0008124641365051346, Model learning rate 0.0008124641608446836\n",
      "\n",
      " Optimizer iteration 557, batch 166\n",
      "\n",
      " Learning rate 0.0008118348632229007, Model learning rate 0.0008118348778225482\n",
      "167/391 [===========>..................] - ETA: 16s - loss: 1.3278 - acc: 0.5776\n",
      " Optimizer iteration 558, batch 167\n",
      "\n",
      " Learning rate 0.0008112047805557692, Model learning rate 0.0008112047798931599\n",
      "\n",
      " Optimizer iteration 559, batch 168\n",
      "\n",
      " Learning rate 0.0008105738901391552, Model learning rate 0.0008105738670565188\n",
      "169/391 [===========>..................] - ETA: 16s - loss: 1.3264 - acc: 0.5781\n",
      " Optimizer iteration 560, batch 169\n",
      "\n",
      " Learning rate 0.0008099421936105702, Model learning rate 0.0008099421975202858\n",
      "170/391 [============>.................] - ETA: 16s - loss: 1.3260 - acc: 0.5784\n",
      " Optimizer iteration 561, batch 170\n",
      "\n",
      " Learning rate 0.0008093096926096177, Model learning rate 0.0008093097130768001\n",
      "171/391 [============>.................] - ETA: 16s - loss: 1.3260 - acc: 0.5784\n",
      " Optimizer iteration 562, batch 171\n",
      "\n",
      " Learning rate 0.00080867638877799, Model learning rate 0.0008086764137260616\n",
      "172/391 [============>.................] - ETA: 16s - loss: 1.3254 - acc: 0.5786\n",
      " Optimizer iteration 563, batch 172\n",
      "\n",
      " Learning rate 0.0008080422837594627, Model learning rate 0.0008080422994680703\n",
      "173/391 [============>.................] - ETA: 16s - loss: 1.3256 - acc: 0.5787\n",
      " Optimizer iteration 564, batch 173\n",
      "\n",
      " Learning rate 0.0008074073791998906, Model learning rate 0.0008074073703028262\n",
      "174/391 [============>.................] - ETA: 16s - loss: 1.3250 - acc: 0.5788\n",
      " Optimizer iteration 565, batch 174\n",
      "\n",
      " Learning rate 0.0008067716767472045, Model learning rate 0.0008067716844379902\n",
      "\n",
      " Optimizer iteration 566, batch 175\n",
      "\n",
      " Learning rate 0.0008061351780514057, Model learning rate 0.0008061351836659014\n",
      "176/391 [============>.................] - ETA: 15s - loss: 1.3259 - acc: 0.5786\n",
      " Optimizer iteration 567, batch 176\n",
      "\n",
      " Learning rate 0.0008054978847645622, Model learning rate 0.0008054978679865599\n",
      "177/391 [============>.................] - ETA: 15s - loss: 1.3258 - acc: 0.5786\n",
      " Optimizer iteration 568, batch 177\n",
      "\n",
      " Learning rate 0.0008048597985408047, Model learning rate 0.0008048597956076264\n",
      "178/391 [============>.................] - ETA: 15s - loss: 1.3262 - acc: 0.5786\n",
      " Optimizer iteration 569, batch 178\n",
      "\n",
      " Learning rate 0.0008042209210363216, Model learning rate 0.0008042209083214402\n",
      "179/391 [============>.................] - ETA: 15s - loss: 1.3257 - acc: 0.5789\n",
      " Optimizer iteration 570, batch 179\n",
      "\n",
      " Learning rate 0.0008035812539093556, Model learning rate 0.0008035812643356621\n",
      "\n",
      " Optimizer iteration 571, batch 180\n",
      "\n",
      " Learning rate 0.0008029407988201985, Model learning rate 0.0008029408054426312\n",
      "181/391 [============>.................] - ETA: 15s - loss: 1.3258 - acc: 0.5789\n",
      " Optimizer iteration 572, batch 181\n",
      "\n",
      " Learning rate 0.0008022995574311875, Model learning rate 0.0008022995316423476\n",
      "182/391 [============>.................] - ETA: 15s - loss: 1.3251 - acc: 0.5791\n",
      " Optimizer iteration 573, batch 182\n",
      "\n",
      " Learning rate 0.0008016575314067005, Model learning rate 0.0008016575593501329\n",
      "\n",
      " Optimizer iteration 574, batch 183\n",
      "\n",
      " Learning rate 0.0008010147224131523, Model learning rate 0.0008010147139430046\n",
      "184/391 [=============>................] - ETA: 15s - loss: 1.3243 - acc: 0.5796\n",
      " Optimizer iteration 575, batch 184\n",
      "\n",
      " Learning rate 0.0008003711321189895, Model learning rate 0.0008003711118362844\n",
      "185/391 [=============>................] - ETA: 15s - loss: 1.3241 - acc: 0.5797\n",
      " Optimizer iteration 576, batch 185\n",
      "\n",
      " Learning rate 0.000799726762194687, Model learning rate 0.0007997267530299723\n",
      "\n",
      " Optimizer iteration 577, batch 186\n",
      "\n",
      " Learning rate 0.0007990816143127431, Model learning rate 0.0007990816375240684\n",
      "187/391 [=============>................] - ETA: 15s - loss: 1.3233 - acc: 0.5802\n",
      " Optimizer iteration 578, batch 187\n",
      "\n",
      " Learning rate 0.0007984356901476755, Model learning rate 0.0007984357071109116\n",
      "188/391 [=============>................] - ETA: 14s - loss: 1.3231 - acc: 0.5800\n",
      " Optimizer iteration 579, batch 188\n",
      "\n",
      " Learning rate 0.0007977889913760163, Model learning rate 0.000797789019998163\n",
      "189/391 [=============>................] - ETA: 14s - loss: 1.3226 - acc: 0.5799\n",
      " Optimizer iteration 580, batch 189\n",
      "\n",
      " Learning rate 0.0007971415196763087, Model learning rate 0.0007971415179781616\n",
      "\n",
      " Optimizer iteration 581, batch 190\n",
      "\n",
      " Learning rate 0.0007964932767291019, Model learning rate 0.0007964932592585683\n",
      "191/391 [=============>................] - ETA: 14s - loss: 1.3223 - acc: 0.5799\n",
      " Optimizer iteration 582, batch 191\n",
      "\n",
      " Learning rate 0.0007958442642169468, Model learning rate 0.0007958442438393831\n",
      "192/391 [=============>................] - ETA: 14s - loss: 1.3221 - acc: 0.5800\n",
      " Optimizer iteration 583, batch 192\n",
      "\n",
      " Learning rate 0.0007951944838243916, Model learning rate 0.0007951944717206061\n",
      "193/391 [=============>................] - ETA: 14s - loss: 1.3217 - acc: 0.5801\n",
      " Optimizer iteration 584, batch 193\n",
      "\n",
      " Learning rate 0.0007945439372379782, Model learning rate 0.0007945439429022372\n",
      "194/391 [=============>................] - ETA: 14s - loss: 1.3223 - acc: 0.5800\n",
      " Optimizer iteration 585, batch 194\n",
      "\n",
      " Learning rate 0.0007938926261462366, Model learning rate 0.0007938925991766155\n",
      "195/391 [=============>................] - ETA: 14s - loss: 1.3221 - acc: 0.5804\n",
      " Optimizer iteration 586, batch 195\n",
      "\n",
      " Learning rate 0.0007932405522396812, Model learning rate 0.0007932405569590628\n",
      "\n",
      " Optimizer iteration 587, batch 196\n",
      "\n",
      " Learning rate 0.0007925877172108067, Model learning rate 0.0007925876998342574\n",
      "197/391 [==============>...............] - ETA: 14s - loss: 1.3212 - acc: 0.5809\n",
      " Optimizer iteration 588, batch 197\n",
      "\n",
      " Learning rate 0.0007919341227540828, Model learning rate 0.000791934144217521\n",
      "\n",
      " Optimizer iteration 589, batch 198\n",
      "\n",
      " Learning rate 0.0007912797705659507, Model learning rate 0.0007912797736935318\n",
      "199/391 [==============>...............] - ETA: 14s - loss: 1.3200 - acc: 0.5811\n",
      " Optimizer iteration 590, batch 199\n",
      "\n",
      " Learning rate 0.0007906246623448183, Model learning rate 0.0007906246464699507\n",
      "200/391 [==============>...............] - ETA: 14s - loss: 1.3197 - acc: 0.5811\n",
      " Optimizer iteration 591, batch 200\n",
      "\n",
      " Learning rate 0.0007899687997910558, Model learning rate 0.0007899688207544386\n",
      "201/391 [==============>...............] - ETA: 14s - loss: 1.3188 - acc: 0.5812\n",
      " Optimizer iteration 592, batch 201\n",
      "\n",
      " Learning rate 0.0007893121846069913, Model learning rate 0.0007893121801316738\n",
      "\n",
      " Optimizer iteration 593, batch 202\n",
      "\n",
      " Learning rate 0.0007886548184969063, Model learning rate 0.000788654841016978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/391 [==============>...............] - ETA: 13s - loss: 1.3180 - acc: 0.5815\n",
      " Optimizer iteration 594, batch 203\n",
      "\n",
      " Learning rate 0.0007879967031670313, Model learning rate 0.0007879966869950294\n",
      "204/391 [==============>...............] - ETA: 13s - loss: 1.3169 - acc: 0.5820\n",
      " Optimizer iteration 595, batch 204\n",
      "\n",
      " Learning rate 0.0007873378403255419, Model learning rate 0.0007873378344811499\n",
      "205/391 [==============>...............] - ETA: 13s - loss: 1.3165 - acc: 0.5821\n",
      " Optimizer iteration 596, batch 205\n",
      "\n",
      " Learning rate 0.0007866782316825535, Model learning rate 0.0007866782252676785\n",
      "\n",
      " Optimizer iteration 597, batch 206\n",
      "\n",
      " Learning rate 0.0007860178789501172, Model learning rate 0.0007860178593546152\n",
      "207/391 [==============>...............] - ETA: 13s - loss: 1.3156 - acc: 0.5825\n",
      " Optimizer iteration 598, batch 207\n",
      "\n",
      " Learning rate 0.000785356783842216, Model learning rate 0.000785356794949621\n",
      "\n",
      " Optimizer iteration 599, batch 208\n",
      "\n",
      " Learning rate 0.0007846949480747588, Model learning rate 0.0007846949738450348\n",
      "209/391 [===============>..............] - ETA: 13s - loss: 1.3147 - acc: 0.5828\n",
      " Optimizer iteration 600, batch 209\n",
      "\n",
      " Learning rate 0.0007840323733655779, Model learning rate 0.0007840323960408568\n",
      "210/391 [===============>..............] - ETA: 13s - loss: 1.3143 - acc: 0.5830\n",
      " Optimizer iteration 601, batch 210\n",
      "\n",
      " Learning rate 0.0007833690614344231, Model learning rate 0.000783369061537087\n",
      "\n",
      " Optimizer iteration 602, batch 211\n",
      "\n",
      " Learning rate 0.0007827050140029577, Model learning rate 0.0007827050285413861\n",
      "212/391 [===============>..............] - ETA: 13s - loss: 1.3146 - acc: 0.5828\n",
      " Optimizer iteration 603, batch 212\n",
      "\n",
      " Learning rate 0.0007820402327947542, Model learning rate 0.0007820402388460934\n",
      "213/391 [===============>..............] - ETA: 13s - loss: 1.3144 - acc: 0.5828\n",
      " Optimizer iteration 604, batch 213\n",
      "\n",
      " Learning rate 0.0007813747195352895, Model learning rate 0.0007813746924512088\n",
      "214/391 [===============>..............] - ETA: 13s - loss: 1.3148 - acc: 0.5829\n",
      " Optimizer iteration 605, batch 214\n",
      "\n",
      " Learning rate 0.0007807084759519405, Model learning rate 0.0007807084475643933\n",
      "\n",
      " Optimizer iteration 606, batch 215\n",
      "\n",
      " Learning rate 0.0007800415037739801, Model learning rate 0.0007800415041856468\n",
      "216/391 [===============>..............] - ETA: 12s - loss: 1.3151 - acc: 0.5831\n",
      " Optimizer iteration 607, batch 216\n",
      "\n",
      " Learning rate 0.0007793738047325717, Model learning rate 0.0007793738041073084\n",
      "217/391 [===============>..............] - ETA: 12s - loss: 1.3156 - acc: 0.5830\n",
      " Optimizer iteration 608, batch 217\n",
      "\n",
      " Learning rate 0.0007787053805607659, Model learning rate 0.000778705405537039\n",
      "218/391 [===============>..............] - ETA: 12s - loss: 1.3150 - acc: 0.5832\n",
      " Optimizer iteration 609, batch 218\n",
      "\n",
      " Learning rate 0.0007780362329934951, Model learning rate 0.0007780362502671778\n",
      "219/391 [===============>..............] - ETA: 12s - loss: 1.3154 - acc: 0.5829\n",
      " Optimizer iteration 610, batch 219\n",
      "\n",
      " Learning rate 0.0007773663637675694, Model learning rate 0.0007773663382977247\n",
      "\n",
      " Optimizer iteration 611, batch 220\n",
      "\n",
      " Learning rate 0.000776695774621672, Model learning rate 0.0007766957860440016\n",
      "221/391 [===============>..............] - ETA: 12s - loss: 1.3152 - acc: 0.5827\n",
      " Optimizer iteration 612, batch 221\n",
      "\n",
      " Learning rate 0.0007760244672963548, Model learning rate 0.0007760244770906866\n",
      "222/391 [================>.............] - ETA: 12s - loss: 1.3156 - acc: 0.5827\n",
      " Optimizer iteration 613, batch 222\n",
      "\n",
      " Learning rate 0.0007753524435340334, Model learning rate 0.0007753524696454406\n",
      "223/391 [================>.............] - ETA: 12s - loss: 1.3151 - acc: 0.5830\n",
      " Optimizer iteration 614, batch 223\n",
      "\n",
      " Learning rate 0.0007746797050789834, Model learning rate 0.0007746797055006027\n",
      "\n",
      " Optimizer iteration 615, batch 224\n",
      "\n",
      " Learning rate 0.0007740062536773351, Model learning rate 0.0007740062428638339\n",
      "225/391 [================>.............] - ETA: 12s - loss: 1.3146 - acc: 0.5834\n",
      " Optimizer iteration 616, batch 225\n",
      "\n",
      " Learning rate 0.0007733320910770693, Model learning rate 0.0007733320817351341\n",
      "226/391 [================>.............] - ETA: 12s - loss: 1.3139 - acc: 0.5835\n",
      " Optimizer iteration 617, batch 226\n",
      "\n",
      " Learning rate 0.0007726572190280134, Model learning rate 0.0007726572221145034\n",
      "227/391 [================>.............] - ETA: 12s - loss: 1.3133 - acc: 0.5836\n",
      " Optimizer iteration 618, batch 227\n",
      "\n",
      " Learning rate 0.0007719816392818353, Model learning rate 0.0007719816640019417\n",
      "\n",
      " Optimizer iteration 619, batch 228\n",
      "\n",
      " Learning rate 0.0007713053535920402, Model learning rate 0.0007713053491897881\n",
      "229/391 [================>.............] - ETA: 12s - loss: 1.3128 - acc: 0.5839\n",
      " Optimizer iteration 620, batch 229\n",
      "\n",
      " Learning rate 0.0007706283637139657, Model learning rate 0.0007706283358857036\n",
      "230/391 [================>.............] - ETA: 11s - loss: 1.3127 - acc: 0.5838\n",
      " Optimizer iteration 621, batch 230\n",
      "\n",
      " Learning rate 0.000769950671404777, Model learning rate 0.000769950682297349\n",
      "\n",
      " Optimizer iteration 622, batch 231\n",
      "\n",
      " Learning rate 0.0007692722784234624, Model learning rate 0.0007692722720094025\n",
      "232/391 [================>.............] - ETA: 11s - loss: 1.3123 - acc: 0.5837\n",
      " Optimizer iteration 623, batch 232\n",
      "\n",
      " Learning rate 0.0007685931865308292, Model learning rate 0.0007685931632295251\n",
      "233/391 [================>.............] - ETA: 11s - loss: 1.3124 - acc: 0.5837\n",
      " Optimizer iteration 624, batch 233\n",
      "\n",
      " Learning rate 0.0007679133974894983, Model learning rate 0.0007679134141653776\n",
      "234/391 [================>.............] - ETA: 11s - loss: 1.3119 - acc: 0.5838\n",
      " Optimizer iteration 625, batch 234\n",
      "\n",
      " Learning rate 0.0007672329130639005, Model learning rate 0.0007672329084016383\n",
      "235/391 [=================>............] - ETA: 11s - loss: 1.3118 - acc: 0.5838\n",
      " Optimizer iteration 626, batch 235\n",
      "\n",
      " Learning rate 0.0007665517350202715, Model learning rate 0.0007665517623536289\n",
      "236/391 [=================>............] - ETA: 11s - loss: 1.3114 - acc: 0.5840\n",
      " Optimizer iteration 627, batch 236\n",
      "\n",
      " Learning rate 0.0007658698651266467, Model learning rate 0.0007658698596060276\n",
      "237/391 [=================>............] - ETA: 11s - loss: 1.3108 - acc: 0.5842\n",
      " Optimizer iteration 628, batch 237\n",
      "\n",
      " Learning rate 0.000765187305152858, Model learning rate 0.0007651873165741563\n",
      "238/391 [=================>............] - ETA: 11s - loss: 1.3098 - acc: 0.5847\n",
      " Optimizer iteration 629, batch 238\n",
      "\n",
      " Learning rate 0.0007645040568705282, Model learning rate 0.000764504075050354\n",
      "239/391 [=================>............] - ETA: 11s - loss: 1.3100 - acc: 0.5848\n",
      " Optimizer iteration 630, batch 239\n",
      "\n",
      " Learning rate 0.0007638201220530663, Model learning rate 0.0007638201350346208\n",
      "240/391 [=================>............] - ETA: 11s - loss: 1.3098 - acc: 0.5848\n",
      " Optimizer iteration 631, batch 240\n",
      "\n",
      " Learning rate 0.0007631355024756639, Model learning rate 0.0007631354965269566\n",
      "241/391 [=================>............] - ETA: 11s - loss: 1.3094 - acc: 0.5850\n",
      " Optimizer iteration 632, batch 241\n",
      "\n",
      " Learning rate 0.0007624501999152893, Model learning rate 0.0007624502177350223\n",
      "\n",
      " Optimizer iteration 633, batch 242\n",
      "\n",
      " Learning rate 0.0007617642161506837, Model learning rate 0.0007617642404511571\n",
      "243/391 [=================>............] - ETA: 11s - loss: 1.3093 - acc: 0.5848\n",
      " Optimizer iteration 634, batch 243\n",
      "\n",
      " Learning rate 0.0007610775529623568, Model learning rate 0.0007610775646753609\n",
      "244/391 [=================>............] - ETA: 10s - loss: 1.3100 - acc: 0.5845\n",
      " Optimizer iteration 635, batch 244\n",
      "\n",
      " Learning rate 0.0007603902121325811, Model learning rate 0.0007603901904076338\n",
      "245/391 [=================>............] - ETA: 10s - loss: 1.3095 - acc: 0.5847\n",
      " Optimizer iteration 636, batch 245\n",
      "\n",
      " Learning rate 0.0007597021954453886, Model learning rate 0.0007597021758556366\n",
      "\n",
      " Optimizer iteration 637, batch 246\n",
      "\n",
      " Learning rate 0.0007590135046865651, Model learning rate 0.0007590135210193694\n",
      "247/391 [=================>............] - ETA: 10s - loss: 1.3089 - acc: 0.5848\n",
      " Optimizer iteration 638, batch 247\n",
      "\n",
      " Learning rate 0.0007583241416436461, Model learning rate 0.0007583241676911712\n",
      "\n",
      " Optimizer iteration 639, batch 248\n",
      "\n",
      " Learning rate 0.0007576341081059123, Model learning rate 0.000757634115871042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/391 [==================>...........] - ETA: 10s - loss: 1.3089 - acc: 0.5847\n",
      " Optimizer iteration 640, batch 249\n",
      "\n",
      " Learning rate 0.0007569434058643844, Model learning rate 0.0007569434237666428\n",
      "250/391 [==================>...........] - ETA: 10s - loss: 1.3088 - acc: 0.5847\n",
      " Optimizer iteration 641, batch 250\n",
      "\n",
      " Learning rate 0.0007562520367118186, Model learning rate 0.0007562520331703126\n",
      "251/391 [==================>...........] - ETA: 10s - loss: 1.3085 - acc: 0.5847\n",
      " Optimizer iteration 642, batch 251\n",
      "\n",
      " Learning rate 0.0007555600024427027, Model learning rate 0.0007555600022897124\n",
      "252/391 [==================>...........] - ETA: 10s - loss: 1.3076 - acc: 0.5850\n",
      " Optimizer iteration 643, batch 252\n",
      "\n",
      " Learning rate 0.0007548673048532504, Model learning rate 0.0007548673311248422\n",
      "253/391 [==================>...........] - ETA: 10s - loss: 1.3074 - acc: 0.5851\n",
      " Optimizer iteration 644, batch 253\n",
      "\n",
      " Learning rate 0.000754173945741397, Model learning rate 0.0007541739614680409\n",
      "254/391 [==================>...........] - ETA: 10s - loss: 1.3067 - acc: 0.5853\n",
      " Optimizer iteration 645, batch 254\n",
      "\n",
      " Learning rate 0.0007534799269067953, Model learning rate 0.0007534799515269697\n",
      "255/391 [==================>...........] - ETA: 10s - loss: 1.3064 - acc: 0.5854\n",
      " Optimizer iteration 646, batch 255\n",
      "\n",
      " Learning rate 0.0007527852501508099, Model learning rate 0.0007527852430939674\n",
      "256/391 [==================>...........] - ETA: 10s - loss: 1.3062 - acc: 0.5855\n",
      " Optimizer iteration 647, batch 256\n",
      "\n",
      " Learning rate 0.0007520899172765136, Model learning rate 0.0007520898943766952\n",
      "257/391 [==================>...........] - ETA: 10s - loss: 1.3057 - acc: 0.5856\n",
      " Optimizer iteration 648, batch 257\n",
      "\n",
      " Learning rate 0.0007513939300886816, Model learning rate 0.0007513939053751528\n",
      "258/391 [==================>...........] - ETA: 9s - loss: 1.3057 - acc: 0.5856 \n",
      " Optimizer iteration 649, batch 258\n",
      "\n",
      " Learning rate 0.0007506972903937878, Model learning rate 0.0007506972760893404\n",
      "\n",
      " Optimizer iteration 650, batch 259\n",
      "\n",
      " Learning rate 0.00075, Model learning rate 0.000750000006519258\n",
      "260/391 [==================>...........] - ETA: 9s - loss: 1.3063 - acc: 0.5856\n",
      " Optimizer iteration 651, batch 260\n",
      "\n",
      " Learning rate 0.0007493020607171743, Model learning rate 0.0007493020384572446\n",
      "261/391 [===================>..........] - ETA: 9s - loss: 1.3056 - acc: 0.5860\n",
      " Optimizer iteration 652, batch 261\n",
      "\n",
      " Learning rate 0.0007486034743568511, Model learning rate 0.0007486034883186221\n",
      "262/391 [===================>..........] - ETA: 9s - loss: 1.3052 - acc: 0.5862\n",
      " Optimizer iteration 653, batch 262\n",
      "\n",
      " Learning rate 0.0007479042427322508, Model learning rate 0.0007479042396880686\n",
      "263/391 [===================>..........] - ETA: 9s - loss: 1.3054 - acc: 0.5864\n",
      " Optimizer iteration 654, batch 263\n",
      "\n",
      " Learning rate 0.0007472043676582685, Model learning rate 0.0007472043507732451\n",
      "\n",
      " Optimizer iteration 655, batch 264\n",
      "\n",
      " Learning rate 0.0007465038509514688, Model learning rate 0.0007465038797818124\n",
      "265/391 [===================>..........] - ETA: 9s - loss: 1.3048 - acc: 0.5867\n",
      " Optimizer iteration 656, batch 265\n",
      "\n",
      " Learning rate 0.0007458026944300824, Model learning rate 0.0007458027102984488\n",
      "266/391 [===================>..........] - ETA: 9s - loss: 1.3045 - acc: 0.5868\n",
      " Optimizer iteration 657, batch 266\n",
      "\n",
      " Learning rate 0.0007451008999140005, Model learning rate 0.0007451009005308151\n",
      "267/391 [===================>..........] - ETA: 9s - loss: 1.3031 - acc: 0.5875\n",
      " Optimizer iteration 658, batch 267\n",
      "\n",
      " Learning rate 0.0007443984692247701, Model learning rate 0.0007443984504789114\n",
      "268/391 [===================>..........] - ETA: 9s - loss: 1.3027 - acc: 0.5874\n",
      " Optimizer iteration 659, batch 268\n",
      "\n",
      " Learning rate 0.0007436954041855892, Model learning rate 0.0007436954183503985\n",
      "269/391 [===================>..........] - ETA: 9s - loss: 1.3029 - acc: 0.5872\n",
      " Optimizer iteration 660, batch 269\n",
      "\n",
      " Learning rate 0.000742991706621303, Model learning rate 0.0007429916877299547\n",
      "270/391 [===================>..........] - ETA: 9s - loss: 1.3020 - acc: 0.5876\n",
      " Optimizer iteration 661, batch 270\n",
      "\n",
      " Learning rate 0.0007422873783583981, Model learning rate 0.0007422873750329018\n",
      "\n",
      " Optimizer iteration 662, batch 271\n",
      "\n",
      " Learning rate 0.0007415824212249977, Model learning rate 0.0007415824220515788\n",
      "272/391 [===================>..........] - ETA: 8s - loss: 1.3010 - acc: 0.5881\n",
      " Optimizer iteration 663, batch 272\n",
      "\n",
      " Learning rate 0.0007408768370508576, Model learning rate 0.0007408768287859857\n",
      "273/391 [===================>..........] - ETA: 8s - loss: 1.3006 - acc: 0.5882\n",
      " Optimizer iteration 664, batch 273\n",
      "\n",
      " Learning rate 0.0007401706276673615, Model learning rate 0.0007401706534437835\n",
      "274/391 [====================>.........] - ETA: 8s - loss: 1.3005 - acc: 0.5882\n",
      " Optimizer iteration 665, batch 274\n",
      "\n",
      " Learning rate 0.0007394637949075153, Model learning rate 0.0007394637796096504\n",
      "\n",
      " Optimizer iteration 666, batch 275\n",
      "\n",
      " Learning rate 0.0007387563406059432, Model learning rate 0.0007387563236989081\n",
      "276/391 [====================>.........] - ETA: 8s - loss: 1.2998 - acc: 0.5886\n",
      " Optimizer iteration 667, batch 276\n",
      "\n",
      " Learning rate 0.0007380482665988826, Model learning rate 0.0007380482857115567\n",
      "277/391 [====================>.........] - ETA: 8s - loss: 1.2992 - acc: 0.5887\n",
      " Optimizer iteration 668, batch 277\n",
      "\n",
      " Learning rate 0.0007373395747241791, Model learning rate 0.0007373395492322743\n",
      "278/391 [====================>.........] - ETA: 8s - loss: 1.2986 - acc: 0.5889\n",
      " Optimizer iteration 669, batch 278\n",
      "\n",
      " Learning rate 0.0007366302668212826, Model learning rate 0.0007366302888840437\n",
      "279/391 [====================>.........] - ETA: 8s - loss: 1.2984 - acc: 0.5889\n",
      " Optimizer iteration 670, batch 279\n",
      "\n",
      " Learning rate 0.000735920344731241, Model learning rate 0.0007359203300438821\n",
      "280/391 [====================>.........] - ETA: 8s - loss: 1.2988 - acc: 0.5888\n",
      " Optimizer iteration 671, batch 280\n",
      "\n",
      " Learning rate 0.0007352098102966978, Model learning rate 0.0007352097891271114\n",
      "281/391 [====================>.........] - ETA: 8s - loss: 1.2987 - acc: 0.5887\n",
      " Optimizer iteration 672, batch 281\n",
      "\n",
      " Learning rate 0.0007344986653618844, Model learning rate 0.0007344986661337316\n",
      "282/391 [====================>.........] - ETA: 8s - loss: 1.2982 - acc: 0.5890\n",
      " Optimizer iteration 673, batch 282\n",
      "\n",
      " Learning rate 0.0007337869117726176, Model learning rate 0.0007337869028560817\n",
      "283/391 [====================>.........] - ETA: 8s - loss: 1.2979 - acc: 0.5891\n",
      " Optimizer iteration 674, batch 283\n",
      "\n",
      " Learning rate 0.0007330745513762936, Model learning rate 0.0007330745575018227\n",
      "\n",
      " Optimizer iteration 675, batch 284\n",
      "\n",
      " Learning rate 0.0007323615860218843, Model learning rate 0.0007323615718632936\n",
      "285/391 [====================>.........] - ETA: 7s - loss: 1.2971 - acc: 0.5895\n",
      " Optimizer iteration 676, batch 285\n",
      "\n",
      " Learning rate 0.0007316480175599309, Model learning rate 0.0007316480041481555\n",
      "286/391 [====================>.........] - ETA: 7s - loss: 1.2972 - acc: 0.5894\n",
      " Optimizer iteration 677, batch 286\n",
      "\n",
      " Learning rate 0.0007309338478425404, Model learning rate 0.0007309338543564081\n",
      "\n",
      " Optimizer iteration 678, batch 287\n",
      "\n",
      " Learning rate 0.0007302190787233807, Model learning rate 0.0007302190642803907\n",
      "288/391 [=====================>........] - ETA: 7s - loss: 1.2972 - acc: 0.5897\n",
      " Optimizer iteration 679, batch 288\n",
      "\n",
      " Learning rate 0.0007295037120576749, Model learning rate 0.0007295036921277642\n",
      "289/391 [=====================>........] - ETA: 7s - loss: 1.2966 - acc: 0.5899\n",
      " Optimizer iteration 680, batch 289\n",
      "\n",
      " Learning rate 0.0007287877497021977, Model learning rate 0.0007287877378985286\n",
      "290/391 [=====================>........] - ETA: 7s - loss: 1.2962 - acc: 0.5901\n",
      " Optimizer iteration 681, batch 290\n",
      "\n",
      " Learning rate 0.0007280711935152691, Model learning rate 0.0007280712015926838\n",
      "\n",
      " Optimizer iteration 682, batch 291\n",
      "\n",
      " Learning rate 0.0007273540453567512, Model learning rate 0.000727354025002569\n",
      "292/391 [=====================>........] - ETA: 7s - loss: 1.2966 - acc: 0.5899\n",
      " Optimizer iteration 683, batch 292\n",
      "\n",
      " Learning rate 0.0007266363070880424, Model learning rate 0.0007266363245435059\n",
      "293/391 [=====================>........] - ETA: 7s - loss: 1.2963 - acc: 0.5900\n",
      " Optimizer iteration 684, batch 293\n",
      "\n",
      " Learning rate 0.0007259179805720726, Model learning rate 0.0007259179838001728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/391 [=====================>........] - ETA: 7s - loss: 1.2958 - acc: 0.5903\n",
      " Optimizer iteration 685, batch 294\n",
      "\n",
      " Learning rate 0.0007251990676732984, Model learning rate 0.0007251990609802306\n",
      "295/391 [=====================>........] - ETA: 7s - loss: 1.2958 - acc: 0.5904\n",
      " Optimizer iteration 686, batch 295\n",
      "\n",
      " Learning rate 0.0007244795702576989, Model learning rate 0.0007244795560836792\n",
      "296/391 [=====================>........] - ETA: 7s - loss: 1.2956 - acc: 0.5905\n",
      " Optimizer iteration 687, batch 296\n",
      "\n",
      " Learning rate 0.0007237594901927699, Model learning rate 0.0007237594691105187\n",
      "297/391 [=====================>........] - ETA: 7s - loss: 1.2958 - acc: 0.5905\n",
      " Optimizer iteration 688, batch 297\n",
      "\n",
      " Learning rate 0.0007230388293475197, Model learning rate 0.00072303885826841\n",
      "298/391 [=====================>........] - ETA: 6s - loss: 1.2952 - acc: 0.5908\n",
      " Optimizer iteration 689, batch 298\n",
      "\n",
      " Learning rate 0.0007223175895924637, Model learning rate 0.0007223176071420312\n",
      "\n",
      " Optimizer iteration 690, batch 299\n",
      "\n",
      " Learning rate 0.0007215957727996207, Model learning rate 0.0007215957739390433\n",
      "300/391 [======================>.......] - ETA: 6s - loss: 1.2953 - acc: 0.5909\n",
      " Optimizer iteration 691, batch 300\n",
      "\n",
      " Learning rate 0.0007208733808425063, Model learning rate 0.0007208733586594462\n",
      "301/391 [======================>.......] - ETA: 6s - loss: 1.2954 - acc: 0.5908\n",
      " Optimizer iteration 692, batch 301\n",
      "\n",
      " Learning rate 0.0007201504155961296, Model learning rate 0.000720150419510901\n",
      "302/391 [======================>.......] - ETA: 6s - loss: 1.2952 - acc: 0.5909\n",
      " Optimizer iteration 693, batch 302\n",
      "\n",
      " Learning rate 0.0007194268789369874, Model learning rate 0.0007194268982857466\n",
      "303/391 [======================>.......] - ETA: 6s - loss: 1.2954 - acc: 0.5909\n",
      " Optimizer iteration 694, batch 303\n",
      "\n",
      " Learning rate 0.00071870277274306, Model learning rate 0.000718702794983983\n",
      "304/391 [======================>.......] - ETA: 6s - loss: 1.2965 - acc: 0.5905\n",
      " Optimizer iteration 695, batch 304\n",
      "\n",
      " Learning rate 0.000717978098893805, Model learning rate 0.0007179781096056104\n",
      "305/391 [======================>.......] - ETA: 6s - loss: 1.2963 - acc: 0.5906\n",
      " Optimizer iteration 696, batch 305\n",
      "\n",
      " Learning rate 0.000717252859270155, Model learning rate 0.0007172528421506286\n",
      "306/391 [======================>.......] - ETA: 6s - loss: 1.2960 - acc: 0.5906\n",
      " Optimizer iteration 697, batch 306\n",
      "\n",
      " Learning rate 0.0007165270557545094, Model learning rate 0.0007165270508266985\n",
      "307/391 [======================>.......] - ETA: 6s - loss: 1.2956 - acc: 0.5907\n",
      " Optimizer iteration 698, batch 307\n",
      "\n",
      " Learning rate 0.0007158006902307321, Model learning rate 0.0007158006774261594\n",
      "308/391 [======================>.......] - ETA: 6s - loss: 1.2955 - acc: 0.5909\n",
      " Optimizer iteration 699, batch 308\n",
      "\n",
      " Learning rate 0.000715073764584146, Model learning rate 0.000715073780156672\n",
      "309/391 [======================>.......] - ETA: 6s - loss: 1.2949 - acc: 0.5911\n",
      " Optimizer iteration 700, batch 309\n",
      "\n",
      " Learning rate 0.000714346280701527, Model learning rate 0.0007143463008105755\n",
      "310/391 [======================>.......] - ETA: 6s - loss: 1.2950 - acc: 0.5911\n",
      " Optimizer iteration 701, batch 310\n",
      "\n",
      " Learning rate 0.0007136182404711008, Model learning rate 0.0007136182393878698\n",
      "\n",
      " Optimizer iteration 702, batch 311\n",
      "\n",
      " Learning rate 0.0007128896457825364, Model learning rate 0.000712889654096216\n",
      "312/391 [======================>.......] - ETA: 5s - loss: 1.2945 - acc: 0.5912\n",
      " Optimizer iteration 703, batch 312\n",
      "\n",
      " Learning rate 0.0007121604985269422, Model learning rate 0.000712160486727953\n",
      "313/391 [=======================>......] - ETA: 5s - loss: 1.2936 - acc: 0.5915\n",
      " Optimizer iteration 704, batch 313\n",
      "\n",
      " Learning rate 0.0007114308005968609, Model learning rate 0.0007114307954907417\n",
      "\n",
      " Optimizer iteration 705, batch 314\n",
      "\n",
      " Learning rate 0.0007107005538862646, Model learning rate 0.0007107005803845823\n",
      "315/391 [=======================>......] - ETA: 5s - loss: 1.2930 - acc: 0.5918\n",
      " Optimizer iteration 706, batch 315\n",
      "\n",
      " Learning rate 0.0007099697602905493, Model learning rate 0.0007099697832018137\n",
      "316/391 [=======================>......] - ETA: 5s - loss: 1.2932 - acc: 0.5918\n",
      " Optimizer iteration 707, batch 316\n",
      "\n",
      " Learning rate 0.0007092384217065314, Model learning rate 0.000709238403942436\n",
      "317/391 [=======================>......] - ETA: 5s - loss: 1.2929 - acc: 0.5917\n",
      " Optimizer iteration 708, batch 317\n",
      "\n",
      " Learning rate 0.0007085065400324407, Model learning rate 0.000708506559021771\n",
      "318/391 [=======================>......] - ETA: 5s - loss: 1.2927 - acc: 0.5919\n",
      " Optimizer iteration 709, batch 318\n",
      "\n",
      " Learning rate 0.0007077741171679173, Model learning rate 0.0007077741320244968\n",
      "319/391 [=======================>......] - ETA: 5s - loss: 1.2925 - acc: 0.5920\n",
      " Optimizer iteration 710, batch 319\n",
      "\n",
      " Learning rate 0.000707041155014006, Model learning rate 0.0007070411811582744\n",
      "320/391 [=======================>......] - ETA: 5s - loss: 1.2917 - acc: 0.5923\n",
      " Optimizer iteration 711, batch 320\n",
      "\n",
      " Learning rate 0.0007063076554731512, Model learning rate 0.0007063076482154429\n",
      "321/391 [=======================>......] - ETA: 5s - loss: 1.2918 - acc: 0.5924\n",
      " Optimizer iteration 712, batch 321\n",
      "\n",
      " Learning rate 0.0007055736204491922, Model learning rate 0.0007055735914036632\n",
      "322/391 [=======================>......] - ETA: 5s - loss: 1.2919 - acc: 0.5923\n",
      " Optimizer iteration 713, batch 322\n",
      "\n",
      " Learning rate 0.0007048390518473579, Model learning rate 0.0007048390689305961\n",
      "\n",
      " Optimizer iteration 714, batch 323\n",
      "\n",
      " Learning rate 0.0007041039515742625, Model learning rate 0.0007041039643809199\n",
      "324/391 [=======================>......] - ETA: 5s - loss: 1.2914 - acc: 0.5924\n",
      " Optimizer iteration 715, batch 324\n",
      "\n",
      " Learning rate 0.0007033683215379002, Model learning rate 0.0007033683359622955\n",
      "325/391 [=======================>......] - ETA: 4s - loss: 1.2913 - acc: 0.5923\n",
      " Optimizer iteration 716, batch 325\n",
      "\n",
      " Learning rate 0.0007026321636476397, Model learning rate 0.0007026321836747229\n",
      "326/391 [========================>.....] - ETA: 4s - loss: 1.2910 - acc: 0.5925\n",
      " Optimizer iteration 717, batch 326\n",
      "\n",
      " Learning rate 0.0007018954798142204, Model learning rate 0.0007018955075182021\n",
      "327/391 [========================>.....] - ETA: 4s - loss: 1.2905 - acc: 0.5927\n",
      " Optimizer iteration 718, batch 327\n",
      "\n",
      " Learning rate 0.0007011582719497466, Model learning rate 0.0007011582492850721\n",
      "\n",
      " Optimizer iteration 719, batch 328\n",
      "\n",
      " Learning rate 0.0007004205419676825, Model learning rate 0.0007004205253906548\n",
      "329/391 [========================>.....] - ETA: 4s - loss: 1.2900 - acc: 0.5930\n",
      " Optimizer iteration 720, batch 329\n",
      "\n",
      " Learning rate 0.0006996822917828477, Model learning rate 0.0006996822776272893\n",
      "330/391 [========================>.....] - ETA: 4s - loss: 1.2902 - acc: 0.5929\n",
      " Optimizer iteration 721, batch 330\n",
      "\n",
      " Learning rate 0.0006989435233114123, Model learning rate 0.0006989435059949756\n",
      "331/391 [========================>.....] - ETA: 4s - loss: 1.2903 - acc: 0.5930\n",
      " Optimizer iteration 722, batch 331\n",
      "\n",
      " Learning rate 0.000698204238470891, Model learning rate 0.0006982042104937136\n",
      "\n",
      " Optimizer iteration 723, batch 332\n",
      "\n",
      " Learning rate 0.0006974644391801395, Model learning rate 0.0006974644493311644\n",
      "333/391 [========================>.....] - ETA: 4s - loss: 1.2905 - acc: 0.5929\n",
      " Optimizer iteration 724, batch 333\n",
      "\n",
      " Learning rate 0.000696724127359348, Model learning rate 0.000696724106092006\n",
      "\n",
      " Optimizer iteration 725, batch 334\n",
      "\n",
      " Learning rate 0.0006959833049300376, Model learning rate 0.0006959832971915603\n",
      "335/391 [========================>.....] - ETA: 4s - loss: 1.2902 - acc: 0.5932\n",
      " Optimizer iteration 726, batch 335\n",
      "\n",
      " Learning rate 0.0006952419738150545, Model learning rate 0.0006952419644221663\n",
      "\n",
      " Optimizer iteration 727, batch 336\n",
      "\n",
      " Learning rate 0.0006945001359385651, Model learning rate 0.0006945001077838242\n",
      "337/391 [========================>.....] - ETA: 4s - loss: 1.2902 - acc: 0.5934\n",
      " Optimizer iteration 728, batch 337\n",
      "\n",
      " Learning rate 0.0006937577932260515, Model learning rate 0.0006937577854841948\n",
      "\n",
      " Optimizer iteration 729, batch 338\n",
      "\n",
      " Learning rate 0.0006930149476043058, Model learning rate 0.0006930149393156171\n",
      "339/391 [=========================>....] - ETA: 3s - loss: 1.2894 - acc: 0.5936\n",
      " Optimizer iteration 730, batch 339\n",
      "\n",
      " Learning rate 0.0006922716010014255, Model learning rate 0.0006922716274857521\n",
      "340/391 [=========================>....] - ETA: 3s - loss: 1.2888 - acc: 0.5938\n",
      " Optimizer iteration 731, batch 340\n",
      "\n",
      " Learning rate 0.0006915277553468083, Model learning rate 0.000691527733579278\n",
      "341/391 [=========================>....] - ETA: 3s - loss: 1.2889 - acc: 0.5939\n",
      " Optimizer iteration 732, batch 341\n",
      "\n",
      " Learning rate 0.0006907834125711476, Model learning rate 0.0006907834322191775\n",
      "\n",
      " Optimizer iteration 733, batch 342\n",
      "\n",
      " Learning rate 0.0006900385746064268, Model learning rate 0.0006900385487824678\n",
      "343/391 [=========================>....] - ETA: 3s - loss: 1.2874 - acc: 0.5944\n",
      " Optimizer iteration 734, batch 343\n",
      "\n",
      " Learning rate 0.0006892932433859147, Model learning rate 0.0006892932578921318\n",
      "\n",
      " Optimizer iteration 735, batch 344\n",
      "\n",
      " Learning rate 0.0006885474208441603, Model learning rate 0.0006885474431328475\n",
      "345/391 [=========================>....] - ETA: 3s - loss: 1.2872 - acc: 0.5946\n",
      " Optimizer iteration 736, batch 345\n",
      "\n",
      " Learning rate 0.0006878011089169878, Model learning rate 0.0006878011045046151\n",
      "346/391 [=========================>....] - ETA: 3s - loss: 1.2870 - acc: 0.5945\n",
      " Optimizer iteration 737, batch 346\n",
      "\n",
      " Learning rate 0.0006870543095414918, Model learning rate 0.0006870543002150953\n",
      "347/391 [=========================>....] - ETA: 3s - loss: 1.2873 - acc: 0.5945\n",
      " Optimizer iteration 738, batch 347\n",
      "\n",
      " Learning rate 0.0006863070246560319, Model learning rate 0.0006863070302642882\n",
      "348/391 [=========================>....] - ETA: 3s - loss: 1.2866 - acc: 0.5947\n",
      " Optimizer iteration 739, batch 348\n",
      "\n",
      " Learning rate 0.0006855592562002281, Model learning rate 0.0006855592364445329\n",
      "349/391 [=========================>....] - ETA: 3s - loss: 1.2865 - acc: 0.5948\n",
      " Optimizer iteration 740, batch 349\n",
      "\n",
      " Learning rate 0.0006848110061149555, Model learning rate 0.0006848110351711512\n",
      "350/391 [=========================>....] - ETA: 3s - loss: 1.2865 - acc: 0.5946\n",
      " Optimizer iteration 741, batch 350\n",
      "\n",
      " Learning rate 0.0006840622763423391, Model learning rate 0.0006840622518211603\n",
      "351/391 [=========================>....] - ETA: 3s - loss: 1.2863 - acc: 0.5948\n",
      " Optimizer iteration 742, batch 351\n",
      "\n",
      " Learning rate 0.0006833130688257489, Model learning rate 0.0006833130610175431\n",
      "352/391 [==========================>...] - ETA: 2s - loss: 1.2858 - acc: 0.5949\n",
      " Optimizer iteration 743, batch 352\n",
      "\n",
      " Learning rate 0.0006825633855097954, Model learning rate 0.0006825634045526385\n",
      "353/391 [==========================>...] - ETA: 2s - loss: 1.2857 - acc: 0.5950\n",
      " Optimizer iteration 744, batch 353\n",
      "\n",
      " Learning rate 0.0006818132283403235, Model learning rate 0.0006818132242187858\n",
      "354/391 [==========================>...] - ETA: 2s - loss: 1.2854 - acc: 0.5951\n",
      " Optimizer iteration 745, batch 354\n",
      "\n",
      " Learning rate 0.0006810625992644084, Model learning rate 0.0006810625782236457\n",
      "355/391 [==========================>...] - ETA: 2s - loss: 1.2852 - acc: 0.5950\n",
      " Optimizer iteration 746, batch 355\n",
      "\n",
      " Learning rate 0.0006803115002303499, Model learning rate 0.0006803115247748792\n",
      "356/391 [==========================>...] - ETA: 2s - loss: 1.2849 - acc: 0.5952\n",
      " Optimizer iteration 747, batch 356\n",
      "\n",
      " Learning rate 0.0006795599331876678, Model learning rate 0.0006795599474571645\n",
      "357/391 [==========================>...] - ETA: 2s - loss: 1.2843 - acc: 0.5954\n",
      " Optimizer iteration 748, batch 357\n",
      "\n",
      " Learning rate 0.0006788079000870966, Model learning rate 0.0006788079044781625\n",
      "358/391 [==========================>...] - ETA: 2s - loss: 1.2838 - acc: 0.5955\n",
      " Optimizer iteration 749, batch 358\n",
      "\n",
      " Learning rate 0.0006780554028805803, Model learning rate 0.0006780553958378732\n",
      "359/391 [==========================>...] - ETA: 2s - loss: 1.2837 - acc: 0.5956\n",
      " Optimizer iteration 750, batch 359\n",
      "\n",
      " Learning rate 0.0006773024435212678, Model learning rate 0.0006773024215362966\n",
      "360/391 [==========================>...] - ETA: 2s - loss: 1.2835 - acc: 0.5956\n",
      " Optimizer iteration 751, batch 360\n",
      "\n",
      " Learning rate 0.0006765490239635075, Model learning rate 0.0006765490397810936\n",
      "361/391 [==========================>...] - ETA: 2s - loss: 1.2829 - acc: 0.5958\n",
      " Optimizer iteration 752, batch 361\n",
      "\n",
      " Learning rate 0.0006757951461628416, Model learning rate 0.0006757951341569424\n",
      "362/391 [==========================>...] - ETA: 2s - loss: 1.2822 - acc: 0.5960\n",
      " Optimizer iteration 753, batch 362\n",
      "\n",
      " Learning rate 0.0006750408120760029, Model learning rate 0.0006750408210791647\n",
      "363/391 [==========================>...] - ETA: 2s - loss: 1.2814 - acc: 0.5963\n",
      " Optimizer iteration 754, batch 363\n",
      "\n",
      " Learning rate 0.0006742860236609076, Model learning rate 0.0006742860423400998\n",
      "364/391 [==========================>...] - ETA: 2s - loss: 1.2812 - acc: 0.5963\n",
      " Optimizer iteration 755, batch 364\n",
      "\n",
      " Learning rate 0.0006735307828766515, Model learning rate 0.0006735307979397476\n",
      "365/391 [===========================>..] - ETA: 1s - loss: 1.2813 - acc: 0.5963\n",
      " Optimizer iteration 756, batch 365\n",
      "\n",
      " Learning rate 0.0006727750916835043, Model learning rate 0.000672775087878108\n",
      "\n",
      " Optimizer iteration 757, batch 366\n",
      "\n",
      " Learning rate 0.0006720189520429052, Model learning rate 0.0006720189703628421\n",
      "367/391 [===========================>..] - ETA: 1s - loss: 1.2807 - acc: 0.5963\n",
      " Optimizer iteration 758, batch 367\n",
      "\n",
      " Learning rate 0.0006712623659174569, Model learning rate 0.0006712623871862888\n",
      "368/391 [===========================>..] - ETA: 1s - loss: 1.2805 - acc: 0.5964\n",
      " Optimizer iteration 759, batch 368\n",
      "\n",
      " Learning rate 0.0006705053352709212, Model learning rate 0.0006705053383484483\n",
      "369/391 [===========================>..] - ETA: 1s - loss: 1.2804 - acc: 0.5964\n",
      " Optimizer iteration 760, batch 369\n",
      "\n",
      " Learning rate 0.0006697478620682136, Model learning rate 0.0006697478820569813\n",
      "\n",
      " Optimizer iteration 761, batch 370\n",
      "\n",
      " Learning rate 0.0006689899482753984, Model learning rate 0.0006689899601042271\n",
      "371/391 [===========================>..] - ETA: 1s - loss: 1.2808 - acc: 0.5964\n",
      " Optimizer iteration 762, batch 371\n",
      "\n",
      " Learning rate 0.0006682315958596836, Model learning rate 0.0006682315724901855\n",
      "\n",
      " Optimizer iteration 763, batch 372\n",
      "\n",
      " Learning rate 0.0006674728067894149, Model learning rate 0.0006674728356301785\n",
      "373/391 [===========================>..] - ETA: 1s - loss: 1.2803 - acc: 0.5965\n",
      " Optimizer iteration 764, batch 373\n",
      "\n",
      " Learning rate 0.0006667135830340727, Model learning rate 0.0006667135749012232\n",
      "374/391 [===========================>..] - ETA: 1s - loss: 1.2801 - acc: 0.5966\n",
      " Optimizer iteration 765, batch 374\n",
      "\n",
      " Learning rate 0.0006659539265642643, Model learning rate 0.0006659539067186415\n",
      "375/391 [===========================>..] - ETA: 1s - loss: 1.2804 - acc: 0.5966\n",
      " Optimizer iteration 766, batch 375\n",
      "\n",
      " Learning rate 0.000665193839351721, Model learning rate 0.0006651938310824335\n",
      "376/391 [===========================>..] - ETA: 1s - loss: 1.2800 - acc: 0.5967\n",
      " Optimizer iteration 767, batch 376\n",
      "\n",
      " Learning rate 0.0006644333233692916, Model learning rate 0.000664433347992599\n",
      "377/391 [===========================>..] - ETA: 1s - loss: 1.2798 - acc: 0.5968\n",
      " Optimizer iteration 768, batch 377\n",
      "\n",
      " Learning rate 0.0006636723805909383, Model learning rate 0.0006636723992414773\n",
      "\n",
      " Optimizer iteration 769, batch 378\n",
      "\n",
      " Learning rate 0.0006629110129917308, Model learning rate 0.0006629109848290682\n",
      "379/391 [============================>.] - ETA: 0s - loss: 1.2793 - acc: 0.5969\n",
      " Optimizer iteration 770, batch 379\n",
      "\n",
      " Learning rate 0.0006621492225478414, Model learning rate 0.0006621492211706936\n",
      "380/391 [============================>.] - ETA: 0s - loss: 1.2790 - acc: 0.5971\n",
      " Optimizer iteration 771, batch 380\n",
      "\n",
      " Learning rate 0.0006613870112365398, Model learning rate 0.0006613869918510318\n",
      "381/391 [============================>.] - ETA: 0s - loss: 1.2789 - acc: 0.5972\n",
      " Optimizer iteration 772, batch 381\n",
      "\n",
      " Learning rate 0.0006606243810361885, Model learning rate 0.0006606243550777435\n",
      "382/391 [============================>.] - ETA: 0s - loss: 1.2788 - acc: 0.5972\n",
      " Optimizer iteration 773, batch 382\n",
      "\n",
      " Learning rate 0.0006598613339262369, Model learning rate 0.0006598613108508289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/391 [============================>.] - ETA: 0s - loss: 1.2787 - acc: 0.5972\n",
      " Optimizer iteration 774, batch 383\n",
      "\n",
      " Learning rate 0.0006590978718872166, Model learning rate 0.0006590978591702878\n",
      "384/391 [============================>.] - ETA: 0s - loss: 1.2784 - acc: 0.5973\n",
      " Optimizer iteration 775, batch 384\n",
      "\n",
      " Learning rate 0.0006583339969007363, Model learning rate 0.0006583340000361204\n",
      "385/391 [============================>.] - ETA: 0s - loss: 1.2787 - acc: 0.5972\n",
      " Optimizer iteration 776, batch 385\n",
      "\n",
      " Learning rate 0.0006575697109494763, Model learning rate 0.0006575697334483266\n",
      "\n",
      " Optimizer iteration 777, batch 386\n",
      "\n",
      " Learning rate 0.0006568050160171837, Model learning rate 0.0006568050011992455\n",
      "387/391 [============================>.] - ETA: 0s - loss: 1.2778 - acc: 0.5975\n",
      " Optimizer iteration 778, batch 387\n",
      "\n",
      " Learning rate 0.0006560399140886673, Model learning rate 0.0006560399197041988\n",
      "388/391 [============================>.] - ETA: 0s - loss: 1.2778 - acc: 0.5977\n",
      " Optimizer iteration 779, batch 388\n",
      "\n",
      " Learning rate 0.0006552744071497918, Model learning rate 0.0006552744307555258\n",
      "389/391 [============================>.] - ETA: 0s - loss: 1.2775 - acc: 0.5978\n",
      " Optimizer iteration 780, batch 389\n",
      "\n",
      " Learning rate 0.0006545084971874737, Model learning rate 0.0006545084761455655\n",
      "\n",
      " Optimizer iteration 781, batch 390\n",
      "\n",
      " Learning rate 0.0006537421861896752, Model learning rate 0.0006537421722896397\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 1.2770 - acc: 0.5980 - val_loss: 1.2552 - val_acc: 0.5998\n",
      "\n",
      "Epoch 00002: saving model to /home/ubuntu/Projects/hybrid-ensemble/model/run_200/cifar10_ResNet20v1_model-0002.h5\n",
      "Epoch 3/10\n",
      "\n",
      " Optimizer iteration 782, batch 0\n",
      "\n",
      " Learning rate 0.0006529754761453999, Model learning rate 0.0006529754609800875\n",
      "  1/391 [..............................] - ETA: 16s - loss: 1.0918 - acc: 0.6328\n",
      " Optimizer iteration 783, batch 1\n",
      "\n",
      " Learning rate 0.0006522083690446862, Model learning rate 0.0006522083422169089\n",
      "  2/391 [..............................] - ETA: 19s - loss: 1.0731 - acc: 0.6445\n",
      " Optimizer iteration 784, batch 2\n",
      "\n",
      " Learning rate 0.0006514408668786038, Model learning rate 0.0006514408742077649\n",
      "  3/391 [..............................] - ETA: 19s - loss: 1.1332 - acc: 0.6172\n",
      " Optimizer iteration 785, batch 3\n",
      "\n",
      " Learning rate 0.000650672971639248, Model learning rate 0.0006506729987449944\n",
      "  4/391 [..............................] - ETA: 20s - loss: 1.0963 - acc: 0.6504\n",
      " Optimizer iteration 786, batch 4\n",
      "\n",
      " Learning rate 0.0006499046853197338, Model learning rate 0.0006499046576209366\n",
      "  5/391 [..............................] - ETA: 20s - loss: 1.1094 - acc: 0.6438\n",
      " Optimizer iteration 787, batch 5\n",
      "\n",
      " Learning rate 0.0006491360099141913, Model learning rate 0.0006491360254585743\n",
      "  6/391 [..............................] - ETA: 20s - loss: 1.1157 - acc: 0.6380\n",
      " Optimizer iteration 788, batch 6\n",
      "\n",
      " Learning rate 0.0006483669474177608, Model learning rate 0.0006483669276349247\n",
      "  7/391 [..............................] - ETA: 20s - loss: 1.1322 - acc: 0.6250\n",
      " Optimizer iteration 789, batch 7\n",
      "\n",
      " Learning rate 0.0006475974998265874, Model learning rate 0.0006475974805653095\n",
      "  8/391 [..............................] - ETA: 21s - loss: 1.1510 - acc: 0.6201\n",
      " Optimizer iteration 790, batch 8\n",
      "\n",
      " Learning rate 0.0006468276691378155, Model learning rate 0.0006468276842497289\n",
      "  9/391 [..............................] - ETA: 21s - loss: 1.1580 - acc: 0.6198\n",
      " Optimizer iteration 791, batch 9\n",
      "\n",
      " Learning rate 0.0006460574573495835, Model learning rate 0.0006460574804805219\n",
      " 10/391 [..............................] - ETA: 21s - loss: 1.1616 - acc: 0.6219\n",
      " Optimizer iteration 792, batch 10\n",
      "\n",
      " Learning rate 0.0006452868664610196, Model learning rate 0.0006452868692576885\n",
      " 11/391 [..............................] - ETA: 21s - loss: 1.1781 - acc: 0.6214\n",
      " Optimizer iteration 793, batch 11\n",
      "\n",
      " Learning rate 0.0006445158984722358, Model learning rate 0.0006445159087888896\n",
      " 12/391 [..............................] - ETA: 21s - loss: 1.1905 - acc: 0.6146\n",
      " Optimizer iteration 794, batch 12\n",
      "\n",
      " Learning rate 0.0006437445553843229, Model learning rate 0.0006437445408664644\n",
      "\n",
      " Optimizer iteration 795, batch 13\n",
      "\n",
      " Learning rate 0.0006429728391993446, Model learning rate 0.0006429728236980736\n",
      " 14/391 [>.............................] - ETA: 20s - loss: 1.1950 - acc: 0.6133\n",
      " Optimizer iteration 796, batch 14\n",
      "\n",
      " Learning rate 0.0006422007519203343, Model learning rate 0.0006422007572837174\n",
      "\n",
      " Optimizer iteration 797, batch 15\n",
      "\n",
      " Learning rate 0.0006414282955512875, Model learning rate 0.0006414282834157348\n",
      " 16/391 [>.............................] - ETA: 20s - loss: 1.1874 - acc: 0.6201\n",
      " Optimizer iteration 798, batch 16\n",
      "\n",
      " Learning rate 0.0006406554720971582, Model learning rate 0.0006406554603017867\n",
      "\n",
      " Optimizer iteration 799, batch 17\n",
      "\n",
      " Learning rate 0.000639882283563853, Model learning rate 0.0006398822879418731\n",
      " 18/391 [>.............................] - ETA: 20s - loss: 1.1888 - acc: 0.6241\n",
      " Optimizer iteration 800, batch 18\n",
      "\n",
      " Learning rate 0.0006391087319582263, Model learning rate 0.0006391087081283331\n",
      " 19/391 [>.............................] - ETA: 20s - loss: 1.1889 - acc: 0.6266\n",
      " Optimizer iteration 801, batch 19\n",
      "\n",
      " Learning rate 0.0006383348192880747, Model learning rate 0.0006383348372764885\n",
      " 20/391 [>.............................] - ETA: 20s - loss: 1.1873 - acc: 0.6262\n",
      " Optimizer iteration 802, batch 20\n",
      "\n",
      " Learning rate 0.0006375605475621318, Model learning rate 0.0006375605589710176\n",
      " 21/391 [>.............................] - ETA: 21s - loss: 1.1837 - acc: 0.6298\n",
      " Optimizer iteration 803, batch 21\n",
      "\n",
      " Learning rate 0.0006367859187900634, Model learning rate 0.0006367859314195812\n",
      "\n",
      " Optimizer iteration 804, batch 22\n",
      "\n",
      " Learning rate 0.0006360109349824621, Model learning rate 0.0006360109546221793\n",
      " 23/391 [>.............................] - ETA: 21s - loss: 1.1827 - acc: 0.6284\n",
      " Optimizer iteration 805, batch 23\n",
      "\n",
      " Learning rate 0.000635235598150842, Model learning rate 0.000635235570371151\n",
      " 24/391 [>.............................] - ETA: 21s - loss: 1.1805 - acc: 0.6309\n",
      " Optimizer iteration 806, batch 24\n",
      "\n",
      " Learning rate 0.0006344599103076329, Model learning rate 0.0006344598950818181\n",
      " 25/391 [>.............................] - ETA: 22s - loss: 1.1762 - acc: 0.6338\n",
      " Optimizer iteration 807, batch 25\n",
      "\n",
      " Learning rate 0.0006336838734661765, Model learning rate 0.0006336838705465198\n",
      " 26/391 [>.............................] - ETA: 22s - loss: 1.1788 - acc: 0.6322\n",
      " Optimizer iteration 808, batch 26\n",
      "\n",
      " Learning rate 0.0006329074896407202, Model learning rate 0.0006329074967652559\n",
      " 27/391 [=>............................] - ETA: 22s - loss: 1.1756 - acc: 0.6317\n",
      " Optimizer iteration 809, batch 27\n",
      "\n",
      " Learning rate 0.0006321307608464113, Model learning rate 0.0006321307737380266\n",
      " 28/391 [=>............................] - ETA: 22s - loss: 1.1764 - acc: 0.6328\n",
      " Optimizer iteration 810, batch 28\n",
      "\n",
      " Learning rate 0.0006313536890992934, Model learning rate 0.0006313537014648318\n",
      " 29/391 [=>............................] - ETA: 23s - loss: 1.1781 - acc: 0.6320\n",
      " Optimizer iteration 811, batch 29\n",
      "\n",
      " Learning rate 0.0006305762764162999, Model learning rate 0.0006305762799456716\n",
      " 30/391 [=>............................] - ETA: 22s - loss: 1.1768 - acc: 0.6318\n",
      " Optimizer iteration 812, batch 30\n",
      "\n",
      " Learning rate 0.0006297985248152486, Model learning rate 0.0006297985091805458\n",
      " 31/391 [=>............................] - ETA: 23s - loss: 1.1730 - acc: 0.6346\n",
      " Optimizer iteration 813, batch 31\n",
      "\n",
      " Learning rate 0.000629020436314838, Model learning rate 0.0006290204473771155\n",
      " 32/391 [=>............................] - ETA: 23s - loss: 1.1664 - acc: 0.6367\n",
      " Optimizer iteration 814, batch 32\n",
      "\n",
      " Learning rate 0.0006282420129346401, Model learning rate 0.0006282420363277197\n",
      " 33/391 [=>............................] - ETA: 23s - loss: 1.1678 - acc: 0.6354\n",
      " Optimizer iteration 815, batch 33\n",
      "\n",
      " Learning rate 0.0006274632566950966, Model learning rate 0.0006274632760323584\n",
      " 34/391 [=>............................] - ETA: 23s - loss: 1.1654 - acc: 0.6376\n",
      " Optimizer iteration 816, batch 34\n",
      "\n",
      " Learning rate 0.0006266841696175131, Model learning rate 0.0006266841664910316\n",
      " 35/391 [=>............................] - ETA: 23s - loss: 1.1605 - acc: 0.6395\n",
      " Optimizer iteration 817, batch 35\n",
      "\n",
      " Learning rate 0.000625904753724054, Model learning rate 0.0006259047659114003\n",
      " 36/391 [=>............................] - ETA: 23s - loss: 1.1588 - acc: 0.6398\n",
      " Optimizer iteration 818, batch 36\n",
      "\n",
      " Learning rate 0.0006251250110377367, Model learning rate 0.0006251250160858035\n",
      " 37/391 [=>............................] - ETA: 23s - loss: 1.1581 - acc: 0.6413\n",
      " Optimizer iteration 819, batch 37\n",
      "\n",
      " Learning rate 0.0006243449435824276, Model learning rate 0.0006243449170142412\n",
      " 38/391 [=>............................] - ETA: 23s - loss: 1.1593 - acc: 0.6394\n",
      " Optimizer iteration 820, batch 38\n",
      "\n",
      " Learning rate 0.0006235645533828348, Model learning rate 0.0006235645269043744\n",
      " 39/391 [=>............................] - ETA: 23s - loss: 1.1656 - acc: 0.6378\n",
      " Optimizer iteration 821, batch 39\n",
      "\n",
      " Learning rate 0.0006227838424645056, Model learning rate 0.0006227838457562029\n",
      " 40/391 [==>...........................] - ETA: 23s - loss: 1.1592 - acc: 0.6408\n",
      " Optimizer iteration 822, batch 40\n",
      "\n",
      " Learning rate 0.0006220028128538187, Model learning rate 0.000622002815362066\n",
      " 41/391 [==>...........................] - ETA: 23s - loss: 1.1609 - acc: 0.6412\n",
      " Optimizer iteration 823, batch 41\n",
      "\n",
      " Learning rate 0.0006212214665779805, Model learning rate 0.0006212214939296246\n",
      "\n",
      " Optimizer iteration 824, batch 42\n",
      "\n",
      " Learning rate 0.000620439805665019, Model learning rate 0.0006204398232512176\n",
      " 43/391 [==>...........................] - ETA: 23s - loss: 1.1566 - acc: 0.6424\n",
      " Optimizer iteration 825, batch 43\n",
      "\n",
      " Learning rate 0.0006196578321437789, Model learning rate 0.0006196578033268452\n",
      " 44/391 [==>...........................] - ETA: 23s - loss: 1.1566 - acc: 0.6424\n",
      " Optimizer iteration 826, batch 44\n",
      "\n",
      " Learning rate 0.0006188755480439165, Model learning rate 0.0006188755505718291\n",
      " 45/391 [==>...........................] - ETA: 23s - loss: 1.1566 - acc: 0.6429\n",
      " Optimizer iteration 827, batch 45\n",
      "\n",
      " Learning rate 0.0006180929553958942, Model learning rate 0.0006180929485708475\n",
      " 46/391 [==>...........................] - ETA: 23s - loss: 1.1568 - acc: 0.6430\n",
      " Optimizer iteration 828, batch 46\n",
      "\n",
      " Learning rate 0.000617310056230975, Model learning rate 0.0006173100555315614\n",
      " 47/391 [==>...........................] - ETA: 23s - loss: 1.1592 - acc: 0.6415\n",
      " Optimizer iteration 829, batch 47\n",
      "\n",
      " Learning rate 0.0006165268525812178, Model learning rate 0.0006165268714539707\n",
      " 48/391 [==>...........................] - ETA: 23s - loss: 1.1584 - acc: 0.6427\n",
      " Optimizer iteration 830, batch 48\n",
      "\n",
      " Learning rate 0.0006157433464794716, Model learning rate 0.0006157433381304145\n",
      " 49/391 [==>...........................] - ETA: 23s - loss: 1.1556 - acc: 0.6433\n",
      " Optimizer iteration 831, batch 49\n",
      "\n",
      " Learning rate 0.0006149595399593703, Model learning rate 0.0006149595137685537\n",
      "\n",
      " Optimizer iteration 832, batch 50\n",
      "\n",
      " Learning rate 0.0006141754350553279, Model learning rate 0.0006141754565760493\n",
      " 51/391 [==>...........................] - ETA: 23s - loss: 1.1583 - acc: 0.6445\n",
      " Optimizer iteration 833, batch 51\n",
      "\n",
      " Learning rate 0.0006133910338025328, Model learning rate 0.0006133910501375794\n",
      " 52/391 [==>...........................] - ETA: 23s - loss: 1.1544 - acc: 0.6459\n",
      " Optimizer iteration 834, batch 52\n",
      "\n",
      " Learning rate 0.0006126063382369423, Model learning rate 0.000612606352660805\n",
      " 53/391 [===>..........................] - ETA: 23s - loss: 1.1535 - acc: 0.6459\n",
      " Optimizer iteration 835, batch 53\n",
      "\n",
      " Learning rate 0.0006118213503952778, Model learning rate 0.000611821364145726\n",
      " 54/391 [===>..........................] - ETA: 23s - loss: 1.1522 - acc: 0.6460\n",
      " Optimizer iteration 836, batch 54\n",
      "\n",
      " Learning rate 0.0006110360723150194, Model learning rate 0.0006110360845923424\n",
      " 55/391 [===>..........................] - ETA: 23s - loss: 1.1505 - acc: 0.6467\n",
      " Optimizer iteration 837, batch 55\n",
      "\n",
      " Learning rate 0.0006102505060344006, Model learning rate 0.0006102505140006542\n",
      " 56/391 [===>..........................] - ETA: 23s - loss: 1.1515 - acc: 0.6462\n",
      " Optimizer iteration 838, batch 56\n",
      "\n",
      " Learning rate 0.0006094646535924025, Model learning rate 0.0006094646523706615\n",
      " 57/391 [===>..........................] - ETA: 23s - loss: 1.1520 - acc: 0.6453\n",
      " Optimizer iteration 839, batch 57\n",
      "\n",
      " Learning rate 0.0006086785170287495, Model learning rate 0.0006086784997023642\n",
      " 58/391 [===>..........................] - ETA: 23s - loss: 1.1527 - acc: 0.6448\n",
      " Optimizer iteration 840, batch 58\n",
      "\n",
      " Learning rate 0.0006078920983839031, Model learning rate 0.0006078921142034233\n",
      " 59/391 [===>..........................] - ETA: 23s - loss: 1.1526 - acc: 0.6451\n",
      " Optimizer iteration 841, batch 59\n",
      "\n",
      " Learning rate 0.000607105399699057, Model learning rate 0.0006071053794585168\n",
      " 60/391 [===>..........................] - ETA: 23s - loss: 1.1543 - acc: 0.6444\n",
      " Optimizer iteration 842, batch 60\n",
      "\n",
      " Learning rate 0.0006063184230161318, Model learning rate 0.0006063184118829668\n",
      " 61/391 [===>..........................] - ETA: 23s - loss: 1.1546 - acc: 0.6443\n",
      " Optimizer iteration 843, batch 61\n",
      "\n",
      " Learning rate 0.0006055311703777698, Model learning rate 0.0006055311532691121\n",
      "\n",
      " Optimizer iteration 844, batch 62\n",
      "\n",
      " Learning rate 0.0006047436438273293, Model learning rate 0.0006047436618246138\n",
      " 63/391 [===>..........................] - ETA: 23s - loss: 1.1528 - acc: 0.6451\n",
      " Optimizer iteration 845, batch 63\n",
      "\n",
      " Learning rate 0.0006039558454088796, Model learning rate 0.00060395582113415\n",
      " 64/391 [===>..........................] - ETA: 23s - loss: 1.1520 - acc: 0.6453\n",
      " Optimizer iteration 846, batch 64\n",
      "\n",
      " Learning rate 0.0006031677771671962, Model learning rate 0.0006031678058207035\n",
      " 65/391 [===>..........................] - ETA: 23s - loss: 1.1528 - acc: 0.6448\n",
      " Optimizer iteration 847, batch 65\n",
      "\n",
      " Learning rate 0.0006023794411477537, Model learning rate 0.0006023794412612915\n",
      " 66/391 [====>.........................] - ETA: 23s - loss: 1.1518 - acc: 0.6460\n",
      " Optimizer iteration 848, batch 66\n",
      "\n",
      " Learning rate 0.0006015908393967232, Model learning rate 0.0006015908438712358\n",
      " 67/391 [====>.........................] - ETA: 23s - loss: 1.1525 - acc: 0.6458\n",
      " Optimizer iteration 849, batch 67\n",
      "\n",
      " Learning rate 0.0006008019739609646, Model learning rate 0.0006008019554428756\n",
      " 68/391 [====>.........................] - ETA: 23s - loss: 1.1516 - acc: 0.6464\n",
      " Optimizer iteration 850, batch 68\n",
      "\n",
      " Learning rate 0.0006000128468880223, Model learning rate 0.0006000128341838717\n",
      " 69/391 [====>.........................] - ETA: 23s - loss: 1.1511 - acc: 0.6464\n",
      " Optimizer iteration 851, batch 69\n",
      "\n",
      " Learning rate 0.00059922346022612, Model learning rate 0.0005992234800942242\n",
      " 70/391 [====>.........................] - ETA: 22s - loss: 1.1497 - acc: 0.6465\n",
      " Optimizer iteration 852, batch 70\n",
      "\n",
      " Learning rate 0.0005984338160241551, Model learning rate 0.0005984338349662721\n",
      " 71/391 [====>.........................] - ETA: 23s - loss: 1.1500 - acc: 0.6462\n",
      " Optimizer iteration 853, batch 71\n",
      "\n",
      " Learning rate 0.0005976439163316936, Model learning rate 0.0005976438988000154\n",
      " 72/391 [====>.........................] - ETA: 23s - loss: 1.1506 - acc: 0.6462\n",
      " Optimizer iteration 854, batch 72\n",
      "\n",
      " Learning rate 0.0005968537631989645, Model learning rate 0.000596853788010776\n",
      "\n",
      " Optimizer iteration 855, batch 73\n",
      "\n",
      " Learning rate 0.0005960633586768543, Model learning rate 0.0005960633861832321\n",
      " 74/391 [====>.........................] - ETA: 22s - loss: 1.1529 - acc: 0.6443\n",
      " Optimizer iteration 856, batch 74\n",
      "\n",
      " Learning rate 0.0005952727048169024, Model learning rate 0.0005952726933173835\n",
      " 75/391 [====>.........................] - ETA: 22s - loss: 1.1543 - acc: 0.6439\n",
      " Optimizer iteration 857, batch 75\n",
      "\n",
      " Learning rate 0.0005944818036712959, Model learning rate 0.0005944818258285522\n",
      " 76/391 [====>.........................] - ETA: 22s - loss: 1.1550 - acc: 0.6433\n",
      " Optimizer iteration 858, batch 76\n",
      "\n",
      " Learning rate 0.0005936906572928624, Model learning rate 0.0005936906673014164\n",
      " 77/391 [====>.........................] - ETA: 22s - loss: 1.1548 - acc: 0.6433\n",
      " Optimizer iteration 859, batch 77\n",
      "\n",
      " Learning rate 0.000592899267735067, Model learning rate 0.0005928992759436369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 78/391 [====>.........................] - ETA: 22s - loss: 1.1551 - acc: 0.6422\n",
      " Optimizer iteration 860, batch 78\n",
      "\n",
      " Learning rate 0.0005921076370520058, Model learning rate 0.0005921076517552137\n",
      " 79/391 [=====>........................] - ETA: 22s - loss: 1.1530 - acc: 0.6423\n",
      " Optimizer iteration 861, batch 79\n",
      "\n",
      " Learning rate 0.0005913157672984006, Model learning rate 0.0005913157947361469\n",
      " 80/391 [=====>........................] - ETA: 22s - loss: 1.1553 - acc: 0.6426\n",
      " Optimizer iteration 862, batch 80\n",
      "\n",
      " Learning rate 0.000590523660529594, Model learning rate 0.0005905236466787755\n",
      " 81/391 [=====>........................] - ETA: 22s - loss: 1.1563 - acc: 0.6425\n",
      " Optimizer iteration 863, batch 81\n",
      "\n",
      " Learning rate 0.0005897313188015433, Model learning rate 0.0005897313239984214\n",
      "\n",
      " Optimizer iteration 864, batch 82\n",
      "\n",
      " Learning rate 0.0005889387441708161, Model learning rate 0.0005889387684874237\n",
      " 83/391 [=====>........................] - ETA: 22s - loss: 1.1563 - acc: 0.6418\n",
      " Optimizer iteration 865, batch 83\n",
      "\n",
      " Learning rate 0.0005881459386945845, Model learning rate 0.0005881459219381213\n",
      " 84/391 [=====>........................] - ETA: 22s - loss: 1.1555 - acc: 0.6430\n",
      " Optimizer iteration 866, batch 84\n",
      "\n",
      " Learning rate 0.0005873529044306193, Model learning rate 0.0005873529007658362\n",
      "\n",
      " Optimizer iteration 867, batch 85\n",
      "\n",
      " Learning rate 0.0005865596434372857, Model learning rate 0.0005865596467629075\n",
      " 86/391 [=====>........................] - ETA: 22s - loss: 1.1556 - acc: 0.6434\n",
      " Optimizer iteration 868, batch 86\n",
      "\n",
      " Learning rate 0.0005857661577735372, Model learning rate 0.0005857661599293351\n",
      " 87/391 [=====>........................] - ETA: 22s - loss: 1.1535 - acc: 0.6444\n",
      " Optimizer iteration 869, batch 87\n",
      "\n",
      " Learning rate 0.0005849724494989103, Model learning rate 0.0005849724402651191\n",
      " 88/391 [=====>........................] - ETA: 22s - loss: 1.1545 - acc: 0.6437\n",
      " Optimizer iteration 870, batch 88\n",
      "\n",
      " Learning rate 0.0005841785206735191, Model learning rate 0.0005841785459779203\n",
      " 89/391 [=====>........................] - ETA: 22s - loss: 1.1546 - acc: 0.6440\n",
      " Optimizer iteration 871, batch 89\n",
      "\n",
      " Learning rate 0.0005833843733580511, Model learning rate 0.0005833843606524169\n",
      "\n",
      " Optimizer iteration 872, batch 90\n",
      "\n",
      " Learning rate 0.0005825900096137599, Model learning rate 0.0005825900007039309\n",
      " 91/391 [=====>........................] - ETA: 21s - loss: 1.1538 - acc: 0.6443\n",
      " Optimizer iteration 873, batch 91\n",
      "\n",
      " Learning rate 0.0005817954315024609, Model learning rate 0.0005817954079248011\n",
      " 92/391 [======>.......................] - ETA: 22s - loss: 1.1523 - acc: 0.6445\n",
      " Optimizer iteration 874, batch 92\n",
      "\n",
      " Learning rate 0.0005810006410865267, Model learning rate 0.0005810006405226886\n",
      "\n",
      " Optimizer iteration 875, batch 93\n",
      "\n",
      " Learning rate 0.0005802056404288802, Model learning rate 0.0005802056402899325\n",
      " 94/391 [======>.......................] - ETA: 21s - loss: 1.1527 - acc: 0.6448\n",
      " Optimizer iteration 876, batch 94\n",
      "\n",
      " Learning rate 0.0005794104315929903, Model learning rate 0.0005794104072265327\n",
      " 95/391 [======>.......................] - ETA: 21s - loss: 1.1510 - acc: 0.6453\n",
      " Optimizer iteration 877, batch 95\n",
      "\n",
      " Learning rate 0.0005786150166428661, Model learning rate 0.0005786149995401502\n",
      " 96/391 [======>.......................] - ETA: 21s - loss: 1.1507 - acc: 0.6456\n",
      " Optimizer iteration 878, batch 96\n",
      "\n",
      " Learning rate 0.0005778193976430518, Model learning rate 0.0005778194172307849\n",
      " 97/391 [======>.......................] - ETA: 21s - loss: 1.1497 - acc: 0.6459\n",
      " Optimizer iteration 879, batch 97\n",
      "\n",
      " Learning rate 0.0005770235766586215, Model learning rate 0.000577023602090776\n",
      " 98/391 [======>.......................] - ETA: 21s - loss: 1.1500 - acc: 0.6458\n",
      " Optimizer iteration 880, batch 98\n",
      "\n",
      " Learning rate 0.0005762275557551727, Model learning rate 0.0005762275541201234\n",
      " 99/391 [======>.......................] - ETA: 21s - loss: 1.1518 - acc: 0.6446\n",
      " Optimizer iteration 881, batch 99\n",
      "\n",
      " Learning rate 0.0005754313369988227, Model learning rate 0.0005754313315264881\n",
      "100/391 [======>.......................] - ETA: 21s - loss: 1.1516 - acc: 0.6452\n",
      " Optimizer iteration 882, batch 100\n",
      "\n",
      " Learning rate 0.0005746349224562021, Model learning rate 0.00057463493430987\n",
      "101/391 [======>.......................] - ETA: 21s - loss: 1.1518 - acc: 0.6452\n",
      " Optimizer iteration 883, batch 101\n",
      "\n",
      " Learning rate 0.0005738383141944493, Model learning rate 0.0005738383042626083\n",
      "102/391 [======>.......................] - ETA: 21s - loss: 1.1499 - acc: 0.6458\n",
      " Optimizer iteration 884, batch 102\n",
      "\n",
      " Learning rate 0.0005730415142812059, Model learning rate 0.0005730414995923638\n",
      "103/391 [======>.......................] - ETA: 21s - loss: 1.1490 - acc: 0.6462\n",
      " Optimizer iteration 885, batch 103\n",
      "\n",
      " Learning rate 0.0005722445247846107, Model learning rate 0.0005722445202991366\n",
      "104/391 [======>.......................] - ETA: 21s - loss: 1.1483 - acc: 0.6466\n",
      " Optimizer iteration 886, batch 104\n",
      "\n",
      " Learning rate 0.0005714473477732947, Model learning rate 0.0005714473663829267\n",
      "105/391 [=======>......................] - ETA: 21s - loss: 1.1481 - acc: 0.6469\n",
      " Optimizer iteration 887, batch 105\n",
      "\n",
      " Learning rate 0.000570649985316376, Model learning rate 0.0005706499796360731\n",
      "106/391 [=======>......................] - ETA: 20s - loss: 1.1481 - acc: 0.6471\n",
      " Optimizer iteration 888, batch 106\n",
      "\n",
      " Learning rate 0.000569852439483453, Model learning rate 0.0005698524182662368\n",
      "107/391 [=======>......................] - ETA: 20s - loss: 1.1488 - acc: 0.6465\n",
      " Optimizer iteration 889, batch 107\n",
      "\n",
      " Learning rate 0.000569054712344601, Model learning rate 0.0005690547404810786\n",
      "108/391 [=======>......................] - ETA: 20s - loss: 1.1485 - acc: 0.6466\n",
      " Optimizer iteration 890, batch 108\n",
      "\n",
      " Learning rate 0.0005682568059703659, Model learning rate 0.0005682568298652768\n",
      "109/391 [=======>......................] - ETA: 20s - loss: 1.1484 - acc: 0.6465\n",
      " Optimizer iteration 891, batch 109\n",
      "\n",
      " Learning rate 0.0005674587224317579, Model learning rate 0.0005674587446264923\n",
      "110/391 [=======>......................] - ETA: 20s - loss: 1.1488 - acc: 0.6459\n",
      " Optimizer iteration 892, batch 110\n",
      "\n",
      " Learning rate 0.000566660463800248, Model learning rate 0.000566660484764725\n",
      "111/391 [=======>......................] - ETA: 20s - loss: 1.1473 - acc: 0.6463\n",
      " Optimizer iteration 893, batch 111\n",
      "\n",
      " Learning rate 0.0005658620321477612, Model learning rate 0.0005658620502799749\n",
      "112/391 [=======>......................] - ETA: 20s - loss: 1.1474 - acc: 0.6459\n",
      " Optimizer iteration 894, batch 112\n",
      "\n",
      " Learning rate 0.0005650634295466716, Model learning rate 0.0005650634411722422\n",
      "113/391 [=======>......................] - ETA: 20s - loss: 1.1473 - acc: 0.6455\n",
      " Optimizer iteration 895, batch 113\n",
      "\n",
      " Learning rate 0.0005642646580697973, Model learning rate 0.0005642646574415267\n",
      "114/391 [=======>......................] - ETA: 20s - loss: 1.1476 - acc: 0.6451\n",
      " Optimizer iteration 896, batch 114\n",
      "\n",
      " Learning rate 0.0005634657197903944, Model learning rate 0.0005634656990878284\n",
      "115/391 [=======>......................] - ETA: 20s - loss: 1.1476 - acc: 0.6450\n",
      " Optimizer iteration 897, batch 115\n",
      "\n",
      " Learning rate 0.0005626666167821521, Model learning rate 0.0005626666243188083\n",
      "116/391 [=======>......................] - ETA: 20s - loss: 1.1464 - acc: 0.6453\n",
      " Optimizer iteration 898, batch 116\n",
      "\n",
      " Learning rate 0.0005618673511191873, Model learning rate 0.0005618673749268055\n",
      "117/391 [=======>......................] - ETA: 20s - loss: 1.1458 - acc: 0.6457\n",
      " Optimizer iteration 899, batch 117\n",
      "\n",
      " Learning rate 0.0005610679248760384, Model learning rate 0.0005610679509118199\n",
      "118/391 [========>.....................] - ETA: 20s - loss: 1.1446 - acc: 0.6465\n",
      " Optimizer iteration 900, batch 118\n",
      "\n",
      " Learning rate 0.0005602683401276614, Model learning rate 0.0005602683522738516\n",
      "119/391 [========>.....................] - ETA: 20s - loss: 1.1436 - acc: 0.6467\n",
      " Optimizer iteration 901, batch 119\n",
      "\n",
      " Learning rate 0.0005594685989494238, Model learning rate 0.0005594685790129006\n",
      "120/391 [========>.....................] - ETA: 20s - loss: 1.1433 - acc: 0.6469\n",
      " Optimizer iteration 902, batch 120\n",
      "\n",
      " Learning rate 0.0005586687034170981, Model learning rate 0.0005586686893366277\n",
      "121/391 [========>.....................] - ETA: 20s - loss: 1.1438 - acc: 0.6468\n",
      " Optimizer iteration 903, batch 121\n",
      "\n",
      " Learning rate 0.0005578686556068585, Model learning rate 0.000557868683245033\n",
      "122/391 [========>.....................] - ETA: 19s - loss: 1.1438 - acc: 0.6470\n",
      " Optimizer iteration 904, batch 122\n",
      "\n",
      " Learning rate 0.0005570684575952737, Model learning rate 0.0005570684443227947\n",
      "123/391 [========>.....................] - ETA: 19s - loss: 1.1422 - acc: 0.6475\n",
      " Optimizer iteration 905, batch 123\n",
      "\n",
      " Learning rate 0.0005562681114593028, Model learning rate 0.0005562680889852345\n",
      "124/391 [========>.....................] - ETA: 19s - loss: 1.1425 - acc: 0.6471\n",
      " Optimizer iteration 906, batch 124\n",
      "\n",
      " Learning rate 0.0005554676192762891, Model learning rate 0.0005554676172323525\n",
      "125/391 [========>.....................] - ETA: 19s - loss: 1.1426 - acc: 0.6474\n",
      " Optimizer iteration 907, batch 125\n",
      "\n",
      " Learning rate 0.000554666983123955, Model learning rate 0.0005546669708564878\n",
      "126/391 [========>.....................] - ETA: 19s - loss: 1.1432 - acc: 0.6473\n",
      " Optimizer iteration 908, batch 126\n",
      "\n",
      " Learning rate 0.0005538662050803964, Model learning rate 0.0005538662080653012\n",
      "\n",
      " Optimizer iteration 909, batch 127\n",
      "\n",
      " Learning rate 0.0005530652872240779, Model learning rate 0.0005530652706511319\n",
      "128/391 [========>.....................] - ETA: 19s - loss: 1.1435 - acc: 0.6470\n",
      " Optimizer iteration 910, batch 128\n",
      "\n",
      " Learning rate 0.0005522642316338268, Model learning rate 0.0005522642168216407\n",
      "129/391 [========>.....................] - ETA: 19s - loss: 1.1419 - acc: 0.6476\n",
      " Optimizer iteration 911, batch 129\n",
      "\n",
      " Learning rate 0.0005514630403888278, Model learning rate 0.0005514630465768278\n",
      "130/391 [========>.....................] - ETA: 19s - loss: 1.1414 - acc: 0.6477\n",
      " Optimizer iteration 912, batch 130\n",
      "\n",
      " Learning rate 0.0005506617155686176, Model learning rate 0.0005506617017090321\n",
      "131/391 [=========>....................] - ETA: 19s - loss: 1.1421 - acc: 0.6474\n",
      " Optimizer iteration 913, batch 131\n",
      "\n",
      " Learning rate 0.0005498602592530799, Model learning rate 0.0005498602404259145\n",
      "132/391 [=========>....................] - ETA: 19s - loss: 1.1414 - acc: 0.6473\n",
      " Optimizer iteration 914, batch 132\n",
      "\n",
      " Learning rate 0.0005490586735224398, Model learning rate 0.0005490586627274752\n",
      "133/391 [=========>....................] - ETA: 19s - loss: 1.1409 - acc: 0.6474\n",
      " Optimizer iteration 915, batch 133\n",
      "\n",
      " Learning rate 0.0005482569604572576, Model learning rate 0.000548256968613714\n",
      "\n",
      " Optimizer iteration 916, batch 134\n",
      "\n",
      " Learning rate 0.000547455122138425, Model learning rate 0.00054745509987697\n",
      "135/391 [=========>....................] - ETA: 19s - loss: 1.1397 - acc: 0.6480\n",
      " Optimizer iteration 917, batch 135\n",
      "\n",
      " Learning rate 0.000546653160647158, Model learning rate 0.0005466531729325652\n",
      "136/391 [=========>....................] - ETA: 19s - loss: 1.1392 - acc: 0.6478\n",
      " Optimizer iteration 918, batch 136\n",
      "\n",
      " Learning rate 0.0005458510780649931, Model learning rate 0.0005458510713651776\n",
      "\n",
      " Optimizer iteration 919, batch 137\n",
      "\n",
      " Learning rate 0.0005450488764737804, Model learning rate 0.0005450488533824682\n",
      "138/391 [=========>....................] - ETA: 18s - loss: 1.1387 - acc: 0.6470\n",
      " Optimizer iteration 920, batch 138\n",
      "\n",
      " Learning rate 0.0005442465579556792, Model learning rate 0.0005442465771920979\n",
      "139/391 [=========>....................] - ETA: 18s - loss: 1.1378 - acc: 0.6474\n",
      " Optimizer iteration 921, batch 139\n",
      "\n",
      " Learning rate 0.0005434441245931524, Model learning rate 0.0005434441263787448\n",
      "140/391 [=========>....................] - ETA: 18s - loss: 1.1390 - acc: 0.6468\n",
      " Optimizer iteration 922, batch 140\n",
      "\n",
      " Learning rate 0.0005426415784689607, Model learning rate 0.00054264155915007\n",
      "141/391 [=========>....................] - ETA: 18s - loss: 1.1396 - acc: 0.6467\n",
      " Optimizer iteration 923, batch 141\n",
      "\n",
      " Learning rate 0.0005418389216661579, Model learning rate 0.0005418389337137341\n",
      "142/391 [=========>....................] - ETA: 18s - loss: 1.1399 - acc: 0.6464\n",
      " Optimizer iteration 924, batch 142\n",
      "\n",
      " Learning rate 0.0005410361562680842, Model learning rate 0.0005410361336544156\n",
      "143/391 [=========>....................] - ETA: 18s - loss: 1.1399 - acc: 0.6464\n",
      " Optimizer iteration 925, batch 143\n",
      "\n",
      " Learning rate 0.000540233284358363, Model learning rate 0.0005402332753874362\n",
      "144/391 [==========>...................] - ETA: 18s - loss: 1.1394 - acc: 0.6466\n",
      " Optimizer iteration 926, batch 144\n",
      "\n",
      " Learning rate 0.000539430308020893, Model learning rate 0.0005394303007051349\n",
      "145/391 [==========>...................] - ETA: 18s - loss: 1.1388 - acc: 0.6469\n",
      " Optimizer iteration 927, batch 145\n",
      "\n",
      " Learning rate 0.0005386272293398444, Model learning rate 0.0005386272096075118\n",
      "\n",
      " Optimizer iteration 928, batch 146\n",
      "\n",
      " Learning rate 0.0005378240503996531, Model learning rate 0.0005378240603022277\n",
      "147/391 [==========>...................] - ETA: 18s - loss: 1.1394 - acc: 0.6467\n",
      " Optimizer iteration 929, batch 147\n",
      "\n",
      " Learning rate 0.000537020773285015, Model learning rate 0.0005370207945816219\n",
      "148/391 [==========>...................] - ETA: 18s - loss: 1.1388 - acc: 0.6467\n",
      " Optimizer iteration 930, batch 148\n",
      "\n",
      " Learning rate 0.0005362174000808813, Model learning rate 0.0005362174124456942\n",
      "\n",
      " Optimizer iteration 931, batch 149\n",
      "\n",
      " Learning rate 0.0005354139328724518, Model learning rate 0.0005354139138944447\n",
      "150/391 [==========>...................] - ETA: 17s - loss: 1.1370 - acc: 0.6473\n",
      " Optimizer iteration 932, batch 150\n",
      "\n",
      " Learning rate 0.000534610373745171, Model learning rate 0.0005346103571355343\n",
      "\n",
      " Optimizer iteration 933, batch 151\n",
      "\n",
      " Learning rate 0.0005338067247847219, Model learning rate 0.000533806742168963\n",
      "152/391 [==========>...................] - ETA: 17s - loss: 1.1362 - acc: 0.6476\n",
      " Optimizer iteration 934, batch 152\n",
      "\n",
      " Learning rate 0.0005330029880770201, Model learning rate 0.0005330030107870698\n",
      "\n",
      " Optimizer iteration 935, batch 153\n",
      "\n",
      " Learning rate 0.0005321991657082097, Model learning rate 0.0005321991629898548\n",
      "154/391 [==========>...................] - ETA: 17s - loss: 1.1361 - acc: 0.6479\n",
      " Optimizer iteration 936, batch 154\n",
      "\n",
      " Learning rate 0.0005313952597646568, Model learning rate 0.0005313952569849789\n",
      "155/391 [==========>...................] - ETA: 17s - loss: 1.1357 - acc: 0.6480\n",
      " Optimizer iteration 937, batch 155\n",
      "\n",
      " Learning rate 0.0005305912723329441, Model learning rate 0.0005305912927724421\n",
      "156/391 [==========>...................] - ETA: 17s - loss: 1.1363 - acc: 0.6477\n",
      " Optimizer iteration 938, batch 156\n",
      "\n",
      " Learning rate 0.0005297872054998662, Model learning rate 0.0005297872121445835\n",
      "\n",
      " Optimizer iteration 939, batch 157\n",
      "\n",
      " Learning rate 0.0005289830613524241, Model learning rate 0.0005289830733090639\n",
      "158/391 [===========>..................] - ETA: 17s - loss: 1.1351 - acc: 0.6481\n",
      " Optimizer iteration 940, batch 158\n",
      "\n",
      " Learning rate 0.0005281788419778188, Model learning rate 0.0005281788180582225\n",
      "\n",
      " Optimizer iteration 941, batch 159\n",
      "\n",
      " Learning rate 0.0005273745494634467, Model learning rate 0.0005273745628073812\n",
      "160/391 [===========>..................] - ETA: 17s - loss: 1.1348 - acc: 0.6482\n",
      " Optimizer iteration 942, batch 160\n",
      "\n",
      " Learning rate 0.0005265701858968943, Model learning rate 0.000526570191141218\n",
      "\n",
      " Optimizer iteration 943, batch 161\n",
      "\n",
      " Learning rate 0.0005257657533659325, Model learning rate 0.0005257657612673938\n",
      "162/391 [===========>..................] - ETA: 17s - loss: 1.1341 - acc: 0.6483\n",
      " Optimizer iteration 944, batch 162\n",
      "\n",
      " Learning rate 0.0005249612539585113, Model learning rate 0.0005249612731859088\n",
      "163/391 [===========>..................] - ETA: 17s - loss: 1.1334 - acc: 0.6486\n",
      " Optimizer iteration 945, batch 163\n",
      "\n",
      " Learning rate 0.0005241566897627535, Model learning rate 0.0005241566686891019\n",
      "164/391 [===========>..................] - ETA: 17s - loss: 1.1331 - acc: 0.6489\n",
      " Optimizer iteration 946, batch 164\n",
      "\n",
      " Learning rate 0.0005233520628669512, Model learning rate 0.0005233520641922951\n",
      "165/391 [===========>..................] - ETA: 16s - loss: 1.1336 - acc: 0.6484\n",
      " Optimizer iteration 947, batch 165\n",
      "\n",
      " Learning rate 0.0005225473753595585, Model learning rate 0.0005225474014878273\n",
      "\n",
      " Optimizer iteration 948, batch 166\n",
      "\n",
      " Learning rate 0.0005217426293291868, Model learning rate 0.0005217426223680377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/391 [===========>..................] - ETA: 16s - loss: 1.1330 - acc: 0.6483\n",
      " Optimizer iteration 949, batch 167\n",
      "\n",
      " Learning rate 0.0005209378268645998, Model learning rate 0.0005209378432482481\n",
      "168/391 [===========>..................] - ETA: 16s - loss: 1.1330 - acc: 0.6484\n",
      " Optimizer iteration 950, batch 168\n",
      "\n",
      " Learning rate 0.0005201329700547076, Model learning rate 0.0005201329477131367\n",
      "169/391 [===========>..................] - ETA: 16s - loss: 1.1341 - acc: 0.6480\n",
      " Optimizer iteration 951, batch 169\n",
      "\n",
      " Learning rate 0.0005193280609885611, Model learning rate 0.0005193280521780252\n",
      "170/391 [============>.................] - ETA: 16s - loss: 1.1337 - acc: 0.6480\n",
      " Optimizer iteration 952, batch 170\n",
      "\n",
      " Learning rate 0.000518523101755347, Model learning rate 0.0005185230984352529\n",
      "171/391 [============>.................] - ETA: 16s - loss: 1.1337 - acc: 0.6481\n",
      " Optimizer iteration 953, batch 171\n",
      "\n",
      " Learning rate 0.0005177180944443821, Model learning rate 0.0005177180864848197\n",
      "172/391 [============>.................] - ETA: 16s - loss: 1.1335 - acc: 0.6482\n",
      " Optimizer iteration 954, batch 172\n",
      "\n",
      " Learning rate 0.0005169130411451083, Model learning rate 0.0005169130163267255\n",
      "\n",
      " Optimizer iteration 955, batch 173\n",
      "\n",
      " Learning rate 0.0005161079439470866, Model learning rate 0.0005161079461686313\n",
      "174/391 [============>.................] - ETA: 16s - loss: 1.1336 - acc: 0.6479\n",
      " Optimizer iteration 956, batch 174\n",
      "\n",
      " Learning rate 0.0005153028049399916, Model learning rate 0.0005153028178028762\n",
      "175/391 [============>.................] - ETA: 16s - loss: 1.1338 - acc: 0.6476\n",
      " Optimizer iteration 957, batch 175\n",
      "\n",
      " Learning rate 0.0005144976262136073, Model learning rate 0.0005144976312294602\n",
      "176/391 [============>.................] - ETA: 16s - loss: 1.1325 - acc: 0.6483\n",
      " Optimizer iteration 958, batch 176\n",
      "\n",
      " Learning rate 0.00051369240985782, Model learning rate 0.0005136923864483833\n",
      "177/391 [============>.................] - ETA: 16s - loss: 1.1322 - acc: 0.6485\n",
      " Optimizer iteration 959, batch 177\n",
      "\n",
      " Learning rate 0.0005128871579626142, Model learning rate 0.0005128871416673064\n",
      "178/391 [============>.................] - ETA: 15s - loss: 1.1321 - acc: 0.6487\n",
      " Optimizer iteration 960, batch 178\n",
      "\n",
      " Learning rate 0.0005120818726180662, Model learning rate 0.0005120818968862295\n",
      "\n",
      " Optimizer iteration 961, batch 179\n",
      "\n",
      " Learning rate 0.0005112765559143394, Model learning rate 0.0005112765356898308\n",
      "180/391 [============>.................] - ETA: 15s - loss: 1.1314 - acc: 0.6490\n",
      " Optimizer iteration 962, batch 180\n",
      "\n",
      " Learning rate 0.0005104712099416785, Model learning rate 0.000510471232701093\n",
      "181/391 [============>.................] - ETA: 15s - loss: 1.1317 - acc: 0.6491\n",
      " Optimizer iteration 963, batch 181\n",
      "\n",
      " Learning rate 0.0005096658367904042, Model learning rate 0.0005096658132970333\n",
      "182/391 [============>.................] - ETA: 15s - loss: 1.1317 - acc: 0.6492\n",
      " Optimizer iteration 964, batch 182\n",
      "\n",
      " Learning rate 0.0005088604385509079, Model learning rate 0.0005088604521006346\n",
      "183/391 [=============>................] - ETA: 15s - loss: 1.1314 - acc: 0.6492\n",
      " Optimizer iteration 965, batch 183\n",
      "\n",
      " Learning rate 0.0005080550173136456, Model learning rate 0.0005080550326965749\n",
      "184/391 [=============>................] - ETA: 15s - loss: 1.1316 - acc: 0.6489\n",
      " Optimizer iteration 966, batch 184\n",
      "\n",
      " Learning rate 0.0005072495751691338, Model learning rate 0.0005072495550848544\n",
      "185/391 [=============>................] - ETA: 15s - loss: 1.1315 - acc: 0.6491\n",
      " Optimizer iteration 967, batch 185\n",
      "\n",
      " Learning rate 0.0005064441142079425, Model learning rate 0.0005064441356807947\n",
      "\n",
      " Optimizer iteration 968, batch 186\n",
      "\n",
      " Learning rate 0.0005056386365206907, Model learning rate 0.0005056386580690742\n",
      "187/391 [=============>................] - ETA: 15s - loss: 1.1315 - acc: 0.6491\n",
      " Optimizer iteration 969, batch 187\n",
      "\n",
      " Learning rate 0.0005048331441980416, Model learning rate 0.0005048331222496927\n",
      "188/391 [=============>................] - ETA: 15s - loss: 1.1305 - acc: 0.6498\n",
      " Optimizer iteration 970, batch 188\n",
      "\n",
      " Learning rate 0.0005040276393306949, Model learning rate 0.0005040276446379721\n",
      "189/391 [=============>................] - ETA: 15s - loss: 1.1305 - acc: 0.6496\n",
      " Optimizer iteration 971, batch 189\n",
      "\n",
      " Learning rate 0.0005032221240093846, Model learning rate 0.0005032221088185906\n",
      "190/391 [=============>................] - ETA: 15s - loss: 1.1301 - acc: 0.6497\n",
      " Optimizer iteration 972, batch 190\n",
      "\n",
      " Learning rate 0.0005024166003248702, Model learning rate 0.0005024165729992092\n",
      "191/391 [=============>................] - ETA: 15s - loss: 1.1299 - acc: 0.6497\n",
      " Optimizer iteration 973, batch 191\n",
      "\n",
      " Learning rate 0.000501611070367934, Model learning rate 0.0005016110953874886\n",
      "192/391 [=============>................] - ETA: 14s - loss: 1.1300 - acc: 0.6496\n",
      " Optimizer iteration 974, batch 192\n",
      "\n",
      " Learning rate 0.0005008055362293743, Model learning rate 0.0005008055595681071\n",
      "193/391 [=============>................] - ETA: 14s - loss: 1.1297 - acc: 0.6499\n",
      " Optimizer iteration 975, batch 193\n",
      "\n",
      " Learning rate 0.0005, Model learning rate 0.0005000000237487257\n",
      "\n",
      " Optimizer iteration 976, batch 194\n",
      "\n",
      " Learning rate 0.0004991944637706257, Model learning rate 0.0004991944879293442\n",
      "195/391 [=============>................] - ETA: 14s - loss: 1.1299 - acc: 0.6500\n",
      " Optimizer iteration 977, batch 195\n",
      "\n",
      " Learning rate 0.000498388929632066, Model learning rate 0.0004983889521099627\n",
      "196/391 [==============>...............] - ETA: 14s - loss: 1.1302 - acc: 0.6498\n",
      " Optimizer iteration 978, batch 196\n",
      "\n",
      " Learning rate 0.0004975833996751299, Model learning rate 0.0004975834162905812\n",
      "197/391 [==============>...............] - ETA: 14s - loss: 1.1300 - acc: 0.6497\n",
      " Optimizer iteration 979, batch 197\n",
      "\n",
      " Learning rate 0.0004967778759906157, Model learning rate 0.0004967778804711998\n",
      "\n",
      " Optimizer iteration 980, batch 198\n",
      "\n",
      " Learning rate 0.0004959723606693051, Model learning rate 0.0004959723446518183\n",
      "199/391 [==============>...............] - ETA: 14s - loss: 1.1296 - acc: 0.6500\n",
      " Optimizer iteration 981, batch 199\n",
      "\n",
      " Learning rate 0.0004951668558019585, Model learning rate 0.0004951668670400977\n",
      "200/391 [==============>...............] - ETA: 14s - loss: 1.1300 - acc: 0.6498\n",
      " Optimizer iteration 982, batch 200\n",
      "\n",
      " Learning rate 0.0004943613634793092, Model learning rate 0.0004943613894283772\n",
      "201/391 [==============>...............] - ETA: 14s - loss: 1.1305 - acc: 0.6494\n",
      " Optimizer iteration 983, batch 201\n",
      "\n",
      " Learning rate 0.0004935558857920576, Model learning rate 0.0004935559118166566\n",
      "202/391 [==============>...............] - ETA: 14s - loss: 1.1301 - acc: 0.6498\n",
      " Optimizer iteration 984, batch 202\n",
      "\n",
      " Learning rate 0.0004927504248308663, Model learning rate 0.000492750434204936\n",
      "\n",
      " Optimizer iteration 985, batch 203\n",
      "\n",
      " Learning rate 0.0004919449826863544, Model learning rate 0.0004919449565932155\n",
      "204/391 [==============>...............] - ETA: 14s - loss: 1.1305 - acc: 0.6495\n",
      " Optimizer iteration 986, batch 204\n",
      "\n",
      " Learning rate 0.0004911395614490922, Model learning rate 0.0004911395371891558\n",
      "205/391 [==============>...............] - ETA: 14s - loss: 1.1313 - acc: 0.6493\n",
      " Optimizer iteration 987, batch 205\n",
      "\n",
      " Learning rate 0.0004903341632095958, Model learning rate 0.0004903341759927571\n",
      "206/391 [==============>...............] - ETA: 13s - loss: 1.1314 - acc: 0.6492\n",
      " Optimizer iteration 988, batch 206\n",
      "\n",
      " Learning rate 0.0004895287900583215, Model learning rate 0.0004895288147963583\n",
      "\n",
      " Optimizer iteration 989, batch 207\n",
      "\n",
      " Learning rate 0.0004887234440856608, Model learning rate 0.0004887234535999596\n",
      "208/391 [==============>...............] - ETA: 13s - loss: 1.1319 - acc: 0.6489\n",
      " Optimizer iteration 990, batch 208\n",
      "\n",
      " Learning rate 0.000487918127381934, Model learning rate 0.00048791812150739133\n",
      "209/391 [===============>..............] - ETA: 13s - loss: 1.1308 - acc: 0.6494\n",
      " Optimizer iteration 991, batch 209\n",
      "\n",
      " Learning rate 0.0004871128420373859, Model learning rate 0.00048711284762248397\n",
      "210/391 [===============>..............] - ETA: 13s - loss: 1.1306 - acc: 0.6496\n",
      " Optimizer iteration 992, batch 210\n",
      "\n",
      " Learning rate 0.00048630759014218, Model learning rate 0.00048630760284140706\n",
      "\n",
      " Optimizer iteration 993, batch 211\n",
      "\n",
      " Learning rate 0.00048550237378639275, Model learning rate 0.0004855023871641606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/391 [===============>..............] - ETA: 13s - loss: 1.1306 - acc: 0.6494\n",
      " Optimizer iteration 994, batch 212\n",
      "\n",
      " Learning rate 0.00048469719506000837, Model learning rate 0.0004846972005907446\n",
      "213/391 [===============>..............] - ETA: 13s - loss: 1.1303 - acc: 0.6495\n",
      " Optimizer iteration 995, batch 213\n",
      "\n",
      " Learning rate 0.00048389205605291365, Model learning rate 0.0004838920431211591\n",
      "214/391 [===============>..............] - ETA: 13s - loss: 1.1310 - acc: 0.6492\n",
      " Optimizer iteration 996, batch 214\n",
      "\n",
      " Learning rate 0.0004830869588548918, Model learning rate 0.0004830869729630649\n",
      "215/391 [===============>..............] - ETA: 13s - loss: 1.1310 - acc: 0.6492\n",
      " Optimizer iteration 997, batch 215\n",
      "\n",
      " Learning rate 0.0004822819055556179, Model learning rate 0.00048228190280497074\n",
      "216/391 [===============>..............] - ETA: 13s - loss: 1.1308 - acc: 0.6494\n",
      " Optimizer iteration 998, batch 216\n",
      "\n",
      " Learning rate 0.00048147689824465313, Model learning rate 0.0004814768908545375\n",
      "217/391 [===============>..............] - ETA: 13s - loss: 1.1310 - acc: 0.6495\n",
      " Optimizer iteration 999, batch 217\n",
      "\n",
      " Learning rate 0.00048067193901143887, Model learning rate 0.00048067193711176515\n",
      "218/391 [===============>..............] - ETA: 13s - loss: 1.1309 - acc: 0.6495\n",
      " Optimizer iteration 1000, batch 218\n",
      "\n",
      " Learning rate 0.0004798670299452926, Model learning rate 0.0004798670415766537\n",
      "219/391 [===============>..............] - ETA: 12s - loss: 1.1310 - acc: 0.6494\n",
      " Optimizer iteration 1001, batch 219\n",
      "\n",
      " Learning rate 0.0004790621731354003, Model learning rate 0.00047906217514537275\n",
      "220/391 [===============>..............] - ETA: 12s - loss: 1.1312 - acc: 0.6493\n",
      " Optimizer iteration 1002, batch 220\n",
      "\n",
      " Learning rate 0.00047825737067081327, Model learning rate 0.0004782573669217527\n",
      "221/391 [===============>..............] - ETA: 12s - loss: 1.1313 - acc: 0.6493\n",
      " Optimizer iteration 1003, batch 221\n",
      "\n",
      " Learning rate 0.00047745262464044165, Model learning rate 0.00047745261690579355\n",
      "222/391 [================>.............] - ETA: 12s - loss: 1.1310 - acc: 0.6495\n",
      " Optimizer iteration 1004, batch 222\n",
      "\n",
      " Learning rate 0.0004766479371330488, Model learning rate 0.0004766479250974953\n",
      "\n",
      " Optimizer iteration 1005, batch 223\n",
      "\n",
      " Learning rate 0.00047584331023724653, Model learning rate 0.00047584332060068846\n",
      "224/391 [================>.............] - ETA: 12s - loss: 1.1305 - acc: 0.6494\n",
      " Optimizer iteration 1006, batch 224\n",
      "\n",
      " Learning rate 0.0004750387460414889, Model learning rate 0.00047503874520771205\n",
      "225/391 [================>.............] - ETA: 12s - loss: 1.1307 - acc: 0.6493\n",
      " Optimizer iteration 1007, batch 225\n",
      "\n",
      " Learning rate 0.00047423424663406747, Model learning rate 0.000474234257126227\n",
      "226/391 [================>.............] - ETA: 12s - loss: 1.1309 - acc: 0.6493\n",
      " Optimizer iteration 1008, batch 226\n",
      "\n",
      " Learning rate 0.0004734298141031057, Model learning rate 0.0004734298272524029\n",
      "227/391 [================>.............] - ETA: 12s - loss: 1.1305 - acc: 0.6495\n",
      " Optimizer iteration 1009, batch 227\n",
      "\n",
      " Learning rate 0.00047262545053655344, Model learning rate 0.0004726254555862397\n",
      "228/391 [================>.............] - ETA: 12s - loss: 1.1297 - acc: 0.6499\n",
      " Optimizer iteration 1010, batch 228\n",
      "\n",
      " Learning rate 0.0004718211580221812, Model learning rate 0.00047182117123156786\n",
      "229/391 [================>.............] - ETA: 12s - loss: 1.1293 - acc: 0.6502\n",
      " Optimizer iteration 1011, batch 229\n",
      "\n",
      " Learning rate 0.00047101693864757605, Model learning rate 0.00047101694508455694\n",
      "\n",
      " Optimizer iteration 1012, batch 230\n",
      "\n",
      " Learning rate 0.00047021279450013383, Model learning rate 0.0004702128062490374\n",
      "231/391 [================>.............] - ETA: 12s - loss: 1.1287 - acc: 0.6505\n",
      " Optimizer iteration 1013, batch 231\n",
      "\n",
      " Learning rate 0.000469408727667056, Model learning rate 0.00046940872562117875\n",
      "232/391 [================>.............] - ETA: 12s - loss: 1.1284 - acc: 0.6507\n",
      " Optimizer iteration 1014, batch 232\n",
      "\n",
      " Learning rate 0.0004686047402353433, Model learning rate 0.0004686047323048115\n",
      "233/391 [================>.............] - ETA: 11s - loss: 1.1282 - acc: 0.6505\n",
      " Optimizer iteration 1015, batch 233\n",
      "\n",
      " Learning rate 0.00046780083429179026, Model learning rate 0.0004678008262999356\n",
      "\n",
      " Optimizer iteration 1016, batch 234\n",
      "\n",
      " Learning rate 0.00046699701192297994, Model learning rate 0.00046699700760655105\n",
      "235/391 [=================>............] - ETA: 11s - loss: 1.1296 - acc: 0.6499\n",
      " Optimizer iteration 1017, batch 235\n",
      "\n",
      " Learning rate 0.00046619327521527825, Model learning rate 0.0004661932762246579\n",
      "236/391 [=================>............] - ETA: 11s - loss: 1.1294 - acc: 0.6502\n",
      " Optimizer iteration 1018, batch 236\n",
      "\n",
      " Learning rate 0.00046538962625482905, Model learning rate 0.0004653896321542561\n",
      "237/391 [=================>............] - ETA: 11s - loss: 1.1294 - acc: 0.6499\n",
      " Optimizer iteration 1019, batch 237\n",
      "\n",
      " Learning rate 0.0004645860671275483, Model learning rate 0.0004645860753953457\n",
      "\n",
      " Optimizer iteration 1020, batch 238\n",
      "\n",
      " Learning rate 0.00046378259991911887, Model learning rate 0.00046378260594792664\n",
      "239/391 [=================>............] - ETA: 11s - loss: 1.1295 - acc: 0.6503\n",
      " Optimizer iteration 1021, batch 239\n",
      "\n",
      " Learning rate 0.0004629792267149849, Model learning rate 0.00046297922381199896\n",
      "240/391 [=================>............] - ETA: 11s - loss: 1.1295 - acc: 0.6503\n",
      " Optimizer iteration 1022, batch 240\n",
      "\n",
      " Learning rate 0.00046217594960034714, Model learning rate 0.0004621759580913931\n",
      "241/391 [=================>............] - ETA: 11s - loss: 1.1300 - acc: 0.6502\n",
      " Optimizer iteration 1023, batch 241\n",
      "\n",
      " Learning rate 0.0004613727706601557, Model learning rate 0.00046137277968227863\n",
      "\n",
      " Optimizer iteration 1024, batch 242\n",
      "\n",
      " Learning rate 0.00046056969197910707, Model learning rate 0.0004605696885846555\n",
      "243/391 [=================>............] - ETA: 11s - loss: 1.1289 - acc: 0.6507\n",
      " Optimizer iteration 1025, batch 243\n",
      "\n",
      " Learning rate 0.00045976671564163706, Model learning rate 0.00045976671390235424\n",
      "244/391 [=================>............] - ETA: 11s - loss: 1.1283 - acc: 0.6508\n",
      " Optimizer iteration 1026, batch 244\n",
      "\n",
      " Learning rate 0.0004589638437319157, Model learning rate 0.0004589638556353748\n",
      "245/391 [=================>............] - ETA: 11s - loss: 1.1277 - acc: 0.6512\n",
      " Optimizer iteration 1027, batch 245\n",
      "\n",
      " Learning rate 0.00045816107833384235, Model learning rate 0.0004581610846798867\n",
      "\n",
      " Optimizer iteration 1028, batch 246\n",
      "\n",
      " Learning rate 0.00045735842153103934, Model learning rate 0.00045735843013972044\n",
      "247/391 [=================>............] - ETA: 10s - loss: 1.1272 - acc: 0.6513\n",
      " Optimizer iteration 1029, batch 247\n",
      "\n",
      " Learning rate 0.0004565558754068477, Model learning rate 0.00045655586291104555\n",
      "248/391 [==================>...........] - ETA: 10s - loss: 1.1272 - acc: 0.6512\n",
      " Optimizer iteration 1030, batch 248\n",
      "\n",
      " Learning rate 0.00045575344204432084, Model learning rate 0.00045575344120152295\n",
      "249/391 [==================>...........] - ETA: 10s - loss: 1.1271 - acc: 0.6514\n",
      " Optimizer iteration 1031, batch 249\n",
      "\n",
      " Learning rate 0.00045495112352621957, Model learning rate 0.00045495113590732217\n",
      "250/391 [==================>...........] - ETA: 10s - loss: 1.1266 - acc: 0.6515\n",
      " Optimizer iteration 1032, batch 250\n",
      "\n",
      " Learning rate 0.0004541489219350069, Model learning rate 0.00045414891792461276\n",
      "\n",
      " Optimizer iteration 1033, batch 251\n",
      "\n",
      " Learning rate 0.0004533468393528421, Model learning rate 0.00045334684546105564\n",
      "252/391 [==================>...........] - ETA: 10s - loss: 1.1270 - acc: 0.6511\n",
      " Optimizer iteration 1034, batch 252\n",
      "\n",
      " Learning rate 0.0004525448778615752, Model learning rate 0.00045254488941282034\n",
      "253/391 [==================>...........] - ETA: 10s - loss: 1.1263 - acc: 0.6515\n",
      " Optimizer iteration 1035, batch 253\n",
      "\n",
      " Learning rate 0.00045174303954274245, Model learning rate 0.00045174304977990687\n",
      "\n",
      " Optimizer iteration 1036, batch 254\n",
      "\n",
      " Learning rate 0.0004509413264775604, Model learning rate 0.0004509413265623152\n",
      "255/391 [==================>...........] - ETA: 10s - loss: 1.1249 - acc: 0.6519\n",
      " Optimizer iteration 1037, batch 255\n",
      "\n",
      " Learning rate 0.0004501397407469201, Model learning rate 0.00045013974886387587\n",
      "256/391 [==================>...........] - ETA: 10s - loss: 1.1244 - acc: 0.6520\n",
      " Optimizer iteration 1038, batch 256\n",
      "\n",
      " Learning rate 0.0004493382844313826, Model learning rate 0.00044933828758075833\n",
      "257/391 [==================>...........] - ETA: 10s - loss: 1.1239 - acc: 0.6523\n",
      " Optimizer iteration 1039, batch 257\n",
      "\n",
      " Learning rate 0.0004485369596111724, Model learning rate 0.0004485369718167931\n",
      "258/391 [==================>...........] - ETA: 10s - loss: 1.1239 - acc: 0.6522\n",
      " Optimizer iteration 1040, batch 258\n",
      "\n",
      " Learning rate 0.00044773576836617336, Model learning rate 0.00044773577246814966\n",
      "259/391 [==================>...........] - ETA: 9s - loss: 1.1235 - acc: 0.6525 \n",
      " Optimizer iteration 1041, batch 259\n",
      "\n",
      " Learning rate 0.0004469347127759222, Model learning rate 0.0004469347186386585\n",
      "260/391 [==================>...........] - ETA: 9s - loss: 1.1235 - acc: 0.6526\n",
      " Optimizer iteration 1042, batch 260\n",
      "\n",
      " Learning rate 0.0004461337949196036, Model learning rate 0.0004461337812244892\n",
      "261/391 [===================>..........] - ETA: 9s - loss: 1.1227 - acc: 0.6530\n",
      " Optimizer iteration 1043, batch 261\n",
      "\n",
      " Learning rate 0.0004453330168760451, Model learning rate 0.00044533301843330264\n",
      "\n",
      " Optimizer iteration 1044, batch 262\n",
      "\n",
      " Learning rate 0.00044453238072371116, Model learning rate 0.0004445323720574379\n",
      "263/391 [===================>..........] - ETA: 9s - loss: 1.1212 - acc: 0.6538\n",
      " Optimizer iteration 1045, batch 263\n",
      "\n",
      " Learning rate 0.0004437318885406973, Model learning rate 0.0004437319003045559\n",
      "264/391 [===================>..........] - ETA: 9s - loss: 1.1206 - acc: 0.6541\n",
      " Optimizer iteration 1046, batch 264\n",
      "\n",
      " Learning rate 0.0004429315424047263, Model learning rate 0.0004429315449669957\n",
      "265/391 [===================>..........] - ETA: 9s - loss: 1.1207 - acc: 0.6540\n",
      " Optimizer iteration 1047, batch 265\n",
      "\n",
      " Learning rate 0.0004421313443931416, Model learning rate 0.0004421313351485878\n",
      "266/391 [===================>..........] - ETA: 9s - loss: 1.1204 - acc: 0.6542\n",
      " Optimizer iteration 1048, batch 266\n",
      "\n",
      " Learning rate 0.00044133129658290195, Model learning rate 0.00044133129995316267\n",
      "\n",
      " Optimizer iteration 1049, batch 267\n",
      "\n",
      " Learning rate 0.00044053140105057635, Model learning rate 0.0004405314102768898\n",
      "268/391 [===================>..........] - ETA: 9s - loss: 1.1196 - acc: 0.6543\n",
      " Optimizer iteration 1050, batch 268\n",
      "\n",
      " Learning rate 0.00043973165987233853, Model learning rate 0.0004397316661197692\n",
      "\n",
      " Optimizer iteration 1051, batch 269\n",
      "\n",
      " Learning rate 0.0004389320751239617, Model learning rate 0.0004389320674818009\n",
      "270/391 [===================>..........] - ETA: 9s - loss: 1.1190 - acc: 0.6544\n",
      " Optimizer iteration 1052, batch 270\n",
      "\n",
      " Learning rate 0.00043813264888081284, Model learning rate 0.00043813264346681535\n",
      "\n",
      " Optimizer iteration 1053, batch 271\n",
      "\n",
      " Learning rate 0.00043733338321784784, Model learning rate 0.00043733339407481253\n",
      "272/391 [===================>..........] - ETA: 9s - loss: 1.1189 - acc: 0.6542\n",
      " Optimizer iteration 1054, batch 272\n",
      "\n",
      " Learning rate 0.0004365342802096057, Model learning rate 0.000436534290201962\n",
      "\n",
      " Optimizer iteration 1055, batch 273\n",
      "\n",
      " Learning rate 0.00043573534193020277, Model learning rate 0.00043573533184826374\n",
      "274/391 [====================>.........] - ETA: 8s - loss: 1.1189 - acc: 0.6543\n",
      " Optimizer iteration 1056, batch 274\n",
      "\n",
      " Learning rate 0.0004349365704533284, Model learning rate 0.0004349365772213787\n",
      "275/391 [====================>.........] - ETA: 8s - loss: 1.1184 - acc: 0.6546\n",
      " Optimizer iteration 1057, batch 275\n",
      "\n",
      " Learning rate 0.0004341379678522389, Model learning rate 0.0004341379681136459\n",
      "276/391 [====================>.........] - ETA: 8s - loss: 1.1179 - acc: 0.6549\n",
      " Optimizer iteration 1058, batch 276\n",
      "\n",
      " Learning rate 0.00043333953619975206, Model learning rate 0.0004333395336288959\n",
      "277/391 [====================>.........] - ETA: 8s - loss: 1.1171 - acc: 0.6552\n",
      " Optimizer iteration 1059, batch 277\n",
      "\n",
      " Learning rate 0.00043254127756824214, Model learning rate 0.0004325412737671286\n",
      "\n",
      " Optimizer iteration 1060, batch 278\n",
      "\n",
      " Learning rate 0.0004317431940296343, Model learning rate 0.00043174318852834404\n",
      "279/391 [====================>.........] - ETA: 8s - loss: 1.1164 - acc: 0.6555\n",
      " Optimizer iteration 1061, batch 279\n",
      "\n",
      " Learning rate 0.000430945287655399, Model learning rate 0.0004309452779125422\n",
      "280/391 [====================>.........] - ETA: 8s - loss: 1.1158 - acc: 0.6558\n",
      " Optimizer iteration 1062, batch 280\n",
      "\n",
      " Learning rate 0.00043014756051654705, Model learning rate 0.0004301475710235536\n",
      "281/391 [====================>.........] - ETA: 8s - loss: 1.1153 - acc: 0.6560\n",
      " Optimizer iteration 1063, batch 281\n",
      "\n",
      " Learning rate 0.00042935001468362405, Model learning rate 0.0004293500096537173\n",
      "\n",
      " Optimizer iteration 1064, batch 282\n",
      "\n",
      " Learning rate 0.00042855265222670517, Model learning rate 0.00042855265201069415\n",
      "283/391 [====================>.........] - ETA: 8s - loss: 1.1150 - acc: 0.6561\n",
      " Optimizer iteration 1065, batch 283\n",
      "\n",
      " Learning rate 0.0004277554752153895, Model learning rate 0.00042775546899065375\n",
      "284/391 [====================>.........] - ETA: 8s - loss: 1.1151 - acc: 0.6562\n",
      " Optimizer iteration 1066, batch 284\n",
      "\n",
      " Learning rate 0.00042695848571879425, Model learning rate 0.00042695848969742656\n",
      "285/391 [====================>.........] - ETA: 8s - loss: 1.1152 - acc: 0.6563\n",
      " Optimizer iteration 1067, batch 285\n",
      "\n",
      " Learning rate 0.0004261616858055508, Model learning rate 0.0004261616850271821\n",
      "286/391 [====================>.........] - ETA: 7s - loss: 1.1150 - acc: 0.6564\n",
      " Optimizer iteration 1068, batch 286\n",
      "\n",
      " Learning rate 0.000425365077543798, Model learning rate 0.00042536508408375084\n",
      "287/391 [=====================>........] - ETA: 7s - loss: 1.1150 - acc: 0.6564\n",
      " Optimizer iteration 1069, batch 287\n",
      "\n",
      " Learning rate 0.00042456866300117724, Model learning rate 0.0004245686577633023\n",
      "288/391 [=====================>........] - ETA: 7s - loss: 1.1151 - acc: 0.6564\n",
      " Optimizer iteration 1070, batch 288\n",
      "\n",
      " Learning rate 0.0004237724442448273, Model learning rate 0.000423772435169667\n",
      "\n",
      " Optimizer iteration 1071, batch 289\n",
      "\n",
      " Learning rate 0.00042297642334137875, Model learning rate 0.0004229764163028449\n",
      "290/391 [=====================>........] - ETA: 7s - loss: 1.1151 - acc: 0.6565\n",
      " Optimizer iteration 1072, batch 290\n",
      "\n",
      " Learning rate 0.00042218060235694826, Model learning rate 0.00042218060116283596\n",
      "\n",
      " Optimizer iteration 1073, batch 291\n",
      "\n",
      " Learning rate 0.000421384983357134, Model learning rate 0.0004213849897496402\n",
      "292/391 [=====================>........] - ETA: 7s - loss: 1.1154 - acc: 0.6565\n",
      " Optimizer iteration 1074, batch 292\n",
      "\n",
      " Learning rate 0.0004205895684070098, Model learning rate 0.0004205895820632577\n",
      "293/391 [=====================>........] - ETA: 7s - loss: 1.1150 - acc: 0.6566\n",
      " Optimizer iteration 1075, batch 293\n",
      "\n",
      " Learning rate 0.0004197943595711198, Model learning rate 0.0004197943489998579\n",
      "\n",
      " Optimizer iteration 1076, batch 294\n",
      "\n",
      " Learning rate 0.0004189993589134735, Model learning rate 0.00041899934876710176\n",
      "295/391 [=====================>........] - ETA: 7s - loss: 1.1141 - acc: 0.6569\n",
      " Optimizer iteration 1077, batch 295\n",
      "\n",
      " Learning rate 0.0004182045684975391, Model learning rate 0.0004182045813649893\n",
      "296/391 [=====================>........] - ETA: 7s - loss: 1.1146 - acc: 0.6569\n",
      " Optimizer iteration 1078, batch 296\n",
      "\n",
      " Learning rate 0.0004174099903862403, Model learning rate 0.00041740998858585954\n",
      "297/391 [=====================>........] - ETA: 7s - loss: 1.1143 - acc: 0.6571\n",
      " Optimizer iteration 1079, batch 297\n",
      "\n",
      " Learning rate 0.0004166156266419489, Model learning rate 0.00041661562863737345\n",
      "298/391 [=====================>........] - ETA: 7s - loss: 1.1142 - acc: 0.6571\n",
      " Optimizer iteration 1080, batch 298\n",
      "\n",
      " Learning rate 0.0004158214793264807, Model learning rate 0.00041582147241570055\n",
      "299/391 [=====================>........] - ETA: 6s - loss: 1.1138 - acc: 0.6574\n",
      " Optimizer iteration 1081, batch 299\n",
      "\n",
      " Learning rate 0.00041502755050108975, Model learning rate 0.0004150275490246713\n",
      "300/391 [======================>.......] - ETA: 6s - loss: 1.1136 - acc: 0.6575\n",
      " Optimizer iteration 1082, batch 300\n",
      "\n",
      " Learning rate 0.00041423384222646285, Model learning rate 0.0004142338293604553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/391 [======================>.......] - ETA: 6s - loss: 1.1133 - acc: 0.6575\n",
      " Optimizer iteration 1083, batch 301\n",
      "\n",
      " Learning rate 0.00041344035656271436, Model learning rate 0.0004134403425268829\n",
      "302/391 [======================>.......] - ETA: 6s - loss: 1.1126 - acc: 0.6578\n",
      " Optimizer iteration 1084, batch 302\n",
      "\n",
      " Learning rate 0.0004126470955693806, Model learning rate 0.00041264708852395415\n",
      "303/391 [======================>.......] - ETA: 6s - loss: 1.1128 - acc: 0.6577\n",
      " Optimizer iteration 1085, batch 303\n",
      "\n",
      " Learning rate 0.0004118540613054155, Model learning rate 0.0004118540673516691\n",
      "304/391 [======================>.......] - ETA: 6s - loss: 1.1130 - acc: 0.6577\n",
      " Optimizer iteration 1086, batch 304\n",
      "\n",
      " Learning rate 0.00041106125582918385, Model learning rate 0.0004110612499061972\n",
      "305/391 [======================>.......] - ETA: 6s - loss: 1.1126 - acc: 0.6578\n",
      " Optimizer iteration 1087, batch 305\n",
      "\n",
      " Learning rate 0.0004102686811984568, Model learning rate 0.0004102686943951994\n",
      "306/391 [======================>.......] - ETA: 6s - loss: 1.1125 - acc: 0.6576\n",
      " Optimizer iteration 1088, batch 306\n",
      "\n",
      " Learning rate 0.00040947633947040614, Model learning rate 0.00040947634261101484\n",
      "\n",
      " Optimizer iteration 1089, batch 307\n",
      "\n",
      " Learning rate 0.00040868423270159945, Model learning rate 0.0004086842236574739\n",
      "308/391 [======================>.......] - ETA: 6s - loss: 1.1124 - acc: 0.6577\n",
      " Optimizer iteration 1090, batch 308\n",
      "\n",
      " Learning rate 0.0004078923629479943, Model learning rate 0.0004078923666384071\n",
      "309/391 [======================>.......] - ETA: 6s - loss: 1.1123 - acc: 0.6578\n",
      " Optimizer iteration 1091, batch 309\n",
      "\n",
      " Learning rate 0.00040710073226493307, Model learning rate 0.00040710074244998395\n",
      "310/391 [======================>.......] - ETA: 6s - loss: 1.1127 - acc: 0.6578\n",
      " Optimizer iteration 1092, batch 310\n",
      "\n",
      " Learning rate 0.0004063093427071376, Model learning rate 0.00040630935109220445\n",
      "311/391 [======================>.......] - ETA: 6s - loss: 1.1126 - acc: 0.6579\n",
      " Optimizer iteration 1093, batch 311\n",
      "\n",
      " Learning rate 0.00040551819632870433, Model learning rate 0.0004055181925650686\n",
      "312/391 [======================>.......] - ETA: 6s - loss: 1.1127 - acc: 0.6578\n",
      " Optimizer iteration 1094, batch 312\n",
      "\n",
      " Learning rate 0.0004047272951830976, Model learning rate 0.00040472729597240686\n",
      "313/391 [=======================>......] - ETA: 5s - loss: 1.1124 - acc: 0.6578\n",
      " Optimizer iteration 1095, batch 313\n",
      "\n",
      " Learning rate 0.00040393664132314577, Model learning rate 0.0004039366322103888\n",
      "\n",
      " Optimizer iteration 1096, batch 314\n",
      "\n",
      " Learning rate 0.0004031462368010357, Model learning rate 0.0004031462303828448\n",
      "315/391 [=======================>......] - ETA: 5s - loss: 1.1121 - acc: 0.6581\n",
      " Optimizer iteration 1097, batch 315\n",
      "\n",
      " Learning rate 0.0004023560836683065, Model learning rate 0.00040235609048977494\n",
      "316/391 [=======================>......] - ETA: 5s - loss: 1.1122 - acc: 0.6579\n",
      " Optimizer iteration 1098, batch 316\n",
      "\n",
      " Learning rate 0.000401566183975845, Model learning rate 0.00040156618342734873\n",
      "317/391 [=======================>......] - ETA: 5s - loss: 1.1117 - acc: 0.6580\n",
      " Optimizer iteration 1099, batch 317\n",
      "\n",
      " Learning rate 0.00040077653977388015, Model learning rate 0.00040077653829939663\n",
      "318/391 [=======================>......] - ETA: 5s - loss: 1.1117 - acc: 0.6579\n",
      " Optimizer iteration 1100, batch 318\n",
      "\n",
      " Learning rate 0.0003999871531119779, Model learning rate 0.00039998715510591865\n",
      "319/391 [=======================>......] - ETA: 5s - loss: 1.1114 - acc: 0.6581\n",
      " Optimizer iteration 1101, batch 319\n",
      "\n",
      " Learning rate 0.00039919802603903553, Model learning rate 0.00039919803384691477\n",
      "320/391 [=======================>......] - ETA: 5s - loss: 1.1117 - acc: 0.6580\n",
      " Optimizer iteration 1102, batch 320\n",
      "\n",
      " Learning rate 0.0003984091606032768, Model learning rate 0.000398409174522385\n",
      "321/391 [=======================>......] - ETA: 5s - loss: 1.1114 - acc: 0.6580\n",
      " Optimizer iteration 1103, batch 321\n",
      "\n",
      " Learning rate 0.0003976205588522461, Model learning rate 0.0003976205480284989\n",
      "\n",
      " Optimizer iteration 1104, batch 322\n",
      "\n",
      " Learning rate 0.0003968322228328041, Model learning rate 0.00039683221257291734\n",
      "323/391 [=======================>......] - ETA: 5s - loss: 1.1115 - acc: 0.6580\n",
      " Optimizer iteration 1105, batch 323\n",
      "\n",
      " Learning rate 0.0003960441545911204, Model learning rate 0.00039604416815564036\n",
      "324/391 [=======================>......] - ETA: 5s - loss: 1.1113 - acc: 0.6582\n",
      " Optimizer iteration 1106, batch 324\n",
      "\n",
      " Learning rate 0.00039525635617267075, Model learning rate 0.00039525635656900704\n",
      "325/391 [=======================>......] - ETA: 5s - loss: 1.1110 - acc: 0.6583\n",
      " Optimizer iteration 1107, batch 325\n",
      "\n",
      " Learning rate 0.00039446882962223027, Model learning rate 0.0003944688360206783\n",
      "326/391 [========================>.....] - ETA: 4s - loss: 1.1105 - acc: 0.6586\n",
      " Optimizer iteration 1108, batch 326\n",
      "\n",
      " Learning rate 0.0003936815769838682, Model learning rate 0.00039368157740682364\n",
      "327/391 [========================>.....] - ETA: 4s - loss: 1.1100 - acc: 0.6589\n",
      " Optimizer iteration 1109, batch 327\n",
      "\n",
      " Learning rate 0.0003928946003009431, Model learning rate 0.00039289460983127356\n",
      "328/391 [========================>.....] - ETA: 4s - loss: 1.1098 - acc: 0.6588\n",
      " Optimizer iteration 1110, batch 328\n",
      "\n",
      " Learning rate 0.000392107901616097, Model learning rate 0.0003921079041901976\n",
      "329/391 [========================>.....] - ETA: 4s - loss: 1.1100 - acc: 0.6587\n",
      " Optimizer iteration 1111, batch 329\n",
      "\n",
      " Learning rate 0.00039132148297125053, Model learning rate 0.0003913214895874262\n",
      "330/391 [========================>.....] - ETA: 4s - loss: 1.1103 - acc: 0.6586\n",
      " Optimizer iteration 1112, batch 330\n",
      "\n",
      " Learning rate 0.0003905353464075975, Model learning rate 0.0003905353369191289\n",
      "\n",
      " Optimizer iteration 1113, batch 331\n",
      "\n",
      " Learning rate 0.0003897494939655995, Model learning rate 0.00038974950439296663\n",
      "332/391 [========================>.....] - ETA: 4s - loss: 1.1097 - acc: 0.6588\n",
      " Optimizer iteration 1114, batch 332\n",
      "\n",
      " Learning rate 0.00038896392768498074, Model learning rate 0.00038896393380127847\n",
      "333/391 [========================>.....] - ETA: 4s - loss: 1.1095 - acc: 0.6590\n",
      " Optimizer iteration 1115, batch 333\n",
      "\n",
      " Learning rate 0.00038817864960472237, Model learning rate 0.0003881786542478949\n",
      "334/391 [========================>.....] - ETA: 4s - loss: 1.1092 - acc: 0.6590\n",
      " Optimizer iteration 1116, batch 334\n",
      "\n",
      " Learning rate 0.00038739366176305785, Model learning rate 0.00038739366573281586\n",
      "\n",
      " Optimizer iteration 1117, batch 335\n",
      "\n",
      " Learning rate 0.00038660896619746734, Model learning rate 0.0003866089682560414\n",
      "336/391 [========================>.....] - ETA: 4s - loss: 1.1088 - acc: 0.6593\n",
      " Optimizer iteration 1118, batch 336\n",
      "\n",
      " Learning rate 0.0003858245649446721, Model learning rate 0.0003858245618175715\n",
      "337/391 [========================>.....] - ETA: 4s - loss: 1.1089 - acc: 0.6593\n",
      " Optimizer iteration 1119, batch 337\n",
      "\n",
      " Learning rate 0.00038504046004062974, Model learning rate 0.0003850404464174062\n",
      "338/391 [========================>.....] - ETA: 4s - loss: 1.1085 - acc: 0.6594\n",
      " Optimizer iteration 1120, batch 338\n",
      "\n",
      " Learning rate 0.0003842566535205286, Model learning rate 0.0003842566511593759\n",
      "339/391 [=========================>....] - ETA: 3s - loss: 1.1084 - acc: 0.6595\n",
      " Optimizer iteration 1121, batch 339\n",
      "\n",
      " Learning rate 0.00038347314741878227, Model learning rate 0.0003834731469396502\n",
      "340/391 [=========================>....] - ETA: 3s - loss: 1.1084 - acc: 0.6595\n",
      " Optimizer iteration 1122, batch 340\n",
      "\n",
      " Learning rate 0.000382689943769025, Model learning rate 0.000382689933758229\n",
      "341/391 [=========================>....] - ETA: 3s - loss: 1.1085 - acc: 0.6595\n",
      " Optimizer iteration 1123, batch 341\n",
      "\n",
      " Learning rate 0.00038190704460410585, Model learning rate 0.0003819070407189429\n",
      "342/391 [=========================>....] - ETA: 3s - loss: 1.1079 - acc: 0.6598\n",
      " Optimizer iteration 1124, batch 342\n",
      "\n",
      " Learning rate 0.00038112445195608334, Model learning rate 0.0003811244387179613\n",
      "343/391 [=========================>....] - ETA: 3s - loss: 1.1074 - acc: 0.6600\n",
      " Optimizer iteration 1125, batch 343\n",
      "\n",
      " Learning rate 0.00038034216785622126, Model learning rate 0.00038034215685911477\n",
      "\n",
      " Optimizer iteration 1126, batch 344\n",
      "\n",
      " Learning rate 0.00037956019433498124, Model learning rate 0.00037956019514240324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/391 [=========================>....] - ETA: 3s - loss: 1.1078 - acc: 0.6601\n",
      " Optimizer iteration 1127, batch 345\n",
      "\n",
      " Learning rate 0.0003787785334220196, Model learning rate 0.0003787785244639963\n",
      "346/391 [=========================>....] - ETA: 3s - loss: 1.1072 - acc: 0.6604\n",
      " Optimizer iteration 1128, batch 346\n",
      "\n",
      " Learning rate 0.00037799718714618124, Model learning rate 0.00037799717392772436\n",
      "\n",
      " Optimizer iteration 1129, batch 347\n",
      "\n",
      " Learning rate 0.00037721615753549446, Model learning rate 0.00037721614353358746\n",
      "348/391 [=========================>....] - ETA: 3s - loss: 1.1069 - acc: 0.6605\n",
      " Optimizer iteration 1130, batch 348\n",
      "\n",
      " Learning rate 0.0003764354466171652, Model learning rate 0.0003764354332815856\n",
      "\n",
      " Optimizer iteration 1131, batch 349\n",
      "\n",
      " Learning rate 0.0003756550564175727, Model learning rate 0.0003756550431717187\n",
      "350/391 [=========================>....] - ETA: 3s - loss: 1.1066 - acc: 0.6604\n",
      " Optimizer iteration 1132, batch 350\n",
      "\n",
      " Learning rate 0.00037487498896226335, Model learning rate 0.00037487500230781734\n",
      "351/391 [=========================>....] - ETA: 3s - loss: 1.1064 - acc: 0.6606\n",
      " Optimizer iteration 1133, batch 351\n",
      "\n",
      " Learning rate 0.00037409524627594605, Model learning rate 0.00037409525248222053\n",
      "\n",
      " Optimizer iteration 1134, batch 352\n",
      "\n",
      " Learning rate 0.0003733158303824868, Model learning rate 0.00037331582279875875\n",
      "353/391 [==========================>...] - ETA: 2s - loss: 1.1064 - acc: 0.6607\n",
      " Optimizer iteration 1135, batch 353\n",
      "\n",
      " Learning rate 0.0003725367433049033, Model learning rate 0.00037253674236126244\n",
      "354/391 [==========================>...] - ETA: 2s - loss: 1.1064 - acc: 0.6606\n",
      " Optimizer iteration 1136, batch 354\n",
      "\n",
      " Learning rate 0.0003717579870653601, Model learning rate 0.00037175798206590116\n",
      "\n",
      " Optimizer iteration 1137, batch 355\n",
      "\n",
      " Learning rate 0.0003709795636851622, Model learning rate 0.00037097957101650536\n",
      "356/391 [==========================>...] - ETA: 2s - loss: 1.1060 - acc: 0.6605\n",
      " Optimizer iteration 1138, batch 356\n",
      "\n",
      " Learning rate 0.00037020147518475137, Model learning rate 0.0003702014801092446\n",
      "357/391 [==========================>...] - ETA: 2s - loss: 1.1057 - acc: 0.6607\n",
      " Optimizer iteration 1139, batch 357\n",
      "\n",
      " Learning rate 0.00036942372358370024, Model learning rate 0.00036942370934411883\n",
      "358/391 [==========================>...] - ETA: 2s - loss: 1.1053 - acc: 0.6609\n",
      " Optimizer iteration 1140, batch 358\n",
      "\n",
      " Learning rate 0.0003686463109007065, Model learning rate 0.000368646316928789\n",
      "359/391 [==========================>...] - ETA: 2s - loss: 1.1051 - acc: 0.6609\n",
      " Optimizer iteration 1141, batch 359\n",
      "\n",
      " Learning rate 0.0003678692391535886, Model learning rate 0.00036786924465559423\n",
      "360/391 [==========================>...] - ETA: 2s - loss: 1.1048 - acc: 0.6609\n",
      " Optimizer iteration 1142, batch 360\n",
      "\n",
      " Learning rate 0.00036709251035927997, Model learning rate 0.0003670925216283649\n",
      "361/391 [==========================>...] - ETA: 2s - loss: 1.1047 - acc: 0.6609\n",
      " Optimizer iteration 1143, batch 361\n",
      "\n",
      " Learning rate 0.0003663161265338235, Model learning rate 0.00036631611874327064\n",
      "362/391 [==========================>...] - ETA: 2s - loss: 1.1049 - acc: 0.6607\n",
      " Optimizer iteration 1144, batch 362\n",
      "\n",
      " Learning rate 0.00036554008969236717, Model learning rate 0.0003655400942079723\n",
      "\n",
      " Optimizer iteration 1145, batch 363\n",
      "\n",
      " Learning rate 0.0003647644018491582, Model learning rate 0.00036476438981480896\n",
      "364/391 [==========================>...] - ETA: 2s - loss: 1.1055 - acc: 0.6606\n",
      " Optimizer iteration 1146, batch 364\n",
      "\n",
      " Learning rate 0.00036398906501753785, Model learning rate 0.0003639890637714416\n",
      "365/391 [===========================>..] - ETA: 1s - loss: 1.1051 - acc: 0.6607\n",
      " Optimizer iteration 1147, batch 365\n",
      "\n",
      " Learning rate 0.0003632140812099368, Model learning rate 0.0003632140869740397\n",
      "366/391 [===========================>..] - ETA: 1s - loss: 1.1049 - acc: 0.6609\n",
      " Optimizer iteration 1148, batch 366\n",
      "\n",
      " Learning rate 0.00036243945243786836, Model learning rate 0.00036243945942260325\n",
      "367/391 [===========================>..] - ETA: 1s - loss: 1.1047 - acc: 0.6610\n",
      " Optimizer iteration 1149, batch 367\n",
      "\n",
      " Learning rate 0.00036166518071192546, Model learning rate 0.0003616651811171323\n",
      "368/391 [===========================>..] - ETA: 1s - loss: 1.1049 - acc: 0.6609\n",
      " Optimizer iteration 1150, batch 368\n",
      "\n",
      " Learning rate 0.0003608912680417737, Model learning rate 0.0003608912811614573\n",
      "369/391 [===========================>..] - ETA: 1s - loss: 1.1046 - acc: 0.6610\n",
      " Optimizer iteration 1151, batch 369\n",
      "\n",
      " Learning rate 0.0003601177164361469, Model learning rate 0.0003601177304517478\n",
      "370/391 [===========================>..] - ETA: 1s - loss: 1.1043 - acc: 0.6610\n",
      " Optimizer iteration 1152, batch 370\n",
      "\n",
      " Learning rate 0.00035934452790284177, Model learning rate 0.00035934452898800373\n",
      "371/391 [===========================>..] - ETA: 1s - loss: 1.1040 - acc: 0.6611\n",
      " Optimizer iteration 1153, batch 371\n",
      "\n",
      " Learning rate 0.00035857170444871254, Model learning rate 0.0003585717058740556\n",
      "\n",
      " Optimizer iteration 1154, batch 372\n",
      "\n",
      " Learning rate 0.0003577992480796658, Model learning rate 0.00035779926110990345\n",
      "373/391 [===========================>..] - ETA: 1s - loss: 1.1037 - acc: 0.6610\n",
      " Optimizer iteration 1155, batch 373\n",
      "\n",
      " Learning rate 0.00035702716080065545, Model learning rate 0.00035702716559171677\n",
      "374/391 [===========================>..] - ETA: 1s - loss: 1.1038 - acc: 0.6611\n",
      " Optimizer iteration 1156, batch 374\n",
      "\n",
      " Learning rate 0.00035625544461567727, Model learning rate 0.000356255448423326\n",
      "375/391 [===========================>..] - ETA: 1s - loss: 1.1038 - acc: 0.6611\n",
      " Optimizer iteration 1157, batch 375\n",
      "\n",
      " Learning rate 0.0003554841015277641, Model learning rate 0.0003554841096047312\n",
      "376/391 [===========================>..] - ETA: 1s - loss: 1.1040 - acc: 0.6609\n",
      " Optimizer iteration 1158, batch 376\n",
      "\n",
      " Learning rate 0.00035471313353898054, Model learning rate 0.00035471312003210187\n",
      "377/391 [===========================>..] - ETA: 1s - loss: 1.1038 - acc: 0.6610\n",
      " Optimizer iteration 1159, batch 377\n",
      "\n",
      " Learning rate 0.00035394254265041657, Model learning rate 0.00035394253791309893\n",
      "378/391 [============================>.] - ETA: 0s - loss: 1.1036 - acc: 0.6610\n",
      " Optimizer iteration 1160, batch 378\n",
      "\n",
      " Learning rate 0.0003531723308621847, Model learning rate 0.00035317233414389193\n",
      "379/391 [============================>.] - ETA: 0s - loss: 1.1033 - acc: 0.6612\n",
      " Optimizer iteration 1161, batch 379\n",
      "\n",
      " Learning rate 0.0003524025001734126, Model learning rate 0.00035240250872448087\n",
      "\n",
      " Optimizer iteration 1162, batch 380\n",
      "\n",
      " Learning rate 0.0003516330525822391, Model learning rate 0.00035163306165486574\n",
      "381/391 [============================>.] - ETA: 0s - loss: 1.1029 - acc: 0.6614\n",
      " Optimizer iteration 1163, batch 381\n",
      "\n",
      " Learning rate 0.00035086399008580884, Model learning rate 0.00035086399293504655\n",
      "382/391 [============================>.] - ETA: 0s - loss: 1.1023 - acc: 0.6615\n",
      " Optimizer iteration 1164, batch 382\n",
      "\n",
      " Learning rate 0.00035009531468026647, Model learning rate 0.0003500953025650233\n",
      "383/391 [============================>.] - ETA: 0s - loss: 1.1024 - acc: 0.6613\n",
      " Optimizer iteration 1165, batch 383\n",
      "\n",
      " Learning rate 0.00034932702836075214, Model learning rate 0.00034932701964862645\n",
      "\n",
      " Optimizer iteration 1166, batch 384\n",
      "\n",
      " Learning rate 0.0003485591331213962, Model learning rate 0.000348559144185856\n",
      "385/391 [============================>.] - ETA: 0s - loss: 1.1030 - acc: 0.6611\n",
      " Optimizer iteration 1167, batch 385\n",
      "\n",
      " Learning rate 0.00034779163095531384, Model learning rate 0.000347791617969051\n",
      "386/391 [============================>.] - ETA: 0s - loss: 1.1025 - acc: 0.6611\n",
      " Optimizer iteration 1168, batch 386\n",
      "\n",
      " Learning rate 0.00034702452385460014, Model learning rate 0.0003470245283097029\n",
      "387/391 [============================>.] - ETA: 0s - loss: 1.1023 - acc: 0.6612\n",
      " Optimizer iteration 1169, batch 387\n",
      "\n",
      " Learning rate 0.00034625781381032484, Model learning rate 0.0003462578170001507\n",
      "388/391 [============================>.] - ETA: 0s - loss: 1.1024 - acc: 0.6611\n",
      " Optimizer iteration 1170, batch 388\n",
      "\n",
      " Learning rate 0.00034549150281252633, Model learning rate 0.0003454915131442249\n",
      "389/391 [============================>.] - ETA: 0s - loss: 1.1022 - acc: 0.6612\n",
      " Optimizer iteration 1171, batch 389\n",
      "\n",
      " Learning rate 0.00034472559285020826, Model learning rate 0.000344725587638095\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.1019 - acc: 0.6612\n",
      " Optimizer iteration 1172, batch 390\n",
      "\n",
      " Learning rate 0.0003439600859113329, Model learning rate 0.000343960098689422\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 1.1017 - acc: 0.6613 - val_loss: 1.4019 - val_acc: 0.5747\n",
      "\n",
      "Epoch 00003: saving model to /home/ubuntu/Projects/hybrid-ensemble/model/run_200/cifar10_ResNet20v1_model-0003.h5\n",
      "Epoch 4/10\n",
      "\n",
      " Optimizer iteration 1173, batch 0\n",
      "\n",
      " Learning rate 0.00034319498398281635, Model learning rate 0.00034319498809054494\n",
      "  1/391 [..............................] - ETA: 20s - loss: 1.1213 - acc: 0.6875\n",
      " Optimizer iteration 1174, batch 1\n",
      "\n",
      " Learning rate 0.00034243028905052387, Model learning rate 0.00034243028494529426\n",
      "  2/391 [..............................] - ETA: 21s - loss: 1.0673 - acc: 0.6719\n",
      " Optimizer iteration 1175, batch 2\n",
      "\n",
      " Learning rate 0.00034166600309926387, Model learning rate 0.00034166598925367\n",
      "  3/391 [..............................] - ETA: 20s - loss: 1.0893 - acc: 0.6615\n",
      " Optimizer iteration 1176, batch 3\n",
      "\n",
      " Learning rate 0.0003409021281127835, Model learning rate 0.00034090213011950254\n",
      "  4/391 [..............................] - ETA: 20s - loss: 1.0670 - acc: 0.6680\n",
      " Optimizer iteration 1177, batch 4\n",
      "\n",
      " Learning rate 0.00034013866607376307, Model learning rate 0.0003401386784389615\n",
      "  5/391 [..............................] - ETA: 20s - loss: 1.0616 - acc: 0.6672\n",
      " Optimizer iteration 1178, batch 5\n",
      "\n",
      " Learning rate 0.00033937561896381146, Model learning rate 0.0003393756051082164\n",
      "\n",
      " Optimizer iteration 1179, batch 6\n",
      "\n",
      " Learning rate 0.0003386129887634601, Model learning rate 0.0003386129974387586\n",
      "  7/391 [..............................] - ETA: 19s - loss: 1.0398 - acc: 0.6752\n",
      " Optimizer iteration 1180, batch 7\n",
      "\n",
      " Learning rate 0.0003378507774521587, Model learning rate 0.00033785076811909676\n",
      "  8/391 [..............................] - ETA: 19s - loss: 1.0365 - acc: 0.6777\n",
      " Optimizer iteration 1181, batch 8\n",
      "\n",
      " Learning rate 0.0003370889870082692, Model learning rate 0.00033708897535689175\n",
      "  9/391 [..............................] - ETA: 19s - loss: 1.0365 - acc: 0.6780\n",
      " Optimizer iteration 1182, batch 9\n",
      "\n",
      " Learning rate 0.0003363276194090617, Model learning rate 0.0003363276191521436\n",
      "\n",
      " Optimizer iteration 1183, batch 10\n",
      "\n",
      " Learning rate 0.00033556667663070836, Model learning rate 0.00033556667040102184\n",
      " 11/391 [..............................] - ETA: 19s - loss: 1.0291 - acc: 0.6768\n",
      " Optimizer iteration 1184, batch 11\n",
      "\n",
      " Learning rate 0.0003348061606482791, Model learning rate 0.00033480615820735693\n",
      "\n",
      " Optimizer iteration 1185, batch 12\n",
      "\n",
      " Learning rate 0.0003340460734357359, Model learning rate 0.00033404608257114887\n",
      " 13/391 [..............................] - ETA: 19s - loss: 1.0232 - acc: 0.6797\n",
      " Optimizer iteration 1186, batch 13\n",
      "\n",
      " Learning rate 0.0003332864169659275, Model learning rate 0.0003332864143885672\n",
      "\n",
      " Optimizer iteration 1187, batch 14\n",
      "\n",
      " Learning rate 0.0003325271932105851, Model learning rate 0.0003325271827634424\n",
      " 15/391 [>.............................] - ETA: 19s - loss: 1.0319 - acc: 0.6760\n",
      " Optimizer iteration 1188, batch 15\n",
      "\n",
      " Learning rate 0.0003317684041403165, Model learning rate 0.0003317684167996049\n",
      "\n",
      " Optimizer iteration 1189, batch 16\n",
      "\n",
      " Learning rate 0.00033101005172460156, Model learning rate 0.0003310100582893938\n",
      " 17/391 [>.............................] - ETA: 18s - loss: 1.0183 - acc: 0.6820\n",
      " Optimizer iteration 1190, batch 17\n",
      "\n",
      " Learning rate 0.00033025213793178644, Model learning rate 0.0003302521363366395\n",
      "\n",
      " Optimizer iteration 1191, batch 18\n",
      "\n",
      " Learning rate 0.00032949466472907896, Model learning rate 0.0003294946509413421\n",
      " 19/391 [>.............................] - ETA: 18s - loss: 1.0173 - acc: 0.6854\n",
      " Optimizer iteration 1192, batch 19\n",
      "\n",
      " Learning rate 0.0003287376340825432, Model learning rate 0.000328737631207332\n",
      " 20/391 [>.............................] - ETA: 19s - loss: 1.0150 - acc: 0.6859\n",
      " Optimizer iteration 1193, batch 20\n",
      "\n",
      " Learning rate 0.0003279810479570948, Model learning rate 0.00032798104803077877\n",
      " 21/391 [>.............................] - ETA: 19s - loss: 1.0105 - acc: 0.6882\n",
      " Optimizer iteration 1194, batch 21\n",
      "\n",
      " Learning rate 0.0003272249083164957, Model learning rate 0.00032722490141168237\n",
      " 22/391 [>.............................] - ETA: 20s - loss: 1.0155 - acc: 0.6861\n",
      " Optimizer iteration 1195, batch 22\n",
      "\n",
      " Learning rate 0.00032646921712334856, Model learning rate 0.0003264692204538733\n",
      " 23/391 [>.............................] - ETA: 20s - loss: 1.0212 - acc: 0.6861\n",
      " Optimizer iteration 1196, batch 23\n",
      "\n",
      " Learning rate 0.0003257139763390925, Model learning rate 0.00032571397605352104\n",
      " 24/391 [>.............................] - ETA: 20s - loss: 1.0240 - acc: 0.6839\n",
      " Optimizer iteration 1197, batch 24\n",
      "\n",
      " Learning rate 0.0003249591879239972, Model learning rate 0.0003249591973144561\n",
      " 25/391 [>.............................] - ETA: 20s - loss: 1.0253 - acc: 0.6834\n",
      " Optimizer iteration 1198, batch 25\n",
      "\n",
      " Learning rate 0.00032420485383715845, Model learning rate 0.000324204855132848\n",
      " 26/391 [>.............................] - ETA: 21s - loss: 1.0225 - acc: 0.6845\n",
      " Optimizer iteration 1199, batch 26\n",
      "\n",
      " Learning rate 0.00032345097603649265, Model learning rate 0.00032345097861252725\n",
      " 27/391 [=>............................] - ETA: 21s - loss: 1.0186 - acc: 0.6861\n",
      " Optimizer iteration 1200, batch 27\n",
      "\n",
      " Learning rate 0.00032269755647873217, Model learning rate 0.0003226975677534938\n",
      " 28/391 [=>............................] - ETA: 21s - loss: 1.0171 - acc: 0.6864\n",
      " Optimizer iteration 1201, batch 28\n",
      "\n",
      " Learning rate 0.00032194459711941967, Model learning rate 0.00032194459345191717\n",
      " 29/391 [=>............................] - ETA: 21s - loss: 1.0191 - acc: 0.6856\n",
      " Optimizer iteration 1202, batch 29\n",
      "\n",
      " Learning rate 0.00032119209991290346, Model learning rate 0.0003211921139154583\n",
      " 30/391 [=>............................] - ETA: 21s - loss: 1.0175 - acc: 0.6865\n",
      " Optimizer iteration 1203, batch 30\n",
      "\n",
      " Learning rate 0.0003204400668123322, Model learning rate 0.0003204400709364563\n",
      " 31/391 [=>............................] - ETA: 22s - loss: 1.0166 - acc: 0.6867\n",
      " Optimizer iteration 1204, batch 31\n",
      "\n",
      " Learning rate 0.00031968849976965014, Model learning rate 0.00031968849361874163\n",
      "\n",
      " Optimizer iteration 1205, batch 32\n",
      "\n",
      " Learning rate 0.00031893740073559167, Model learning rate 0.0003189374110661447\n",
      " 33/391 [=>............................] - ETA: 22s - loss: 1.0140 - acc: 0.6868\n",
      " Optimizer iteration 1206, batch 33\n",
      "\n",
      " Learning rate 0.00031818677165967646, Model learning rate 0.00031818676507100463\n",
      " 34/391 [=>............................] - ETA: 22s - loss: 1.0165 - acc: 0.6857\n",
      " Optimizer iteration 1207, batch 34\n",
      "\n",
      " Learning rate 0.0003174366144902048, Model learning rate 0.0003174366138409823\n",
      " 35/391 [=>............................] - ETA: 22s - loss: 1.0164 - acc: 0.6862\n",
      " Optimizer iteration 1208, batch 35\n",
      "\n",
      " Learning rate 0.00031668693117425126, Model learning rate 0.0003166869282722473\n",
      "\n",
      " Optimizer iteration 1209, batch 36\n",
      "\n",
      " Learning rate 0.00031593772365766105, Model learning rate 0.0003159377374686301\n",
      " 37/391 [=>............................] - ETA: 22s - loss: 1.0111 - acc: 0.6875\n",
      " Optimizer iteration 1210, batch 37\n",
      "\n",
      " Learning rate 0.0003151889938850445, Model learning rate 0.0003151889832224697\n",
      " 38/391 [=>............................] - ETA: 22s - loss: 1.0114 - acc: 0.6867\n",
      " Optimizer iteration 1211, batch 38\n",
      "\n",
      " Learning rate 0.00031444074379977186, Model learning rate 0.0003144407528452575\n",
      " 39/391 [=>............................] - ETA: 22s - loss: 1.0138 - acc: 0.6857\n",
      " Optimizer iteration 1212, batch 39\n",
      "\n",
      " Learning rate 0.00031369297534396826, Model learning rate 0.00031369298812933266\n",
      " 40/391 [==>...........................] - ETA: 22s - loss: 1.0125 - acc: 0.6867\n",
      " Optimizer iteration 1213, batch 40\n",
      "\n",
      " Learning rate 0.0003129456904585084, Model learning rate 0.0003129456890746951\n",
      " 41/391 [==>...........................] - ETA: 22s - loss: 1.0113 - acc: 0.6869\n",
      " Optimizer iteration 1214, batch 41\n",
      "\n",
      " Learning rate 0.00031219889108301236, Model learning rate 0.0003121988847851753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42/391 [==>...........................] - ETA: 22s - loss: 1.0111 - acc: 0.6882\n",
      " Optimizer iteration 1215, batch 42\n",
      "\n",
      " Learning rate 0.0003114525791558398, Model learning rate 0.0003114525752607733\n",
      " 43/391 [==>...........................] - ETA: 22s - loss: 1.0100 - acc: 0.6891\n",
      " Optimizer iteration 1216, batch 43\n",
      "\n",
      " Learning rate 0.0003107067566140853, Model learning rate 0.00031070676050148904\n",
      "\n",
      " Optimizer iteration 1217, batch 44\n",
      "\n",
      " Learning rate 0.0003099614253935731, Model learning rate 0.0003099614114034921\n",
      " 45/391 [==>...........................] - ETA: 22s - loss: 1.0075 - acc: 0.6896\n",
      " Optimizer iteration 1218, batch 45\n",
      "\n",
      " Learning rate 0.0003092165874288525, Model learning rate 0.00030921658617444336\n",
      " 46/391 [==>...........................] - ETA: 22s - loss: 1.0063 - acc: 0.6904\n",
      " Optimizer iteration 1219, batch 46\n",
      "\n",
      " Learning rate 0.0003084722446531918, Model learning rate 0.0003084722557105124\n",
      " 47/391 [==>...........................] - ETA: 23s - loss: 1.0062 - acc: 0.6898\n",
      " Optimizer iteration 1220, batch 47\n",
      "\n",
      " Learning rate 0.00030772839899857464, Model learning rate 0.00030772839090786874\n",
      " 48/391 [==>...........................] - ETA: 22s - loss: 1.0028 - acc: 0.6914\n",
      " Optimizer iteration 1221, batch 48\n",
      "\n",
      " Learning rate 0.00030698505239569424, Model learning rate 0.0003069850499741733\n",
      " 49/391 [==>...........................] - ETA: 23s - loss: 1.0046 - acc: 0.6915\n",
      " Optimizer iteration 1222, batch 49\n",
      "\n",
      " Learning rate 0.0003062422067739485, Model learning rate 0.00030624220380559564\n",
      " 50/391 [==>...........................] - ETA: 23s - loss: 1.0047 - acc: 0.6919\n",
      " Optimizer iteration 1223, batch 50\n",
      "\n",
      " Learning rate 0.00030549986406143496, Model learning rate 0.00030549985240213573\n",
      " 51/391 [==>...........................] - ETA: 22s - loss: 1.0049 - acc: 0.6918\n",
      " Optimizer iteration 1224, batch 51\n",
      "\n",
      " Learning rate 0.0003047580261849456, Model learning rate 0.00030475802486762404\n",
      " 52/391 [==>...........................] - ETA: 22s - loss: 1.0075 - acc: 0.6910\n",
      " Optimizer iteration 1225, batch 52\n",
      "\n",
      " Learning rate 0.0003040166950699625, Model learning rate 0.0003040166920982301\n",
      " 53/391 [===>..........................] - ETA: 23s - loss: 1.0067 - acc: 0.6906\n",
      " Optimizer iteration 1226, batch 53\n",
      "\n",
      " Learning rate 0.0003032758726406521, Model learning rate 0.0003032758831977844\n",
      " 54/391 [===>..........................] - ETA: 23s - loss: 1.0069 - acc: 0.6907\n",
      " Optimizer iteration 1227, batch 54\n",
      "\n",
      " Learning rate 0.0003025355608198606, Model learning rate 0.0003025355690624565\n",
      "\n",
      " Optimizer iteration 1228, batch 55\n",
      "\n",
      " Learning rate 0.0003017957615291088, Model learning rate 0.0003017957496922463\n",
      " 56/391 [===>..........................] - ETA: 22s - loss: 1.0105 - acc: 0.6895\n",
      " Optimizer iteration 1229, batch 56\n",
      "\n",
      " Learning rate 0.0003010564766885878, Model learning rate 0.0003010564832948148\n",
      " 57/391 [===>..........................] - ETA: 23s - loss: 1.0115 - acc: 0.6889\n",
      " Optimizer iteration 1230, batch 57\n",
      "\n",
      " Learning rate 0.0003003177082171523, Model learning rate 0.0003003177116625011\n",
      " 58/391 [===>..........................] - ETA: 23s - loss: 1.0124 - acc: 0.6884\n",
      " Optimizer iteration 1231, batch 58\n",
      "\n",
      " Learning rate 0.00029957945803231754, Model learning rate 0.0002995794638991356\n",
      " 59/391 [===>..........................] - ETA: 22s - loss: 1.0141 - acc: 0.6878\n",
      " Optimizer iteration 1232, batch 59\n",
      "\n",
      " Learning rate 0.00029884172805025343, Model learning rate 0.0002988417400047183\n",
      " 60/391 [===>..........................] - ETA: 22s - loss: 1.0152 - acc: 0.6878\n",
      " Optimizer iteration 1233, batch 60\n",
      "\n",
      " Learning rate 0.0002981045201857796, Model learning rate 0.0002981045108754188\n",
      " 61/391 [===>..........................] - ETA: 23s - loss: 1.0145 - acc: 0.6883\n",
      " Optimizer iteration 1234, batch 61\n",
      "\n",
      " Learning rate 0.0002973678363523604, Model learning rate 0.00029736783471889794\n",
      " 62/391 [===>..........................] - ETA: 23s - loss: 1.0124 - acc: 0.6894\n",
      " Optimizer iteration 1235, batch 62\n",
      "\n",
      " Learning rate 0.0002966316784621, Model learning rate 0.0002966316824313253\n",
      " 63/391 [===>..........................] - ETA: 23s - loss: 1.0131 - acc: 0.6890\n",
      " Optimizer iteration 1236, batch 63\n",
      "\n",
      " Learning rate 0.00029589604842573757, Model learning rate 0.0002958960540127009\n",
      " 64/391 [===>..........................] - ETA: 23s - loss: 1.0131 - acc: 0.6891\n",
      " Optimizer iteration 1237, batch 64\n",
      "\n",
      " Learning rate 0.00029516094815264216, Model learning rate 0.00029516094946302474\n",
      " 65/391 [===>..........................] - ETA: 23s - loss: 1.0120 - acc: 0.6888\n",
      " Optimizer iteration 1238, batch 65\n",
      "\n",
      " Learning rate 0.00029442637955080786, Model learning rate 0.0002944263687822968\n",
      " 66/391 [====>.........................] - ETA: 23s - loss: 1.0123 - acc: 0.6890\n",
      " Optimizer iteration 1239, batch 66\n",
      "\n",
      " Learning rate 0.0002936923445268488, Model learning rate 0.0002936923410743475\n",
      " 67/391 [====>.........................] - ETA: 22s - loss: 1.0126 - acc: 0.6890\n",
      " Optimizer iteration 1240, batch 67\n",
      "\n",
      " Learning rate 0.0002929588449859941, Model learning rate 0.00029295883723534644\n",
      " 68/391 [====>.........................] - ETA: 22s - loss: 1.0108 - acc: 0.6901\n",
      " Optimizer iteration 1241, batch 68\n",
      "\n",
      " Learning rate 0.00029222588283208274, Model learning rate 0.00029222588636912405\n",
      " 69/391 [====>.........................] - ETA: 23s - loss: 1.0139 - acc: 0.6887\n",
      " Optimizer iteration 1242, batch 69\n",
      "\n",
      " Learning rate 0.00029149345996755936, Model learning rate 0.0002914934593718499\n",
      " 70/391 [====>.........................] - ETA: 22s - loss: 1.0124 - acc: 0.6900\n",
      " Optimizer iteration 1243, batch 70\n",
      "\n",
      " Learning rate 0.00029076157829346883, Model learning rate 0.0002907615853473544\n",
      " 71/391 [====>.........................] - ETA: 22s - loss: 1.0123 - acc: 0.6901\n",
      " Optimizer iteration 1244, batch 71\n",
      "\n",
      " Learning rate 0.00029003023970945057, Model learning rate 0.00029003023519180715\n",
      " 72/391 [====>.........................] - ETA: 22s - loss: 1.0130 - acc: 0.6900\n",
      " Optimizer iteration 1245, batch 72\n",
      "\n",
      " Learning rate 0.00028929944611373555, Model learning rate 0.00028929943800903857\n",
      " 73/391 [====>.........................] - ETA: 22s - loss: 1.0115 - acc: 0.6908\n",
      " Optimizer iteration 1246, batch 73\n",
      "\n",
      " Learning rate 0.0002885691994031393, Model learning rate 0.00028856919379904866\n",
      " 74/391 [====>.........................] - ETA: 22s - loss: 1.0110 - acc: 0.6908\n",
      " Optimizer iteration 1247, batch 74\n",
      "\n",
      " Learning rate 0.0002878395014730579, Model learning rate 0.00028783950256183743\n",
      " 75/391 [====>.........................] - ETA: 22s - loss: 1.0119 - acc: 0.6901\n",
      " Optimizer iteration 1248, batch 75\n",
      "\n",
      " Learning rate 0.00028711035421746366, Model learning rate 0.0002871103642974049\n",
      "\n",
      " Optimizer iteration 1249, batch 76\n",
      "\n",
      " Learning rate 0.0002863817595288993, Model learning rate 0.00028638174990192056\n",
      " 77/391 [====>.........................] - ETA: 22s - loss: 1.0121 - acc: 0.6900\n",
      " Optimizer iteration 1250, batch 77\n",
      "\n",
      " Learning rate 0.00028565371929847286, Model learning rate 0.00028565371758304536\n",
      " 78/391 [====>.........................] - ETA: 22s - loss: 1.0129 - acc: 0.6900\n",
      " Optimizer iteration 1251, batch 78\n",
      "\n",
      " Learning rate 0.00028492623541585404, Model learning rate 0.00028492623823694885\n",
      " 79/391 [=====>........................] - ETA: 22s - loss: 1.0139 - acc: 0.6900\n",
      " Optimizer iteration 1252, batch 79\n",
      "\n",
      " Learning rate 0.000284199309769268, Model learning rate 0.000284199311863631\n",
      " 80/391 [=====>........................] - ETA: 22s - loss: 1.0130 - acc: 0.6904\n",
      " Optimizer iteration 1253, batch 80\n",
      "\n",
      " Learning rate 0.00028347294424549077, Model learning rate 0.00028347293846309185\n",
      " 81/391 [=====>........................] - ETA: 22s - loss: 1.0145 - acc: 0.6895\n",
      " Optimizer iteration 1254, batch 81\n",
      "\n",
      " Learning rate 0.00028274714072984506, Model learning rate 0.0002827471471391618\n",
      " 82/391 [=====>........................] - ETA: 22s - loss: 1.0155 - acc: 0.6892\n",
      " Optimizer iteration 1255, batch 82\n",
      "\n",
      " Learning rate 0.0002820219011061949, Model learning rate 0.0002820219087880105\n",
      " 83/391 [=====>........................] - ETA: 22s - loss: 1.0162 - acc: 0.6889\n",
      " Optimizer iteration 1256, batch 83\n",
      "\n",
      " Learning rate 0.0002812972272569402, Model learning rate 0.0002812972234096378\n",
      " 84/391 [=====>........................] - ETA: 22s - loss: 1.0171 - acc: 0.6886\n",
      " Optimizer iteration 1257, batch 84\n",
      "\n",
      " Learning rate 0.00028057312106301253, Model learning rate 0.0002805731201078743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85/391 [=====>........................] - ETA: 22s - loss: 1.0182 - acc: 0.6878\n",
      " Optimizer iteration 1258, batch 85\n",
      "\n",
      " Learning rate 0.00027984958440387044, Model learning rate 0.0002798495988827199\n",
      " 86/391 [=====>........................] - ETA: 22s - loss: 1.0173 - acc: 0.6880\n",
      " Optimizer iteration 1259, batch 86\n",
      "\n",
      " Learning rate 0.0002791266191574936, Model learning rate 0.00027912663063034415\n",
      "\n",
      " Optimizer iteration 1260, batch 87\n",
      "\n",
      " Learning rate 0.0002784042272003794, Model learning rate 0.0002784042153507471\n",
      " 88/391 [=====>........................] - ETA: 21s - loss: 1.0178 - acc: 0.6879\n",
      " Optimizer iteration 1261, batch 88\n",
      "\n",
      " Learning rate 0.00027768241040753637, Model learning rate 0.00027768241125158966\n",
      " 89/391 [=====>........................] - ETA: 22s - loss: 1.0186 - acc: 0.6878\n",
      " Optimizer iteration 1262, batch 89\n",
      "\n",
      " Learning rate 0.0002769611706524805, Model learning rate 0.0002769611601252109\n",
      " 90/391 [=====>........................] - ETA: 21s - loss: 1.0192 - acc: 0.6875\n",
      " Optimizer iteration 1263, batch 90\n",
      "\n",
      " Learning rate 0.0002762405098072303, Model learning rate 0.0002762405201792717\n",
      " 91/391 [=====>........................] - ETA: 21s - loss: 1.0201 - acc: 0.6871\n",
      " Optimizer iteration 1264, batch 91\n",
      "\n",
      " Learning rate 0.00027552042974230117, Model learning rate 0.0002755204332061112\n",
      " 92/391 [======>.......................] - ETA: 21s - loss: 1.0191 - acc: 0.6875\n",
      " Optimizer iteration 1265, batch 92\n",
      "\n",
      " Learning rate 0.0002748009323267016, Model learning rate 0.0002748009283095598\n",
      " 93/391 [======>.......................] - ETA: 21s - loss: 1.0178 - acc: 0.6881\n",
      " Optimizer iteration 1266, batch 93\n",
      "\n",
      " Learning rate 0.00027408201942792756, Model learning rate 0.0002740820054896176\n",
      " 94/391 [======>.......................] - ETA: 21s - loss: 1.0166 - acc: 0.6886\n",
      " Optimizer iteration 1267, batch 94\n",
      "\n",
      " Learning rate 0.00027336369291195773, Model learning rate 0.00027336369385011494\n",
      "\n",
      " Optimizer iteration 1268, batch 95\n",
      "\n",
      " Learning rate 0.00027264595464324875, Model learning rate 0.00027264596428722143\n",
      " 96/391 [======>.......................] - ETA: 21s - loss: 1.0156 - acc: 0.6891\n",
      " Optimizer iteration 1269, batch 96\n",
      "\n",
      " Learning rate 0.000271928806484731, Model learning rate 0.00027192881680093706\n",
      " 97/391 [======>.......................] - ETA: 21s - loss: 1.0153 - acc: 0.6894\n",
      " Optimizer iteration 1270, batch 97\n",
      "\n",
      " Learning rate 0.0002712122502978024, Model learning rate 0.0002712122513912618\n",
      "\n",
      " Optimizer iteration 1271, batch 98\n",
      "\n",
      " Learning rate 0.00027049628794232505, Model learning rate 0.00027049629716202617\n",
      " 99/391 [======>.......................] - ETA: 21s - loss: 1.0142 - acc: 0.6901\n",
      " Optimizer iteration 1272, batch 99\n",
      "\n",
      " Learning rate 0.00026978092127661945, Model learning rate 0.00026978092500939965\n",
      "100/391 [======>.......................] - ETA: 21s - loss: 1.0127 - acc: 0.6905\n",
      " Optimizer iteration 1273, batch 100\n",
      "\n",
      " Learning rate 0.0002690661521574596, Model learning rate 0.00026906616403721273\n",
      "101/391 [======>.......................] - ETA: 21s - loss: 1.0131 - acc: 0.6905\n",
      " Optimizer iteration 1274, batch 101\n",
      "\n",
      " Learning rate 0.00026835198244006924, Model learning rate 0.00026835198514163494\n",
      "\n",
      " Optimizer iteration 1275, batch 102\n",
      "\n",
      " Learning rate 0.00026763841397811573, Model learning rate 0.00026763841742649674\n",
      "103/391 [======>.......................] - ETA: 21s - loss: 1.0146 - acc: 0.6900\n",
      " Optimizer iteration 1276, batch 103\n",
      "\n",
      " Learning rate 0.0002669254486237062, Model learning rate 0.00026692546089179814\n",
      "104/391 [======>.......................] - ETA: 20s - loss: 1.0143 - acc: 0.6906\n",
      " Optimizer iteration 1277, batch 104\n",
      "\n",
      " Learning rate 0.0002662130882273825, Model learning rate 0.00026621308643370867\n",
      "105/391 [=======>......................] - ETA: 21s - loss: 1.0126 - acc: 0.6914\n",
      " Optimizer iteration 1278, batch 105\n",
      "\n",
      " Learning rate 0.0002655013346381158, Model learning rate 0.0002655013231560588\n",
      "106/391 [=======>......................] - ETA: 20s - loss: 1.0131 - acc: 0.6915\n",
      " Optimizer iteration 1279, batch 106\n",
      "\n",
      " Learning rate 0.00026479018970330227, Model learning rate 0.00026479020016267896\n",
      "107/391 [=======>......................] - ETA: 20s - loss: 1.0144 - acc: 0.6914\n",
      " Optimizer iteration 1280, batch 107\n",
      "\n",
      " Learning rate 0.000264079655268759, Model learning rate 0.00026407965924590826\n",
      "\n",
      " Optimizer iteration 1281, batch 108\n",
      "\n",
      " Learning rate 0.00026336973317871756, Model learning rate 0.00026336972950957716\n",
      "109/391 [=======>......................] - ETA: 20s - loss: 1.0161 - acc: 0.6907\n",
      " Optimizer iteration 1282, batch 109\n",
      "\n",
      " Learning rate 0.000262660425275821, Model learning rate 0.00026266041095368564\n",
      "110/391 [=======>......................] - ETA: 20s - loss: 1.0157 - acc: 0.6906\n",
      " Optimizer iteration 1283, batch 110\n",
      "\n",
      " Learning rate 0.0002619517334011177, Model learning rate 0.0002619517326820642\n",
      "111/391 [=======>......................] - ETA: 20s - loss: 1.0162 - acc: 0.6904\n",
      " Optimizer iteration 1284, batch 111\n",
      "\n",
      " Learning rate 0.0002612436593940568, Model learning rate 0.0002612436655908823\n",
      "112/391 [=======>......................] - ETA: 20s - loss: 1.0164 - acc: 0.6907\n",
      " Optimizer iteration 1285, batch 112\n",
      "\n",
      " Learning rate 0.0002605362050924848, Model learning rate 0.00026053620968014\n",
      "113/391 [=======>......................] - ETA: 20s - loss: 1.0179 - acc: 0.6905\n",
      " Optimizer iteration 1286, batch 113\n",
      "\n",
      " Learning rate 0.00025982937233263846, Model learning rate 0.0002598293649498373\n",
      "\n",
      " Optimizer iteration 1287, batch 114\n",
      "\n",
      " Learning rate 0.0002591231629491423, Model learning rate 0.0002591231605038047\n",
      "115/391 [=======>......................] - ETA: 20s - loss: 1.0194 - acc: 0.6905\n",
      " Optimizer iteration 1288, batch 115\n",
      "\n",
      " Learning rate 0.0002584175787750024, Model learning rate 0.00025841756723821163\n",
      "\n",
      " Optimizer iteration 1289, batch 116\n",
      "\n",
      " Learning rate 0.00025771262164160213, Model learning rate 0.00025771261425688863\n",
      "117/391 [=======>......................] - ETA: 20s - loss: 1.0191 - acc: 0.6909\n",
      " Optimizer iteration 1290, batch 117\n",
      "\n",
      " Learning rate 0.00025700829337869696, Model learning rate 0.00025700830155983567\n",
      "118/391 [========>.....................] - ETA: 20s - loss: 1.0200 - acc: 0.6908\n",
      " Optimizer iteration 1291, batch 118\n",
      "\n",
      " Learning rate 0.0002563045958144109, Model learning rate 0.0002563046000432223\n",
      "119/391 [========>.....................] - ETA: 20s - loss: 1.0217 - acc: 0.6905\n",
      " Optimizer iteration 1292, batch 119\n",
      "\n",
      " Learning rate 0.0002556015307752301, Model learning rate 0.000255601538810879\n",
      "\n",
      " Optimizer iteration 1293, batch 120\n",
      "\n",
      " Learning rate 0.00025489910008599966, Model learning rate 0.00025489908875897527\n",
      "121/391 [========>.....................] - ETA: 20s - loss: 1.0208 - acc: 0.6909\n",
      " Optimizer iteration 1294, batch 121\n",
      "\n",
      " Learning rate 0.0002541973055699178, Model learning rate 0.00025419730809517205\n",
      "122/391 [========>.....................] - ETA: 19s - loss: 1.0202 - acc: 0.6912\n",
      " Optimizer iteration 1295, batch 122\n",
      "\n",
      " Learning rate 0.0002534961490485313, Model learning rate 0.0002534961386118084\n",
      "123/391 [========>.....................] - ETA: 19s - loss: 1.0197 - acc: 0.6912\n",
      " Optimizer iteration 1296, batch 123\n",
      "\n",
      " Learning rate 0.00025279563234173176, Model learning rate 0.0002527956385165453\n",
      "\n",
      " Optimizer iteration 1297, batch 124\n",
      "\n",
      " Learning rate 0.00025209575726774914, Model learning rate 0.00025209574960172176\n",
      "125/391 [========>.....................] - ETA: 19s - loss: 1.0185 - acc: 0.6911\n",
      " Optimizer iteration 1298, batch 125\n",
      "\n",
      " Learning rate 0.0002513965256431488, Model learning rate 0.00025139653007499874\n",
      "126/391 [========>.....................] - ETA: 19s - loss: 1.0171 - acc: 0.6918\n",
      " Optimizer iteration 1299, batch 126\n",
      "\n",
      " Learning rate 0.0002506979392828258, Model learning rate 0.00025069795083254576\n",
      "127/391 [========>.....................] - ETA: 19s - loss: 1.0181 - acc: 0.6913\n",
      " Optimizer iteration 1300, batch 127\n",
      "\n",
      " Learning rate 0.0002500000000000001, Model learning rate 0.0002500000118743628\n",
      "128/391 [========>.....................] - ETA: 19s - loss: 1.0182 - acc: 0.6912\n",
      " Optimizer iteration 1301, batch 128\n",
      "\n",
      " Learning rate 0.0002493027096062121, Model learning rate 0.00024930271320044994\n",
      "129/391 [========>.....................] - ETA: 19s - loss: 1.0178 - acc: 0.6914\n",
      " Optimizer iteration 1302, batch 129\n",
      "\n",
      " Learning rate 0.00024860606991131855, Model learning rate 0.00024860608391463757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/391 [========>.....................] - ETA: 19s - loss: 1.0182 - acc: 0.6913\n",
      " Optimizer iteration 1303, batch 130\n",
      "\n",
      " Learning rate 0.00024791008272348654, Model learning rate 0.00024791009491309524\n",
      "131/391 [=========>....................] - ETA: 19s - loss: 1.0189 - acc: 0.6910\n",
      " Optimizer iteration 1304, batch 131\n",
      "\n",
      " Learning rate 0.0002472147498491902, Model learning rate 0.00024721474619582295\n",
      "132/391 [=========>....................] - ETA: 19s - loss: 1.0185 - acc: 0.6910\n",
      " Optimizer iteration 1305, batch 132\n",
      "\n",
      " Learning rate 0.00024652007309320496, Model learning rate 0.0002465200668666512\n",
      "133/391 [=========>....................] - ETA: 19s - loss: 1.0180 - acc: 0.6911\n",
      " Optimizer iteration 1306, batch 133\n",
      "\n",
      " Learning rate 0.00024582605425860313, Model learning rate 0.0002458260569255799\n",
      "134/391 [=========>....................] - ETA: 19s - loss: 1.0187 - acc: 0.6908\n",
      " Optimizer iteration 1307, batch 134\n",
      "\n",
      " Learning rate 0.00024513269514674985, Model learning rate 0.0002451326872687787\n",
      "135/391 [=========>....................] - ETA: 18s - loss: 1.0176 - acc: 0.6907\n",
      " Optimizer iteration 1308, batch 135\n",
      "\n",
      " Learning rate 0.0002444399975572974, Model learning rate 0.00024443998700007796\n",
      "\n",
      " Optimizer iteration 1309, batch 136\n",
      "\n",
      " Learning rate 0.00024374796328818134, Model learning rate 0.00024374795611947775\n",
      "137/391 [=========>....................] - ETA: 18s - loss: 1.0160 - acc: 0.6917\n",
      " Optimizer iteration 1310, batch 137\n",
      "\n",
      " Learning rate 0.00024305659413561572, Model learning rate 0.00024305659462697804\n",
      "138/391 [=========>....................] - ETA: 18s - loss: 1.0158 - acc: 0.6915\n",
      " Optimizer iteration 1311, batch 138\n",
      "\n",
      " Learning rate 0.0002423658918940878, Model learning rate 0.0002423658879706636\n",
      "139/391 [=========>....................] - ETA: 18s - loss: 1.0156 - acc: 0.6918\n",
      " Optimizer iteration 1312, batch 139\n",
      "\n",
      " Learning rate 0.0002416758583563538, Model learning rate 0.0002416758652543649\n",
      "\n",
      " Optimizer iteration 1313, batch 140\n",
      "\n",
      " Learning rate 0.00024098649531343496, Model learning rate 0.00024098649737425148\n",
      "141/391 [=========>....................] - ETA: 18s - loss: 1.0143 - acc: 0.6922\n",
      " Optimizer iteration 1314, batch 141\n",
      "\n",
      " Learning rate 0.00024029780455461138, Model learning rate 0.00024029779888223857\n",
      "142/391 [=========>....................] - ETA: 18s - loss: 1.0142 - acc: 0.6921\n",
      " Optimizer iteration 1315, batch 142\n",
      "\n",
      " Learning rate 0.00023960978786741877, Model learning rate 0.00023960978433024138\n",
      "143/391 [=========>....................] - ETA: 18s - loss: 1.0134 - acc: 0.6919\n",
      " Optimizer iteration 1316, batch 143\n",
      "\n",
      " Learning rate 0.00023892244703764342, Model learning rate 0.00023892245371825993\n",
      "144/391 [==========>...................] - ETA: 18s - loss: 1.0117 - acc: 0.6924\n",
      " Optimizer iteration 1317, batch 144\n",
      "\n",
      " Learning rate 0.00023823578384931632, Model learning rate 0.00023823577794246376\n",
      "145/391 [==========>...................] - ETA: 18s - loss: 1.0111 - acc: 0.6927\n",
      " Optimizer iteration 1318, batch 145\n",
      "\n",
      " Learning rate 0.0002375498000847107, Model learning rate 0.00023754980065859854\n",
      "146/391 [==========>...................] - ETA: 18s - loss: 1.0114 - acc: 0.6926\n",
      " Optimizer iteration 1319, batch 146\n",
      "\n",
      " Learning rate 0.00023686449752433614, Model learning rate 0.00023686449276283383\n",
      "147/391 [==========>...................] - ETA: 18s - loss: 1.0117 - acc: 0.6925\n",
      " Optimizer iteration 1320, batch 147\n",
      "\n",
      " Learning rate 0.00023617987794693357, Model learning rate 0.0002361798833590001\n",
      "148/391 [==========>...................] - ETA: 18s - loss: 1.0119 - acc: 0.6925\n",
      " Optimizer iteration 1321, batch 148\n",
      "\n",
      " Learning rate 0.00023549594312947188, Model learning rate 0.00023549594334326684\n",
      "149/391 [==========>...................] - ETA: 18s - loss: 1.0128 - acc: 0.6925\n",
      " Optimizer iteration 1322, batch 149\n",
      "\n",
      " Learning rate 0.00023481269484714208, Model learning rate 0.00023481270181946456\n",
      "150/391 [==========>...................] - ETA: 18s - loss: 1.0126 - acc: 0.6925\n",
      " Optimizer iteration 1323, batch 150\n",
      "\n",
      " Learning rate 0.00023413013487335333, Model learning rate 0.0002341301296837628\n",
      "151/391 [==========>...................] - ETA: 17s - loss: 1.0127 - acc: 0.6924\n",
      " Optimizer iteration 1324, batch 151\n",
      "\n",
      " Learning rate 0.0002334482649797287, Model learning rate 0.0002334482705919072\n",
      "152/391 [==========>...................] - ETA: 17s - loss: 1.0123 - acc: 0.6926\n",
      " Optimizer iteration 1325, batch 152\n",
      "\n",
      " Learning rate 0.00023276708693609945, Model learning rate 0.00023276708088815212\n",
      "153/391 [==========>...................] - ETA: 17s - loss: 1.0122 - acc: 0.6925\n",
      " Optimizer iteration 1326, batch 153\n",
      "\n",
      " Learning rate 0.00023208660251050156, Model learning rate 0.00023208660422824323\n",
      "154/391 [==========>...................] - ETA: 17s - loss: 1.0123 - acc: 0.6925\n",
      " Optimizer iteration 1327, batch 154\n",
      "\n",
      " Learning rate 0.00023140681346917104, Model learning rate 0.00023140681150835007\n",
      "\n",
      " Optimizer iteration 1328, batch 155\n",
      "\n",
      " Learning rate 0.00023072772157653766, Model learning rate 0.00023072771728038788\n",
      "156/391 [==========>...................] - ETA: 17s - loss: 1.0132 - acc: 0.6925\n",
      " Optimizer iteration 1329, batch 156\n",
      "\n",
      " Learning rate 0.00023004932859522305, Model learning rate 0.00023004932154435664\n",
      "157/391 [===========>..................] - ETA: 17s - loss: 1.0126 - acc: 0.6928\n",
      " Optimizer iteration 1330, batch 157\n",
      "\n",
      " Learning rate 0.00022937163628603436, Model learning rate 0.0002293716388521716\n",
      "158/391 [===========>..................] - ETA: 17s - loss: 1.0123 - acc: 0.6929\n",
      " Optimizer iteration 1331, batch 158\n",
      "\n",
      " Learning rate 0.00022869464640795973, Model learning rate 0.0002286946401000023\n",
      "159/391 [===========>..................] - ETA: 17s - loss: 1.0118 - acc: 0.6932\n",
      " Optimizer iteration 1332, batch 159\n",
      "\n",
      " Learning rate 0.00022801836071816473, Model learning rate 0.00022801835439167917\n",
      "\n",
      " Optimizer iteration 1333, batch 160\n",
      "\n",
      " Learning rate 0.00022734278097198669, Model learning rate 0.00022734278172720224\n",
      "161/391 [===========>..................] - ETA: 17s - loss: 1.0111 - acc: 0.6935\n",
      " Optimizer iteration 1334, batch 161\n",
      "\n",
      " Learning rate 0.0002266679089229306, Model learning rate 0.00022666790755465627\n",
      "162/391 [===========>..................] - ETA: 17s - loss: 1.0115 - acc: 0.6931\n",
      " Optimizer iteration 1335, batch 162\n",
      "\n",
      " Learning rate 0.00022599374632266512, Model learning rate 0.0002259937464259565\n",
      "\n",
      " Optimizer iteration 1336, batch 163\n",
      "\n",
      " Learning rate 0.00022532029492101674, Model learning rate 0.0002253202983411029\n",
      "164/391 [===========>..................] - ETA: 16s - loss: 1.0103 - acc: 0.6935\n",
      " Optimizer iteration 1337, batch 164\n",
      "\n",
      " Learning rate 0.0002246475564659666, Model learning rate 0.0002246475633000955\n",
      "165/391 [===========>..................] - ETA: 16s - loss: 1.0107 - acc: 0.6936\n",
      " Optimizer iteration 1338, batch 165\n",
      "\n",
      " Learning rate 0.00022397553270364545, Model learning rate 0.00022397552675101906\n",
      "166/391 [===========>..................] - ETA: 16s - loss: 1.0114 - acc: 0.6935\n",
      " Optimizer iteration 1339, batch 166\n",
      "\n",
      " Learning rate 0.000223304225378328, Model learning rate 0.00022330423234961927\n",
      "167/391 [===========>..................] - ETA: 16s - loss: 1.0120 - acc: 0.6934\n",
      " Optimizer iteration 1340, batch 167\n",
      "\n",
      " Learning rate 0.00022263363623243056, Model learning rate 0.00022263363644015044\n",
      "168/391 [===========>..................] - ETA: 16s - loss: 1.0110 - acc: 0.6938\n",
      " Optimizer iteration 1341, batch 168\n",
      "\n",
      " Learning rate 0.00022196376700650496, Model learning rate 0.00022196376812644303\n",
      "\n",
      " Optimizer iteration 1342, batch 169\n",
      "\n",
      " Learning rate 0.00022129461943923406, Model learning rate 0.0002212946128565818\n",
      "170/391 [============>.................] - ETA: 16s - loss: 1.0105 - acc: 0.6937\n",
      " Optimizer iteration 1343, batch 170\n",
      "\n",
      " Learning rate 0.0002206261952674284, Model learning rate 0.00022062619973439723\n",
      "171/391 [============>.................] - ETA: 16s - loss: 1.0101 - acc: 0.6939\n",
      " Optimizer iteration 1344, batch 171\n",
      "\n",
      " Learning rate 0.00021995849622602015, Model learning rate 0.00021995849965605885\n",
      "172/391 [============>.................] - ETA: 16s - loss: 1.0108 - acc: 0.6937\n",
      " Optimizer iteration 1345, batch 172\n",
      "\n",
      " Learning rate 0.00021929152404805957, Model learning rate 0.00021929152717348188\n",
      "\n",
      " Optimizer iteration 1346, batch 173\n",
      "\n",
      " Learning rate 0.0002186252804647107, Model learning rate 0.00021862528228666633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174/391 [============>.................] - ETA: 16s - loss: 1.0107 - acc: 0.6939\n",
      " Optimizer iteration 1347, batch 174\n",
      "\n",
      " Learning rate 0.0002179597672052458, Model learning rate 0.0002179597649956122\n",
      "\n",
      " Optimizer iteration 1348, batch 175\n",
      "\n",
      " Learning rate 0.00021729498599704216, Model learning rate 0.00021729498985223472\n",
      "176/391 [============>.................] - ETA: 16s - loss: 1.0110 - acc: 0.6940\n",
      " Optimizer iteration 1349, batch 176\n",
      "\n",
      " Learning rate 0.00021663093856557708, Model learning rate 0.00021663094230461866\n",
      "177/391 [============>.................] - ETA: 15s - loss: 1.0109 - acc: 0.6940\n",
      " Optimizer iteration 1350, batch 177\n",
      "\n",
      " Learning rate 0.00021596762663442215, Model learning rate 0.000215967622352764\n",
      "178/391 [============>.................] - ETA: 16s - loss: 1.0120 - acc: 0.6939\n",
      " Optimizer iteration 1351, batch 178\n",
      "\n",
      " Learning rate 0.00021530505192524118, Model learning rate 0.00021530505910050124\n",
      "179/391 [============>.................] - ETA: 15s - loss: 1.0120 - acc: 0.6939\n",
      " Optimizer iteration 1352, batch 179\n",
      "\n",
      " Learning rate 0.0002146432161577842, Model learning rate 0.00021464320889208466\n",
      "180/391 [============>.................] - ETA: 15s - loss: 1.0122 - acc: 0.6938\n",
      " Optimizer iteration 1353, batch 180\n",
      "\n",
      " Learning rate 0.00021398212104988275, Model learning rate 0.00021398211538325995\n",
      "181/391 [============>.................] - ETA: 15s - loss: 1.0119 - acc: 0.6939\n",
      " Optimizer iteration 1354, batch 181\n",
      "\n",
      " Learning rate 0.0002133217683174466, Model learning rate 0.0002133217640221119\n",
      "182/391 [============>.................] - ETA: 15s - loss: 1.0122 - acc: 0.6939\n",
      " Optimizer iteration 1355, batch 182\n",
      "\n",
      " Learning rate 0.00021266215967445824, Model learning rate 0.00021266215480864048\n",
      "183/391 [=============>................] - ETA: 15s - loss: 1.0115 - acc: 0.6943\n",
      " Optimizer iteration 1356, batch 183\n",
      "\n",
      " Learning rate 0.0002120032968329687, Model learning rate 0.00021200330229476094\n",
      "184/391 [=============>................] - ETA: 15s - loss: 1.0112 - acc: 0.6944\n",
      " Optimizer iteration 1357, batch 184\n",
      "\n",
      " Learning rate 0.0002113451815030939, Model learning rate 0.00021134517737664282\n",
      "185/391 [=============>................] - ETA: 15s - loss: 1.0115 - acc: 0.6940\n",
      " Optimizer iteration 1358, batch 185\n",
      "\n",
      " Learning rate 0.00021068781539300874, Model learning rate 0.00021068780915811658\n",
      "186/391 [=============>................] - ETA: 15s - loss: 1.0117 - acc: 0.6941\n",
      " Optimizer iteration 1359, batch 186\n",
      "\n",
      " Learning rate 0.0002100312002089441, Model learning rate 0.0002100311976391822\n",
      "\n",
      " Optimizer iteration 1360, batch 187\n",
      "\n",
      " Learning rate 0.00020937533765518184, Model learning rate 0.00020937534281983972\n",
      "188/391 [=============>................] - ETA: 15s - loss: 1.0103 - acc: 0.6944\n",
      " Optimizer iteration 1361, batch 188\n",
      "\n",
      " Learning rate 0.0002087202294340494, Model learning rate 0.00020872023014817387\n",
      "189/391 [=============>................] - ETA: 15s - loss: 1.0108 - acc: 0.6943\n",
      " Optimizer iteration 1362, batch 189\n",
      "\n",
      " Learning rate 0.00020806587724591725, Model learning rate 0.0002080658741760999\n",
      "190/391 [=============>................] - ETA: 15s - loss: 1.0105 - acc: 0.6944\n",
      " Optimizer iteration 1363, batch 190\n",
      "\n",
      " Learning rate 0.00020741228278919343, Model learning rate 0.00020741228945553303\n",
      "191/391 [=============>................] - ETA: 15s - loss: 1.0098 - acc: 0.6947\n",
      " Optimizer iteration 1364, batch 191\n",
      "\n",
      " Learning rate 0.00020675944776031875, Model learning rate 0.0002067594468826428\n",
      "\n",
      " Optimizer iteration 1365, batch 192\n",
      "\n",
      " Learning rate 0.00020610737385376348, Model learning rate 0.0002061073755612597\n",
      "193/391 [=============>................] - ETA: 14s - loss: 1.0110 - acc: 0.6939\n",
      " Optimizer iteration 1366, batch 193\n",
      "\n",
      " Learning rate 0.0002054560627620219, Model learning rate 0.00020545606093946844\n",
      "194/391 [=============>................] - ETA: 14s - loss: 1.0109 - acc: 0.6940\n",
      " Optimizer iteration 1367, batch 194\n",
      "\n",
      " Learning rate 0.00020480551617560832, Model learning rate 0.0002048055175691843\n",
      "195/391 [=============>................] - ETA: 14s - loss: 1.0106 - acc: 0.6942\n",
      " Optimizer iteration 1368, batch 195\n",
      "\n",
      " Learning rate 0.0002041557357830534, Model learning rate 0.00020415573089849204\n",
      "\n",
      " Optimizer iteration 1369, batch 196\n",
      "\n",
      " Learning rate 0.00020350672327089814, Model learning rate 0.0002035067300312221\n",
      "197/391 [==============>...............] - ETA: 14s - loss: 1.0108 - acc: 0.6945\n",
      " Optimizer iteration 1370, batch 197\n",
      "\n",
      " Learning rate 0.00020285848032369137, Model learning rate 0.00020285848586354405\n",
      "198/391 [==============>...............] - ETA: 14s - loss: 1.0108 - acc: 0.6947\n",
      " Optimizer iteration 1371, batch 198\n",
      "\n",
      " Learning rate 0.00020221100862398374, Model learning rate 0.0002022110129473731\n",
      "\n",
      " Optimizer iteration 1372, batch 199\n",
      "\n",
      " Learning rate 0.00020156430985232465, Model learning rate 0.00020156431128270924\n",
      "200/391 [==============>...............] - ETA: 14s - loss: 1.0098 - acc: 0.6951\n",
      " Optimizer iteration 1373, batch 200\n",
      "\n",
      " Learning rate 0.00020091838568725683, Model learning rate 0.0002009183808695525\n",
      "201/391 [==============>...............] - ETA: 14s - loss: 1.0094 - acc: 0.6952\n",
      " Optimizer iteration 1374, batch 201\n",
      "\n",
      " Learning rate 0.0002002732378053131, Model learning rate 0.00020027323625981808\n",
      "202/391 [==============>...............] - ETA: 14s - loss: 1.0095 - acc: 0.6949\n",
      " Optimizer iteration 1375, batch 202\n",
      "\n",
      " Learning rate 0.00019962886788101047, Model learning rate 0.00019962886290159076\n",
      "\n",
      " Optimizer iteration 1376, batch 203\n",
      "\n",
      " Learning rate 0.00019898527758684787, Model learning rate 0.00019898527534678578\n",
      "204/391 [==============>...............] - ETA: 14s - loss: 1.0089 - acc: 0.6952\n",
      " Optimizer iteration 1377, batch 204\n",
      "\n",
      " Learning rate 0.00019834246859329964, Model learning rate 0.00019834247359540313\n",
      "205/391 [==============>...............] - ETA: 13s - loss: 1.0082 - acc: 0.6956\n",
      " Optimizer iteration 1378, batch 205\n",
      "\n",
      " Learning rate 0.00019770044256881258, Model learning rate 0.0001977004430955276\n",
      "206/391 [==============>...............] - ETA: 13s - loss: 1.0086 - acc: 0.6955\n",
      " Optimizer iteration 1379, batch 206\n",
      "\n",
      " Learning rate 0.00019705920117980147, Model learning rate 0.00019705919839907438\n",
      "207/391 [==============>...............] - ETA: 13s - loss: 1.0074 - acc: 0.6960\n",
      " Optimizer iteration 1380, batch 207\n",
      "\n",
      " Learning rate 0.00019641874609064441, Model learning rate 0.0001964187395060435\n",
      "208/391 [==============>...............] - ETA: 13s - loss: 1.0069 - acc: 0.6962\n",
      " Optimizer iteration 1381, batch 208\n",
      "\n",
      " Learning rate 0.00019577907896367846, Model learning rate 0.00019577908096835017\n",
      "\n",
      " Optimizer iteration 1382, batch 209\n",
      "\n",
      " Learning rate 0.00019514020145919536, Model learning rate 0.00019514020823407918\n",
      "210/391 [===============>..............] - ETA: 13s - loss: 1.0066 - acc: 0.6961\n",
      " Optimizer iteration 1383, batch 210\n",
      "\n",
      " Learning rate 0.00019450211523543793, Model learning rate 0.00019450212130323052\n",
      "\n",
      " Optimizer iteration 1384, batch 211\n",
      "\n",
      " Learning rate 0.0001938648219485944, Model learning rate 0.0001938648201758042\n",
      "212/391 [===============>..............] - ETA: 13s - loss: 1.0058 - acc: 0.6963\n",
      " Optimizer iteration 1385, batch 212\n",
      "\n",
      " Learning rate 0.0001932283232527956, Model learning rate 0.00019322831940371543\n",
      "\n",
      " Optimizer iteration 1386, batch 213\n",
      "\n",
      " Learning rate 0.00019259262080010937, Model learning rate 0.00019259261898696423\n",
      "214/391 [===============>..............] - ETA: 13s - loss: 1.0051 - acc: 0.6966\n",
      " Optimizer iteration 1387, batch 214\n",
      "\n",
      " Learning rate 0.00019195771624053743, Model learning rate 0.00019195771892555058\n",
      "215/391 [===============>..............] - ETA: 13s - loss: 1.0051 - acc: 0.6967\n",
      " Optimizer iteration 1388, batch 215\n",
      "\n",
      " Learning rate 0.0001913236112220101, Model learning rate 0.00019132360466755927\n",
      "\n",
      " Optimizer iteration 1389, batch 216\n",
      "\n",
      " Learning rate 0.00019069030739038222, Model learning rate 0.00019069030531682074\n",
      "217/391 [===============>..............] - ETA: 13s - loss: 1.0045 - acc: 0.6967\n",
      " Optimizer iteration 1390, batch 217\n",
      "\n",
      " Learning rate 0.00019005780638942983, Model learning rate 0.00019005780632141978\n",
      "218/391 [===============>..............] - ETA: 13s - loss: 1.0047 - acc: 0.6966\n",
      " Optimizer iteration 1391, batch 218\n",
      "\n",
      " Learning rate 0.00018942610986084484, Model learning rate 0.00018942610768135637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/391 [===============>..............] - ETA: 12s - loss: 1.0049 - acc: 0.6967\n",
      " Optimizer iteration 1392, batch 219\n",
      "\n",
      " Learning rate 0.0001887952194442309, Model learning rate 0.00018879522394854575\n",
      "220/391 [===============>..............] - ETA: 12s - loss: 1.0049 - acc: 0.6968\n",
      " Optimizer iteration 1393, batch 220\n",
      "\n",
      " Learning rate 0.00018816513677709934, Model learning rate 0.0001881651405710727\n",
      "221/391 [===============>..............] - ETA: 12s - loss: 1.0046 - acc: 0.6969\n",
      " Optimizer iteration 1394, batch 221\n",
      "\n",
      " Learning rate 0.00018753586349486552, Model learning rate 0.0001875358575489372\n",
      "222/391 [================>.............] - ETA: 12s - loss: 1.0039 - acc: 0.6973\n",
      " Optimizer iteration 1395, batch 222\n",
      "\n",
      " Learning rate 0.00018690740123084316, Model learning rate 0.00018690740398596972\n",
      "\n",
      " Optimizer iteration 1396, batch 223\n",
      "\n",
      " Learning rate 0.00018627975161624165, Model learning rate 0.0001862797507783398\n",
      "224/391 [================>.............] - ETA: 12s - loss: 1.0034 - acc: 0.6975\n",
      " Optimizer iteration 1397, batch 224\n",
      "\n",
      " Learning rate 0.00018565291628016062, Model learning rate 0.00018565291247796267\n",
      "\n",
      " Optimizer iteration 1398, batch 225\n",
      "\n",
      " Learning rate 0.00018502689684958662, Model learning rate 0.00018502690363675356\n",
      "226/391 [================>.............] - ETA: 12s - loss: 1.0035 - acc: 0.6977\n",
      " Optimizer iteration 1399, batch 226\n",
      "\n",
      " Learning rate 0.00018440169494938802, Model learning rate 0.000184401695150882\n",
      "227/391 [================>.............] - ETA: 12s - loss: 1.0037 - acc: 0.6974\n",
      " Optimizer iteration 1400, batch 227\n",
      "\n",
      " Learning rate 0.0001837773122023114, Model learning rate 0.00018377731612417847\n",
      "228/391 [================>.............] - ETA: 12s - loss: 1.0035 - acc: 0.6974\n",
      " Optimizer iteration 1401, batch 228\n",
      "\n",
      " Learning rate 0.00018315375022897736, Model learning rate 0.00018315375200472772\n",
      "\n",
      " Optimizer iteration 1402, batch 229\n",
      "\n",
      " Learning rate 0.0001825310106478762, Model learning rate 0.000182531017344445\n",
      "230/391 [================>.............] - ETA: 12s - loss: 1.0030 - acc: 0.6976\n",
      " Optimizer iteration 1403, batch 230\n",
      "\n",
      " Learning rate 0.00018190909507536323, Model learning rate 0.00018190909759141505\n",
      "231/391 [================>.............] - ETA: 12s - loss: 1.0031 - acc: 0.6977\n",
      " Optimizer iteration 1404, batch 231\n",
      "\n",
      " Learning rate 0.00018128800512565513, Model learning rate 0.00018128800729755312\n",
      "232/391 [================>.............] - ETA: 11s - loss: 1.0040 - acc: 0.6976\n",
      " Optimizer iteration 1405, batch 232\n",
      "\n",
      " Learning rate 0.00018066774241082612, Model learning rate 0.0001806677464628592\n",
      "233/391 [================>.............] - ETA: 11s - loss: 1.0041 - acc: 0.6975\n",
      " Optimizer iteration 1406, batch 233\n",
      "\n",
      " Learning rate 0.0001800483085408025, Model learning rate 0.00018004831508733332\n",
      "234/391 [================>.............] - ETA: 11s - loss: 1.0041 - acc: 0.6974\n",
      " Optimizer iteration 1407, batch 234\n",
      "\n",
      " Learning rate 0.00017942970512335998, Model learning rate 0.00017942969861906022\n",
      "235/391 [=================>............] - ETA: 11s - loss: 1.0039 - acc: 0.6975\n",
      " Optimizer iteration 1408, batch 235\n",
      "\n",
      " Learning rate 0.00017881193376411818, Model learning rate 0.0001788119407137856\n",
      "\n",
      " Optimizer iteration 1409, batch 236\n",
      "\n",
      " Learning rate 0.00017819499606653772, Model learning rate 0.00017819499771576375\n",
      "237/391 [=================>............] - ETA: 11s - loss: 1.0032 - acc: 0.6978\n",
      " Optimizer iteration 1410, batch 237\n",
      "\n",
      " Learning rate 0.00017757889363191482, Model learning rate 0.00017757889872882515\n",
      "238/391 [=================>............] - ETA: 11s - loss: 1.0033 - acc: 0.6976\n",
      " Optimizer iteration 1411, batch 238\n",
      "\n",
      " Learning rate 0.00017696362805937776, Model learning rate 0.00017696362920105457\n",
      "239/391 [=================>............] - ETA: 11s - loss: 1.0030 - acc: 0.6978\n",
      " Optimizer iteration 1412, batch 239\n",
      "\n",
      " Learning rate 0.00017634920094588308, Model learning rate 0.00017634920368436724\n",
      "\n",
      " Optimizer iteration 1413, batch 240\n",
      "\n",
      " Learning rate 0.00017573561388621101, Model learning rate 0.00017573560762684792\n",
      "241/391 [=================>............] - ETA: 11s - loss: 1.0026 - acc: 0.6979\n",
      " Optimizer iteration 1414, batch 241\n",
      "\n",
      " Learning rate 0.00017512286847296105, Model learning rate 0.00017512287013232708\n",
      "242/391 [=================>............] - ETA: 11s - loss: 1.0032 - acc: 0.6977\n",
      " Optimizer iteration 1415, batch 242\n",
      "\n",
      " Learning rate 0.0001745109662965481, Model learning rate 0.00017451096209697425\n",
      "243/391 [=================>............] - ETA: 11s - loss: 1.0035 - acc: 0.6976\n",
      " Optimizer iteration 1416, batch 243\n",
      "\n",
      " Learning rate 0.0001738999089451991, Model learning rate 0.0001738999126246199\n",
      "244/391 [=================>............] - ETA: 11s - loss: 1.0031 - acc: 0.6980\n",
      " Optimizer iteration 1417, batch 244\n",
      "\n",
      " Learning rate 0.00017328969800494727, Model learning rate 0.00017328969261143357\n",
      "\n",
      " Optimizer iteration 1418, batch 245\n",
      "\n",
      " Learning rate 0.0001726803350596297, Model learning rate 0.0001726803311612457\n",
      "246/391 [=================>............] - ETA: 10s - loss: 1.0027 - acc: 0.6983\n",
      " Optimizer iteration 1419, batch 246\n",
      "\n",
      " Learning rate 0.00017207182169088204, Model learning rate 0.00017207182827405632\n",
      "247/391 [=================>............] - ETA: 10s - loss: 1.0030 - acc: 0.6981\n",
      " Optimizer iteration 1420, batch 247\n",
      "\n",
      " Learning rate 0.00017146415947813472, Model learning rate 0.00017146415484603494\n",
      "248/391 [==================>...........] - ETA: 10s - loss: 1.0026 - acc: 0.6982\n",
      " Optimizer iteration 1421, batch 248\n",
      "\n",
      " Learning rate 0.00017085734999860937, Model learning rate 0.00017085735453292727\n",
      "249/391 [==================>...........] - ETA: 10s - loss: 1.0029 - acc: 0.6982\n",
      " Optimizer iteration 1422, batch 249\n",
      "\n",
      " Learning rate 0.00017025139482731384, Model learning rate 0.00017025139823090285\n",
      "250/391 [==================>...........] - ETA: 10s - loss: 1.0030 - acc: 0.6982\n",
      " Optimizer iteration 1423, batch 250\n",
      "\n",
      " Learning rate 0.00016964629553703893, Model learning rate 0.0001696463004918769\n",
      "251/391 [==================>...........] - ETA: 10s - loss: 1.0022 - acc: 0.6985\n",
      " Optimizer iteration 1424, batch 251\n",
      "\n",
      " Learning rate 0.000169042053698354, Model learning rate 0.0001690420467639342\n",
      "\n",
      " Optimizer iteration 1425, batch 252\n",
      "\n",
      " Learning rate 0.00016843867087960252, Model learning rate 0.0001684386661509052\n",
      "253/391 [==================>...........] - ETA: 10s - loss: 1.0017 - acc: 0.6987\n",
      " Optimizer iteration 1426, batch 253\n",
      "\n",
      " Learning rate 0.00016783614864689827, Model learning rate 0.00016783614410087466\n",
      "254/391 [==================>...........] - ETA: 10s - loss: 1.0014 - acc: 0.6989\n",
      " Optimizer iteration 1427, batch 254\n",
      "\n",
      " Learning rate 0.00016723448856412188, Model learning rate 0.00016723449516575783\n",
      "255/391 [==================>...........] - ETA: 10s - loss: 1.0018 - acc: 0.6987\n",
      " Optimizer iteration 1428, batch 255\n",
      "\n",
      " Learning rate 0.00016663369219291558, Model learning rate 0.00016663369024172425\n",
      "\n",
      " Optimizer iteration 1429, batch 256\n",
      "\n",
      " Learning rate 0.00016603376109268042, Model learning rate 0.00016603375843260437\n",
      "257/391 [==================>...........] - ETA: 10s - loss: 1.0009 - acc: 0.6993\n",
      " Optimizer iteration 1430, batch 257\n",
      "\n",
      " Learning rate 0.00016543469682057105, Model learning rate 0.0001654346997383982\n",
      "258/391 [==================>...........] - ETA: 10s - loss: 1.0011 - acc: 0.6991\n",
      " Optimizer iteration 1431, batch 258\n",
      "\n",
      " Learning rate 0.00016483650093149227, Model learning rate 0.0001648364996071905\n",
      "259/391 [==================>...........] - ETA: 9s - loss: 1.0009 - acc: 0.6989 \n",
      " Optimizer iteration 1432, batch 259\n",
      "\n",
      " Learning rate 0.00016423917497809532, Model learning rate 0.0001642391725908965\n",
      "260/391 [==================>...........] - ETA: 9s - loss: 1.0009 - acc: 0.6989\n",
      " Optimizer iteration 1433, batch 260\n",
      "\n",
      " Learning rate 0.00016364272051077333, Model learning rate 0.0001636427186895162\n",
      "261/391 [===================>..........] - ETA: 9s - loss: 1.0009 - acc: 0.6989\n",
      " Optimizer iteration 1434, batch 261\n",
      "\n",
      " Learning rate 0.00016304713907765712, Model learning rate 0.0001630471379030496\n",
      "262/391 [===================>..........] - ETA: 9s - loss: 1.0012 - acc: 0.6987\n",
      " Optimizer iteration 1435, batch 262\n",
      "\n",
      " Learning rate 0.00016245243222461197, Model learning rate 0.0001624524302314967\n",
      "263/391 [===================>..........] - ETA: 9s - loss: 1.0010 - acc: 0.6987\n",
      " Optimizer iteration 1436, batch 263\n",
      "\n",
      " Learning rate 0.00016185860149523285, Model learning rate 0.0001618585956748575\n",
      "264/391 [===================>..........] - ETA: 9s - loss: 1.0007 - acc: 0.6989\n",
      " Optimizer iteration 1437, batch 264\n",
      "\n",
      " Learning rate 0.00016126564843084052, Model learning rate 0.00016126564878504723\n",
      "265/391 [===================>..........] - ETA: 9s - loss: 1.0006 - acc: 0.6989\n",
      " Optimizer iteration 1438, batch 265\n",
      "\n",
      " Learning rate 0.00016067357457047837, Model learning rate 0.00016067357501015067\n",
      "266/391 [===================>..........] - ETA: 9s - loss: 1.0003 - acc: 0.6990\n",
      " Optimizer iteration 1439, batch 266\n",
      "\n",
      " Learning rate 0.00016008238145090692, Model learning rate 0.0001600823743501678\n",
      "267/391 [===================>..........] - ETA: 9s - loss: 1.0000 - acc: 0.6991\n",
      " Optimizer iteration 1440, batch 267\n",
      "\n",
      " Learning rate 0.00015949207060660136, Model learning rate 0.0001594920759089291\n",
      "268/391 [===================>..........] - ETA: 9s - loss: 0.9995 - acc: 0.6994\n",
      " Optimizer iteration 1441, batch 268\n",
      "\n",
      " Learning rate 0.00015890264356974688, Model learning rate 0.0001589026505826041\n",
      "\n",
      " Optimizer iteration 1442, batch 269\n",
      "\n",
      " Learning rate 0.00015831410187023388, Model learning rate 0.0001583140983711928\n",
      "270/391 [===================>..........] - ETA: 9s - loss: 0.9987 - acc: 0.6997\n",
      " Optimizer iteration 1443, batch 270\n",
      "\n",
      " Learning rate 0.00015772644703565563, Model learning rate 0.00015772644837852567\n",
      "\n",
      " Optimizer iteration 1444, batch 271\n",
      "\n",
      " Learning rate 0.00015713968059130346, Model learning rate 0.00015713968605268747\n",
      "272/391 [===================>..........] - ETA: 8s - loss: 0.9974 - acc: 0.7001\n",
      " Optimizer iteration 1445, batch 272\n",
      "\n",
      " Learning rate 0.00015655380406016234, Model learning rate 0.00015655379684176296\n",
      "273/391 [===================>..........] - ETA: 8s - loss: 0.9968 - acc: 0.7001\n",
      " Optimizer iteration 1446, batch 273\n",
      "\n",
      " Learning rate 0.000155968818962908, Model learning rate 0.00015596882440149784\n",
      "274/391 [====================>.........] - ETA: 8s - loss: 0.9965 - acc: 0.7002\n",
      " Optimizer iteration 1447, batch 274\n",
      "\n",
      " Learning rate 0.00015538472681790185, Model learning rate 0.00015538472507614642\n",
      "\n",
      " Optimizer iteration 1448, batch 275\n",
      "\n",
      " Learning rate 0.00015480152914118783, Model learning rate 0.00015480152796953917\n",
      "276/391 [====================>.........] - ETA: 8s - loss: 0.9959 - acc: 0.7004\n",
      " Optimizer iteration 1449, batch 276\n",
      "\n",
      " Learning rate 0.00015421922744648846, Model learning rate 0.00015421923308167607\n",
      "\n",
      " Optimizer iteration 1450, batch 277\n",
      "\n",
      " Learning rate 0.00015363782324520031, Model learning rate 0.0001536378258606419\n",
      "278/391 [====================>.........] - ETA: 8s - loss: 0.9960 - acc: 0.7005\n",
      " Optimizer iteration 1451, batch 278\n",
      "\n",
      " Learning rate 0.00015305731804639066, Model learning rate 0.00015305732085835189\n",
      "279/391 [====================>.........] - ETA: 8s - loss: 0.9966 - acc: 0.7004\n",
      " Optimizer iteration 1452, batch 279\n",
      "\n",
      " Learning rate 0.00015247771335679373, Model learning rate 0.00015247771807480603\n",
      "\n",
      " Optimizer iteration 1453, batch 280\n",
      "\n",
      " Learning rate 0.00015189901068080535, Model learning rate 0.00015189901751000434\n",
      "281/391 [====================>.........] - ETA: 8s - loss: 0.9966 - acc: 0.7005\n",
      " Optimizer iteration 1454, batch 281\n",
      "\n",
      " Learning rate 0.00015132121152048117, Model learning rate 0.00015132120461203158\n",
      "282/391 [====================>.........] - ETA: 8s - loss: 0.9963 - acc: 0.7007\n",
      " Optimizer iteration 1455, batch 282\n",
      "\n",
      " Learning rate 0.00015074431737553158, Model learning rate 0.00015074432303663343\n",
      "283/391 [====================>.........] - ETA: 8s - loss: 0.9960 - acc: 0.7007\n",
      " Optimizer iteration 1456, batch 283\n",
      "\n",
      " Learning rate 0.00015016832974331724, Model learning rate 0.00015016832912806422\n",
      "284/391 [====================>.........] - ETA: 8s - loss: 0.9954 - acc: 0.7010\n",
      " Optimizer iteration 1457, batch 284\n",
      "\n",
      " Learning rate 0.00014959325011884683, Model learning rate 0.00014959325199015439\n",
      "285/391 [====================>.........] - ETA: 8s - loss: 0.9958 - acc: 0.7009\n",
      " Optimizer iteration 1458, batch 285\n",
      "\n",
      " Learning rate 0.00014901907999477165, Model learning rate 0.00014901907707098871\n",
      "286/391 [====================>.........] - ETA: 7s - loss: 0.9961 - acc: 0.7007\n",
      " Optimizer iteration 1459, batch 286\n",
      "\n",
      " Learning rate 0.00014844582086138232, Model learning rate 0.00014844581892248243\n",
      "\n",
      " Optimizer iteration 1460, batch 287\n",
      "\n",
      " Learning rate 0.0001478734742066054, Model learning rate 0.00014787347754463553\n",
      "288/391 [=====================>........] - ETA: 7s - loss: 0.9957 - acc: 0.7008\n",
      " Optimizer iteration 1461, batch 288\n",
      "\n",
      " Learning rate 0.00014730204151599846, Model learning rate 0.0001473020383855328\n",
      "289/391 [=====================>........] - ETA: 7s - loss: 0.9956 - acc: 0.7007\n",
      " Optimizer iteration 1462, batch 289\n",
      "\n",
      " Learning rate 0.00014673152427274738, Model learning rate 0.00014673153054900467\n",
      "290/391 [=====================>........] - ETA: 7s - loss: 0.9954 - acc: 0.7008\n",
      " Optimizer iteration 1463, batch 290\n",
      "\n",
      " Learning rate 0.00014616192395766186, Model learning rate 0.0001461619249312207\n",
      "\n",
      " Optimizer iteration 1464, batch 291\n",
      "\n",
      " Learning rate 0.000145593242049171, Model learning rate 0.00014559323608409613\n",
      "292/391 [=====================>........] - ETA: 7s - loss: 0.9949 - acc: 0.7009\n",
      " Optimizer iteration 1465, batch 292\n",
      "\n",
      " Learning rate 0.00014502548002332088, Model learning rate 0.00014502547855954617\n",
      "\n",
      " Optimizer iteration 1466, batch 293\n",
      "\n",
      " Learning rate 0.0001444586393537699, Model learning rate 0.0001444586378056556\n",
      "294/391 [=====================>........] - ETA: 7s - loss: 0.9946 - acc: 0.7012\n",
      " Optimizer iteration 1467, batch 294\n",
      "\n",
      " Learning rate 0.00014389272151178452, Model learning rate 0.00014389272837433964\n",
      "295/391 [=====================>........] - ETA: 7s - loss: 0.9944 - acc: 0.7012\n",
      " Optimizer iteration 1468, batch 295\n",
      "\n",
      " Learning rate 0.00014332772796623656, Model learning rate 0.00014332772116176784\n",
      "\n",
      " Optimizer iteration 1469, batch 296\n",
      "\n",
      " Learning rate 0.00014276366018359842, Model learning rate 0.00014276365982368588\n",
      "297/391 [=====================>........] - ETA: 7s - loss: 0.9949 - acc: 0.7010\n",
      " Optimizer iteration 1470, batch 297\n",
      "\n",
      " Learning rate 0.00014220051962793951, Model learning rate 0.00014220051525626332\n",
      "298/391 [=====================>........] - ETA: 7s - loss: 0.9955 - acc: 0.7009\n",
      " Optimizer iteration 1471, batch 298\n",
      "\n",
      " Learning rate 0.000141638307760923, Model learning rate 0.00014163830201141536\n",
      "299/391 [=====================>........] - ETA: 6s - loss: 0.9951 - acc: 0.7010\n",
      " Optimizer iteration 1472, batch 299\n",
      "\n",
      " Learning rate 0.00014107702604180118, Model learning rate 0.00014107702008914202\n",
      "\n",
      " Optimizer iteration 1473, batch 300\n",
      "\n",
      " Learning rate 0.0001405166759274123, Model learning rate 0.0001405166694894433\n",
      "301/391 [======================>.......] - ETA: 6s - loss: 0.9947 - acc: 0.7012\n",
      " Optimizer iteration 1474, batch 301\n",
      "\n",
      " Learning rate 0.0001399572588721769, Model learning rate 0.00013995726476423442\n",
      "302/391 [======================>.......] - ETA: 6s - loss: 0.9943 - acc: 0.7013\n",
      " Optimizer iteration 1475, batch 302\n",
      "\n",
      " Learning rate 0.0001393987763280928, Model learning rate 0.00013939877680968493\n",
      "\n",
      " Optimizer iteration 1476, batch 303\n",
      "\n",
      " Learning rate 0.00013884122974473307, Model learning rate 0.00013884123472962528\n",
      "304/391 [======================>.......] - ETA: 6s - loss: 0.9944 - acc: 0.7012\n",
      " Optimizer iteration 1477, batch 304\n",
      "\n",
      " Learning rate 0.0001382846205692413, Model learning rate 0.00013828462397214025\n",
      "305/391 [======================>.......] - ETA: 6s - loss: 0.9942 - acc: 0.7012\n",
      " Optimizer iteration 1478, batch 305\n",
      "\n",
      " Learning rate 0.00013772895024632753, Model learning rate 0.00013772894453722984\n",
      "306/391 [======================>.......] - ETA: 6s - loss: 0.9945 - acc: 0.7010\n",
      " Optimizer iteration 1479, batch 306\n",
      "\n",
      " Learning rate 0.00013717422021826569, Model learning rate 0.0001371742255287245\n",
      "307/391 [======================>.......] - ETA: 6s - loss: 0.9945 - acc: 0.7010\n",
      " Optimizer iteration 1480, batch 307\n",
      "\n",
      " Learning rate 0.0001366204319248885, Model learning rate 0.00013662043784279376\n",
      "308/391 [======================>.......] - ETA: 6s - loss: 0.9942 - acc: 0.7011\n",
      " Optimizer iteration 1481, batch 308\n",
      "\n",
      " Learning rate 0.00013606758680358445, Model learning rate 0.00013606758147943765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizer iteration 1482, batch 309\n",
      "\n",
      " Learning rate 0.00013551568628929433, Model learning rate 0.0001355156855424866\n",
      "310/391 [======================>.......] - ETA: 6s - loss: 0.9937 - acc: 0.7014\n",
      " Optimizer iteration 1483, batch 310\n",
      "\n",
      " Learning rate 0.0001349647318145067, Model learning rate 0.0001349647354800254\n",
      "311/391 [======================>.......] - ETA: 6s - loss: 0.9934 - acc: 0.7015\n",
      " Optimizer iteration 1484, batch 311\n",
      "\n",
      " Learning rate 0.00013441472480925492, Model learning rate 0.00013441473129205406\n",
      "312/391 [======================>.......] - ETA: 5s - loss: 0.9937 - acc: 0.7014\n",
      " Optimizer iteration 1485, batch 312\n",
      "\n",
      " Learning rate 0.0001338656667011134, Model learning rate 0.00013386567297857255\n",
      "313/391 [=======================>......] - ETA: 5s - loss: 0.9932 - acc: 0.7017\n",
      " Optimizer iteration 1486, batch 313\n",
      "\n",
      " Learning rate 0.00013331755891519265, Model learning rate 0.00013331756053958088\n",
      "314/391 [=======================>......] - ETA: 5s - loss: 0.9929 - acc: 0.7017\n",
      " Optimizer iteration 1487, batch 314\n",
      "\n",
      " Learning rate 0.00013277040287413755, Model learning rate 0.0001327704085269943\n",
      "315/391 [=======================>......] - ETA: 5s - loss: 0.9932 - acc: 0.7015\n",
      " Optimizer iteration 1488, batch 315\n",
      "\n",
      " Learning rate 0.00013222419999812246, Model learning rate 0.00013222420238889754\n",
      "316/391 [=======================>......] - ETA: 5s - loss: 0.9931 - acc: 0.7016\n",
      " Optimizer iteration 1489, batch 316\n",
      "\n",
      " Learning rate 0.0001316789517048473, Model learning rate 0.00013167895667720586\n",
      "\n",
      " Optimizer iteration 1490, batch 317\n",
      "\n",
      " Learning rate 0.00013113465940953496, Model learning rate 0.00013113465684000403\n",
      "318/391 [=======================>......] - ETA: 5s - loss: 0.9933 - acc: 0.7015\n",
      " Optimizer iteration 1491, batch 318\n",
      "\n",
      " Learning rate 0.00013059132452492651, Model learning rate 0.00013059131742920727\n",
      "319/391 [=======================>......] - ETA: 5s - loss: 0.9931 - acc: 0.7017\n",
      " Optimizer iteration 1492, batch 319\n",
      "\n",
      " Learning rate 0.0001300489484612779, Model learning rate 0.0001300489529967308\n",
      "\n",
      " Optimizer iteration 1493, batch 320\n",
      "\n",
      " Learning rate 0.00012950753262635711, Model learning rate 0.0001295075344387442\n",
      "321/391 [=======================>......] - ETA: 5s - loss: 0.9929 - acc: 0.7018\n",
      " Optimizer iteration 1494, batch 321\n",
      "\n",
      " Learning rate 0.00012896707842543898, Model learning rate 0.00012896707630716264\n",
      "322/391 [=======================>......] - ETA: 5s - loss: 0.9925 - acc: 0.7020\n",
      " Optimizer iteration 1495, batch 322\n",
      "\n",
      " Learning rate 0.00012842758726130281, Model learning rate 0.0001284275931539014\n",
      "323/391 [=======================>......] - ETA: 5s - loss: 0.9925 - acc: 0.7020\n",
      " Optimizer iteration 1496, batch 323\n",
      "\n",
      " Learning rate 0.0001278890605342285, Model learning rate 0.00012788905587513\n",
      "324/391 [=======================>......] - ETA: 5s - loss: 0.9926 - acc: 0.7020\n",
      " Optimizer iteration 1497, batch 324\n",
      "\n",
      " Learning rate 0.0001273514996419921, Model learning rate 0.0001273514935746789\n",
      "325/391 [=======================>......] - ETA: 5s - loss: 0.9925 - acc: 0.7020\n",
      " Optimizer iteration 1498, batch 325\n",
      "\n",
      " Learning rate 0.00012681490597986312, Model learning rate 0.0001268149062525481\n",
      "326/391 [========================>.....] - ETA: 4s - loss: 0.9921 - acc: 0.7021\n",
      " Optimizer iteration 1499, batch 326\n",
      "\n",
      " Learning rate 0.00012627928094060065, Model learning rate 0.00012627927935682237\n",
      "327/391 [========================>.....] - ETA: 4s - loss: 0.9923 - acc: 0.7020\n",
      " Optimizer iteration 1500, batch 327\n",
      "\n",
      " Learning rate 0.0001257446259144494, Model learning rate 0.00012574462743941694\n",
      "328/391 [========================>.....] - ETA: 4s - loss: 0.9922 - acc: 0.7020\n",
      " Optimizer iteration 1501, batch 328\n",
      "\n",
      " Learning rate 0.00012521094228913683, Model learning rate 0.0001252109359484166\n",
      "329/391 [========================>.....] - ETA: 4s - loss: 0.9928 - acc: 0.7018\n",
      " Optimizer iteration 1502, batch 329\n",
      "\n",
      " Learning rate 0.00012467823144986844, Model learning rate 0.00012467823398765177\n",
      "330/391 [========================>.....] - ETA: 4s - loss: 0.9928 - acc: 0.7017\n",
      " Optimizer iteration 1503, batch 330\n",
      "\n",
      " Learning rate 0.0001241464947793251, Model learning rate 0.000124146492453292\n",
      "331/391 [========================>.....] - ETA: 4s - loss: 0.9929 - acc: 0.7017\n",
      " Optimizer iteration 1504, batch 331\n",
      "\n",
      " Learning rate 0.00012361573365765938, Model learning rate 0.0001236157404491678\n",
      "\n",
      " Optimizer iteration 1505, batch 332\n",
      "\n",
      " Learning rate 0.00012308594946249164, Model learning rate 0.00012308594887144864\n",
      "333/391 [========================>.....] - ETA: 4s - loss: 0.9930 - acc: 0.7016\n",
      " Optimizer iteration 1506, batch 333\n",
      "\n",
      " Learning rate 0.0001225571435689062, Model learning rate 0.000122557146823965\n",
      "334/391 [========================>.....] - ETA: 4s - loss: 0.9927 - acc: 0.7017\n",
      " Optimizer iteration 1507, batch 334\n",
      "\n",
      " Learning rate 0.00012202931734944878, Model learning rate 0.00012202931975480169\n",
      "335/391 [========================>.....] - ETA: 4s - loss: 0.9928 - acc: 0.7017\n",
      " Optimizer iteration 1508, batch 335\n",
      "\n",
      " Learning rate 0.00012150247217412185, Model learning rate 0.00012150247493991628\n",
      "336/391 [========================>.....] - ETA: 4s - loss: 0.9932 - acc: 0.7016\n",
      " Optimizer iteration 1509, batch 336\n",
      "\n",
      " Learning rate 0.00012097660941038147, Model learning rate 0.00012097661237930879\n",
      "\n",
      " Optimizer iteration 1510, batch 337\n",
      "\n",
      " Learning rate 0.00012045173042313429, Model learning rate 0.00012045173207297921\n",
      "338/391 [========================>.....] - ETA: 4s - loss: 0.9926 - acc: 0.7018\n",
      " Optimizer iteration 1511, batch 338\n",
      "\n",
      " Learning rate 0.00011992783657473289, Model learning rate 0.00011992783402092755\n",
      "339/391 [=========================>....] - ETA: 3s - loss: 0.9921 - acc: 0.7019\n",
      " Optimizer iteration 1512, batch 339\n",
      "\n",
      " Learning rate 0.00011940492922497337, Model learning rate 0.00011940493277506903\n",
      "340/391 [=========================>....] - ETA: 3s - loss: 0.9919 - acc: 0.7020\n",
      " Optimizer iteration 1513, batch 340\n",
      "\n",
      " Learning rate 0.00011888300973109112, Model learning rate 0.00011888300650753081\n",
      "341/391 [=========================>....] - ETA: 3s - loss: 0.9917 - acc: 0.7021\n",
      " Optimizer iteration 1514, batch 341\n",
      "\n",
      " Learning rate 0.00011836207944775728, Model learning rate 0.00011836207704618573\n",
      "342/391 [=========================>....] - ETA: 3s - loss: 0.9917 - acc: 0.7021\n",
      " Optimizer iteration 1515, batch 342\n",
      "\n",
      " Learning rate 0.0001178421397270758, Model learning rate 0.00011784213711507618\n",
      "343/391 [=========================>....] - ETA: 3s - loss: 0.9920 - acc: 0.7020\n",
      " Optimizer iteration 1516, batch 343\n",
      "\n",
      " Learning rate 0.00011732319191857954, Model learning rate 0.00011732319399015978\n",
      "344/391 [=========================>....] - ETA: 3s - loss: 0.9918 - acc: 0.7020\n",
      " Optimizer iteration 1517, batch 344\n",
      "\n",
      " Learning rate 0.0001168052373692266, Model learning rate 0.0001168052403954789\n",
      "345/391 [=========================>....] - ETA: 3s - loss: 0.9916 - acc: 0.7021\n",
      " Optimizer iteration 1518, batch 345\n",
      "\n",
      " Learning rate 0.00011628827742339687, Model learning rate 0.00011628827633103356\n",
      "346/391 [=========================>....] - ETA: 3s - loss: 0.9917 - acc: 0.7021\n",
      " Optimizer iteration 1519, batch 346\n",
      "\n",
      " Learning rate 0.0001157723134228893, Model learning rate 0.00011577231634873897\n",
      "347/391 [=========================>....] - ETA: 3s - loss: 0.9915 - acc: 0.7022\n",
      " Optimizer iteration 1520, batch 347\n",
      "\n",
      " Learning rate 0.00011525734670691701, Model learning rate 0.00011525734589667991\n",
      "\n",
      " Optimizer iteration 1521, batch 348\n",
      "\n",
      " Learning rate 0.00011474337861210544, Model learning rate 0.0001147433795267716\n",
      "349/391 [=========================>....] - ETA: 3s - loss: 0.9915 - acc: 0.7021\n",
      " Optimizer iteration 1522, batch 349\n",
      "\n",
      " Learning rate 0.00011423041047248728, Model learning rate 0.00011423040996305645\n",
      "350/391 [=========================>....] - ETA: 3s - loss: 0.9913 - acc: 0.7021\n",
      " Optimizer iteration 1523, batch 350\n",
      "\n",
      " Learning rate 0.00011371844361950045, Model learning rate 0.00011371844448149204\n",
      "\n",
      " Optimizer iteration 1524, batch 351\n",
      "\n",
      " Learning rate 0.00011320747938198356, Model learning rate 0.00011320747580612078\n",
      "352/391 [==========================>...] - ETA: 2s - loss: 0.9910 - acc: 0.7023\n",
      " Optimizer iteration 1525, batch 352\n",
      "\n",
      " Learning rate 0.00011269751908617276, Model learning rate 0.0001126975184888579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/391 [==========================>...] - ETA: 2s - loss: 0.9906 - acc: 0.7025\n",
      " Optimizer iteration 1526, batch 353\n",
      "\n",
      " Learning rate 0.00011218856405569883, Model learning rate 0.00011218856525374576\n",
      "354/391 [==========================>...] - ETA: 2s - loss: 0.9906 - acc: 0.7025\n",
      " Optimizer iteration 1527, batch 354\n",
      "\n",
      " Learning rate 0.00011168061561158321, Model learning rate 0.00011168061610078439\n",
      "355/391 [==========================>...] - ETA: 2s - loss: 0.9903 - acc: 0.7027\n",
      " Optimizer iteration 1528, batch 355\n",
      "\n",
      " Learning rate 0.00011117367507223452, Model learning rate 0.00011117367830593139\n",
      "\n",
      " Optimizer iteration 1529, batch 356\n",
      "\n",
      " Learning rate 0.0001106677437534453, Model learning rate 0.00011066774459322914\n",
      "357/391 [==========================>...] - ETA: 2s - loss: 0.9900 - acc: 0.7027\n",
      " Optimizer iteration 1530, batch 357\n",
      "\n",
      " Learning rate 0.00011016282296838886, Model learning rate 0.00011016282223863527\n",
      "358/391 [==========================>...] - ETA: 2s - loss: 0.9898 - acc: 0.7027\n",
      " Optimizer iteration 1531, batch 358\n",
      "\n",
      " Learning rate 0.0001096589140276153, Model learning rate 0.00010965891124214977\n",
      "\n",
      " Optimizer iteration 1532, batch 359\n",
      "\n",
      " Learning rate 0.00010915601823904875, Model learning rate 0.00010915601887973025\n",
      "360/391 [==========================>...] - ETA: 2s - loss: 0.9890 - acc: 0.7031\n",
      " Optimizer iteration 1533, batch 360\n",
      "\n",
      " Learning rate 0.00010865413690798321, Model learning rate 0.00010865413787541911\n",
      "361/391 [==========================>...] - ETA: 2s - loss: 0.9893 - acc: 0.7030\n",
      " Optimizer iteration 1534, batch 361\n",
      "\n",
      " Learning rate 0.00010815327133708014, Model learning rate 0.00010815326822921634\n",
      "362/391 [==========================>...] - ETA: 2s - loss: 0.9892 - acc: 0.7031\n",
      " Optimizer iteration 1535, batch 362\n",
      "\n",
      " Learning rate 0.00010765342282636414, Model learning rate 0.00010765342449303716\n",
      "363/391 [==========================>...] - ETA: 2s - loss: 0.9888 - acc: 0.7031\n",
      " Optimizer iteration 1536, batch 363\n",
      "\n",
      " Learning rate 0.00010715459267321997, Model learning rate 0.00010715459211496636\n",
      "364/391 [==========================>...] - ETA: 2s - loss: 0.9884 - acc: 0.7032\n",
      " Optimizer iteration 1537, batch 364\n",
      "\n",
      " Learning rate 0.00010665678217238933, Model learning rate 0.00010665678564691916\n",
      "365/391 [===========================>..] - ETA: 1s - loss: 0.9883 - acc: 0.7031\n",
      " Optimizer iteration 1538, batch 365\n",
      "\n",
      " Learning rate 0.0001061599926159676, Model learning rate 0.00010615999053698033\n",
      "366/391 [===========================>..] - ETA: 1s - loss: 0.9882 - acc: 0.7033\n",
      " Optimizer iteration 1539, batch 366\n",
      "\n",
      " Learning rate 0.0001056642252933997, Model learning rate 0.00010566422861302271\n",
      "367/391 [===========================>..] - ETA: 1s - loss: 0.9882 - acc: 0.7034\n",
      " Optimizer iteration 1540, batch 367\n",
      "\n",
      " Learning rate 0.00010516948149147753, Model learning rate 0.00010516947804717347\n",
      "\n",
      " Optimizer iteration 1541, batch 368\n",
      "\n",
      " Learning rate 0.00010467576249433663, Model learning rate 0.00010467576066730544\n",
      "369/391 [===========================>..] - ETA: 1s - loss: 0.9879 - acc: 0.7035\n",
      " Optimizer iteration 1542, batch 369\n",
      "\n",
      " Learning rate 0.00010418306958345213, Model learning rate 0.00010418306919746101\n",
      "370/391 [===========================>..] - ETA: 1s - loss: 0.9881 - acc: 0.7034\n",
      " Optimizer iteration 1543, batch 370\n",
      "\n",
      " Learning rate 0.00010369140403763638, Model learning rate 0.00010369140363764018\n",
      "371/391 [===========================>..] - ETA: 1s - loss: 0.9881 - acc: 0.7035\n",
      " Optimizer iteration 1544, batch 371\n",
      "\n",
      " Learning rate 0.00010320076713303467, Model learning rate 0.00010320076398784295\n",
      "\n",
      " Optimizer iteration 1545, batch 372\n",
      "\n",
      " Learning rate 0.00010271116014312292, Model learning rate 0.00010271115752402693\n",
      "373/391 [===========================>..] - ETA: 1s - loss: 0.9882 - acc: 0.7036\n",
      " Optimizer iteration 1546, batch 373\n",
      "\n",
      " Learning rate 0.00010222258433870341, Model learning rate 0.00010222258424619213\n",
      "374/391 [===========================>..] - ETA: 1s - loss: 0.9885 - acc: 0.7035\n",
      " Optimizer iteration 1547, batch 374\n",
      "\n",
      " Learning rate 0.00010173504098790188, Model learning rate 0.00010173504415433854\n",
      "375/391 [===========================>..] - ETA: 1s - loss: 0.9882 - acc: 0.7036\n",
      " Optimizer iteration 1548, batch 375\n",
      "\n",
      " Learning rate 0.00010124853135616475, Model learning rate 0.00010124852997250855\n",
      "\n",
      " Optimizer iteration 1549, batch 376\n",
      "\n",
      " Learning rate 0.00010076305670625507, Model learning rate 0.00010076305625261739\n",
      "377/391 [===========================>..] - ETA: 1s - loss: 0.9882 - acc: 0.7036\n",
      " Optimizer iteration 1550, batch 377\n",
      "\n",
      " Learning rate 0.00010027861829824952, Model learning rate 0.00010027861571870744\n",
      "378/391 [============================>.] - ETA: 0s - loss: 0.9878 - acc: 0.7037\n",
      " Optimizer iteration 1551, batch 378\n",
      "\n",
      " Learning rate 9.9795217389535e-05, Model learning rate 9.979521564673632e-05\n",
      "379/391 [============================>.] - ETA: 0s - loss: 0.9876 - acc: 0.7038\n",
      " Optimizer iteration 1552, batch 379\n",
      "\n",
      " Learning rate 9.931285523480604e-05, Model learning rate 9.931285603670403e-05\n",
      "380/391 [============================>.] - ETA: 0s - loss: 0.9873 - acc: 0.7038\n",
      " Optimizer iteration 1553, batch 380\n",
      "\n",
      " Learning rate 9.883153308606035e-05, Model learning rate 9.883152961265296e-05\n",
      "381/391 [============================>.] - ETA: 0s - loss: 0.9872 - acc: 0.7038\n",
      " Optimizer iteration 1554, batch 381\n",
      "\n",
      " Learning rate 9.835125219259695e-05, Model learning rate 9.835125092649832e-05\n",
      "382/391 [============================>.] - ETA: 0s - loss: 0.9871 - acc: 0.7037\n",
      " Optimizer iteration 1555, batch 382\n",
      "\n",
      " Learning rate 9.787201380101158e-05, Model learning rate 9.787201270228252e-05\n",
      "\n",
      " Optimizer iteration 1556, batch 383\n",
      "\n",
      " Learning rate 9.739381915519457e-05, Model learning rate 9.739382221596316e-05\n",
      "384/391 [============================>.] - ETA: 0s - loss: 0.9865 - acc: 0.7040\n",
      " Optimizer iteration 1557, batch 384\n",
      "\n",
      " Learning rate 9.691666949632683e-05, Model learning rate 9.691667219158262e-05\n",
      "385/391 [============================>.] - ETA: 0s - loss: 0.9868 - acc: 0.7039\n",
      " Optimizer iteration 1558, batch 385\n",
      "\n",
      " Learning rate 9.644056606287727e-05, Model learning rate 9.644056262914091e-05\n",
      "386/391 [============================>.] - ETA: 0s - loss: 0.9869 - acc: 0.7039\n",
      " Optimizer iteration 1559, batch 386\n",
      "\n",
      " Learning rate 9.596551009059884e-05, Model learning rate 9.596550808055326e-05\n",
      "387/391 [============================>.] - ETA: 0s - loss: 0.9866 - acc: 0.7040\n",
      " Optimizer iteration 1560, batch 387\n",
      "\n",
      " Learning rate 9.549150281252633e-05, Model learning rate 9.549150126986206e-05\n",
      "\n",
      " Optimizer iteration 1561, batch 388\n",
      "\n",
      " Learning rate 9.501854545897203e-05, Model learning rate 9.501854219706729e-05\n",
      "389/391 [============================>.] - ETA: 0s - loss: 0.9863 - acc: 0.7042\n",
      " Optimizer iteration 1562, batch 389\n",
      "\n",
      " Learning rate 9.454663925752316e-05, Model learning rate 9.454663813812658e-05\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.9864 - acc: 0.7043\n",
      " Optimizer iteration 1563, batch 390\n",
      "\n",
      " Learning rate 9.407578543303913e-05, Model learning rate 9.407578181708232e-05\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.9859 - acc: 0.7046 - val_loss: 1.0302 - val_acc: 0.6782\n",
      "\n",
      "Epoch 00004: saving model to /home/ubuntu/Projects/hybrid-ensemble/model/run_200/cifar10_ResNet20v1_model-0004.h5\n",
      "Epoch 5/10\n",
      "\n",
      " Optimizer iteration 1564, batch 0\n",
      "\n",
      " Learning rate 9.360598520764712e-05, Model learning rate 9.360598778584972e-05\n",
      "  1/391 [..............................] - ETA: 15s - loss: 0.7638 - acc: 0.8047\n",
      " Optimizer iteration 1565, batch 1\n",
      "\n",
      " Learning rate 9.313723980074018e-05, Model learning rate 9.313724149251357e-05\n",
      "\n",
      " Optimizer iteration 1566, batch 2\n",
      "\n",
      " Learning rate 9.266955042897358e-05, Model learning rate 9.266955021303147e-05\n",
      "  3/391 [..............................] - ETA: 17s - loss: 0.9335 - acc: 0.7266\n",
      " Optimizer iteration 1567, batch 3\n",
      "\n",
      " Learning rate 9.220291830626082e-05, Model learning rate 9.220292122336105e-05\n",
      "\n",
      " Optimizer iteration 1568, batch 4\n",
      "\n",
      " Learning rate 9.173734464377204e-05, Model learning rate 9.173734724754468e-05\n",
      "  5/391 [..............................] - ETA: 17s - loss: 0.9090 - acc: 0.7391\n",
      " Optimizer iteration 1569, batch 5\n",
      "\n",
      " Learning rate 9.127283064992997e-05, Model learning rate 9.127282828558236e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizer iteration 1570, batch 6\n",
      "\n",
      " Learning rate 9.080937753040646e-05, Model learning rate 9.080937888938934e-05\n",
      "  7/391 [..............................] - ETA: 18s - loss: 0.9266 - acc: 0.7243\n",
      " Optimizer iteration 1571, batch 7\n",
      "\n",
      " Learning rate 9.034698648812046e-05, Model learning rate 9.034698450705037e-05\n",
      "  8/391 [..............................] - ETA: 18s - loss: 0.9164 - acc: 0.7324\n",
      " Optimizer iteration 1572, batch 8\n",
      "\n",
      " Learning rate 8.988565872323362e-05, Model learning rate 8.988565969048068e-05\n",
      "\n",
      " Optimizer iteration 1573, batch 9\n",
      "\n",
      " Learning rate 8.942539543314798e-05, Model learning rate 8.942539716372266e-05\n",
      " 10/391 [..............................] - ETA: 18s - loss: 0.9188 - acc: 0.7352\n",
      " Optimizer iteration 1574, batch 10\n",
      "\n",
      " Learning rate 8.896619781250309e-05, Model learning rate 8.896619692677632e-05\n",
      "\n",
      " Optimizer iteration 1575, batch 11\n",
      "\n",
      " Learning rate 8.850806705317183e-05, Model learning rate 8.850806625559926e-05\n",
      " 12/391 [..............................] - ETA: 18s - loss: 0.9355 - acc: 0.7272\n",
      " Optimizer iteration 1576, batch 12\n",
      "\n",
      " Learning rate 8.805100434425845e-05, Model learning rate 8.805100515019149e-05\n",
      " 13/391 [..............................] - ETA: 18s - loss: 0.9381 - acc: 0.7248\n",
      " Optimizer iteration 1577, batch 13\n",
      "\n",
      " Learning rate 8.75950108720951e-05, Model learning rate 8.7595013610553e-05\n",
      " 14/391 [>.............................] - ETA: 18s - loss: 0.9300 - acc: 0.7294\n",
      " Optimizer iteration 1578, batch 14\n",
      "\n",
      " Learning rate 8.714008782023796e-05, Model learning rate 8.714008436072618e-05\n",
      "\n",
      " Optimizer iteration 1579, batch 15\n",
      "\n",
      " Learning rate 8.668623636946566e-05, Model learning rate 8.668623922858387e-05\n",
      " 16/391 [>.............................] - ETA: 18s - loss: 0.9269 - acc: 0.7305\n",
      " Optimizer iteration 1580, batch 16\n",
      "\n",
      " Learning rate 8.623345769777513e-05, Model learning rate 8.623345638625324e-05\n",
      " 17/391 [>.............................] - ETA: 18s - loss: 0.9239 - acc: 0.7312\n",
      " Optimizer iteration 1581, batch 17\n",
      "\n",
      " Learning rate 8.578175298037872e-05, Model learning rate 8.57817503856495e-05\n",
      "\n",
      " Optimizer iteration 1582, batch 18\n",
      "\n",
      " Learning rate 8.533112338970156e-05, Model learning rate 8.533112122677267e-05\n",
      " 19/391 [>.............................] - ETA: 18s - loss: 0.9323 - acc: 0.7307\n",
      " Optimizer iteration 1583, batch 19\n",
      "\n",
      " Learning rate 8.488157009537794e-05, Model learning rate 8.488156890962273e-05\n",
      " 20/391 [>.............................] - ETA: 18s - loss: 0.9367 - acc: 0.7281\n",
      " Optimizer iteration 1584, batch 20\n",
      "\n",
      " Learning rate 8.443309426424861e-05, Model learning rate 8.443309343419969e-05\n",
      " 21/391 [>.............................] - ETA: 19s - loss: 0.9415 - acc: 0.7269\n",
      " Optimizer iteration 1585, batch 21\n",
      "\n",
      " Learning rate 8.398569706035791e-05, Model learning rate 8.398569480050355e-05\n",
      " 22/391 [>.............................] - ETA: 19s - loss: 0.9472 - acc: 0.7255\n",
      " Optimizer iteration 1586, batch 22\n",
      "\n",
      " Learning rate 8.353937964495028e-05, Model learning rate 8.353938028449193e-05\n",
      "\n",
      " Optimizer iteration 1587, batch 23\n",
      "\n",
      " Learning rate 8.309414317646769e-05, Model learning rate 8.30941426102072e-05\n",
      " 24/391 [>.............................] - ETA: 20s - loss: 0.9413 - acc: 0.7279\n",
      " Optimizer iteration 1588, batch 24\n",
      "\n",
      " Learning rate 8.264998881054659e-05, Model learning rate 8.264998905360699e-05\n",
      " 25/391 [>.............................] - ETA: 20s - loss: 0.9383 - acc: 0.7278\n",
      " Optimizer iteration 1589, batch 25\n",
      "\n",
      " Learning rate 8.220691770001421e-05, Model learning rate 8.220691961469129e-05\n",
      " 26/391 [>.............................] - ETA: 20s - loss: 0.9442 - acc: 0.7251\n",
      " Optimizer iteration 1590, batch 26\n",
      "\n",
      " Learning rate 8.176493099488664e-05, Model learning rate 8.17649342934601e-05\n",
      "\n",
      " Optimizer iteration 1591, batch 27\n",
      "\n",
      " Learning rate 8.132402984236531e-05, Model learning rate 8.132403308991343e-05\n",
      " 28/391 [=>............................] - ETA: 21s - loss: 0.9530 - acc: 0.7218\n",
      " Optimizer iteration 1592, batch 28\n",
      "\n",
      " Learning rate 8.088421538683377e-05, Model learning rate 8.088421600405127e-05\n",
      " 29/391 [=>............................] - ETA: 21s - loss: 0.9500 - acc: 0.7225\n",
      " Optimizer iteration 1593, batch 29\n",
      "\n",
      " Learning rate 8.04454887698553e-05, Model learning rate 8.044549031183124e-05\n",
      " 30/391 [=>............................] - ETA: 22s - loss: 0.9497 - acc: 0.7232\n",
      " Optimizer iteration 1594, batch 30\n",
      "\n",
      " Learning rate 8.000785113016939e-05, Model learning rate 8.000784873729572e-05\n",
      " 31/391 [=>............................] - ETA: 22s - loss: 0.9474 - acc: 0.7228\n",
      " Optimizer iteration 1595, batch 31\n",
      "\n",
      " Learning rate 7.957130360368897e-05, Model learning rate 7.957130583235994e-05\n",
      " 32/391 [=>............................] - ETA: 22s - loss: 0.9445 - acc: 0.7239\n",
      " Optimizer iteration 1596, batch 32\n",
      "\n",
      " Learning rate 7.913584732349787e-05, Model learning rate 7.913584704510868e-05\n",
      "\n",
      " Optimizer iteration 1597, batch 33\n",
      "\n",
      " Learning rate 7.870148341984713e-05, Model learning rate 7.870148692745715e-05\n",
      " 34/391 [=>............................] - ETA: 22s - loss: 0.9454 - acc: 0.7254\n",
      " Optimizer iteration 1598, batch 34\n",
      "\n",
      " Learning rate 7.826821302015275e-05, Model learning rate 7.826821092749014e-05\n",
      " 35/391 [=>............................] - ETA: 22s - loss: 0.9433 - acc: 0.7257\n",
      " Optimizer iteration 1599, batch 35\n",
      "\n",
      " Learning rate 7.783603724899258e-05, Model learning rate 7.783604087308049e-05\n",
      " 36/391 [=>............................] - ETA: 22s - loss: 0.9409 - acc: 0.7248\n",
      " Optimizer iteration 1600, batch 36\n",
      "\n",
      " Learning rate 7.74049572281027e-05, Model learning rate 7.740495493635535e-05\n",
      " 37/391 [=>............................] - ETA: 22s - loss: 0.9392 - acc: 0.7264\n",
      " Optimizer iteration 1601, batch 37\n",
      "\n",
      " Learning rate 7.697497407637566e-05, Model learning rate 7.697497494518757e-05\n",
      " 38/391 [=>............................] - ETA: 22s - loss: 0.9411 - acc: 0.7251\n",
      " Optimizer iteration 1602, batch 38\n",
      "\n",
      " Learning rate 7.654608890985709e-05, Model learning rate 7.654608634766191e-05\n",
      " 39/391 [=>............................] - ETA: 22s - loss: 0.9397 - acc: 0.7252\n",
      " Optimizer iteration 1603, batch 39\n",
      "\n",
      " Learning rate 7.611830284174221e-05, Model learning rate 7.611830369569361e-05\n",
      " 40/391 [==>...........................] - ETA: 23s - loss: 0.9410 - acc: 0.7248\n",
      " Optimizer iteration 1604, batch 40\n",
      "\n",
      " Learning rate 7.569161698237404e-05, Model learning rate 7.569161971332505e-05\n",
      "\n",
      " Optimizer iteration 1605, batch 41\n",
      "\n",
      " Learning rate 7.526603243923958e-05, Model learning rate 7.526603440055624e-05\n",
      " 42/391 [==>...........................] - ETA: 22s - loss: 0.9362 - acc: 0.7262\n",
      " Optimizer iteration 1606, batch 42\n",
      "\n",
      " Learning rate 7.484155031696727e-05, Model learning rate 7.484154775738716e-05\n",
      "\n",
      " Optimizer iteration 1607, batch 43\n",
      "\n",
      " Learning rate 7.441817171732457e-05, Model learning rate 7.441817433573306e-05\n",
      " 44/391 [==>...........................] - ETA: 23s - loss: 0.9336 - acc: 0.7269\n",
      " Optimizer iteration 1608, batch 44\n",
      "\n",
      " Learning rate 7.399589773921412e-05, Model learning rate 7.399589958367869e-05\n",
      "\n",
      " Optimizer iteration 1609, batch 45\n",
      "\n",
      " Learning rate 7.357472947867188e-05, Model learning rate 7.357473077718168e-05\n",
      " 46/391 [==>...........................] - ETA: 22s - loss: 0.9330 - acc: 0.7284\n",
      " Optimizer iteration 1610, batch 46\n",
      "\n",
      " Learning rate 7.315466802886401e-05, Model learning rate 7.315466791624203e-05\n",
      " 47/391 [==>...........................] - ETA: 22s - loss: 0.9343 - acc: 0.7276\n",
      " Optimizer iteration 1611, batch 47\n",
      "\n",
      " Learning rate 7.273571448008304e-05, Model learning rate 7.273571100085974e-05\n",
      " 48/391 [==>...........................] - ETA: 23s - loss: 0.9356 - acc: 0.7269\n",
      " Optimizer iteration 1612, batch 48\n",
      "\n",
      " Learning rate 7.23178699197467e-05, Model learning rate 7.231786730699241e-05\n",
      "\n",
      " Optimizer iteration 1613, batch 49\n",
      "\n",
      " Learning rate 7.190113543239407e-05, Model learning rate 7.190113683464006e-05\n",
      " 50/391 [==>...........................] - ETA: 23s - loss: 0.9350 - acc: 0.7262\n",
      " Optimizer iteration 1614, batch 50\n",
      "\n",
      " Learning rate 7.148551209968279e-05, Model learning rate 7.148551230784506e-05\n",
      " 51/391 [==>...........................] - ETA: 22s - loss: 0.9324 - acc: 0.7275\n",
      " Optimizer iteration 1615, batch 51\n",
      "\n",
      " Learning rate 7.107100100038672e-05, Model learning rate 7.107100100256503e-05\n",
      " 52/391 [==>...........................] - ETA: 23s - loss: 0.9335 - acc: 0.7272\n",
      " Optimizer iteration 1616, batch 52\n",
      "\n",
      " Learning rate 7.06576032103926e-05, Model learning rate 7.065760291879997e-05\n",
      " 53/391 [===>..........................] - ETA: 22s - loss: 0.9306 - acc: 0.7274\n",
      " Optimizer iteration 1617, batch 53\n",
      "\n",
      " Learning rate 7.024531980269744e-05, Model learning rate 7.024531805654988e-05\n",
      " 54/391 [===>..........................] - ETA: 23s - loss: 0.9296 - acc: 0.7279\n",
      " Optimizer iteration 1618, batch 54\n",
      "\n",
      " Learning rate 6.983415184740616e-05, Model learning rate 6.983415369177237e-05\n",
      "\n",
      " Optimizer iteration 1619, batch 55\n",
      "\n",
      " Learning rate 6.942410041172836e-05, Model learning rate 6.942410254850984e-05\n",
      " 56/391 [===>..........................] - ETA: 23s - loss: 0.9309 - acc: 0.7271\n",
      " Optimizer iteration 1620, batch 56\n",
      "\n",
      " Learning rate 6.901516655997537e-05, Model learning rate 6.901516462676227e-05\n",
      " 57/391 [===>..........................] - ETA: 22s - loss: 0.9305 - acc: 0.7274\n",
      " Optimizer iteration 1621, batch 57\n",
      "\n",
      " Learning rate 6.860735135355811e-05, Model learning rate 6.86073544784449e-05\n",
      " 58/391 [===>..........................] - ETA: 22s - loss: 0.9287 - acc: 0.7278\n",
      " Optimizer iteration 1622, batch 58\n",
      "\n",
      " Learning rate 6.820065585098378e-05, Model learning rate 6.820065755164251e-05\n",
      " 59/391 [===>..........................] - ETA: 22s - loss: 0.9293 - acc: 0.7276\n",
      " Optimizer iteration 1623, batch 59\n",
      "\n",
      " Learning rate 6.779508110785331e-05, Model learning rate 6.77950811223127e-05\n",
      " 60/391 [===>..........................] - ETA: 23s - loss: 0.9309 - acc: 0.7267\n",
      " Optimizer iteration 1624, batch 60\n",
      "\n",
      " Learning rate 6.739062817685892e-05, Model learning rate 6.739062519045547e-05\n",
      " 61/391 [===>..........................] - ETA: 22s - loss: 0.9318 - acc: 0.7262\n",
      " Optimizer iteration 1625, batch 61\n",
      "\n",
      " Learning rate 6.698729810778065e-05, Model learning rate 6.698729703202844e-05\n",
      "\n",
      " Optimizer iteration 1626, batch 62\n",
      "\n",
      " Learning rate 6.658509194748463e-05, Model learning rate 6.658508937107399e-05\n",
      " 63/391 [===>..........................] - ETA: 22s - loss: 0.9331 - acc: 0.7258\n",
      " Optimizer iteration 1627, batch 63\n",
      "\n",
      " Learning rate 6.618401073991936e-05, Model learning rate 6.618400948354974e-05\n",
      " 64/391 [===>..........................] - ETA: 22s - loss: 0.9327 - acc: 0.7261\n",
      " Optimizer iteration 1628, batch 64\n",
      "\n",
      " Learning rate 6.57840555261136e-05, Model learning rate 6.57840573694557e-05\n",
      "\n",
      " Optimizer iteration 1629, batch 65\n",
      "\n",
      " Learning rate 6.538522734417357e-05, Model learning rate 6.538522575283423e-05\n",
      " 66/391 [====>.........................] - ETA: 22s - loss: 0.9351 - acc: 0.7248\n",
      " Optimizer iteration 1630, batch 66\n",
      "\n",
      " Learning rate 6.498752722928042e-05, Model learning rate 6.498752918560058e-05\n",
      " 67/391 [====>.........................] - ETA: 22s - loss: 0.9373 - acc: 0.7238\n",
      " Optimizer iteration 1631, batch 67\n",
      "\n",
      " Learning rate 6.459095621368682e-05, Model learning rate 6.459095311583951e-05\n",
      " 68/391 [====>.........................] - ETA: 22s - loss: 0.9382 - acc: 0.7235\n",
      " Optimizer iteration 1632, batch 68\n",
      "\n",
      " Learning rate 6.419551532671541e-05, Model learning rate 6.419551209546626e-05\n",
      " 69/391 [====>.........................] - ETA: 22s - loss: 0.9411 - acc: 0.7220\n",
      " Optimizer iteration 1633, batch 69\n",
      "\n",
      " Learning rate 6.380120559475506e-05, Model learning rate 6.380120612448081e-05\n",
      " 70/391 [====>.........................] - ETA: 22s - loss: 0.9386 - acc: 0.7231\n",
      " Optimizer iteration 1634, batch 70\n",
      "\n",
      " Learning rate 6.340802804125873e-05, Model learning rate 6.340802792692557e-05\n",
      " 71/391 [====>.........................] - ETA: 22s - loss: 0.9376 - acc: 0.7234\n",
      " Optimizer iteration 1635, batch 71\n",
      "\n",
      " Learning rate 6.301598368674105e-05, Model learning rate 6.301598477875814e-05\n",
      " 72/391 [====>.........................] - ETA: 22s - loss: 0.9376 - acc: 0.7233\n",
      " Optimizer iteration 1636, batch 72\n",
      "\n",
      " Learning rate 6.262507354877494e-05, Model learning rate 6.262507667997852e-05\n",
      " 73/391 [====>.........................] - ETA: 22s - loss: 0.9355 - acc: 0.7243\n",
      " Optimizer iteration 1637, batch 73\n",
      "\n",
      " Learning rate 6.223529864198984e-05, Model learning rate 6.22352963546291e-05\n",
      " 74/391 [====>.........................] - ETA: 22s - loss: 0.9341 - acc: 0.7250\n",
      " Optimizer iteration 1638, batch 74\n",
      "\n",
      " Learning rate 6.184665997806832e-05, Model learning rate 6.18466583546251e-05\n",
      "\n",
      " Optimizer iteration 1639, batch 75\n",
      "\n",
      " Learning rate 6.145915856574363e-05, Model learning rate 6.145915540400892e-05\n",
      " 76/391 [====>.........................] - ETA: 22s - loss: 0.9349 - acc: 0.7243\n",
      " Optimizer iteration 1640, batch 76\n",
      "\n",
      " Learning rate 6.10727954107977e-05, Model learning rate 6.107279477873817e-05\n",
      " 77/391 [====>.........................] - ETA: 22s - loss: 0.9348 - acc: 0.7243\n",
      " Optimizer iteration 1641, batch 77\n",
      "\n",
      " Learning rate 6.0687571516057803e-05, Model learning rate 6.0687572840834036e-05\n",
      " 78/391 [====>.........................] - ETA: 22s - loss: 0.9356 - acc: 0.7239\n",
      " Optimizer iteration 1642, batch 78\n",
      "\n",
      " Learning rate 6.030348788139406e-05, Model learning rate 6.030348959029652e-05\n",
      " 79/391 [=====>........................] - ETA: 22s - loss: 0.9382 - acc: 0.7229\n",
      " Optimizer iteration 1643, batch 79\n",
      "\n",
      " Learning rate 5.9920545503717226e-05, Model learning rate 5.992054502712563e-05\n",
      " 80/391 [=====>........................] - ETA: 22s - loss: 0.9385 - acc: 0.7231\n",
      " Optimizer iteration 1644, batch 80\n",
      "\n",
      " Learning rate 5.9538745376975736e-05, Model learning rate 5.9538746427278966e-05\n",
      " 81/391 [=====>........................] - ETA: 22s - loss: 0.9384 - acc: 0.7234\n",
      " Optimizer iteration 1645, batch 81\n",
      "\n",
      " Learning rate 5.9158088492153036e-05, Model learning rate 5.915809015277773e-05\n",
      " 82/391 [=====>........................] - ETA: 22s - loss: 0.9414 - acc: 0.7223\n",
      " Optimizer iteration 1646, batch 82\n",
      "\n",
      " Learning rate 5.8778575837265754e-05, Model learning rate 5.8778576203621924e-05\n",
      "\n",
      " Optimizer iteration 1647, batch 83\n",
      "\n",
      " Learning rate 5.84002083973601e-05, Model learning rate 5.840020821779035e-05\n",
      " 84/391 [=====>........................] - ETA: 22s - loss: 0.9411 - acc: 0.7224\n",
      " Optimizer iteration 1648, batch 84\n",
      "\n",
      " Learning rate 5.8022987154510154e-05, Model learning rate 5.802298619528301e-05\n",
      " 85/391 [=====>........................] - ETA: 22s - loss: 0.9404 - acc: 0.7224\n",
      " Optimizer iteration 1649, batch 85\n",
      "\n",
      " Learning rate 5.7646913087814725e-05, Model learning rate 5.764691377407871e-05\n",
      " 86/391 [=====>........................] - ETA: 21s - loss: 0.9396 - acc: 0.7227\n",
      " Optimizer iteration 1650, batch 86\n",
      "\n",
      " Learning rate 5.72719871733951e-05, Model learning rate 5.727198731619865e-05\n",
      " 87/391 [=====>........................] - ETA: 21s - loss: 0.9387 - acc: 0.7232\n",
      " Optimizer iteration 1651, batch 87\n",
      "\n",
      " Learning rate 5.6898210384392636e-05, Model learning rate 5.689821045962162e-05\n",
      " 88/391 [=====>........................] - ETA: 21s - loss: 0.9394 - acc: 0.7236\n",
      " Optimizer iteration 1652, batch 88\n",
      "\n",
      " Learning rate 5.6525583690966056e-05, Model learning rate 5.652558320434764e-05\n",
      " 89/391 [=====>........................] - ETA: 21s - loss: 0.9396 - acc: 0.7236\n",
      " Optimizer iteration 1653, batch 89\n",
      "\n",
      " Learning rate 5.6154108060288756e-05, Model learning rate 5.6154109188355505e-05\n",
      "\n",
      " Optimizer iteration 1654, batch 90\n",
      "\n",
      " Learning rate 5.578378445654664e-05, Model learning rate 5.578378477366641e-05\n",
      " 91/391 [=====>........................] - ETA: 21s - loss: 0.9391 - acc: 0.7242\n",
      " Optimizer iteration 1655, batch 91\n",
      "\n",
      " Learning rate 5.541461384093549e-05, Model learning rate 5.5414613598259166e-05\n",
      " 92/391 [======>.......................] - ETA: 21s - loss: 0.9378 - acc: 0.7248\n",
      " Optimizer iteration 1656, batch 92\n",
      "\n",
      " Learning rate 5.5046597171658106e-05, Model learning rate 5.504659566213377e-05\n",
      "\n",
      " Optimizer iteration 1657, batch 93\n",
      "\n",
      " Learning rate 5.467973540392274e-05, Model learning rate 5.4679734603269026e-05\n",
      " 94/391 [======>.......................] - ETA: 21s - loss: 0.9379 - acc: 0.7246\n",
      " Optimizer iteration 1658, batch 94\n",
      "\n",
      " Learning rate 5.4314029489939464e-05, Model learning rate 5.431403042166494e-05\n",
      " 95/391 [======>.......................] - ETA: 21s - loss: 0.9378 - acc: 0.7247\n",
      " Optimizer iteration 1659, batch 95\n",
      "\n",
      " Learning rate 5.394948037891867e-05, Model learning rate 5.39494794793427e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/391 [======>.......................] - ETA: 21s - loss: 0.9383 - acc: 0.7241\n",
      " Optimizer iteration 1660, batch 96\n",
      "\n",
      " Learning rate 5.358608901706802e-05, Model learning rate 5.358608905225992e-05\n",
      " 97/391 [======>.......................] - ETA: 21s - loss: 0.9396 - acc: 0.7234\n",
      " Optimizer iteration 1661, batch 97\n",
      "\n",
      " Learning rate 5.3223856347590084e-05, Model learning rate 5.32238555024378e-05\n",
      " 98/391 [======>.......................] - ETA: 21s - loss: 0.9395 - acc: 0.7231\n",
      " Optimizer iteration 1662, batch 98\n",
      "\n",
      " Learning rate 5.286278331068017e-05, Model learning rate 5.286278246785514e-05\n",
      " 99/391 [======>.......................] - ETA: 21s - loss: 0.9398 - acc: 0.7232\n",
      " Optimizer iteration 1663, batch 99\n",
      "\n",
      " Learning rate 5.250287084352373e-05, Model learning rate 5.250286994851194e-05\n",
      "100/391 [======>.......................] - ETA: 21s - loss: 0.9387 - acc: 0.7237\n",
      " Optimizer iteration 1664, batch 100\n",
      "\n",
      " Learning rate 5.214411988029355e-05, Model learning rate 5.2144121582387015e-05\n",
      "\n",
      " Optimizer iteration 1665, batch 101\n",
      "\n",
      " Learning rate 5.1786531352148115e-05, Model learning rate 5.178653009352274e-05\n",
      "102/391 [======>.......................] - ETA: 20s - loss: 0.9392 - acc: 0.7232\n",
      " Optimizer iteration 1666, batch 102\n",
      "\n",
      " Learning rate 5.1430106187228485e-05, Model learning rate 5.1430106395855546e-05\n",
      "\n",
      " Optimizer iteration 1667, batch 103\n",
      "\n",
      " Learning rate 5.107484531065604e-05, Model learning rate 5.107484685140662e-05\n",
      "104/391 [======>.......................] - ETA: 20s - loss: 0.9402 - acc: 0.7225\n",
      " Optimizer iteration 1668, batch 104\n",
      "\n",
      " Learning rate 5.072074964453055e-05, Model learning rate 5.072075146017596e-05\n",
      "\n",
      " Optimizer iteration 1669, batch 105\n",
      "\n",
      " Learning rate 5.0367820107926957e-05, Model learning rate 5.036782022216357e-05\n",
      "106/391 [=======>......................] - ETA: 20s - loss: 0.9393 - acc: 0.7224\n",
      " Optimizer iteration 1670, batch 106\n",
      "\n",
      " Learning rate 5.0016057616893987e-05, Model learning rate 5.001605677534826e-05\n",
      "107/391 [=======>......................] - ETA: 20s - loss: 0.9405 - acc: 0.7217\n",
      " Optimizer iteration 1671, batch 107\n",
      "\n",
      " Learning rate 4.966546308445074e-05, Model learning rate 4.966546475770883e-05\n",
      "108/391 [=======>......................] - ETA: 20s - loss: 0.9399 - acc: 0.7218\n",
      " Optimizer iteration 1672, batch 108\n",
      "\n",
      " Learning rate 4.9316037420584935e-05, Model learning rate 4.9316036893287674e-05\n",
      "109/391 [=======>......................] - ETA: 20s - loss: 0.9395 - acc: 0.7219\n",
      " Optimizer iteration 1673, batch 109\n",
      "\n",
      " Learning rate 4.896778153225062e-05, Model learning rate 4.89677804580424e-05\n",
      "\n",
      " Optimizer iteration 1674, batch 110\n",
      "\n",
      " Learning rate 4.862069632336558e-05, Model learning rate 4.8620695451973006e-05\n",
      "111/391 [=======>......................] - ETA: 20s - loss: 0.9395 - acc: 0.7218\n",
      " Optimizer iteration 1675, batch 111\n",
      "\n",
      " Learning rate 4.827478269480895e-05, Model learning rate 4.82747818750795e-05\n",
      "112/391 [=======>......................] - ETA: 20s - loss: 0.9401 - acc: 0.7212\n",
      " Optimizer iteration 1676, batch 112\n",
      "\n",
      " Learning rate 4.793004154441877e-05, Model learning rate 4.793003972736187e-05\n",
      "113/391 [=======>......................] - ETA: 20s - loss: 0.9390 - acc: 0.7214\n",
      " Optimizer iteration 1677, batch 113\n",
      "\n",
      " Learning rate 4.758647376699032e-05, Model learning rate 4.758647264679894e-05\n",
      "\n",
      " Optimizer iteration 1678, batch 114\n",
      "\n",
      " Learning rate 4.7244080254272795e-05, Model learning rate 4.7244080633390695e-05\n",
      "115/391 [=======>......................] - ETA: 20s - loss: 0.9392 - acc: 0.7215\n",
      " Optimizer iteration 1679, batch 115\n",
      "\n",
      " Learning rate 4.690286189496795e-05, Model learning rate 4.690286368713714e-05\n",
      "116/391 [=======>......................] - ETA: 20s - loss: 0.9396 - acc: 0.7218\n",
      " Optimizer iteration 1680, batch 116\n",
      "\n",
      " Learning rate 4.65628195747273e-05, Model learning rate 4.656281817005947e-05\n",
      "117/391 [=======>......................] - ETA: 20s - loss: 0.9391 - acc: 0.7222\n",
      " Optimizer iteration 1681, batch 117\n",
      "\n",
      " Learning rate 4.6223954176149606e-05, Model learning rate 4.622395499609411e-05\n",
      "118/391 [========>.....................] - ETA: 20s - loss: 0.9381 - acc: 0.7224\n",
      " Optimizer iteration 1682, batch 118\n",
      "\n",
      " Learning rate 4.588626657877898e-05, Model learning rate 4.5886266889283434e-05\n",
      "\n",
      " Optimizer iteration 1683, batch 119\n",
      "\n",
      " Learning rate 4.5549757659102795e-05, Model learning rate 4.554975748760626e-05\n",
      "120/391 [========>.....................] - ETA: 19s - loss: 0.9391 - acc: 0.7223\n",
      " Optimizer iteration 1684, batch 120\n",
      "\n",
      " Learning rate 4.521442829054856e-05, Model learning rate 4.521442679106258e-05\n",
      "\n",
      " Optimizer iteration 1685, batch 121\n",
      "\n",
      " Learning rate 4.488027934348271e-05, Model learning rate 4.4880278437631205e-05\n",
      "122/391 [========>.....................] - ETA: 19s - loss: 0.9400 - acc: 0.7222\n",
      " Optimizer iteration 1686, batch 122\n",
      "\n",
      " Learning rate 4.4547311685207536e-05, Model learning rate 4.4547312427312136e-05\n",
      "123/391 [========>.....................] - ETA: 19s - loss: 0.9397 - acc: 0.7226\n",
      " Optimizer iteration 1687, batch 123\n",
      "\n",
      " Learning rate 4.4215526179959165e-05, Model learning rate 4.4215525122126564e-05\n",
      "124/391 [========>.....................] - ETA: 19s - loss: 0.9401 - acc: 0.7220\n",
      " Optimizer iteration 1688, batch 124\n",
      "\n",
      " Learning rate 4.388492368890568e-05, Model learning rate 4.3884923798032105e-05\n",
      "125/391 [========>.....................] - ETA: 19s - loss: 0.9401 - acc: 0.7219\n",
      " Optimizer iteration 1689, batch 125\n",
      "\n",
      " Learning rate 4.3555505070144276e-05, Model learning rate 4.355550481704995e-05\n",
      "126/391 [========>.....................] - ETA: 19s - loss: 0.9393 - acc: 0.7224\n",
      " Optimizer iteration 1690, batch 126\n",
      "\n",
      " Learning rate 4.322727117869951e-05, Model learning rate 4.322727181715891e-05\n",
      "127/391 [========>.....................] - ETA: 19s - loss: 0.9394 - acc: 0.7222\n",
      " Optimizer iteration 1691, batch 127\n",
      "\n",
      " Learning rate 4.2900222866521014e-05, Model learning rate 4.290022116038017e-05\n",
      "128/391 [========>.....................] - ETA: 19s - loss: 0.9399 - acc: 0.7224\n",
      " Optimizer iteration 1692, batch 128\n",
      "\n",
      " Learning rate 4.257436098248091e-05, Model learning rate 4.257436012267135e-05\n",
      "129/391 [========>.....................] - ETA: 19s - loss: 0.9398 - acc: 0.7222\n",
      " Optimizer iteration 1693, batch 129\n",
      "\n",
      " Learning rate 4.224968637237198e-05, Model learning rate 4.2249685066053644e-05\n",
      "130/391 [========>.....................] - ETA: 19s - loss: 0.9394 - acc: 0.7225\n",
      " Optimizer iteration 1694, batch 130\n",
      "\n",
      " Learning rate 4.192619987890556e-05, Model learning rate 4.1926199628505856e-05\n",
      "\n",
      " Optimizer iteration 1695, batch 131\n",
      "\n",
      " Learning rate 4.16039023417088e-05, Model learning rate 4.160390381002799e-05\n",
      "132/391 [=========>....................] - ETA: 19s - loss: 0.9408 - acc: 0.7221\n",
      " Optimizer iteration 1696, batch 132\n",
      "\n",
      " Learning rate 4.128279459732326e-05, Model learning rate 4.128279397264123e-05\n",
      "\n",
      " Optimizer iteration 1697, batch 133\n",
      "\n",
      " Learning rate 4.096287747920202e-05, Model learning rate 4.09628773923032e-05\n",
      "134/391 [=========>....................] - ETA: 18s - loss: 0.9408 - acc: 0.7223\n",
      " Optimizer iteration 1698, batch 134\n",
      "\n",
      " Learning rate 4.0644151817707866e-05, Model learning rate 4.0644150431035087e-05\n",
      "135/391 [=========>....................] - ETA: 18s - loss: 0.9410 - acc: 0.7225\n",
      " Optimizer iteration 1699, batch 135\n",
      "\n",
      " Learning rate 4.0326618440111316e-05, Model learning rate 4.03266167268157e-05\n",
      "136/391 [=========>....................] - ETA: 18s - loss: 0.9416 - acc: 0.7220\n",
      " Optimizer iteration 1700, batch 136\n",
      "\n",
      " Learning rate 4.001027817058789e-05, Model learning rate 4.001027991762385e-05\n",
      "\n",
      " Optimizer iteration 1701, batch 137\n",
      "\n",
      " Learning rate 3.9695131830216616e-05, Model learning rate 3.9695132727501914e-05\n",
      "138/391 [=========>....................] - ETA: 18s - loss: 0.9415 - acc: 0.7220\n",
      " Optimizer iteration 1702, batch 138\n",
      "\n",
      " Learning rate 3.938118023697762e-05, Model learning rate 3.9381178794428706e-05\n",
      "\n",
      " Optimizer iteration 1703, batch 139\n",
      "\n",
      " Learning rate 3.90684242057498e-05, Model learning rate 3.906842539436184e-05\n",
      "140/391 [=========>....................] - ETA: 18s - loss: 0.9407 - acc: 0.7219\n",
      " Optimizer iteration 1704, batch 140\n",
      "\n",
      " Learning rate 3.8756864548308845e-05, Model learning rate 3.87568652513437e-05\n",
      "\n",
      " Optimizer iteration 1705, batch 141\n",
      "\n",
      " Learning rate 3.844650207332562e-05, Model learning rate 3.844650200335309e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/391 [=========>....................] - ETA: 18s - loss: 0.9410 - acc: 0.7219\n",
      " Optimizer iteration 1706, batch 142\n",
      "\n",
      " Learning rate 3.8137337586363063e-05, Model learning rate 3.813733928836882e-05\n",
      "143/391 [=========>....................] - ETA: 18s - loss: 0.9414 - acc: 0.7214\n",
      " Optimizer iteration 1707, batch 143\n",
      "\n",
      " Learning rate 3.782937188987523e-05, Model learning rate 3.7829373468412086e-05\n",
      "144/391 [==========>...................] - ETA: 18s - loss: 0.9414 - acc: 0.7216\n",
      " Optimizer iteration 1708, batch 144\n",
      "\n",
      " Learning rate 3.7522605783204264e-05, Model learning rate 3.7522604543482885e-05\n",
      "\n",
      " Optimizer iteration 1709, batch 145\n",
      "\n",
      " Learning rate 3.7217040062578756e-05, Model learning rate 3.721703978953883e-05\n",
      "146/391 [==========>...................] - ETA: 18s - loss: 0.9404 - acc: 0.7220\n",
      " Optimizer iteration 1710, batch 146\n",
      "\n",
      " Learning rate 3.691267552111183e-05, Model learning rate 3.6912675568601117e-05\n",
      "\n",
      " Optimizer iteration 1711, batch 147\n",
      "\n",
      " Learning rate 3.660951294879855e-05, Model learning rate 3.660951188066974e-05\n",
      "148/391 [==========>...................] - ETA: 18s - loss: 0.9407 - acc: 0.7219\n",
      " Optimizer iteration 1712, batch 148\n",
      "\n",
      " Learning rate 3.6307553132514546e-05, Model learning rate 3.6307552363723516e-05\n",
      "149/391 [==========>...................] - ETA: 17s - loss: 0.9414 - acc: 0.7215\n",
      " Optimizer iteration 1713, batch 149\n",
      "\n",
      " Learning rate 3.6006796856013493e-05, Model learning rate 3.600679701776244e-05\n",
      "150/391 [==========>...................] - ETA: 17s - loss: 0.9411 - acc: 0.7214\n",
      " Optimizer iteration 1714, batch 150\n",
      "\n",
      " Learning rate 3.5707244899925164e-05, Model learning rate 3.5707245842786506e-05\n",
      "151/391 [==========>...................] - ETA: 17s - loss: 0.9414 - acc: 0.7213\n",
      " Optimizer iteration 1715, batch 151\n",
      "\n",
      " Learning rate 3.54088980417534e-05, Model learning rate 3.540889883879572e-05\n",
      "152/391 [==========>...................] - ETA: 17s - loss: 0.9409 - acc: 0.7214\n",
      " Optimizer iteration 1716, batch 152\n",
      "\n",
      " Learning rate 3.5111757055874326e-05, Model learning rate 3.5111756005790085e-05\n",
      "153/391 [==========>...................] - ETA: 17s - loss: 0.9407 - acc: 0.7213\n",
      " Optimizer iteration 1717, batch 153\n",
      "\n",
      " Learning rate 3.4815822713533954e-05, Model learning rate 3.48158209817484e-05\n",
      "154/391 [==========>...................] - ETA: 17s - loss: 0.9402 - acc: 0.7214\n",
      " Optimizer iteration 1718, batch 154\n",
      "\n",
      " Learning rate 3.4521095782846624e-05, Model learning rate 3.452109740464948e-05\n",
      "155/391 [==========>...................] - ETA: 17s - loss: 0.9396 - acc: 0.7215\n",
      " Optimizer iteration 1719, batch 155\n",
      "\n",
      " Learning rate 3.422757702879259e-05, Model learning rate 3.422757799853571e-05\n",
      "156/391 [==========>...................] - ETA: 17s - loss: 0.9398 - acc: 0.7215\n",
      " Optimizer iteration 1720, batch 156\n",
      "\n",
      " Learning rate 3.393526721321616e-05, Model learning rate 3.393526640138589e-05\n",
      "157/391 [===========>..................] - ETA: 17s - loss: 0.9397 - acc: 0.7217\n",
      " Optimizer iteration 1721, batch 157\n",
      "\n",
      " Learning rate 3.3644167094823985e-05, Model learning rate 3.364416625117883e-05\n",
      "158/391 [===========>..................] - ETA: 17s - loss: 0.9395 - acc: 0.7216\n",
      " Optimizer iteration 1722, batch 158\n",
      "\n",
      " Learning rate 3.335427742918262e-05, Model learning rate 3.3354277547914535e-05\n",
      "\n",
      " Optimizer iteration 1723, batch 159\n",
      "\n",
      " Learning rate 3.3065598968717137e-05, Model learning rate 3.3065600291593e-05\n",
      "160/391 [===========>..................] - ETA: 17s - loss: 0.9385 - acc: 0.7221\n",
      " Optimizer iteration 1724, batch 160\n",
      "\n",
      " Learning rate 3.277813246270872e-05, Model learning rate 3.277813084423542e-05\n",
      "161/391 [===========>..................] - ETA: 17s - loss: 0.9383 - acc: 0.7223\n",
      " Optimizer iteration 1725, batch 161\n",
      "\n",
      " Learning rate 3.249187865729264e-05, Model learning rate 3.2491880119778216e-05\n",
      "162/391 [===========>..................] - ETA: 17s - loss: 0.9384 - acc: 0.7224\n",
      " Optimizer iteration 1726, batch 162\n",
      "\n",
      " Learning rate 3.220683829545678e-05, Model learning rate 3.2206837204284966e-05\n",
      "163/391 [===========>..................] - ETA: 16s - loss: 0.9382 - acc: 0.7223\n",
      " Optimizer iteration 1727, batch 163\n",
      "\n",
      " Learning rate 3.192301211703952e-05, Model learning rate 3.192301301169209e-05\n",
      "164/391 [===========>..................] - ETA: 16s - loss: 0.9375 - acc: 0.7227\n",
      " Optimizer iteration 1728, batch 164\n",
      "\n",
      " Learning rate 3.164040085872755e-05, Model learning rate 3.164040026604198e-05\n",
      "165/391 [===========>..................] - ETA: 16s - loss: 0.9371 - acc: 0.7228\n",
      " Optimizer iteration 1729, batch 165\n",
      "\n",
      " Learning rate 3.1359005254054274e-05, Model learning rate 3.135900624329224e-05\n",
      "\n",
      " Optimizer iteration 1730, batch 166\n",
      "\n",
      " Learning rate 3.107882603339785e-05, Model learning rate 3.1078827305464074e-05\n",
      "167/391 [===========>..................] - ETA: 16s - loss: 0.9375 - acc: 0.7223\n",
      " Optimizer iteration 1731, batch 167\n",
      "\n",
      " Learning rate 3.079986392397899e-05, Model learning rate 3.0799863452557474e-05\n",
      "168/391 [===========>..................] - ETA: 16s - loss: 0.9372 - acc: 0.7223\n",
      " Optimizer iteration 1732, batch 168\n",
      "\n",
      " Learning rate 3.052211964985974e-05, Model learning rate 3.052211832255125e-05\n",
      "169/391 [===========>..................] - ETA: 16s - loss: 0.9371 - acc: 0.7228\n",
      " Optimizer iteration 1733, batch 169\n",
      "\n",
      " Learning rate 3.024559393194076e-05, Model learning rate 3.0245593734434806e-05\n",
      "170/391 [============>.................] - ETA: 16s - loss: 0.9370 - acc: 0.7228\n",
      " Optimizer iteration 1734, batch 170\n",
      "\n",
      " Learning rate 2.9970287487960158e-05, Model learning rate 2.9970287869218737e-05\n",
      "171/391 [============>.................] - ETA: 16s - loss: 0.9368 - acc: 0.7228\n",
      " Optimizer iteration 1735, batch 171\n",
      "\n",
      " Learning rate 2.9696201032491433e-05, Model learning rate 2.9696200726903044e-05\n",
      "172/391 [============>.................] - ETA: 16s - loss: 0.9366 - acc: 0.7227\n",
      " Optimizer iteration 1736, batch 172\n",
      "\n",
      " Learning rate 2.942333527694113e-05, Model learning rate 2.9423335945466533e-05\n",
      "173/391 [============>.................] - ETA: 16s - loss: 0.9371 - acc: 0.7224\n",
      " Optimizer iteration 1737, batch 173\n",
      "\n",
      " Learning rate 2.9151690929547726e-05, Model learning rate 2.9151691705919802e-05\n",
      "174/391 [============>.................] - ETA: 16s - loss: 0.9368 - acc: 0.7225\n",
      " Optimizer iteration 1738, batch 174\n",
      "\n",
      " Learning rate 2.8881268695379436e-05, Model learning rate 2.888126800826285e-05\n",
      "175/391 [============>.................] - ETA: 16s - loss: 0.9372 - acc: 0.7223\n",
      " Optimizer iteration 1739, batch 175\n",
      "\n",
      " Learning rate 2.8612069276332253e-05, Model learning rate 2.8612068490474485e-05\n",
      "176/391 [============>.................] - ETA: 16s - loss: 0.9380 - acc: 0.7222\n",
      " Optimizer iteration 1740, batch 176\n",
      "\n",
      " Learning rate 2.834409337112842e-05, Model learning rate 2.8344093152554706e-05\n",
      "177/391 [============>.................] - ETA: 16s - loss: 0.9377 - acc: 0.7225\n",
      " Optimizer iteration 1741, batch 177\n",
      "\n",
      " Learning rate 2.807734167531456e-05, Model learning rate 2.8077341994503513e-05\n",
      "178/391 [============>.................] - ETA: 15s - loss: 0.9387 - acc: 0.7223\n",
      " Optimizer iteration 1742, batch 178\n",
      "\n",
      " Learning rate 2.78118148812595e-05, Model learning rate 2.7811815016320907e-05\n",
      "179/391 [============>.................] - ETA: 15s - loss: 0.9397 - acc: 0.7221\n",
      " Optimizer iteration 1743, batch 179\n",
      "\n",
      " Learning rate 2.7547513678153e-05, Model learning rate 2.754751403699629e-05\n",
      "180/391 [============>.................] - ETA: 15s - loss: 0.9394 - acc: 0.7221\n",
      " Optimizer iteration 1744, batch 180\n",
      "\n",
      " Learning rate 2.7284438752003758e-05, Model learning rate 2.7284439056529664e-05\n",
      "181/391 [============>.................] - ETA: 15s - loss: 0.9396 - acc: 0.7221\n",
      " Optimizer iteration 1745, batch 181\n",
      "\n",
      " Learning rate 2.70225907856374e-05, Model learning rate 2.7022590074921027e-05\n",
      "182/391 [============>.................] - ETA: 15s - loss: 0.9391 - acc: 0.7224\n",
      " Optimizer iteration 1746, batch 182\n",
      "\n",
      " Learning rate 2.6761970458695107e-05, Model learning rate 2.6761970730149187e-05\n",
      "183/391 [=============>................] - ETA: 15s - loss: 0.9390 - acc: 0.7224\n",
      " Optimizer iteration 1747, batch 183\n",
      "\n",
      " Learning rate 2.65025784476316e-05, Model learning rate 2.650257920322474e-05\n",
      "184/391 [=============>................] - ETA: 15s - loss: 0.9384 - acc: 0.7227\n",
      " Optimizer iteration 1748, batch 184\n",
      "\n",
      " Learning rate 2.6244415425713264e-05, Model learning rate 2.6244415494147688e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/391 [=============>................] - ETA: 15s - loss: 0.9383 - acc: 0.7226\n",
      " Optimizer iteration 1749, batch 185\n",
      "\n",
      " Learning rate 2.598748206301682e-05, Model learning rate 2.5987481421907432e-05\n",
      "186/391 [=============>................] - ETA: 15s - loss: 0.9385 - acc: 0.7226\n",
      " Optimizer iteration 1750, batch 186\n",
      "\n",
      " Learning rate 2.573177902642726e-05, Model learning rate 2.5731778805493377e-05\n",
      "187/391 [=============>................] - ETA: 15s - loss: 0.9379 - acc: 0.7228\n",
      " Optimizer iteration 1751, batch 187\n",
      "\n",
      " Learning rate 2.547730697963607e-05, Model learning rate 2.5477307644905522e-05\n",
      "188/391 [=============>................] - ETA: 15s - loss: 0.9370 - acc: 0.7232\n",
      " Optimizer iteration 1752, batch 188\n",
      "\n",
      " Learning rate 2.522406658313997e-05, Model learning rate 2.5224066121154465e-05\n",
      "189/391 [=============>................] - ETA: 15s - loss: 0.9364 - acc: 0.7235\n",
      " Optimizer iteration 1753, batch 189\n",
      "\n",
      " Learning rate 2.4972058494238337e-05, Model learning rate 2.497205787221901e-05\n",
      "190/391 [=============>................] - ETA: 15s - loss: 0.9372 - acc: 0.7235\n",
      " Optimizer iteration 1754, batch 190\n",
      "\n",
      " Learning rate 2.4721283367032387e-05, Model learning rate 2.472128289809916e-05\n",
      "191/391 [=============>................] - ETA: 15s - loss: 0.9374 - acc: 0.7235\n",
      " Optimizer iteration 1755, batch 191\n",
      "\n",
      " Learning rate 2.4471741852423235e-05, Model learning rate 2.4471741198794916e-05\n",
      "192/391 [=============>................] - ETA: 15s - loss: 0.9372 - acc: 0.7236\n",
      " Optimizer iteration 1756, batch 192\n",
      "\n",
      " Learning rate 2.422343459810966e-05, Model learning rate 2.422343459329568e-05\n",
      "\n",
      " Optimizer iteration 1757, batch 193\n",
      "\n",
      " Learning rate 2.3976362248587293e-05, Model learning rate 2.397636308160145e-05\n",
      "194/391 [=============>................] - ETA: 14s - loss: 0.9368 - acc: 0.7236\n",
      " Optimizer iteration 1758, batch 194\n",
      "\n",
      " Learning rate 2.3730525445146145e-05, Model learning rate 2.3730524844722822e-05\n",
      "195/391 [=============>................] - ETA: 14s - loss: 0.9368 - acc: 0.7237\n",
      " Optimizer iteration 1759, batch 195\n",
      "\n",
      " Learning rate 2.348592482586942e-05, Model learning rate 2.348592533962801e-05\n",
      "196/391 [==============>...............] - ETA: 14s - loss: 0.9368 - acc: 0.7238\n",
      " Optimizer iteration 1760, batch 196\n",
      "\n",
      " Learning rate 2.324256102563188e-05, Model learning rate 2.3242560928338207e-05\n",
      "\n",
      " Optimizer iteration 1761, batch 197\n",
      "\n",
      " Learning rate 2.3000434676097803e-05, Model learning rate 2.300043524883222e-05\n",
      "198/391 [==============>...............] - ETA: 14s - loss: 0.9367 - acc: 0.7236\n",
      " Optimizer iteration 1762, batch 198\n",
      "\n",
      " Learning rate 2.275954640571981e-05, Model learning rate 2.275954648212064e-05\n",
      "199/391 [==============>...............] - ETA: 14s - loss: 0.9370 - acc: 0.7235\n",
      " Optimizer iteration 1763, batch 199\n",
      "\n",
      " Learning rate 2.2519896839737097e-05, Model learning rate 2.2519896447192878e-05\n",
      "200/391 [==============>...............] - ETA: 14s - loss: 0.9372 - acc: 0.7232\n",
      " Optimizer iteration 1764, batch 200\n",
      "\n",
      " Learning rate 2.2281486600173206e-05, Model learning rate 2.2281486963038333e-05\n",
      "201/391 [==============>...............] - ETA: 14s - loss: 0.9370 - acc: 0.7234\n",
      " Optimizer iteration 1765, batch 201\n",
      "\n",
      " Learning rate 2.2044316305835478e-05, Model learning rate 2.2044316210667603e-05\n",
      "\n",
      " Optimizer iteration 1766, batch 202\n",
      "\n",
      " Learning rate 2.180838657231282e-05, Model learning rate 2.180838600907009e-05\n",
      "203/391 [==============>...............] - ETA: 14s - loss: 0.9362 - acc: 0.7238\n",
      " Optimizer iteration 1767, batch 203\n",
      "\n",
      " Learning rate 2.1573698011973954e-05, Model learning rate 2.15736981772352e-05\n",
      "204/391 [==============>...............] - ETA: 14s - loss: 0.9363 - acc: 0.7237\n",
      " Optimizer iteration 1768, batch 204\n",
      "\n",
      " Learning rate 2.134025123396638e-05, Model learning rate 2.134025089617353e-05\n",
      "205/391 [==============>...............] - ETA: 14s - loss: 0.9365 - acc: 0.7236\n",
      " Optimizer iteration 1769, batch 205\n",
      "\n",
      " Learning rate 2.1108046844214192e-05, Model learning rate 2.110804598487448e-05\n",
      "206/391 [==============>...............] - ETA: 14s - loss: 0.9363 - acc: 0.7237\n",
      " Optimizer iteration 1770, batch 206\n",
      "\n",
      " Learning rate 2.087708544541689e-05, Model learning rate 2.0877085262327455e-05\n",
      "207/391 [==============>...............] - ETA: 13s - loss: 0.9360 - acc: 0.7238\n",
      " Optimizer iteration 1771, batch 207\n",
      "\n",
      " Learning rate 2.0647367637047887e-05, Model learning rate 2.0647366909543052e-05\n",
      "208/391 [==============>...............] - ETA: 13s - loss: 0.9365 - acc: 0.7238\n",
      " Optimizer iteration 1772, batch 208\n",
      "\n",
      " Learning rate 2.041889401535252e-05, Model learning rate 2.041889456450008e-05\n",
      "\n",
      " Optimizer iteration 1773, batch 209\n",
      "\n",
      " Learning rate 2.019166517334703e-05, Model learning rate 2.0191664589219727e-05\n",
      "210/391 [===============>..............] - ETA: 13s - loss: 0.9355 - acc: 0.7243\n",
      " Optimizer iteration 1774, batch 210\n",
      "\n",
      " Learning rate 1.9965681700816584e-05, Model learning rate 1.9965682440670207e-05\n",
      "\n",
      " Optimizer iteration 1775, batch 211\n",
      "\n",
      " Learning rate 1.974094418431388e-05, Model learning rate 1.9740944480872713e-05\n",
      "212/391 [===============>..............] - ETA: 13s - loss: 0.9362 - acc: 0.7240\n",
      " Optimizer iteration 1776, batch 212\n",
      "\n",
      " Learning rate 1.9517453207157864e-05, Model learning rate 1.9517452528816648e-05\n",
      "213/391 [===============>..............] - ETA: 13s - loss: 0.9364 - acc: 0.7238\n",
      " Optimizer iteration 1777, batch 213\n",
      "\n",
      " Learning rate 1.929520934943191e-05, Model learning rate 1.929521022248082e-05\n",
      "214/391 [===============>..............] - ETA: 13s - loss: 0.9369 - acc: 0.7238\n",
      " Optimizer iteration 1778, batch 214\n",
      "\n",
      " Learning rate 1.9074213187982415e-05, Model learning rate 1.907421392388642e-05\n",
      "215/391 [===============>..............] - ETA: 13s - loss: 0.9369 - acc: 0.7238\n",
      " Optimizer iteration 1779, batch 215\n",
      "\n",
      " Learning rate 1.885446529641732e-05, Model learning rate 1.885446545202285e-05\n",
      "216/391 [===============>..............] - ETA: 13s - loss: 0.9377 - acc: 0.7236\n",
      " Optimizer iteration 1780, batch 216\n",
      "\n",
      " Learning rate 1.8635966245104663e-05, Model learning rate 1.863596662587952e-05\n",
      "217/391 [===============>..............] - ETA: 13s - loss: 0.9380 - acc: 0.7235\n",
      " Optimizer iteration 1781, batch 217\n",
      "\n",
      " Learning rate 1.841871660117095e-05, Model learning rate 1.8418717445456423e-05\n",
      "218/391 [===============>..............] - ETA: 13s - loss: 0.9374 - acc: 0.7237\n",
      " Optimizer iteration 1782, batch 218\n",
      "\n",
      " Learning rate 1.820271692849984e-05, Model learning rate 1.820271609176416e-05\n",
      "219/391 [===============>..............] - ETA: 13s - loss: 0.9374 - acc: 0.7237\n",
      " Optimizer iteration 1783, batch 219\n",
      "\n",
      " Learning rate 1.7987967787730542e-05, Model learning rate 1.798796802177094e-05\n",
      "220/391 [===============>..............] - ETA: 13s - loss: 0.9373 - acc: 0.7236\n",
      " Optimizer iteration 1784, batch 220\n",
      "\n",
      " Learning rate 1.7774469736256683e-05, Model learning rate 1.7774469597497955e-05\n",
      "\n",
      " Optimizer iteration 1785, batch 221\n",
      "\n",
      " Learning rate 1.7562223328224324e-05, Model learning rate 1.756222263793461e-05\n",
      "222/391 [================>.............] - ETA: 12s - loss: 0.9373 - acc: 0.7237\n",
      " Optimizer iteration 1786, batch 222\n",
      "\n",
      " Learning rate 1.735122911453091e-05, Model learning rate 1.735122896207031e-05\n",
      "223/391 [================>.............] - ETA: 12s - loss: 0.9373 - acc: 0.7237\n",
      " Optimizer iteration 1787, batch 223\n",
      "\n",
      " Learning rate 1.7141487642823806e-05, Model learning rate 1.7141486750915647e-05\n",
      "224/391 [================>.............] - ETA: 12s - loss: 0.9382 - acc: 0.7234\n",
      " Optimizer iteration 1788, batch 224\n",
      "\n",
      " Learning rate 1.693299945749882e-05, Model learning rate 1.693299964244943e-05\n",
      "\n",
      " Optimizer iteration 1789, batch 225\n",
      "\n",
      " Learning rate 1.6725765099698698e-05, Model learning rate 1.672576581768226e-05\n",
      "226/391 [================>.............] - ETA: 12s - loss: 0.9372 - acc: 0.7237\n",
      " Optimizer iteration 1790, batch 226\n",
      "\n",
      " Learning rate 1.651978510731189e-05, Model learning rate 1.651978527661413e-05\n",
      "227/391 [================>.............] - ETA: 12s - loss: 0.9377 - acc: 0.7235\n",
      " Optimizer iteration 1791, batch 227\n",
      "\n",
      " Learning rate 1.6315060014971016e-05, Model learning rate 1.6315059838234447e-05\n",
      "228/391 [================>.............] - ETA: 12s - loss: 0.9371 - acc: 0.7237\n",
      " Optimizer iteration 1792, batch 228\n",
      "\n",
      " Learning rate 1.6111590354051464e-05, Model learning rate 1.611158950254321e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/391 [================>.............] - ETA: 12s - loss: 0.9367 - acc: 0.7239\n",
      " Optimizer iteration 1793, batch 229\n",
      "\n",
      " Learning rate 1.5909376652670282e-05, Model learning rate 1.5909376088529825e-05\n",
      "\n",
      " Optimizer iteration 1794, batch 230\n",
      "\n",
      " Learning rate 1.5708419435684463e-05, Model learning rate 1.570841959619429e-05\n",
      "231/391 [================>.............] - ETA: 12s - loss: 0.9362 - acc: 0.7240\n",
      " Optimizer iteration 1795, batch 231\n",
      "\n",
      " Learning rate 1.5508719224689714e-05, Model learning rate 1.5508720025536604e-05\n",
      "232/391 [================>.............] - ETA: 12s - loss: 0.9359 - acc: 0.7240\n",
      " Optimizer iteration 1796, batch 232\n",
      "\n",
      " Learning rate 1.5310276538019196e-05, Model learning rate 1.531027737655677e-05\n",
      "\n",
      " Optimizer iteration 1797, batch 233\n",
      "\n",
      " Learning rate 1.5113091890741948e-05, Model learning rate 1.5113091649254784e-05\n",
      "234/391 [================>.............] - ETA: 11s - loss: 0.9357 - acc: 0.7242\n",
      " Optimizer iteration 1798, batch 234\n",
      "\n",
      " Learning rate 1.4917165794661846e-05, Model learning rate 1.4917165572114754e-05\n",
      "235/391 [=================>............] - ETA: 11s - loss: 0.9362 - acc: 0.7239\n",
      " Optimizer iteration 1799, batch 235\n",
      "\n",
      " Learning rate 1.4722498758316161e-05, Model learning rate 1.472249914513668e-05\n",
      "236/391 [=================>............] - ETA: 11s - loss: 0.9363 - acc: 0.7239\n",
      " Optimizer iteration 1800, batch 236\n",
      "\n",
      " Learning rate 1.4529091286973995e-05, Model learning rate 1.452909145882586e-05\n",
      "237/391 [=================>............] - ETA: 11s - loss: 0.9366 - acc: 0.7240\n",
      " Optimizer iteration 1801, batch 237\n",
      "\n",
      " Learning rate 1.4336943882635345e-05, Model learning rate 1.4336944332171697e-05\n",
      "238/391 [=================>............] - ETA: 11s - loss: 0.9363 - acc: 0.7241\n",
      " Optimizer iteration 1802, batch 238\n",
      "\n",
      " Learning rate 1.414605704402966e-05, Model learning rate 1.414605685567949e-05\n",
      "239/391 [=================>............] - ETA: 11s - loss: 0.9366 - acc: 0.7238\n",
      " Optimizer iteration 1803, batch 239\n",
      "\n",
      " Learning rate 1.3956431266614278e-05, Model learning rate 1.3956430848338641e-05\n",
      "240/391 [=================>............] - ETA: 11s - loss: 0.9361 - acc: 0.7241\n",
      " Optimizer iteration 1804, batch 240\n",
      "\n",
      " Learning rate 1.3768067042573662e-05, Model learning rate 1.3768067219643854e-05\n",
      "241/391 [=================>............] - ETA: 11s - loss: 0.9367 - acc: 0.7240\n",
      " Optimizer iteration 1805, batch 241\n",
      "\n",
      " Learning rate 1.3580964860817779e-05, Model learning rate 1.3580965060100425e-05\n",
      "\n",
      " Optimizer iteration 1806, batch 242\n",
      "\n",
      " Learning rate 1.3395125206980774e-05, Model learning rate 1.3395125279203057e-05\n",
      "243/391 [=================>............] - ETA: 11s - loss: 0.9364 - acc: 0.7242\n",
      " Optimizer iteration 1807, batch 243\n",
      "\n",
      " Learning rate 1.3210548563419855e-05, Model learning rate 1.3210548786446452e-05\n",
      "244/391 [=================>............] - ETA: 11s - loss: 0.9363 - acc: 0.7244\n",
      " Optimizer iteration 1808, batch 244\n",
      "\n",
      " Learning rate 1.3027235409214189e-05, Model learning rate 1.302723558183061e-05\n",
      "245/391 [=================>............] - ETA: 11s - loss: 0.9361 - acc: 0.7243\n",
      " Optimizer iteration 1809, batch 245\n",
      "\n",
      " Learning rate 1.2845186220163286e-05, Model learning rate 1.2845186574850231e-05\n",
      "246/391 [=================>............] - ETA: 11s - loss: 0.9361 - acc: 0.7242\n",
      " Optimizer iteration 1810, batch 246\n",
      "\n",
      " Learning rate 1.2664401468786114e-05, Model learning rate 1.2664401765505318e-05\n",
      "247/391 [=================>............] - ETA: 10s - loss: 0.9361 - acc: 0.7243\n",
      " Optimizer iteration 1811, batch 247\n",
      "\n",
      " Learning rate 1.2484881624319489e-05, Model learning rate 1.248488206329057e-05\n",
      "248/391 [==================>...........] - ETA: 10s - loss: 0.9365 - acc: 0.7240\n",
      " Optimizer iteration 1812, batch 248\n",
      "\n",
      " Learning rate 1.2306627152717408e-05, Model learning rate 1.2306627468205988e-05\n",
      "249/391 [==================>...........] - ETA: 10s - loss: 0.9362 - acc: 0.7242\n",
      " Optimizer iteration 1813, batch 249\n",
      "\n",
      " Learning rate 1.2129638516649278e-05, Model learning rate 1.2129638889746275e-05\n",
      "250/391 [==================>...........] - ETA: 10s - loss: 0.9359 - acc: 0.7243\n",
      " Optimizer iteration 1814, batch 250\n",
      "\n",
      " Learning rate 1.1953916175499068e-05, Model learning rate 1.1953916327911429e-05\n",
      "251/391 [==================>...........] - ETA: 10s - loss: 0.9354 - acc: 0.7244\n",
      " Optimizer iteration 1815, batch 251\n",
      "\n",
      " Learning rate 1.1779460585363943e-05, Model learning rate 1.1779460692196153e-05\n",
      "252/391 [==================>...........] - ETA: 10s - loss: 0.9351 - acc: 0.7246\n",
      " Optimizer iteration 1816, batch 252\n",
      "\n",
      " Learning rate 1.1606272199053247e-05, Model learning rate 1.1606271982600447e-05\n",
      "253/391 [==================>...........] - ETA: 10s - loss: 0.9345 - acc: 0.7247\n",
      " Optimizer iteration 1817, batch 253\n",
      "\n",
      " Learning rate 1.1434351466087178e-05, Model learning rate 1.1434351108619012e-05\n",
      "254/391 [==================>...........] - ETA: 10s - loss: 0.9346 - acc: 0.7246\n",
      " Optimizer iteration 1818, batch 254\n",
      "\n",
      " Learning rate 1.1263698832695512e-05, Model learning rate 1.126369897974655e-05\n",
      "255/391 [==================>...........] - ETA: 10s - loss: 0.9352 - acc: 0.7244\n",
      " Optimizer iteration 1819, batch 255\n",
      "\n",
      " Learning rate 1.1094314741816935e-05, Model learning rate 1.109431468648836e-05\n",
      "256/391 [==================>...........] - ETA: 10s - loss: 0.9356 - acc: 0.7242\n",
      " Optimizer iteration 1820, batch 256\n",
      "\n",
      " Learning rate 1.0926199633097156e-05, Model learning rate 1.0926200047833845e-05\n",
      "257/391 [==================>...........] - ETA: 10s - loss: 0.9352 - acc: 0.7243\n",
      " Optimizer iteration 1821, batch 257\n",
      "\n",
      " Learning rate 1.0759353942888573e-05, Model learning rate 1.0759354154288303e-05\n",
      "258/391 [==================>...........] - ETA: 10s - loss: 0.9352 - acc: 0.7242\n",
      " Optimizer iteration 1822, batch 258\n",
      "\n",
      " Learning rate 1.0593778104248441e-05, Model learning rate 1.0593777915346436e-05\n",
      "\n",
      " Optimizer iteration 1823, batch 259\n",
      "\n",
      " Learning rate 1.0429472546938157e-05, Model learning rate 1.0429472240502946e-05\n",
      "260/391 [==================>...........] - ETA: 10s - loss: 0.9354 - acc: 0.7243\n",
      " Optimizer iteration 1824, batch 260\n",
      "\n",
      " Learning rate 1.0266437697422026e-05, Model learning rate 1.0266438039252535e-05\n",
      "261/391 [===================>..........] - ETA: 9s - loss: 0.9352 - acc: 0.7244 \n",
      " Optimizer iteration 1825, batch 261\n",
      "\n",
      " Learning rate 1.0104673978866164e-05, Model learning rate 1.01046744021005e-05\n",
      "262/391 [===================>..........] - ETA: 9s - loss: 0.9350 - acc: 0.7245\n",
      " Optimizer iteration 1826, batch 262\n",
      "\n",
      " Learning rate 9.94418181113732e-06, Model learning rate 9.944182238541543e-06\n",
      "263/391 [===================>..........] - ETA: 9s - loss: 0.9348 - acc: 0.7245\n",
      " Optimizer iteration 1827, batch 263\n",
      "\n",
      " Learning rate 9.784961610802113e-06, Model learning rate 9.784961548575666e-06\n",
      "264/391 [===================>..........] - ETA: 9s - loss: 0.9346 - acc: 0.7245\n",
      " Optimizer iteration 1828, batch 264\n",
      "\n",
      " Learning rate 9.627013791125295e-06, Model learning rate 9.62701415119227e-06\n",
      "265/391 [===================>..........] - ETA: 9s - loss: 0.9348 - acc: 0.7244\n",
      " Optimizer iteration 1829, batch 265\n",
      "\n",
      " Learning rate 9.470338762069431e-06, Model learning rate 9.470339136896655e-06\n",
      "266/391 [===================>..........] - ETA: 9s - loss: 0.9349 - acc: 0.7243\n",
      " Optimizer iteration 1830, batch 266\n",
      "\n",
      " Learning rate 9.314936930293284e-06, Model learning rate 9.31493650568882e-06\n",
      "267/391 [===================>..........] - ETA: 9s - loss: 0.9345 - acc: 0.7246\n",
      " Optimizer iteration 1831, batch 267\n",
      "\n",
      " Learning rate 9.16080869915109e-06, Model learning rate 9.16080898605287e-06\n",
      "268/391 [===================>..........] - ETA: 9s - loss: 0.9343 - acc: 0.7247\n",
      " Optimizer iteration 1832, batch 268\n",
      "\n",
      " Learning rate 9.007954468691294e-06, Model learning rate 9.007954758999404e-06\n",
      "269/391 [===================>..........] - ETA: 9s - loss: 0.9345 - acc: 0.7246\n",
      " Optimizer iteration 1833, batch 269\n",
      "\n",
      " Learning rate 8.856374635655695e-06, Model learning rate 8.85637473402312e-06\n",
      "270/391 [===================>..........] - ETA: 9s - loss: 0.9348 - acc: 0.7244\n",
      " Optimizer iteration 1834, batch 270\n",
      "\n",
      " Learning rate 8.706069593478139e-06, Model learning rate 8.706069820618723e-06\n",
      "271/391 [===================>..........] - ETA: 9s - loss: 0.9343 - acc: 0.7245\n",
      " Optimizer iteration 1835, batch 271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Learning rate 8.557039732283945e-06, Model learning rate 8.55704001878621e-06\n",
      "272/391 [===================>..........] - ETA: 9s - loss: 0.9345 - acc: 0.7243\n",
      " Optimizer iteration 1836, batch 272\n",
      "\n",
      " Learning rate 8.409285438888358e-06, Model learning rate 8.409285328525584e-06\n",
      "273/391 [===================>..........] - ETA: 9s - loss: 0.9344 - acc: 0.7245\n",
      " Optimizer iteration 1837, batch 273\n",
      "\n",
      " Learning rate 8.262807096795999e-06, Model learning rate 8.262806659331545e-06\n",
      "274/391 [====================>.........] - ETA: 9s - loss: 0.9345 - acc: 0.7245\n",
      " Optimizer iteration 1838, batch 274\n",
      "\n",
      " Learning rate 8.117605086199687e-06, Model learning rate 8.117604920698795e-06\n",
      "\n",
      " Optimizer iteration 1839, batch 275\n",
      "\n",
      " Learning rate 7.973679783979337e-06, Model learning rate 7.973680112627335e-06\n",
      "276/391 [====================>.........] - ETA: 8s - loss: 0.9342 - acc: 0.7247\n",
      " Optimizer iteration 1840, batch 276\n",
      "\n",
      " Learning rate 7.83103156370113e-06, Model learning rate 7.831031325622462e-06\n",
      "277/391 [====================>.........] - ETA: 8s - loss: 0.9339 - acc: 0.7248\n",
      " Optimizer iteration 1841, batch 277\n",
      "\n",
      " Learning rate 7.689660795616559e-06, Model learning rate 7.68966037867358e-06\n",
      "\n",
      " Optimizer iteration 1842, batch 278\n",
      "\n",
      " Learning rate 7.549567846661387e-06, Model learning rate 7.549567726528039e-06\n",
      "279/391 [====================>.........] - ETA: 8s - loss: 0.9331 - acc: 0.7251\n",
      " Optimizer iteration 1843, batch 279\n",
      "\n",
      " Learning rate 7.410753080454746e-06, Model learning rate 7.41075291443849e-06\n",
      "280/391 [====================>.........] - ETA: 8s - loss: 0.9329 - acc: 0.7250\n",
      " Optimizer iteration 1844, batch 280\n",
      "\n",
      " Learning rate 7.2732168572981485e-06, Model learning rate 7.273216851899633e-06\n",
      "281/391 [====================>.........] - ETA: 8s - loss: 0.9324 - acc: 0.7252\n",
      " Optimizer iteration 1845, batch 281\n",
      "\n",
      " Learning rate 7.136959534174592e-06, Model learning rate 7.136959538911469e-06\n",
      "\n",
      " Optimizer iteration 1846, batch 282\n",
      "\n",
      " Learning rate 7.001981464747565e-06, Model learning rate 7.001981430221349e-06\n",
      "283/391 [====================>.........] - ETA: 8s - loss: 0.9327 - acc: 0.7251\n",
      " Optimizer iteration 1847, batch 283\n",
      "\n",
      " Learning rate 6.868282999360265e-06, Model learning rate 6.868282980576623e-06\n",
      "284/391 [====================>.........] - ETA: 8s - loss: 0.9326 - acc: 0.7253\n",
      " Optimizer iteration 1848, batch 284\n",
      "\n",
      " Learning rate 6.735864485034493e-06, Model learning rate 6.735864644724643e-06\n",
      "285/391 [====================>.........] - ETA: 8s - loss: 0.9322 - acc: 0.7254\n",
      " Optimizer iteration 1849, batch 285\n",
      "\n",
      " Learning rate 6.604726265470096e-06, Model learning rate 6.604726422665408e-06\n",
      "286/391 [====================>.........] - ETA: 8s - loss: 0.9321 - acc: 0.7254\n",
      " Optimizer iteration 1850, batch 286\n",
      "\n",
      " Learning rate 6.474868681043577e-06, Model learning rate 6.474868769146269e-06\n",
      "\n",
      " Optimizer iteration 1851, batch 287\n",
      "\n",
      " Learning rate 6.346292068807602e-06, Model learning rate 6.346292138914578e-06\n",
      "288/391 [=====================>........] - ETA: 7s - loss: 0.9324 - acc: 0.7252\n",
      " Optimizer iteration 1852, batch 288\n",
      "\n",
      " Learning rate 6.2189967624899925e-06, Model learning rate 6.218996986717684e-06\n",
      "289/391 [=====================>........] - ETA: 7s - loss: 0.9326 - acc: 0.7250\n",
      " Optimizer iteration 1853, batch 289\n",
      "\n",
      " Learning rate 6.092983092492843e-06, Model learning rate 6.092983312555589e-06\n",
      "\n",
      " Optimizer iteration 1854, batch 290\n",
      "\n",
      " Learning rate 5.968251385891743e-06, Model learning rate 5.968251571175642e-06\n",
      "291/391 [=====================>........] - ETA: 7s - loss: 0.9333 - acc: 0.7247\n",
      " Optimizer iteration 1855, batch 291\n",
      "\n",
      " Learning rate 5.844801966434832e-06, Model learning rate 5.844801762577845e-06\n",
      "292/391 [=====================>........] - ETA: 7s - loss: 0.9331 - acc: 0.7246\n",
      " Optimizer iteration 1856, batch 292\n",
      "\n",
      " Learning rate 5.722635154541967e-06, Model learning rate 5.722635251004249e-06\n",
      "293/391 [=====================>........] - ETA: 7s - loss: 0.9325 - acc: 0.7249\n",
      " Optimizer iteration 1857, batch 293\n",
      "\n",
      " Learning rate 5.601751267304056e-06, Model learning rate 5.601751126960153e-06\n",
      "294/391 [=====================>........] - ETA: 7s - loss: 0.9325 - acc: 0.7249\n",
      " Optimizer iteration 1858, batch 294\n",
      "\n",
      " Learning rate 5.482150618481952e-06, Model learning rate 5.482150754687609e-06\n",
      "295/391 [=====================>........] - ETA: 7s - loss: 0.9323 - acc: 0.7249\n",
      " Optimizer iteration 1859, batch 295\n",
      "\n",
      " Learning rate 5.363833518505834e-06, Model learning rate 5.363833679439267e-06\n",
      "296/391 [=====================>........] - ETA: 7s - loss: 0.9325 - acc: 0.7249\n",
      " Optimizer iteration 1860, batch 296\n",
      "\n",
      " Learning rate 5.2468002744744395e-06, Model learning rate 5.246800355962478e-06\n",
      "297/391 [=====================>........] - ETA: 7s - loss: 0.9321 - acc: 0.7251\n",
      " Optimizer iteration 1861, batch 297\n",
      "\n",
      " Learning rate 5.131051190154113e-06, Model learning rate 5.1310512390045915e-06\n",
      "298/391 [=====================>........] - ETA: 7s - loss: 0.9315 - acc: 0.7252\n",
      " Optimizer iteration 1862, batch 298\n",
      "\n",
      " Learning rate 5.016586565978087e-06, Model learning rate 5.01658678331296e-06\n",
      "\n",
      " Optimizer iteration 1863, batch 299\n",
      "\n",
      " Learning rate 4.9034066990457095e-06, Model learning rate 4.903406534140231e-06\n",
      "300/391 [======================>.......] - ETA: 7s - loss: 0.9314 - acc: 0.7254\n",
      " Optimizer iteration 1864, batch 300\n",
      "\n",
      " Learning rate 4.791511883121713e-06, Model learning rate 4.791511855728459e-06\n",
      "301/391 [======================>.......] - ETA: 6s - loss: 0.9311 - acc: 0.7254\n",
      " Optimizer iteration 1865, batch 301\n",
      "\n",
      " Learning rate 4.680902408635335e-06, Model learning rate 4.680902293330291e-06\n",
      "\n",
      " Optimizer iteration 1866, batch 302\n",
      "\n",
      " Learning rate 4.571578562679757e-06, Model learning rate 4.571578756440431e-06\n",
      "303/391 [======================>.......] - ETA: 6s - loss: 0.9314 - acc: 0.7252\n",
      " Optimizer iteration 1867, batch 303\n",
      "\n",
      " Learning rate 4.463540629010998e-06, Model learning rate 4.4635407903115265e-06\n",
      "304/391 [======================>.......] - ETA: 6s - loss: 0.9315 - acc: 0.7253\n",
      " Optimizer iteration 1868, batch 304\n",
      "\n",
      " Learning rate 4.356788888047747e-06, Model learning rate 4.356788849690929e-06\n",
      "305/391 [======================>.......] - ETA: 6s - loss: 0.9314 - acc: 0.7253\n",
      " Optimizer iteration 1869, batch 305\n",
      "\n",
      " Learning rate 4.2513236168700845e-06, Model learning rate 4.25132384407334e-06\n",
      "306/391 [======================>.......] - ETA: 6s - loss: 0.9320 - acc: 0.7252\n",
      " Optimizer iteration 1870, batch 306\n",
      "\n",
      " Learning rate 4.147145089218984e-06, Model learning rate 4.1471448639640585e-06\n",
      "\n",
      " Optimizer iteration 1871, batch 307\n",
      "\n",
      " Learning rate 4.04425357549576e-06, Model learning rate 4.044253728352487e-06\n",
      "308/391 [======================>.......] - ETA: 6s - loss: 0.9321 - acc: 0.7251\n",
      " Optimizer iteration 1872, batch 308\n",
      "\n",
      " Learning rate 3.942649342761117e-06, Model learning rate 3.942649527743924e-06\n",
      "309/391 [======================>.......] - ETA: 6s - loss: 0.9320 - acc: 0.7252\n",
      " Optimizer iteration 1873, batch 309\n",
      "\n",
      " Learning rate 3.842332654734437e-06, Model learning rate 3.842332716885721e-06\n",
      "310/391 [======================>.......] - ETA: 6s - loss: 0.9321 - acc: 0.7252\n",
      " Optimizer iteration 1874, batch 310\n",
      "\n",
      " Learning rate 3.7433037717933828e-06, Model learning rate 3.7433037505252287e-06\n",
      "\n",
      " Optimizer iteration 1875, batch 311\n",
      "\n",
      " Learning rate 3.645562950973014e-06, Model learning rate 3.645562856036122e-06\n",
      "312/391 [======================>.......] - ETA: 6s - loss: 0.9320 - acc: 0.7253\n",
      " Optimizer iteration 1876, batch 312\n",
      "\n",
      " Learning rate 3.5491104459650646e-06, Model learning rate 3.549110488165752e-06\n",
      "313/391 [=======================>......] - ETA: 6s - loss: 0.9319 - acc: 0.7253\n",
      " Optimizer iteration 1877, batch 313\n",
      "\n",
      " Learning rate 3.453946507117445e-06, Model learning rate 3.4539464195404435e-06\n",
      "314/391 [=======================>......] - ETA: 5s - loss: 0.9318 - acc: 0.7252\n",
      " Optimizer iteration 1878, batch 314\n",
      "\n",
      " Learning rate 3.3600713814335158e-06, Model learning rate 3.3600713322812226e-06\n",
      "315/391 [=======================>......] - ETA: 5s - loss: 0.9319 - acc: 0.7252\n",
      " Optimizer iteration 1879, batch 315\n",
      "\n",
      " Learning rate 3.2674853125714276e-06, Model learning rate 3.2674852263880894e-06\n",
      "316/391 [=======================>......] - ETA: 5s - loss: 0.9319 - acc: 0.7253\n",
      " Optimizer iteration 1880, batch 316\n",
      "\n",
      " Learning rate 3.1761885408435056e-06, Model learning rate 3.1761885566083947e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/391 [=======================>......] - ETA: 5s - loss: 0.9317 - acc: 0.7254\n",
      " Optimizer iteration 1881, batch 317\n",
      "\n",
      " Learning rate 3.0861813032156404e-06, Model learning rate 3.0861813229421386e-06\n",
      "318/391 [=======================>......] - ETA: 5s - loss: 0.9320 - acc: 0.7252\n",
      " Optimizer iteration 1882, batch 318\n",
      "\n",
      " Learning rate 2.997463833306735e-06, Model learning rate 2.9974637527629966e-06\n",
      "319/391 [=======================>......] - ETA: 5s - loss: 0.9320 - acc: 0.7253\n",
      " Optimizer iteration 1883, batch 319\n",
      "\n",
      " Learning rate 2.9100363613879243e-06, Model learning rate 2.9100363008183194e-06\n",
      "320/391 [=======================>......] - ETA: 5s - loss: 0.9323 - acc: 0.7252\n",
      " Optimizer iteration 1884, batch 320\n",
      "\n",
      " Learning rate 2.823899114382078e-06, Model learning rate 2.8238991944817826e-06\n",
      "321/391 [=======================>......] - ETA: 5s - loss: 0.9323 - acc: 0.7253\n",
      " Optimizer iteration 1885, batch 321\n",
      "\n",
      " Learning rate 2.739052315863355e-06, Model learning rate 2.7390522063797107e-06\n",
      "322/391 [=======================>......] - ETA: 5s - loss: 0.9317 - acc: 0.7255\n",
      " Optimizer iteration 1886, batch 322\n",
      "\n",
      " Learning rate 2.655496186056261e-06, Model learning rate 2.6554962460068054e-06\n",
      "323/391 [=======================>......] - ETA: 5s - loss: 0.9316 - acc: 0.7255\n",
      " Optimizer iteration 1887, batch 323\n",
      "\n",
      " Learning rate 2.573230941835536e-06, Model learning rate 2.573230858615716e-06\n",
      "\n",
      " Optimizer iteration 1888, batch 324\n",
      "\n",
      " Learning rate 2.492256796725212e-06, Model learning rate 2.4922567263274686e-06\n",
      "325/391 [=======================>......] - ETA: 5s - loss: 0.9315 - acc: 0.7255\n",
      " Optimizer iteration 1889, batch 325\n",
      "\n",
      " Learning rate 2.4125739608981124e-06, Model learning rate 2.4125738491420634e-06\n",
      "326/391 [========================>.....] - ETA: 5s - loss: 0.9311 - acc: 0.7256\n",
      " Optimizer iteration 1890, batch 326\n",
      "\n",
      " Learning rate 2.334182641175686e-06, Model learning rate 2.334182681806851e-06\n",
      "\n",
      " Optimizer iteration 1891, batch 327\n",
      "\n",
      " Learning rate 2.2570830410268973e-06, Model learning rate 2.2570829969481565e-06\n",
      "328/391 [========================>.....] - ETA: 4s - loss: 0.9309 - acc: 0.7257\n",
      " Optimizer iteration 1892, batch 328\n",
      "\n",
      " Learning rate 2.181275360568169e-06, Model learning rate 2.1812752493133303e-06\n",
      "329/391 [========================>.....] - ETA: 4s - loss: 0.9313 - acc: 0.7257\n",
      " Optimizer iteration 1893, batch 329\n",
      "\n",
      " Learning rate 2.106759796562496e-06, Model learning rate 2.1067598936497234e-06\n",
      "330/391 [========================>.....] - ETA: 4s - loss: 0.9317 - acc: 0.7256\n",
      " Optimizer iteration 1894, batch 330\n",
      "\n",
      " Learning rate 2.0335365424192786e-06, Model learning rate 2.033536475209985e-06\n",
      "331/391 [========================>.....] - ETA: 4s - loss: 0.9319 - acc: 0.7256\n",
      " Optimizer iteration 1895, batch 331\n",
      "\n",
      " Learning rate 1.9616057881935434e-06, Model learning rate 1.9616056761151413e-06\n",
      "332/391 [========================>.....] - ETA: 4s - loss: 0.9319 - acc: 0.7255\n",
      " Optimizer iteration 1896, batch 332\n",
      "\n",
      " Learning rate 1.890967720585668e-06, Model learning rate 1.8909677237388678e-06\n",
      "333/391 [========================>.....] - ETA: 4s - loss: 0.9320 - acc: 0.7256\n",
      " Optimizer iteration 1897, batch 333\n",
      "\n",
      " Learning rate 1.8216225229406026e-06, Model learning rate 1.8216225043943268e-06\n",
      "334/391 [========================>.....] - ETA: 4s - loss: 0.9322 - acc: 0.7257\n",
      " Optimizer iteration 1898, batch 334\n",
      "\n",
      " Learning rate 1.753570375247815e-06, Model learning rate 1.7535703591420315e-06\n",
      "335/391 [========================>.....] - ETA: 4s - loss: 0.9326 - acc: 0.7257\n",
      " Optimizer iteration 1899, batch 335\n",
      "\n",
      " Learning rate 1.6868114541404577e-06, Model learning rate 1.6868114016688196e-06\n",
      "\n",
      " Optimizer iteration 1900, batch 336\n",
      "\n",
      " Learning rate 1.6213459328950354e-06, Model learning rate 1.6213459730352042e-06\n",
      "337/391 [========================>.....] - ETA: 4s - loss: 0.9328 - acc: 0.7255\n",
      " Optimizer iteration 1901, batch 337\n",
      "\n",
      " Learning rate 1.5571739814309594e-06, Model learning rate 1.5571739595543477e-06\n",
      "338/391 [========================>.....] - ETA: 4s - loss: 0.9331 - acc: 0.7253\n",
      " Optimizer iteration 1902, batch 338\n",
      "\n",
      " Learning rate 1.494295766310161e-06, Model learning rate 1.4942958159736008e-06\n",
      "339/391 [=========================>....] - ETA: 4s - loss: 0.9330 - acc: 0.7254\n",
      " Optimizer iteration 1903, batch 339\n",
      "\n",
      " Learning rate 1.4327114507365346e-06, Model learning rate 1.432711428606126e-06\n",
      "340/391 [=========================>....] - ETA: 3s - loss: 0.9333 - acc: 0.7254\n",
      " Optimizer iteration 1904, batch 340\n",
      "\n",
      " Learning rate 1.372421194555773e-06, Model learning rate 1.3724211385124363e-06\n",
      "341/391 [=========================>....] - ETA: 3s - loss: 0.9336 - acc: 0.7253\n",
      " Optimizer iteration 1905, batch 341\n",
      "\n",
      " Learning rate 1.3134251542544773e-06, Model learning rate 1.3134251730662072e-06\n",
      "342/391 [=========================>....] - ETA: 3s - loss: 0.9334 - acc: 0.7254\n",
      " Optimizer iteration 1906, batch 342\n",
      "\n",
      " Learning rate 1.2557234829601582e-06, Model learning rate 1.2557235322674387e-06\n",
      "\n",
      " Optimizer iteration 1907, batch 343\n",
      "\n",
      " Learning rate 1.1993163304409027e-06, Model learning rate 1.1993163298029685e-06\n",
      "344/391 [=========================>....] - ETA: 3s - loss: 0.9334 - acc: 0.7255\n",
      " Optimizer iteration 1908, batch 344\n",
      "\n",
      " Learning rate 1.1442038431044854e-06, Model learning rate 1.1442037930464721e-06\n",
      "345/391 [=========================>....] - ETA: 3s - loss: 0.9335 - acc: 0.7255\n",
      " Optimizer iteration 1909, batch 345\n",
      "\n",
      " Learning rate 1.0903861639985914e-06, Model learning rate 1.090386149371625e-06\n",
      "346/391 [=========================>....] - ETA: 3s - loss: 0.9330 - acc: 0.7257\n",
      " Optimizer iteration 1910, batch 346\n",
      "\n",
      " Learning rate 1.0378634328099267e-06, Model learning rate 1.037863398778427e-06\n",
      "347/391 [=========================>....] - ETA: 3s - loss: 0.9330 - acc: 0.7255\n",
      " Optimizer iteration 1911, batch 347\n",
      "\n",
      " Learning rate 9.866357858642206e-07, Model learning rate 9.866357686405536e-07\n",
      "348/391 [=========================>....] - ETA: 3s - loss: 0.9333 - acc: 0.7253\n",
      " Optimizer iteration 1912, batch 348\n",
      "\n",
      " Learning rate 9.367033561257233e-07, Model learning rate 9.367033726448426e-07\n",
      "349/391 [=========================>....] - ETA: 3s - loss: 0.9335 - acc: 0.7252\n",
      " Optimizer iteration 1913, batch 349\n",
      "\n",
      " Learning rate 8.880662731968747e-07, Model learning rate 8.880662676347129e-07\n",
      "350/391 [=========================>....] - ETA: 3s - loss: 0.9333 - acc: 0.7253\n",
      " Optimizer iteration 1914, batch 350\n",
      "\n",
      " Learning rate 8.4072466331786e-07, Model learning rate 8.407246809838398e-07\n",
      "351/391 [=========================>....] - ETA: 3s - loss: 0.9334 - acc: 0.7252\n",
      " Optimizer iteration 1915, batch 351\n",
      "\n",
      " Learning rate 7.946786493666647e-07, Model learning rate 7.946786695356423e-07\n",
      "352/391 [==========================>...] - ETA: 3s - loss: 0.9334 - acc: 0.7251\n",
      " Optimizer iteration 1916, batch 352\n",
      "\n",
      " Learning rate 7.499283508581311e-07, Model learning rate 7.49928346976958e-07\n",
      "353/391 [==========================>...] - ETA: 2s - loss: 0.9336 - acc: 0.7251\n",
      " Optimizer iteration 1917, batch 353\n",
      "\n",
      " Learning rate 7.064738839442364e-07, Model learning rate 7.064738838380435e-07\n",
      "354/391 [==========================>...] - ETA: 2s - loss: 0.9337 - acc: 0.7249\n",
      " Optimizer iteration 1918, batch 354\n",
      "\n",
      " Learning rate 6.643153614134811e-07, Model learning rate 6.643153369623178e-07\n",
      "355/391 [==========================>...] - ETA: 2s - loss: 0.9339 - acc: 0.7248\n",
      " Optimizer iteration 1919, batch 355\n",
      "\n",
      " Learning rate 6.234528926907234e-07, Model learning rate 6.234528768800374e-07\n",
      "356/391 [==========================>...] - ETA: 2s - loss: 0.9338 - acc: 0.7249\n",
      " Optimizer iteration 1920, batch 356\n",
      "\n",
      " Learning rate 5.838865838366791e-07, Model learning rate 5.838865604346211e-07\n",
      "357/391 [==========================>...] - ETA: 2s - loss: 0.9340 - acc: 0.7248\n",
      " Optimizer iteration 1921, batch 357\n",
      "\n",
      " Learning rate 5.456165375480882e-07, Model learning rate 5.456165581563255e-07\n",
      "358/391 [==========================>...] - ETA: 2s - loss: 0.9341 - acc: 0.7247\n",
      " Optimizer iteration 1922, batch 358\n",
      "\n",
      " Learning rate 5.08642853156882e-07, Model learning rate 5.086428700451506e-07\n",
      "\n",
      " Optimizer iteration 1923, batch 359\n",
      "\n",
      " Learning rate 4.729656266304061e-07, Model learning rate 4.7296563820964366e-07\n",
      "360/391 [==========================>...] - ETA: 2s - loss: 0.9340 - acc: 0.7248\n",
      " Optimizer iteration 1924, batch 360\n",
      "\n",
      " Learning rate 4.3858495057080837e-07, Model learning rate 4.3858494791493285e-07\n",
      "\n",
      " Optimizer iteration 1925, batch 361\n",
      "\n",
      " Learning rate 4.055009142152066e-07, Model learning rate 4.0550091284785594e-07\n",
      "362/391 [==========================>...] - ETA: 2s - loss: 0.9339 - acc: 0.7248\n",
      " Optimizer iteration 1926, batch 362\n",
      "\n",
      " Learning rate 3.737136034349109e-07, Model learning rate 3.737135898518318e-07\n",
      "363/391 [==========================>...] - ETA: 2s - loss: 0.9345 - acc: 0.7246\n",
      " Optimizer iteration 1927, batch 363\n",
      "\n",
      " Learning rate 3.432231007358122e-07, Model learning rate 3.432230926136981e-07\n",
      "\n",
      " Optimizer iteration 1928, batch 364\n",
      "\n",
      " Learning rate 3.1402948525766086e-07, Model learning rate 3.1402947797687375e-07\n",
      "365/391 [===========================>..] - ETA: 2s - loss: 0.9351 - acc: 0.7244\n",
      " Optimizer iteration 1929, batch 365\n",
      "\n",
      " Learning rate 2.861328327741219e-07, Model learning rate 2.86132831206487e-07\n",
      "366/391 [===========================>..] - ETA: 1s - loss: 0.9352 - acc: 0.7244\n",
      " Optimizer iteration 1930, batch 366\n",
      "\n",
      " Learning rate 2.595332156925534e-07, Model learning rate 2.595332091459568e-07\n",
      "367/391 [===========================>..] - ETA: 1s - loss: 0.9351 - acc: 0.7245\n",
      " Optimizer iteration 1931, batch 367\n",
      "\n",
      " Learning rate 2.3423070305367278e-07, Model learning rate 2.3423069706041133e-07\n",
      "\n",
      " Optimizer iteration 1932, batch 368\n",
      "\n",
      " Learning rate 2.1022536053166842e-07, Model learning rate 2.1022536600412423e-07\n",
      "369/391 [===========================>..] - ETA: 1s - loss: 0.9350 - acc: 0.7244\n",
      " Optimizer iteration 1933, batch 369\n",
      "\n",
      " Learning rate 1.8751725043375523e-07, Model learning rate 1.875172443988049e-07\n",
      "370/391 [===========================>..] - ETA: 1s - loss: 0.9352 - acc: 0.7245\n",
      " Optimizer iteration 1934, batch 370\n",
      "\n",
      " Learning rate 1.6610643170000827e-07, Model learning rate 1.6610643172043638e-07\n",
      "371/391 [===========================>..] - ETA: 1s - loss: 0.9351 - acc: 0.7244\n",
      " Optimizer iteration 1935, batch 371\n",
      "\n",
      " Learning rate 1.4599295990352924e-07, Model learning rate 1.4599295639072807e-07\n",
      "372/391 [===========================>..] - ETA: 1s - loss: 0.9351 - acc: 0.7245\n",
      " Optimizer iteration 1936, batch 372\n",
      "\n",
      " Learning rate 1.271768872498913e-07, Model learning rate 1.2717688946395356e-07\n",
      "\n",
      " Optimizer iteration 1937, batch 373\n",
      "\n",
      " Learning rate 1.096582625772502e-07, Model learning rate 1.0965825936182227e-07\n",
      "374/391 [===========================>..] - ETA: 1s - loss: 0.9358 - acc: 0.7242\n",
      " Optimizer iteration 1938, batch 374\n",
      "\n",
      " Learning rate 9.343713135623322e-08, Model learning rate 9.343713003318044e-08\n",
      "\n",
      " Optimizer iteration 1939, batch 375\n",
      "\n",
      " Learning rate 7.851353568971708e-08, Model learning rate 7.851353700516484e-08\n",
      "376/391 [===========================>..] - ETA: 1s - loss: 0.9361 - acc: 0.7240\n",
      " Optimizer iteration 1940, batch 376\n",
      "\n",
      " Learning rate 6.488751431266149e-08, Model learning rate 6.488751580491225e-08\n",
      "377/391 [===========================>..] - ETA: 1s - loss: 0.9359 - acc: 0.7241\n",
      " Optimizer iteration 1941, batch 377\n",
      "\n",
      " Learning rate 5.2559102592164565e-08, Model learning rate 5.2559101959559484e-08\n",
      "378/391 [============================>.] - ETA: 1s - loss: 0.9362 - acc: 0.7240\n",
      " Optimizer iteration 1942, batch 378\n",
      "\n",
      " Learning rate 4.15283325274074e-08, Model learning rate 4.152833099624331e-08\n",
      "\n",
      " Optimizer iteration 1943, batch 379\n",
      "\n",
      " Learning rate 3.179523274932094e-08, Model learning rate 3.179523133667317e-08\n",
      "380/391 [============================>.] - ETA: 0s - loss: 0.9360 - acc: 0.7238\n",
      " Optimizer iteration 1944, batch 380\n",
      "\n",
      " Learning rate 2.3359828520641556e-08, Model learning rate 2.3359827849844805e-08\n",
      "\n",
      " Optimizer iteration 1945, batch 381\n",
      "\n",
      " Learning rate 1.622214173602199e-08, Model learning rate 1.6222141852040295e-08\n",
      "382/391 [============================>.] - ETA: 0s - loss: 0.9360 - acc: 0.7238\n",
      " Optimizer iteration 1946, batch 382\n",
      "\n",
      " Learning rate 1.0382190921753854e-08, Model learning rate 1.0382191106828031e-08\n",
      "383/391 [============================>.] - ETA: 0s - loss: 0.9362 - acc: 0.7237\n",
      " Optimizer iteration 1947, batch 383\n",
      "\n",
      " Learning rate 5.839991235656594e-09, Model learning rate 5.83999115733036e-09\n",
      "384/391 [============================>.] - ETA: 0s - loss: 0.9362 - acc: 0.7238\n",
      " Optimizer iteration 1948, batch 384\n",
      "\n",
      " Learning rate 2.5955544673550437e-09, Model learning rate 2.5955544380451556e-09\n",
      "\n",
      " Optimizer iteration 1949, batch 385\n",
      "\n",
      " Learning rate 6.48889037890843e-10, Model learning rate 6.488890536004988e-10\n",
      "386/391 [============================>.] - ETA: 0s - loss: 0.9359 - acc: 0.7238\n",
      " Optimizer iteration 1950, batch 386\n",
      "\n",
      " Learning rate 0.001, Model learning rate 0.0010000000474974513\n",
      "387/391 [============================>.] - ETA: 0s - loss: 0.9361 - acc: 0.7239\n",
      " Optimizer iteration 1951, batch 387\n",
      "\n",
      " Learning rate 0.0009999993511109622, Model learning rate 0.0009999993490055203\n",
      "\n",
      " Optimizer iteration 1952, batch 388\n",
      "\n",
      " Learning rate 0.0009999974044455327, Model learning rate 0.0009999973699450493\n",
      "389/391 [============================>.] - ETA: 0s - loss: 0.9359 - acc: 0.7239\n",
      " Optimizer iteration 1953, batch 389\n",
      "\n",
      " Learning rate 0.0009999941600087644, Model learning rate 0.0009999941103160381\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.9359 - acc: 0.7239\n",
      " Optimizer iteration 1954, batch 390\n",
      "\n",
      " Learning rate 0.0009999896178090784, Model learning rate 0.0009999895701184869\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.9365 - acc: 0.7236 - val_loss: 1.2258 - val_acc: 0.6313\n",
      "\n",
      "Epoch 00005: saving model to /home/ubuntu/Projects/hybrid-ensemble/model/run_200/cifar10_ResNet20v1_model-0005.h5\n",
      "Epoch 6/10\n",
      "\n",
      " Optimizer iteration 1955, batch 0\n",
      "\n",
      " Learning rate 0.000999983777858264, Model learning rate 0.0009999837493523955\n",
      "  1/391 [..............................] - ETA: 17s - loss: 0.8673 - acc: 0.7500\n",
      " Optimizer iteration 1956, batch 1\n",
      "\n",
      " Learning rate 0.0009999766401714793, Model learning rate 0.000999976648017764\n",
      "\n",
      " Optimizer iteration 1957, batch 2\n",
      "\n",
      " Learning rate 0.0009999682047672506, Model learning rate 0.0009999681496992707\n",
      "  3/391 [..............................] - ETA: 17s - loss: 0.9885 - acc: 0.7057\n",
      " Optimizer iteration 1958, batch 3\n",
      "\n",
      " Learning rate 0.0009999584716674725, Model learning rate 0.000999958487227559\n",
      "  4/391 [..............................] - ETA: 18s - loss: 0.9858 - acc: 0.6875\n",
      " Optimizer iteration 1959, batch 4\n",
      "\n",
      " Learning rate 0.000999947440897408, Model learning rate 0.0009999474277719855\n",
      "  5/391 [..............................] - ETA: 18s - loss: 0.9767 - acc: 0.6906\n",
      " Optimizer iteration 1960, batch 5\n",
      "\n",
      " Learning rate 0.0009999351124856874, Model learning rate 0.0009999350877478719\n",
      "  6/391 [..............................] - ETA: 18s - loss: 0.9903 - acc: 0.6836\n",
      " Optimizer iteration 1961, batch 6\n",
      "\n",
      " Learning rate 0.0009999214864643102, Model learning rate 0.0009999214671552181\n",
      "  7/391 [..............................] - ETA: 18s - loss: 0.9887 - acc: 0.6830\n",
      " Optimizer iteration 1962, batch 7\n",
      "\n",
      " Learning rate 0.0009999065628686437, Model learning rate 0.0009999065659940243\n",
      "\n",
      " Optimizer iteration 1963, batch 8\n",
      "\n",
      " Learning rate 0.0009998903417374227, Model learning rate 0.0009998903842642903\n",
      "  9/391 [..............................] - ETA: 18s - loss: 0.9974 - acc: 0.6892\n",
      " Optimizer iteration 1964, batch 9\n",
      "\n",
      " Learning rate 0.00099987282311275, Model learning rate 0.0009998728055506945\n",
      "\n",
      " Optimizer iteration 1965, batch 10\n",
      "\n",
      " Learning rate 0.0009998540070400965, Model learning rate 0.0009998540626838803\n",
      " 11/391 [..............................] - ETA: 18s - loss: 1.0065 - acc: 0.6861\n",
      " Optimizer iteration 1966, batch 11\n",
      "\n",
      " Learning rate 0.0009998338935683, Model learning rate 0.0009998339228332043\n",
      " 12/391 [..............................] - ETA: 18s - loss: 1.0166 - acc: 0.6849\n",
      " Optimizer iteration 1967, batch 12\n",
      "\n",
      " Learning rate 0.0009998124827495663, Model learning rate 0.0009998125024139881\n",
      " 13/391 [..............................] - ETA: 18s - loss: 1.0120 - acc: 0.6851\n",
      " Optimizer iteration 1968, batch 13\n",
      "\n",
      " Learning rate 0.0009997897746394685, Model learning rate 0.0009997898014262319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14/391 [>.............................] - ETA: 18s - loss: 1.0150 - acc: 0.6869\n",
      " Optimizer iteration 1969, batch 14\n",
      "\n",
      " Learning rate 0.0009997657692969464, Model learning rate 0.0009997658198699355\n",
      " 15/391 [>.............................] - ETA: 18s - loss: 1.0116 - acc: 0.6880\n",
      " Optimizer iteration 1970, batch 15\n",
      "\n",
      " Learning rate 0.0009997404667843074, Model learning rate 0.0009997404413297772\n",
      "\n",
      " Optimizer iteration 1971, batch 16\n",
      "\n",
      " Learning rate 0.000999713867167226, Model learning rate 0.0009997138986364007\n",
      " 17/391 [>.............................] - ETA: 18s - loss: 1.0302 - acc: 0.6861\n",
      " Optimizer iteration 1972, batch 17\n",
      "\n",
      " Learning rate 0.0009996859705147423, Model learning rate 0.0009996859589591622\n",
      " 18/391 [>.............................] - ETA: 18s - loss: 1.0378 - acc: 0.6832\n",
      " Optimizer iteration 1973, batch 18\n",
      "\n",
      " Learning rate 0.0009996567768992641, Model learning rate 0.0009996567387133837\n",
      " 19/391 [>.............................] - ETA: 18s - loss: 1.0397 - acc: 0.6817\n",
      " Optimizer iteration 1974, batch 19\n",
      "\n",
      " Learning rate 0.000999626286396565, Model learning rate 0.000999626237899065\n",
      " 20/391 [>.............................] - ETA: 19s - loss: 1.0540 - acc: 0.6781\n",
      " Optimizer iteration 1975, batch 20\n",
      "\n",
      " Learning rate 0.0009995944990857848, Model learning rate 0.0009995944565162063\n",
      " 21/391 [>.............................] - ETA: 19s - loss: 1.0531 - acc: 0.6789\n",
      " Optimizer iteration 1976, batch 21\n",
      "\n",
      " Learning rate 0.0009995614150494292, Model learning rate 0.0009995613945648074\n",
      " 22/391 [>.............................] - ETA: 19s - loss: 1.0531 - acc: 0.6790\n",
      " Optimizer iteration 1977, batch 22\n",
      "\n",
      " Learning rate 0.0009995270343733697, Model learning rate 0.0009995270520448685\n",
      " 23/391 [>.............................] - ETA: 20s - loss: 1.0477 - acc: 0.6797\n",
      " Optimizer iteration 1978, batch 23\n",
      "\n",
      " Learning rate 0.000999491357146843, Model learning rate 0.0009994913125410676\n",
      " 24/391 [>.............................] - ETA: 20s - loss: 1.0541 - acc: 0.6784\n",
      " Optimizer iteration 1979, batch 24\n",
      "\n",
      " Learning rate 0.0009994543834624518, Model learning rate 0.0009994544088840485\n",
      " 25/391 [>.............................] - ETA: 21s - loss: 1.0566 - acc: 0.6763\n",
      " Optimizer iteration 1980, batch 25\n",
      "\n",
      " Learning rate 0.0009994161134161633, Model learning rate 0.0009994161082431674\n",
      " 26/391 [>.............................] - ETA: 21s - loss: 1.0644 - acc: 0.6752\n",
      " Optimizer iteration 1981, batch 26\n",
      "\n",
      " Learning rate 0.0009993765471073093, Model learning rate 0.0009993765270337462\n",
      " 27/391 [=>............................] - ETA: 21s - loss: 1.0672 - acc: 0.6748\n",
      " Optimizer iteration 1982, batch 27\n",
      "\n",
      " Learning rate 0.0009993356846385866, Model learning rate 0.000999335665255785\n",
      " 28/391 [=>............................] - ETA: 21s - loss: 1.0630 - acc: 0.6747\n",
      " Optimizer iteration 1983, batch 28\n",
      "\n",
      " Learning rate 0.000999293526116056, Model learning rate 0.0009992935229092836\n",
      " 29/391 [=>............................] - ETA: 22s - loss: 1.0651 - acc: 0.6721\n",
      " Optimizer iteration 1984, batch 29\n",
      "\n",
      " Learning rate 0.000999250071649142, Model learning rate 0.0009992500999942422\n",
      "\n",
      " Optimizer iteration 1985, batch 30\n",
      "\n",
      " Learning rate 0.0009992053213506334, Model learning rate 0.0009992052800953388\n",
      " 31/391 [=>............................] - ETA: 22s - loss: 1.0723 - acc: 0.6691\n",
      " Optimizer iteration 1986, batch 31\n",
      "\n",
      " Learning rate 0.000999159275336682, Model learning rate 0.0009991592960432172\n",
      " 32/391 [=>............................] - ETA: 22s - loss: 1.0755 - acc: 0.6677\n",
      " Optimizer iteration 1987, batch 32\n",
      "\n",
      " Learning rate 0.0009991119337268031, Model learning rate 0.0009991119150072336\n",
      " 33/391 [=>............................] - ETA: 22s - loss: 1.0714 - acc: 0.6695\n",
      " Optimizer iteration 1988, batch 33\n",
      "\n",
      " Learning rate 0.0009990632966438743, Model learning rate 0.00099906325340271\n",
      " 34/391 [=>............................] - ETA: 22s - loss: 1.0730 - acc: 0.6689\n",
      " Optimizer iteration 1989, batch 34\n",
      "\n",
      " Learning rate 0.0009990133642141358, Model learning rate 0.0009990133112296462\n",
      " 35/391 [=>............................] - ETA: 22s - loss: 1.0737 - acc: 0.6670\n",
      " Optimizer iteration 1990, batch 35\n",
      "\n",
      " Learning rate 0.0009989621365671902, Model learning rate 0.0009989620884880424\n",
      "\n",
      " Optimizer iteration 1991, batch 36\n",
      "\n",
      " Learning rate 0.0009989096138360014, Model learning rate 0.0009989095851778984\n",
      " 37/391 [=>............................] - ETA: 22s - loss: 1.0762 - acc: 0.6668\n",
      " Optimizer iteration 1992, batch 37\n",
      "\n",
      " Learning rate 0.0009988557961568955, Model learning rate 0.0009988558012992144\n",
      "\n",
      " Optimizer iteration 1993, batch 38\n",
      "\n",
      " Learning rate 0.000998800683669559, Model learning rate 0.0009988007368519902\n",
      " 39/391 [=>............................] - ETA: 22s - loss: 1.0762 - acc: 0.6671\n",
      " Optimizer iteration 1994, batch 39\n",
      "\n",
      " Learning rate 0.0009987442765170397, Model learning rate 0.0009987442754209042\n",
      " 40/391 [==>...........................] - ETA: 22s - loss: 1.0760 - acc: 0.6682\n",
      " Optimizer iteration 1995, batch 40\n",
      "\n",
      " Learning rate 0.0009986865748457456, Model learning rate 0.000998686533421278\n",
      " 41/391 [==>...........................] - ETA: 22s - loss: 1.0761 - acc: 0.6681\n",
      " Optimizer iteration 1996, batch 41\n",
      "\n",
      " Learning rate 0.000998627578805444, Model learning rate 0.0009986276272684336\n",
      " 42/391 [==>...........................] - ETA: 22s - loss: 1.0752 - acc: 0.6683\n",
      " Optimizer iteration 1997, batch 42\n",
      "\n",
      " Learning rate 0.0009985672885492634, Model learning rate 0.0009985673241317272\n",
      " 43/391 [==>...........................] - ETA: 23s - loss: 1.0724 - acc: 0.6693\n",
      " Optimizer iteration 1998, batch 43\n",
      "\n",
      " Learning rate 0.0009985057042336898, Model learning rate 0.0009985057404264808\n",
      " 44/391 [==>...........................] - ETA: 22s - loss: 1.0703 - acc: 0.6706\n",
      " Optimizer iteration 1999, batch 44\n",
      "\n",
      " Learning rate 0.000998442826018569, Model learning rate 0.0009984428761526942\n",
      " 45/391 [==>...........................] - ETA: 23s - loss: 1.0728 - acc: 0.6712\n",
      " Optimizer iteration 2000, batch 45\n",
      "\n",
      " Learning rate 0.000998378654067105, Model learning rate 0.0009983786148950458\n",
      " 46/391 [==>...........................] - ETA: 22s - loss: 1.0732 - acc: 0.6712\n",
      " Optimizer iteration 2001, batch 46\n",
      "\n",
      " Learning rate 0.0009983131885458597, Model learning rate 0.000998313189484179\n",
      " 47/391 [==>...........................] - ETA: 23s - loss: 1.0717 - acc: 0.6720\n",
      " Optimizer iteration 2002, batch 47\n",
      "\n",
      " Learning rate 0.0009982464296247522, Model learning rate 0.0009982464835047722\n",
      "\n",
      " Optimizer iteration 2003, batch 48\n",
      "\n",
      " Learning rate 0.0009981783774770593, Model learning rate 0.0009981783805415034\n",
      " 49/391 [==>...........................] - ETA: 23s - loss: 1.0729 - acc: 0.6717\n",
      " Optimizer iteration 2004, batch 49\n",
      "\n",
      " Learning rate 0.0009981090322794144, Model learning rate 0.0009981089970096946\n",
      "\n",
      " Optimizer iteration 2005, batch 50\n",
      "\n",
      " Learning rate 0.0009980383942118066, Model learning rate 0.0009980384493246675\n",
      " 51/391 [==>...........................] - ETA: 22s - loss: 1.0673 - acc: 0.6733\n",
      " Optimizer iteration 2006, batch 51\n",
      "\n",
      " Learning rate 0.0009979664634575808, Model learning rate 0.0009979665046557784\n",
      " 52/391 [==>...........................] - ETA: 22s - loss: 1.0642 - acc: 0.6746\n",
      " Optimizer iteration 2007, batch 52\n",
      "\n",
      " Learning rate 0.0009978932402034376, Model learning rate 0.0009978932794183493\n",
      " 53/391 [===>..........................] - ETA: 23s - loss: 1.0663 - acc: 0.6741\n",
      " Optimizer iteration 2008, batch 53\n",
      "\n",
      " Learning rate 0.000997818724639432, Model learning rate 0.00099781877361238\n",
      " 54/391 [===>..........................] - ETA: 22s - loss: 1.0653 - acc: 0.6740\n",
      " Optimizer iteration 2009, batch 54\n",
      "\n",
      " Learning rate 0.0009977429169589732, Model learning rate 0.0009977428708225489\n",
      " 55/391 [===>..........................] - ETA: 22s - loss: 1.0661 - acc: 0.6733\n",
      " Optimizer iteration 2010, batch 55\n",
      "\n",
      " Learning rate 0.0009976658173588243, Model learning rate 0.0009976658038794994\n",
      " 56/391 [===>..........................] - ETA: 22s - loss: 1.0675 - acc: 0.6727\n",
      " Optimizer iteration 2011, batch 56\n",
      "\n",
      " Learning rate 0.0009975874260391019, Model learning rate 0.00099758745636791\n",
      " 57/391 [===>..........................] - ETA: 23s - loss: 1.0654 - acc: 0.6737\n",
      " Optimizer iteration 2012, batch 57\n",
      "\n",
      " Learning rate 0.0009975077432032749, Model learning rate 0.0009975077118724585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 58/391 [===>..........................] - ETA: 22s - loss: 1.0662 - acc: 0.6732\n",
      " Optimizer iteration 2013, batch 58\n",
      "\n",
      " Learning rate 0.0009974267690581644, Model learning rate 0.0009974268032237887\n",
      " 59/391 [===>..........................] - ETA: 22s - loss: 1.0659 - acc: 0.6737\n",
      " Optimizer iteration 2014, batch 59\n",
      "\n",
      " Learning rate 0.0009973445038139437, Model learning rate 0.000997344497591257\n",
      " 60/391 [===>..........................] - ETA: 22s - loss: 1.0674 - acc: 0.6724\n",
      " Optimizer iteration 2015, batch 60\n",
      "\n",
      " Learning rate 0.0009972609476841367, Model learning rate 0.0009972609113901854\n",
      " 61/391 [===>..........................] - ETA: 22s - loss: 1.0655 - acc: 0.6734\n",
      " Optimizer iteration 2016, batch 61\n",
      "\n",
      " Learning rate 0.000997176100885618, Model learning rate 0.0009971760446205735\n",
      "\n",
      " Optimizer iteration 2017, batch 62\n",
      "\n",
      " Learning rate 0.000997089963638612, Model learning rate 0.0009970900136977434\n",
      " 63/391 [===>..........................] - ETA: 22s - loss: 1.0653 - acc: 0.6731\n",
      " Optimizer iteration 2018, batch 63\n",
      "\n",
      " Learning rate 0.0009970025361666932, Model learning rate 0.0009970025857910514\n",
      " 64/391 [===>..........................] - ETA: 22s - loss: 1.0628 - acc: 0.6740\n",
      " Optimizer iteration 2019, batch 64\n",
      "\n",
      " Learning rate 0.0009969138186967843, Model learning rate 0.0009969137609004974\n",
      " 65/391 [===>..........................] - ETA: 22s - loss: 1.0614 - acc: 0.6736\n",
      " Optimizer iteration 2020, batch 65\n",
      "\n",
      " Learning rate 0.0009968238114591566, Model learning rate 0.0009968237718567252\n",
      " 66/391 [====>.........................] - ETA: 22s - loss: 1.0599 - acc: 0.6747\n",
      " Optimizer iteration 2021, batch 66\n",
      "\n",
      " Learning rate 0.0009967325146874287, Model learning rate 0.000996732502244413\n",
      " 67/391 [====>.........................] - ETA: 22s - loss: 1.0600 - acc: 0.6746\n",
      " Optimizer iteration 2022, batch 67\n",
      "\n",
      " Learning rate 0.0009966399286185665, Model learning rate 0.0009966399520635605\n",
      " 68/391 [====>.........................] - ETA: 22s - loss: 1.0611 - acc: 0.6738\n",
      " Optimizer iteration 2023, batch 68\n",
      "\n",
      " Learning rate 0.0009965460534928825, Model learning rate 0.0009965460048988461\n",
      " 69/391 [====>.........................] - ETA: 22s - loss: 1.0616 - acc: 0.6729\n",
      " Optimizer iteration 2024, batch 69\n",
      "\n",
      " Learning rate 0.0009964508895540349, Model learning rate 0.0009964508935809135\n",
      "\n",
      " Optimizer iteration 2025, batch 70\n",
      "\n",
      " Learning rate 0.000996354437049027, Model learning rate 0.000996354385279119\n",
      " 71/391 [====>.........................] - ETA: 22s - loss: 1.0610 - acc: 0.6734\n",
      " Optimizer iteration 2026, batch 71\n",
      "\n",
      " Learning rate 0.0009962566962282066, Model learning rate 0.0009962567128241062\n",
      " 72/391 [====>.........................] - ETA: 22s - loss: 1.0582 - acc: 0.6747\n",
      " Optimizer iteration 2027, batch 72\n",
      "\n",
      " Learning rate 0.0009961576673452655, Model learning rate 0.0009961576433852315\n",
      " 73/391 [====>.........................] - ETA: 22s - loss: 1.0574 - acc: 0.6755\n",
      " Optimizer iteration 2028, batch 73\n",
      "\n",
      " Learning rate 0.000996057350657239, Model learning rate 0.0009960572933778167\n",
      " 74/391 [====>.........................] - ETA: 22s - loss: 1.0590 - acc: 0.6747\n",
      " Optimizer iteration 2029, batch 74\n",
      "\n",
      " Learning rate 0.0009959557464245042, Model learning rate 0.0009959557792171836\n",
      " 75/391 [====>.........................] - ETA: 22s - loss: 1.0582 - acc: 0.6756\n",
      " Optimizer iteration 2030, batch 75\n",
      "\n",
      " Learning rate 0.000995852854910781, Model learning rate 0.0009958528680726886\n",
      " 76/391 [====>.........................] - ETA: 22s - loss: 1.0569 - acc: 0.6764\n",
      " Optimizer iteration 2031, batch 76\n",
      "\n",
      " Learning rate 0.00099574867638313, Model learning rate 0.0009957486763596535\n",
      " 77/391 [====>.........................] - ETA: 22s - loss: 1.0547 - acc: 0.6774\n",
      " Optimizer iteration 2032, batch 77\n",
      "\n",
      " Learning rate 0.0009956432111119522, Model learning rate 0.0009956432040780783\n",
      " 78/391 [====>.........................] - ETA: 22s - loss: 1.0532 - acc: 0.6781\n",
      " Optimizer iteration 2033, batch 78\n",
      "\n",
      " Learning rate 0.000995536459370989, Model learning rate 0.000995536451227963\n",
      "\n",
      " Optimizer iteration 2034, batch 79\n",
      "\n",
      " Learning rate 0.0009954284214373204, Model learning rate 0.0009954284178093076\n",
      " 80/391 [=====>........................] - ETA: 22s - loss: 1.0513 - acc: 0.6789\n",
      " Optimizer iteration 2035, batch 80\n",
      "\n",
      " Learning rate 0.0009953190975913646, Model learning rate 0.000995319103822112\n",
      " 81/391 [=====>........................] - ETA: 22s - loss: 1.0526 - acc: 0.6775\n",
      " Optimizer iteration 2036, batch 81\n",
      "\n",
      " Learning rate 0.0009952084881168783, Model learning rate 0.0009952085092663765\n",
      " 82/391 [=====>........................] - ETA: 21s - loss: 1.0528 - acc: 0.6773\n",
      " Optimizer iteration 2037, batch 82\n",
      "\n",
      " Learning rate 0.0009950965933009544, Model learning rate 0.0009950966341421008\n",
      " 83/391 [=====>........................] - ETA: 21s - loss: 1.0525 - acc: 0.6772\n",
      " Optimizer iteration 2038, batch 83\n",
      "\n",
      " Learning rate 0.000994983413434022, Model learning rate 0.0009949833620339632\n",
      " 84/391 [=====>........................] - ETA: 21s - loss: 1.0541 - acc: 0.6764\n",
      " Optimizer iteration 2039, batch 84\n",
      "\n",
      " Learning rate 0.000994868948809846, Model learning rate 0.0009948689257726073\n",
      " 85/391 [=====>........................] - ETA: 21s - loss: 1.0550 - acc: 0.6762\n",
      " Optimizer iteration 2040, batch 85\n",
      "\n",
      " Learning rate 0.0009947531997255255, Model learning rate 0.0009947532089427114\n",
      " 86/391 [=====>........................] - ETA: 21s - loss: 1.0558 - acc: 0.6763\n",
      " Optimizer iteration 2041, batch 86\n",
      "\n",
      " Learning rate 0.0009946361664814943, Model learning rate 0.0009946362115442753\n",
      " 87/391 [=====>........................] - ETA: 21s - loss: 1.0564 - acc: 0.6758\n",
      " Optimizer iteration 2042, batch 87\n",
      "\n",
      " Learning rate 0.0009945178493815181, Model learning rate 0.0009945178171619773\n",
      " 88/391 [=====>........................] - ETA: 21s - loss: 1.0557 - acc: 0.6765\n",
      " Optimizer iteration 2043, batch 88\n",
      "\n",
      " Learning rate 0.000994398248732696, Model learning rate 0.000994398258626461\n",
      " 89/391 [=====>........................] - ETA: 21s - loss: 1.0554 - acc: 0.6761\n",
      " Optimizer iteration 2044, batch 89\n",
      "\n",
      " Learning rate 0.000994277364845458, Model learning rate 0.0009942774195224047\n",
      " 90/391 [=====>........................] - ETA: 21s - loss: 1.0565 - acc: 0.6759\n",
      " Optimizer iteration 2045, batch 90\n",
      "\n",
      " Learning rate 0.0009941551980335653, Model learning rate 0.0009941551834344864\n",
      "\n",
      " Optimizer iteration 2046, batch 91\n",
      "\n",
      " Learning rate 0.0009940317486141082, Model learning rate 0.0009940317831933498\n",
      " 92/391 [======>.......................] - ETA: 21s - loss: 1.0557 - acc: 0.6760\n",
      " Optimizer iteration 2047, batch 92\n",
      "\n",
      " Learning rate 0.0009939070169075071, Model learning rate 0.0009939069859683514\n",
      " 93/391 [======>.......................] - ETA: 21s - loss: 1.0558 - acc: 0.6761\n",
      " Optimizer iteration 2048, batch 93\n",
      "\n",
      " Learning rate 0.00099378100323751, Model learning rate 0.0009937810245901346\n",
      " 94/391 [======>.......................] - ETA: 21s - loss: 1.0562 - acc: 0.6759\n",
      " Optimizer iteration 2049, batch 94\n",
      "\n",
      " Learning rate 0.0009936537079311926, Model learning rate 0.000993653666228056\n",
      " 95/391 [======>.......................] - ETA: 21s - loss: 1.0559 - acc: 0.6761\n",
      " Optimizer iteration 2050, batch 95\n",
      "\n",
      " Learning rate 0.0009935251313189565, Model learning rate 0.000993525143712759\n",
      " 96/391 [======>.......................] - ETA: 21s - loss: 1.0555 - acc: 0.6766\n",
      " Optimizer iteration 2051, batch 96\n",
      "\n",
      " Learning rate 0.00099339527373453, Model learning rate 0.0009933952242136002\n",
      " 97/391 [======>.......................] - ETA: 21s - loss: 1.0568 - acc: 0.6765\n",
      " Optimizer iteration 2052, batch 97\n",
      "\n",
      " Learning rate 0.0009932641355149653, Model learning rate 0.000993264140561223\n",
      " 98/391 [======>.......................] - ETA: 21s - loss: 1.0566 - acc: 0.6770\n",
      " Optimizer iteration 2053, batch 98\n",
      "\n",
      " Learning rate 0.0009931317170006398, Model learning rate 0.000993131659924984\n",
      " 99/391 [======>.......................] - ETA: 21s - loss: 1.0582 - acc: 0.6763\n",
      " Optimizer iteration 2054, batch 99\n",
      "\n",
      " Learning rate 0.0009929980185352525, Model learning rate 0.0009929980151355267\n",
      "100/391 [======>.......................] - ETA: 20s - loss: 1.0577 - acc: 0.6764\n",
      " Optimizer iteration 2055, batch 100\n",
      "\n",
      " Learning rate 0.0009928630404658254, Model learning rate 0.0009928630897775292\n",
      "101/391 [======>.......................] - ETA: 20s - loss: 1.0587 - acc: 0.6765\n",
      " Optimizer iteration 2056, batch 101\n",
      "\n",
      " Learning rate 0.0009927267831427017, Model learning rate 0.00099272676743567\n",
      "102/391 [======>.......................] - ETA: 20s - loss: 1.0571 - acc: 0.6769\n",
      " Optimizer iteration 2057, batch 102\n",
      "\n",
      " Learning rate 0.0009925892469195452, Model learning rate 0.0009925892809405923\n",
      "103/391 [======>.......................] - ETA: 20s - loss: 1.0567 - acc: 0.6772\n",
      " Optimizer iteration 2058, batch 103\n",
      "\n",
      " Learning rate 0.0009924504321533387, Model learning rate 0.0009924503974616528\n",
      "104/391 [======>.......................] - ETA: 20s - loss: 1.0563 - acc: 0.6777\n",
      " Optimizer iteration 2059, batch 104\n",
      "\n",
      " Learning rate 0.0009923103392043835, Model learning rate 0.000992310349829495\n",
      "105/391 [=======>......................] - ETA: 20s - loss: 1.0556 - acc: 0.6778\n",
      " Optimizer iteration 2060, batch 105\n",
      "\n",
      " Learning rate 0.000992168968436299, Model learning rate 0.000992169021628797\n",
      "106/391 [=======>......................] - ETA: 20s - loss: 1.0577 - acc: 0.6774\n",
      " Optimizer iteration 2061, batch 106\n",
      "\n",
      " Learning rate 0.0009920263202160206, Model learning rate 0.0009920262964442372\n",
      "107/391 [=======>......................] - ETA: 20s - loss: 1.0566 - acc: 0.6777\n",
      " Optimizer iteration 2062, batch 107\n",
      "\n",
      " Learning rate 0.0009918823949138004, Model learning rate 0.0009918824071064591\n",
      "108/391 [=======>......................] - ETA: 20s - loss: 1.0574 - acc: 0.6771\n",
      " Optimizer iteration 2063, batch 108\n",
      "\n",
      " Learning rate 0.000991737192903204, Model learning rate 0.000991737237200141\n",
      "109/391 [=======>......................] - ETA: 20s - loss: 1.0573 - acc: 0.6775\n",
      " Optimizer iteration 2064, batch 109\n",
      "\n",
      " Learning rate 0.0009915907145611115, Model learning rate 0.0009915906703099608\n",
      "110/391 [=======>......................] - ETA: 20s - loss: 1.0553 - acc: 0.6785\n",
      " Optimizer iteration 2065, batch 110\n",
      "\n",
      " Learning rate 0.0009914429602677162, Model learning rate 0.0009914429392665625\n",
      "111/391 [=======>......................] - ETA: 20s - loss: 1.0532 - acc: 0.6795\n",
      " Optimizer iteration 2066, batch 111\n",
      "\n",
      " Learning rate 0.0009912939304065219, Model learning rate 0.000991293927654624\n",
      "112/391 [=======>......................] - ETA: 20s - loss: 1.0545 - acc: 0.6793\n",
      " Optimizer iteration 2067, batch 112\n",
      "\n",
      " Learning rate 0.0009911436253643444, Model learning rate 0.0009911436354741454\n",
      "113/391 [=======>......................] - ETA: 20s - loss: 1.0538 - acc: 0.6796\n",
      " Optimizer iteration 2068, batch 113\n",
      "\n",
      " Learning rate 0.0009909920455313088, Model learning rate 0.0009909920627251267\n",
      "114/391 [=======>......................] - ETA: 20s - loss: 1.0538 - acc: 0.6796\n",
      " Optimizer iteration 2069, batch 114\n",
      "\n",
      " Learning rate 0.000990839191300849, Model learning rate 0.000990839209407568\n",
      "115/391 [=======>......................] - ETA: 20s - loss: 1.0539 - acc: 0.6793\n",
      " Optimizer iteration 2070, batch 115\n",
      "\n",
      " Learning rate 0.0009906850630697068, Model learning rate 0.0009906850755214691\n",
      "116/391 [=======>......................] - ETA: 20s - loss: 1.0537 - acc: 0.6794\n",
      " Optimizer iteration 2071, batch 116\n",
      "\n",
      " Learning rate 0.0009905296612379307, Model learning rate 0.0009905296610668302\n",
      "117/391 [=======>......................] - ETA: 20s - loss: 1.0518 - acc: 0.6800\n",
      " Optimizer iteration 2072, batch 117\n",
      "\n",
      " Learning rate 0.0009903729862088748, Model learning rate 0.000990372966043651\n",
      "118/391 [========>.....................] - ETA: 20s - loss: 1.0514 - acc: 0.6800\n",
      " Optimizer iteration 2073, batch 118\n",
      "\n",
      " Learning rate 0.0009902150383891979, Model learning rate 0.000990214990451932\n",
      "119/391 [========>.....................] - ETA: 20s - loss: 1.0512 - acc: 0.6802\n",
      " Optimizer iteration 2074, batch 119\n",
      "\n",
      " Learning rate 0.0009900558181888627, Model learning rate 0.0009900558507069945\n",
      "120/391 [========>.....................] - ETA: 20s - loss: 1.0519 - acc: 0.6796\n",
      " Optimizer iteration 2075, batch 120\n",
      "\n",
      " Learning rate 0.0009898953260211339, Model learning rate 0.0009898953139781952\n",
      "121/391 [========>.....................] - ETA: 19s - loss: 1.0505 - acc: 0.6803\n",
      " Optimizer iteration 2076, batch 121\n",
      "\n",
      " Learning rate 0.000989733562302578, Model learning rate 0.0009897336130961776\n",
      "122/391 [========>.....................] - ETA: 19s - loss: 1.0506 - acc: 0.6804\n",
      " Optimizer iteration 2077, batch 122\n",
      "\n",
      " Learning rate 0.0009895705274530619, Model learning rate 0.000989570515230298\n",
      "123/391 [========>.....................] - ETA: 19s - loss: 1.0513 - acc: 0.6800\n",
      " Optimizer iteration 2078, batch 123\n",
      "\n",
      " Learning rate 0.0009894062218957515, Model learning rate 0.0009894062532112002\n",
      "124/391 [========>.....................] - ETA: 19s - loss: 1.0510 - acc: 0.6802\n",
      " Optimizer iteration 2079, batch 124\n",
      "\n",
      " Learning rate 0.0009892406460571114, Model learning rate 0.0009892405942082405\n",
      "\n",
      " Optimizer iteration 2080, batch 125\n",
      "\n",
      " Learning rate 0.0009890738003669028, Model learning rate 0.0009890737710520625\n",
      "126/391 [========>.....................] - ETA: 19s - loss: 1.0489 - acc: 0.6806\n",
      " Optimizer iteration 2081, batch 126\n",
      "\n",
      " Learning rate 0.000988905685258183, Model learning rate 0.0009889056673273444\n",
      "\n",
      " Optimizer iteration 2082, batch 127\n",
      "\n",
      " Learning rate 0.0009887363011673045, Model learning rate 0.0009887362830340862\n",
      "128/391 [========>.....................] - ETA: 19s - loss: 1.0491 - acc: 0.6807\n",
      " Optimizer iteration 2083, batch 128\n",
      "\n",
      " Learning rate 0.0009885656485339128, Model learning rate 0.000988565618172288\n",
      "129/391 [========>.....................] - ETA: 19s - loss: 1.0481 - acc: 0.6811\n",
      " Optimizer iteration 2084, batch 129\n",
      "\n",
      " Learning rate 0.0009883937278009466, Model learning rate 0.0009883936727419496\n",
      "130/391 [========>.....................] - ETA: 19s - loss: 1.0458 - acc: 0.6822\n",
      " Optimizer iteration 2085, batch 130\n",
      "\n",
      " Learning rate 0.0009882205394146362, Model learning rate 0.000988220563158393\n",
      "131/391 [=========>....................] - ETA: 19s - loss: 1.0452 - acc: 0.6823\n",
      " Optimizer iteration 2086, batch 131\n",
      "\n",
      " Learning rate 0.000988046083824501, Model learning rate 0.0009880460565909743\n",
      "132/391 [=========>....................] - ETA: 19s - loss: 1.0453 - acc: 0.6818\n",
      " Optimizer iteration 2087, batch 132\n",
      "\n",
      " Learning rate 0.0009878703614833507, Model learning rate 0.0009878703858703375\n",
      "133/391 [=========>....................] - ETA: 19s - loss: 1.0452 - acc: 0.6822\n",
      " Optimizer iteration 2088, batch 133\n",
      "\n",
      " Learning rate 0.0009876933728472826, Model learning rate 0.0009876933181658387\n",
      "134/391 [=========>....................] - ETA: 19s - loss: 1.0441 - acc: 0.6825\n",
      " Optimizer iteration 2089, batch 134\n",
      "\n",
      " Learning rate 0.0009875151183756806, Model learning rate 0.0009875150863081217\n",
      "135/391 [=========>....................] - ETA: 19s - loss: 1.0437 - acc: 0.6822\n",
      " Optimizer iteration 2090, batch 135\n",
      "\n",
      " Learning rate 0.000987335598531214, Model learning rate 0.0009873355738818645\n",
      "136/391 [=========>....................] - ETA: 19s - loss: 1.0434 - acc: 0.6824\n",
      " Optimizer iteration 2091, batch 136\n",
      "\n",
      " Learning rate 0.0009871548137798368, Model learning rate 0.0009871547808870673\n",
      "137/391 [=========>....................] - ETA: 19s - loss: 1.0427 - acc: 0.6826\n",
      " Optimizer iteration 2092, batch 137\n",
      "\n",
      " Learning rate 0.0009869727645907857, Model learning rate 0.00098697270732373\n",
      "138/391 [=========>....................] - ETA: 19s - loss: 1.0426 - acc: 0.6823\n",
      " Optimizer iteration 2093, batch 138\n",
      "\n",
      " Learning rate 0.0009867894514365802, Model learning rate 0.0009867894696071744\n",
      "139/391 [=========>....................] - ETA: 18s - loss: 1.0428 - acc: 0.6825\n",
      " Optimizer iteration 2094, batch 139\n",
      "\n",
      " Learning rate 0.0009866048747930194, Model learning rate 0.0009866048349067569\n",
      "140/391 [=========>....................] - ETA: 18s - loss: 1.0426 - acc: 0.6825\n",
      " Optimizer iteration 2095, batch 140\n",
      "\n",
      " Learning rate 0.0009864190351391822, Model learning rate 0.000986419036053121\n",
      "141/391 [=========>....................] - ETA: 18s - loss: 1.0433 - acc: 0.6821\n",
      " Optimizer iteration 2096, batch 141\n",
      "\n",
      " Learning rate 0.0009862319329574263, Model learning rate 0.0009862319566309452\n",
      "142/391 [=========>....................] - ETA: 18s - loss: 1.0428 - acc: 0.6822\n",
      " Optimizer iteration 2097, batch 142\n",
      "\n",
      " Learning rate 0.0009860435687333857, Model learning rate 0.0009860435966402292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/391 [=========>....................] - ETA: 18s - loss: 1.0425 - acc: 0.6820\n",
      " Optimizer iteration 2098, batch 143\n",
      "\n",
      " Learning rate 0.0009858539429559703, Model learning rate 0.0009858539560809731\n",
      "144/391 [==========>...................] - ETA: 18s - loss: 1.0420 - acc: 0.6822\n",
      " Optimizer iteration 2099, batch 144\n",
      "\n",
      " Learning rate 0.0009856630561173648, Model learning rate 0.000985663034953177\n",
      "145/391 [==========>...................] - ETA: 18s - loss: 1.0415 - acc: 0.6820\n",
      " Optimizer iteration 2100, batch 145\n",
      "\n",
      " Learning rate 0.000985470908713026, Model learning rate 0.0009854709496721625\n",
      "146/391 [==========>...................] - ETA: 18s - loss: 1.0417 - acc: 0.6821\n",
      " Optimizer iteration 2101, batch 146\n",
      "\n",
      " Learning rate 0.000985277501241684, Model learning rate 0.0009852774674072862\n",
      "147/391 [==========>...................] - ETA: 18s - loss: 1.0415 - acc: 0.6823\n",
      " Optimizer iteration 2102, batch 147\n",
      "\n",
      " Learning rate 0.0009850828342053382, Model learning rate 0.0009850828209891915\n",
      "148/391 [==========>...................] - ETA: 18s - loss: 1.0407 - acc: 0.6824\n",
      " Optimizer iteration 2103, batch 148\n",
      "\n",
      " Learning rate 0.000984886908109258, Model learning rate 0.0009848868940025568\n",
      "149/391 [==========>...................] - ETA: 18s - loss: 1.0397 - acc: 0.6830\n",
      " Optimizer iteration 2104, batch 149\n",
      "\n",
      " Learning rate 0.000984689723461981, Model learning rate 0.000984689686447382\n",
      "150/391 [==========>...................] - ETA: 18s - loss: 1.0392 - acc: 0.6830\n",
      " Optimizer iteration 2105, batch 150\n",
      "\n",
      " Learning rate 0.0009844912807753104, Model learning rate 0.0009844913147389889\n",
      "151/391 [==========>...................] - ETA: 18s - loss: 1.0400 - acc: 0.6827\n",
      " Optimizer iteration 2106, batch 151\n",
      "\n",
      " Learning rate 0.0009842915805643156, Model learning rate 0.0009842915460467339\n",
      "152/391 [==========>...................] - ETA: 18s - loss: 1.0404 - acc: 0.6824\n",
      " Optimizer iteration 2107, batch 152\n",
      "\n",
      " Learning rate 0.0009840906233473297, Model learning rate 0.0009840906132012606\n",
      "153/391 [==========>...................] - ETA: 18s - loss: 1.0403 - acc: 0.6823\n",
      " Optimizer iteration 2108, batch 153\n",
      "\n",
      " Learning rate 0.0009838884096459487, Model learning rate 0.0009838883997872472\n",
      "154/391 [==========>...................] - ETA: 18s - loss: 1.0399 - acc: 0.6822\n",
      " Optimizer iteration 2109, batch 154\n",
      "\n",
      " Learning rate 0.0009836849399850291, Model learning rate 0.0009836849058046937\n",
      "155/391 [==========>...................] - ETA: 18s - loss: 1.0387 - acc: 0.6827\n",
      " Optimizer iteration 2110, batch 155\n",
      "\n",
      " Learning rate 0.0009834802148926882, Model learning rate 0.000983480247668922\n",
      "156/391 [==========>...................] - ETA: 17s - loss: 1.0383 - acc: 0.6827\n",
      " Optimizer iteration 2111, batch 156\n",
      "\n",
      " Learning rate 0.0009832742349003014, Model learning rate 0.0009832741925492883\n",
      "157/391 [===========>..................] - ETA: 17s - loss: 1.0371 - acc: 0.6832\n",
      " Optimizer iteration 2112, batch 157\n",
      "\n",
      " Learning rate 0.0009830670005425012, Model learning rate 0.0009830669732764363\n",
      "158/391 [===========>..................] - ETA: 17s - loss: 1.0365 - acc: 0.6831\n",
      " Optimizer iteration 2113, batch 158\n",
      "\n",
      " Learning rate 0.0009828585123571763, Model learning rate 0.0009828584734350443\n",
      "159/391 [===========>..................] - ETA: 17s - loss: 1.0366 - acc: 0.6828\n",
      " Optimizer iteration 2114, batch 159\n",
      "\n",
      " Learning rate 0.0009826487708854692, Model learning rate 0.000982648809440434\n",
      "160/391 [===========>..................] - ETA: 17s - loss: 1.0357 - acc: 0.6832\n",
      " Optimizer iteration 2115, batch 160\n",
      "\n",
      " Learning rate 0.0009824377766717758, Model learning rate 0.0009824377484619617\n",
      "161/391 [===========>..................] - ETA: 17s - loss: 1.0350 - acc: 0.6836\n",
      " Optimizer iteration 2116, batch 161\n",
      "\n",
      " Learning rate 0.0009822255302637435, Model learning rate 0.0009822255233302712\n",
      "162/391 [===========>..................] - ETA: 17s - loss: 1.0346 - acc: 0.6834\n",
      " Optimizer iteration 2117, batch 162\n",
      "\n",
      " Learning rate 0.0009820120322122695, Model learning rate 0.0009820120176300406\n",
      "163/391 [===========>..................] - ETA: 17s - loss: 1.0356 - acc: 0.6831\n",
      " Optimizer iteration 2118, batch 163\n",
      "\n",
      " Learning rate 0.0009817972830715002, Model learning rate 0.00098179723136127\n",
      "164/391 [===========>..................] - ETA: 17s - loss: 1.0360 - acc: 0.6826\n",
      " Optimizer iteration 2119, batch 164\n",
      "\n",
      " Learning rate 0.0009815812833988292, Model learning rate 0.000981581280939281\n",
      "165/391 [===========>..................] - ETA: 17s - loss: 1.0354 - acc: 0.6830\n",
      " Optimizer iteration 2120, batch 165\n",
      "\n",
      " Learning rate 0.0009813640337548953, Model learning rate 0.000981364049948752\n",
      "166/391 [===========>..................] - ETA: 17s - loss: 1.0366 - acc: 0.6827\n",
      " Optimizer iteration 2121, batch 166\n",
      "\n",
      " Learning rate 0.0009811455347035827, Model learning rate 0.0009811455383896828\n",
      "167/391 [===========>..................] - ETA: 17s - loss: 1.0366 - acc: 0.6831\n",
      " Optimizer iteration 2122, batch 167\n",
      "\n",
      " Learning rate 0.0009809257868120176, Model learning rate 0.0009809257462620735\n",
      "168/391 [===========>..................] - ETA: 17s - loss: 1.0366 - acc: 0.6831\n",
      " Optimizer iteration 2123, batch 168\n",
      "\n",
      " Learning rate 0.0009807047906505682, Model learning rate 0.000980704789981246\n",
      "169/391 [===========>..................] - ETA: 17s - loss: 1.0363 - acc: 0.6833\n",
      " Optimizer iteration 2124, batch 169\n",
      "\n",
      " Learning rate 0.0009804825467928423, Model learning rate 0.0009804825531318784\n",
      "170/391 [============>.................] - ETA: 17s - loss: 1.0356 - acc: 0.6835\n",
      " Optimizer iteration 2125, batch 170\n",
      "\n",
      " Learning rate 0.000980259055815686, Model learning rate 0.0009802590357139707\n",
      "171/391 [============>.................] - ETA: 17s - loss: 1.0361 - acc: 0.6830\n",
      " Optimizer iteration 2126, batch 171\n",
      "\n",
      " Learning rate 0.0009800343182991835, Model learning rate 0.0009800343541428447\n",
      "172/391 [============>.................] - ETA: 16s - loss: 1.0371 - acc: 0.6829\n",
      " Optimizer iteration 2127, batch 172\n",
      "\n",
      " Learning rate 0.000979808334826653, Model learning rate 0.0009798083920031786\n",
      "173/391 [============>.................] - ETA: 16s - loss: 1.0358 - acc: 0.6834\n",
      " Optimizer iteration 2128, batch 173\n",
      "\n",
      " Learning rate 0.0009795811059846475, Model learning rate 0.0009795811492949724\n",
      "174/391 [============>.................] - ETA: 16s - loss: 1.0363 - acc: 0.6832\n",
      " Optimizer iteration 2129, batch 174\n",
      "\n",
      " Learning rate 0.000979352632362952, Model learning rate 0.0009793526260182261\n",
      "175/391 [============>.................] - ETA: 16s - loss: 1.0369 - acc: 0.6830\n",
      " Optimizer iteration 2130, batch 175\n",
      "\n",
      " Learning rate 0.0009791229145545831, Model learning rate 0.0009791229385882616\n",
      "176/391 [============>.................] - ETA: 16s - loss: 1.0367 - acc: 0.6830\n",
      " Optimizer iteration 2131, batch 176\n",
      "\n",
      " Learning rate 0.0009788919531557858, Model learning rate 0.000978891970589757\n",
      "177/391 [============>.................] - ETA: 16s - loss: 1.0366 - acc: 0.6828\n",
      " Optimizer iteration 2132, batch 177\n",
      "\n",
      " Learning rate 0.0009786597487660335, Model learning rate 0.0009786597220227122\n",
      "178/391 [============>.................] - ETA: 16s - loss: 1.0365 - acc: 0.6826\n",
      " Optimizer iteration 2133, batch 178\n",
      "\n",
      " Learning rate 0.0009784263019880259, Model learning rate 0.0009784263093024492\n",
      "179/391 [============>.................] - ETA: 16s - loss: 1.0366 - acc: 0.6827\n",
      " Optimizer iteration 2134, batch 179\n",
      "\n",
      " Learning rate 0.0009781916134276871, Model learning rate 0.0009781916160136461\n",
      "180/391 [============>.................] - ETA: 16s - loss: 1.0367 - acc: 0.6826\n",
      " Optimizer iteration 2135, batch 180\n",
      "\n",
      " Learning rate 0.0009779556836941646, Model learning rate 0.000977955642156303\n",
      "181/391 [============>.................] - ETA: 16s - loss: 1.0377 - acc: 0.6823\n",
      " Optimizer iteration 2136, batch 181\n",
      "\n",
      " Learning rate 0.0009777185133998268, Model learning rate 0.0009777185041457415\n",
      "182/391 [============>.................] - ETA: 16s - loss: 1.0376 - acc: 0.6825\n",
      " Optimizer iteration 2137, batch 182\n",
      "\n",
      " Learning rate 0.0009774801031602629, Model learning rate 0.00097748008556664\n",
      "183/391 [=============>................] - ETA: 16s - loss: 1.0374 - acc: 0.6826\n",
      " Optimizer iteration 2138, batch 183\n",
      "\n",
      " Learning rate 0.00097724045359428, Model learning rate 0.00097724050283432\n",
      "184/391 [=============>................] - ETA: 16s - loss: 1.0375 - acc: 0.6829\n",
      " Optimizer iteration 2139, batch 184\n",
      "\n",
      " Learning rate 0.0009769995653239022, Model learning rate 0.0009769995231181383\n",
      "185/391 [=============>................] - ETA: 16s - loss: 1.0374 - acc: 0.6830\n",
      " Optimizer iteration 2140, batch 185\n",
      "\n",
      " Learning rate 0.0009767574389743681, Model learning rate 0.0009767574956640601\n",
      "186/391 [=============>................] - ETA: 16s - loss: 1.0365 - acc: 0.6832\n",
      " Optimizer iteration 2141, batch 186\n",
      "\n",
      " Learning rate 0.0009765140751741306, Model learning rate 0.00097651407122612\n",
      "187/391 [=============>................] - ETA: 15s - loss: 1.0361 - acc: 0.6834\n",
      " Optimizer iteration 2142, batch 187\n",
      "\n",
      " Learning rate 0.000976269474554854, Model learning rate 0.0009762694826349616\n",
      "\n",
      " Optimizer iteration 2143, batch 188\n",
      "\n",
      " Learning rate 0.0009760236377514128, Model learning rate 0.0009760236134752631\n",
      "189/391 [=============>................] - ETA: 15s - loss: 1.0377 - acc: 0.6830\n",
      " Optimizer iteration 2144, batch 189\n",
      "\n",
      " Learning rate 0.0009757765654018904, Model learning rate 0.0009757765801623464\n",
      "190/391 [=============>................] - ETA: 15s - loss: 1.0376 - acc: 0.6830\n",
      " Optimizer iteration 2145, batch 190\n",
      "\n",
      " Learning rate 0.0009755282581475768, Model learning rate 0.0009755282662808895\n",
      "191/391 [=============>................] - ETA: 15s - loss: 1.0378 - acc: 0.6829\n",
      " Optimizer iteration 2146, batch 191\n",
      "\n",
      " Learning rate 0.0009752787166329675, Model learning rate 0.0009752787300385535\n",
      "192/391 [=============>................] - ETA: 15s - loss: 1.0392 - acc: 0.6827\n",
      " Optimizer iteration 2147, batch 192\n",
      "\n",
      " Learning rate 0.0009750279415057616, Model learning rate 0.0009750279132276773\n",
      "193/391 [=============>................] - ETA: 15s - loss: 1.0383 - acc: 0.6830\n",
      " Optimizer iteration 2148, batch 193\n",
      "\n",
      " Learning rate 0.0009747759334168601, Model learning rate 0.0009747759322635829\n",
      "194/391 [=============>................] - ETA: 15s - loss: 1.0379 - acc: 0.6829\n",
      " Optimizer iteration 2149, batch 194\n",
      "\n",
      " Learning rate 0.0009745226930203639, Model learning rate 0.0009745226707309484\n",
      "195/391 [=============>................] - ETA: 15s - loss: 1.0368 - acc: 0.6834\n",
      " Optimizer iteration 2150, batch 195\n",
      "\n",
      " Learning rate 0.0009742682209735727, Model learning rate 0.0009742682450450957\n",
      "\n",
      " Optimizer iteration 2151, batch 196\n",
      "\n",
      " Learning rate 0.0009740125179369832, Model learning rate 0.0009740125387907028\n",
      "197/391 [==============>...............] - ETA: 15s - loss: 1.0365 - acc: 0.6835\n",
      " Optimizer iteration 2152, batch 197\n",
      "\n",
      " Learning rate 0.0009737555845742868, Model learning rate 0.0009737556101754308\n",
      "198/391 [==============>...............] - ETA: 15s - loss: 1.0365 - acc: 0.6836\n",
      " Optimizer iteration 2153, batch 198\n",
      "\n",
      " Learning rate 0.0009734974215523685, Model learning rate 0.0009734974009916186\n",
      "\n",
      " Optimizer iteration 2154, batch 199\n",
      "\n",
      " Learning rate 0.0009732380295413049, Model learning rate 0.0009732380276545882\n",
      "200/391 [==============>...............] - ETA: 14s - loss: 1.0351 - acc: 0.6841\n",
      " Optimizer iteration 2155, batch 200\n",
      "\n",
      " Learning rate 0.0009729774092143626, Model learning rate 0.0009729774319566786\n",
      "\n",
      " Optimizer iteration 2156, batch 201\n",
      "\n",
      " Learning rate 0.0009727155612479963, Model learning rate 0.0009727155556902289\n",
      "202/391 [==============>...............] - ETA: 14s - loss: 1.0347 - acc: 0.6843\n",
      " Optimizer iteration 2157, batch 202\n",
      "\n",
      " Learning rate 0.000972452486321847, Model learning rate 0.000972452515270561\n",
      "203/391 [==============>...............] - ETA: 14s - loss: 1.0340 - acc: 0.6844\n",
      " Optimizer iteration 2158, batch 203\n",
      "\n",
      " Learning rate 0.0009721881851187406, Model learning rate 0.0009721881942823529\n",
      "204/391 [==============>...............] - ETA: 14s - loss: 1.0340 - acc: 0.6844\n",
      " Optimizer iteration 2159, batch 204\n",
      "\n",
      " Learning rate 0.0009719226583246855, Model learning rate 0.0009719226509332657\n",
      "205/391 [==============>...............] - ETA: 14s - loss: 1.0343 - acc: 0.6842\n",
      " Optimizer iteration 2160, batch 205\n",
      "\n",
      " Learning rate 0.0009716559066288715, Model learning rate 0.0009716558852232993\n",
      "206/391 [==============>...............] - ETA: 14s - loss: 1.0336 - acc: 0.6845\n",
      " Optimizer iteration 2161, batch 206\n",
      "\n",
      " Learning rate 0.0009713879307236677, Model learning rate 0.0009713879553601146\n",
      "207/391 [==============>...............] - ETA: 14s - loss: 1.0334 - acc: 0.6846\n",
      " Optimizer iteration 2162, batch 207\n",
      "\n",
      " Learning rate 0.0009711187313046206, Model learning rate 0.0009711187449283898\n",
      "\n",
      " Optimizer iteration 2163, batch 208\n",
      "\n",
      " Learning rate 0.0009708483090704523, Model learning rate 0.0009708483121357858\n",
      "209/391 [===============>..............] - ETA: 14s - loss: 1.0333 - acc: 0.6845\n",
      " Optimizer iteration 2164, batch 209\n",
      "\n",
      " Learning rate 0.0009705766647230589, Model learning rate 0.0009705766569823027\n",
      "210/391 [===============>..............] - ETA: 14s - loss: 1.0335 - acc: 0.6846\n",
      " Optimizer iteration 2165, batch 210\n",
      "\n",
      " Learning rate 0.0009703037989675086, Model learning rate 0.0009703037794679403\n",
      "211/391 [===============>..............] - ETA: 14s - loss: 1.0342 - acc: 0.6844\n",
      " Optimizer iteration 2166, batch 211\n",
      "\n",
      " Learning rate 0.0009700297125120399, Model learning rate 0.0009700297378003597\n",
      "212/391 [===============>..............] - ETA: 14s - loss: 1.0343 - acc: 0.6843\n",
      " Optimizer iteration 2167, batch 212\n",
      "\n",
      " Learning rate 0.0009697544060680594, Model learning rate 0.000969754415564239\n",
      "213/391 [===============>..............] - ETA: 13s - loss: 1.0340 - acc: 0.6844\n",
      " Optimizer iteration 2168, batch 213\n",
      "\n",
      " Learning rate 0.0009694778803501403, Model learning rate 0.0009694778709672391\n",
      "214/391 [===============>..............] - ETA: 13s - loss: 1.0346 - acc: 0.6841\n",
      " Optimizer iteration 2169, batch 214\n",
      "\n",
      " Learning rate 0.0009692001360760211, Model learning rate 0.000969200162217021\n",
      "215/391 [===============>..............] - ETA: 13s - loss: 1.0350 - acc: 0.6840\n",
      " Optimizer iteration 2170, batch 215\n",
      "\n",
      " Learning rate 0.0009689211739666023, Model learning rate 0.0009689211728982627\n",
      "\n",
      " Optimizer iteration 2171, batch 216\n",
      "\n",
      " Learning rate 0.0009686409947459458, Model learning rate 0.0009686410194262862\n",
      "217/391 [===============>..............] - ETA: 13s - loss: 1.0347 - acc: 0.6840\n",
      " Optimizer iteration 2172, batch 217\n",
      "\n",
      " Learning rate 0.0009683595991412725, Model learning rate 0.0009683595853857696\n",
      "218/391 [===============>..............] - ETA: 13s - loss: 1.0355 - acc: 0.6837\n",
      " Optimizer iteration 2173, batch 218\n",
      "\n",
      " Learning rate 0.0009680769878829606, Model learning rate 0.0009680769871920347\n",
      "219/391 [===============>..............] - ETA: 13s - loss: 1.0360 - acc: 0.6835\n",
      " Optimizer iteration 2174, batch 219\n",
      "\n",
      " Learning rate 0.0009677931617045432, Model learning rate 0.0009677931666374207\n",
      "\n",
      " Optimizer iteration 2175, batch 220\n",
      "\n",
      " Learning rate 0.0009675081213427075, Model learning rate 0.0009675081237219274\n",
      "221/391 [===============>..............] - ETA: 13s - loss: 1.0362 - acc: 0.6835\n",
      " Optimizer iteration 2176, batch 221\n",
      "\n",
      " Learning rate 0.0009672218675372913, Model learning rate 0.000967221858445555\n",
      "222/391 [================>.............] - ETA: 13s - loss: 1.0357 - acc: 0.6838\n",
      " Optimizer iteration 2177, batch 222\n",
      "\n",
      " Learning rate 0.0009669344010312829, Model learning rate 0.0009669344290159643\n",
      "\n",
      " Optimizer iteration 2178, batch 223\n",
      "\n",
      " Learning rate 0.0009666457225708174, Model learning rate 0.0009666457190178335\n",
      "224/391 [================>.............] - ETA: 13s - loss: 1.0350 - acc: 0.6842\n",
      " Optimizer iteration 2179, batch 224\n",
      "\n",
      " Learning rate 0.0009663558329051762, Model learning rate 0.0009663558448664844\n",
      "225/391 [================>.............] - ETA: 12s - loss: 1.0351 - acc: 0.6844\n",
      " Optimizer iteration 2180, batch 225\n",
      "\n",
      " Learning rate 0.0009660647327867839, Model learning rate 0.0009660647483542562\n",
      "226/391 [================>.............] - ETA: 12s - loss: 1.0343 - acc: 0.6847\n",
      " Optimizer iteration 2181, batch 226\n",
      "\n",
      " Learning rate 0.0009657724229712075, Model learning rate 0.0009657724294811487\n",
      "\n",
      " Optimizer iteration 2182, batch 227\n",
      "\n",
      " Learning rate 0.0009654789042171535, Model learning rate 0.0009654788882471621\n",
      "228/391 [================>.............] - ETA: 12s - loss: 1.0356 - acc: 0.6842\n",
      " Optimizer iteration 2183, batch 228\n",
      "\n",
      " Learning rate 0.000965184177286466, Model learning rate 0.0009651841828599572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/391 [================>.............] - ETA: 12s - loss: 1.0359 - acc: 0.6840\n",
      " Optimizer iteration 2184, batch 229\n",
      "\n",
      " Learning rate 0.0009648882429441257, Model learning rate 0.0009648882551118731\n",
      "230/391 [================>.............] - ETA: 12s - loss: 1.0352 - acc: 0.6843\n",
      " Optimizer iteration 2185, batch 230\n",
      "\n",
      " Learning rate 0.0009645911019582466, Model learning rate 0.0009645911050029099\n",
      "\n",
      " Optimizer iteration 2186, batch 231\n",
      "\n",
      " Learning rate 0.0009642927551000749, Model learning rate 0.0009642927325330675\n",
      "232/391 [================>.............] - ETA: 12s - loss: 1.0353 - acc: 0.6844\n",
      " Optimizer iteration 2187, batch 232\n",
      "\n",
      " Learning rate 0.0009639932031439866, Model learning rate 0.0009639931959100068\n",
      "\n",
      " Optimizer iteration 2188, batch 233\n",
      "\n",
      " Learning rate 0.0009636924468674854, Model learning rate 0.0009636924369260669\n",
      "234/391 [================>.............] - ETA: 12s - loss: 1.0341 - acc: 0.6849\n",
      " Optimizer iteration 2189, batch 234\n",
      "\n",
      " Learning rate 0.0009633904870512015, Model learning rate 0.0009633905137889087\n",
      "235/391 [=================>............] - ETA: 12s - loss: 1.0337 - acc: 0.6851\n",
      " Optimizer iteration 2190, batch 235\n",
      "\n",
      " Learning rate 0.0009630873244788883, Model learning rate 0.0009630873100832105\n",
      "236/391 [=================>............] - ETA: 12s - loss: 1.0345 - acc: 0.6848\n",
      " Optimizer iteration 2191, batch 236\n",
      "\n",
      " Learning rate 0.0009627829599374214, Model learning rate 0.000962782942224294\n",
      "237/391 [=================>............] - ETA: 12s - loss: 1.0343 - acc: 0.6850\n",
      " Optimizer iteration 2192, batch 237\n",
      "\n",
      " Learning rate 0.0009624773942167958, Model learning rate 0.0009624774102121592\n",
      "238/391 [=================>............] - ETA: 11s - loss: 1.0344 - acc: 0.6849\n",
      " Optimizer iteration 2193, batch 238\n",
      "\n",
      " Learning rate 0.0009621706281101248, Model learning rate 0.0009621706558391452\n",
      "239/391 [=================>............] - ETA: 11s - loss: 1.0344 - acc: 0.6848\n",
      " Optimizer iteration 2194, batch 239\n",
      "\n",
      " Learning rate 0.0009618626624136369, Model learning rate 0.000961862679105252\n",
      "240/391 [=================>............] - ETA: 11s - loss: 1.0338 - acc: 0.6850\n",
      " Optimizer iteration 2195, batch 240\n",
      "\n",
      " Learning rate 0.0009615534979266745, Model learning rate 0.0009615534800104797\n",
      "\n",
      " Optimizer iteration 2196, batch 241\n",
      "\n",
      " Learning rate 0.0009612431354516912, Model learning rate 0.0009612431167624891\n",
      "242/391 [=================>............] - ETA: 11s - loss: 1.0328 - acc: 0.6855\n",
      " Optimizer iteration 2197, batch 242\n",
      "\n",
      " Learning rate 0.0009609315757942503, Model learning rate 0.0009609315893612802\n",
      "243/391 [=================>............] - ETA: 11s - loss: 1.0321 - acc: 0.6856\n",
      " Optimizer iteration 2198, batch 243\n",
      "\n",
      " Learning rate 0.0009606188197630223, Model learning rate 0.0009606188395991921\n",
      "244/391 [=================>............] - ETA: 11s - loss: 1.0313 - acc: 0.6859\n",
      " Optimizer iteration 2199, batch 244\n",
      "\n",
      " Learning rate 0.0009603048681697834, Model learning rate 0.0009603048674762249\n",
      "245/391 [=================>............] - ETA: 11s - loss: 1.0308 - acc: 0.6860\n",
      " Optimizer iteration 2200, batch 245\n",
      "\n",
      " Learning rate 0.0009599897218294122, Model learning rate 0.0009599897312000394\n",
      "246/391 [=================>............] - ETA: 11s - loss: 1.0303 - acc: 0.6863\n",
      " Optimizer iteration 2201, batch 246\n",
      "\n",
      " Learning rate 0.0009596733815598888, Model learning rate 0.0009596733725629747\n",
      "247/391 [=================>............] - ETA: 11s - loss: 1.0301 - acc: 0.6863\n",
      " Optimizer iteration 2202, batch 247\n",
      "\n",
      " Learning rate 0.0009593558481822921, Model learning rate 0.0009593558497726917\n",
      "248/391 [==================>...........] - ETA: 11s - loss: 1.0301 - acc: 0.6862\n",
      " Optimizer iteration 2203, batch 248\n",
      "\n",
      " Learning rate 0.0009590371225207981, Model learning rate 0.0009590371046215296\n",
      "249/391 [==================>...........] - ETA: 11s - loss: 1.0301 - acc: 0.6862\n",
      " Optimizer iteration 2204, batch 249\n",
      "\n",
      " Learning rate 0.0009587172054026768, Model learning rate 0.0009587171953171492\n",
      "250/391 [==================>...........] - ETA: 11s - loss: 1.0300 - acc: 0.6863\n",
      " Optimizer iteration 2205, batch 250\n",
      "\n",
      " Learning rate 0.0009583960976582913, Model learning rate 0.0009583961218595505\n",
      "251/391 [==================>...........] - ETA: 10s - loss: 1.0300 - acc: 0.6864\n",
      " Optimizer iteration 2206, batch 251\n",
      "\n",
      " Learning rate 0.0009580738001210944, Model learning rate 0.0009580738260410726\n",
      "252/391 [==================>...........] - ETA: 10s - loss: 1.0299 - acc: 0.6865\n",
      " Optimizer iteration 2207, batch 252\n",
      "\n",
      " Learning rate 0.000957750313627628, Model learning rate 0.0009577503078617156\n",
      "253/391 [==================>...........] - ETA: 10s - loss: 1.0294 - acc: 0.6867\n",
      " Optimizer iteration 2208, batch 253\n",
      "\n",
      " Learning rate 0.0009574256390175192, Model learning rate 0.0009574256255291402\n",
      "254/391 [==================>...........] - ETA: 10s - loss: 1.0293 - acc: 0.6868\n",
      " Optimizer iteration 2209, batch 254\n",
      "\n",
      " Learning rate 0.0009570997771334791, Model learning rate 0.0009570997790433466\n",
      "255/391 [==================>...........] - ETA: 10s - loss: 1.0293 - acc: 0.6867\n",
      " Optimizer iteration 2210, batch 255\n",
      "\n",
      " Learning rate 0.0009567727288213005, Model learning rate 0.0009567727101966739\n",
      "\n",
      " Optimizer iteration 2211, batch 256\n",
      "\n",
      " Learning rate 0.0009564444949298558, Model learning rate 0.0009564444771967828\n",
      "257/391 [==================>...........] - ETA: 10s - loss: 1.0303 - acc: 0.6865\n",
      " Optimizer iteration 2212, batch 257\n",
      "\n",
      " Learning rate 0.0009561150763110944, Model learning rate 0.0009561150800436735\n",
      "258/391 [==================>...........] - ETA: 10s - loss: 1.0303 - acc: 0.6864\n",
      " Optimizer iteration 2213, batch 258\n",
      "\n",
      " Learning rate 0.0009557844738200408, Model learning rate 0.000955784460529685\n",
      "259/391 [==================>...........] - ETA: 10s - loss: 1.0297 - acc: 0.6867\n",
      " Optimizer iteration 2214, batch 259\n",
      "\n",
      " Learning rate 0.0009554526883147925, Model learning rate 0.0009554526768624783\n",
      "260/391 [==================>...........] - ETA: 10s - loss: 1.0293 - acc: 0.6868\n",
      " Optimizer iteration 2215, batch 260\n",
      "\n",
      " Learning rate 0.0009551197206565174, Model learning rate 0.0009551197290420532\n",
      "\n",
      " Optimizer iteration 2216, batch 261\n",
      "\n",
      " Learning rate 0.0009547855717094515, Model learning rate 0.000954785558860749\n",
      "262/391 [===================>..........] - ETA: 10s - loss: 1.0302 - acc: 0.6867\n",
      " Optimizer iteration 2217, batch 262\n",
      "\n",
      " Learning rate 0.0009544502423408973, Model learning rate 0.0009544502245262265\n",
      "263/391 [===================>..........] - ETA: 10s - loss: 1.0297 - acc: 0.6869\n",
      " Optimizer iteration 2218, batch 263\n",
      "\n",
      " Learning rate 0.0009541137334212211, Model learning rate 0.0009541137260384858\n",
      "\n",
      " Optimizer iteration 2219, batch 264\n",
      "\n",
      " Learning rate 0.0009537760458238505, Model learning rate 0.0009537760633975267\n",
      "265/391 [===================>..........] - ETA: 9s - loss: 1.0304 - acc: 0.6868 \n",
      " Optimizer iteration 2220, batch 265\n",
      "\n",
      " Learning rate 0.0009534371804252727, Model learning rate 0.0009534371783956885\n",
      "266/391 [===================>..........] - ETA: 9s - loss: 1.0301 - acc: 0.6868\n",
      " Optimizer iteration 2221, batch 266\n",
      "\n",
      " Learning rate 0.0009530971381050319, Model learning rate 0.0009530971292406321\n",
      "\n",
      " Optimizer iteration 2222, batch 267\n",
      "\n",
      " Learning rate 0.0009527559197457272, Model learning rate 0.0009527559159323573\n",
      "268/391 [===================>..........] - ETA: 9s - loss: 1.0291 - acc: 0.6873\n",
      " Optimizer iteration 2223, batch 268\n",
      "\n",
      " Learning rate 0.0009524135262330098, Model learning rate 0.0009524135384708643\n",
      "\n",
      " Optimizer iteration 2224, batch 269\n",
      "\n",
      " Learning rate 0.0009520699584555812, Model learning rate 0.0009520699386484921\n",
      "270/391 [===================>..........] - ETA: 9s - loss: 1.0279 - acc: 0.6879\n",
      " Optimizer iteration 2225, batch 270\n",
      "\n",
      " Learning rate 0.0009517252173051911, Model learning rate 0.0009517252328805625\n",
      "\n",
      " Optimizer iteration 2226, batch 271\n",
      "\n",
      " Learning rate 0.0009513793036766345, Model learning rate 0.0009513793047517538\n",
      "272/391 [===================>..........] - ETA: 9s - loss: 1.0280 - acc: 0.6878\n",
      " Optimizer iteration 2227, batch 272\n",
      "\n",
      " Learning rate 0.0009510322184677493, Model learning rate 0.0009510322124697268\n",
      "273/391 [===================>..........] - ETA: 9s - loss: 1.0280 - acc: 0.6878\n",
      " Optimizer iteration 2228, batch 273\n",
      "\n",
      " Learning rate 0.0009506839625794152, Model learning rate 0.0009506839560344815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/391 [====================>.........] - ETA: 9s - loss: 1.0279 - acc: 0.6878\n",
      " Optimizer iteration 2229, batch 274\n",
      "\n",
      " Learning rate 0.0009503345369155494, Model learning rate 0.000950334535446018\n",
      "275/391 [====================>.........] - ETA: 9s - loss: 1.0277 - acc: 0.6879\n",
      " Optimizer iteration 2230, batch 275\n",
      "\n",
      " Learning rate 0.0009499839423831061, Model learning rate 0.0009499839507043362\n",
      "276/391 [====================>.........] - ETA: 8s - loss: 1.0276 - acc: 0.6879\n",
      " Optimizer iteration 2231, batch 276\n",
      "\n",
      " Learning rate 0.0009496321798920731, Model learning rate 0.0009496322018094361\n",
      "277/391 [====================>.........] - ETA: 8s - loss: 1.0276 - acc: 0.6880\n",
      " Optimizer iteration 2232, batch 277\n",
      "\n",
      " Learning rate 0.0009492792503554695, Model learning rate 0.0009492792305536568\n",
      "278/391 [====================>.........] - ETA: 8s - loss: 1.0282 - acc: 0.6878\n",
      " Optimizer iteration 2233, batch 278\n",
      "\n",
      " Learning rate 0.000948925154689344, Model learning rate 0.0009489251533523202\n",
      "279/391 [====================>.........] - ETA: 8s - loss: 1.0279 - acc: 0.6882\n",
      " Optimizer iteration 2234, batch 279\n",
      "\n",
      " Learning rate 0.0009485698938127715, Model learning rate 0.0009485699119977653\n",
      "280/391 [====================>.........] - ETA: 8s - loss: 1.0279 - acc: 0.6883\n",
      " Optimizer iteration 2235, batch 280\n",
      "\n",
      " Learning rate 0.0009482134686478518, Model learning rate 0.0009482134482823312\n",
      "281/391 [====================>.........] - ETA: 8s - loss: 1.0275 - acc: 0.6885\n",
      " Optimizer iteration 2236, batch 281\n",
      "\n",
      " Learning rate 0.0009478558801197064, Model learning rate 0.0009478558786213398\n",
      "282/391 [====================>.........] - ETA: 8s - loss: 1.0274 - acc: 0.6886\n",
      " Optimizer iteration 2237, batch 282\n",
      "\n",
      " Learning rate 0.0009474971291564764, Model learning rate 0.0009474971448071301\n",
      "283/391 [====================>.........] - ETA: 8s - loss: 1.0270 - acc: 0.6887\n",
      " Optimizer iteration 2238, batch 283\n",
      "\n",
      " Learning rate 0.0009471372166893198, Model learning rate 0.0009471371886320412\n",
      "284/391 [====================>.........] - ETA: 8s - loss: 1.0271 - acc: 0.6886\n",
      " Optimizer iteration 2239, batch 284\n",
      "\n",
      " Learning rate 0.0009467761436524099, Model learning rate 0.000946776126511395\n",
      "285/391 [====================>.........] - ETA: 8s - loss: 1.0272 - acc: 0.6883\n",
      " Optimizer iteration 2240, batch 285\n",
      "\n",
      " Learning rate 0.0009464139109829321, Model learning rate 0.0009464139002375305\n",
      "286/391 [====================>.........] - ETA: 8s - loss: 1.0269 - acc: 0.6884\n",
      " Optimizer iteration 2241, batch 286\n",
      "\n",
      " Learning rate 0.0009460505196210813, Model learning rate 0.0009460505098104477\n",
      "\n",
      " Optimizer iteration 2242, batch 287\n",
      "\n",
      " Learning rate 0.0009456859705100606, Model learning rate 0.0009456859552301466\n",
      "288/391 [=====================>........] - ETA: 8s - loss: 1.0260 - acc: 0.6887\n",
      " Optimizer iteration 2243, batch 288\n",
      "\n",
      " Learning rate 0.0009453202645960773, Model learning rate 0.0009453202364966273\n",
      "289/391 [=====================>........] - ETA: 7s - loss: 1.0253 - acc: 0.6889\n",
      " Optimizer iteration 2244, batch 289\n",
      "\n",
      " Learning rate 0.000944953402828342, Model learning rate 0.0009449534118175507\n",
      "290/391 [=====================>........] - ETA: 7s - loss: 1.0248 - acc: 0.6892\n",
      " Optimizer iteration 2245, batch 290\n",
      "\n",
      " Learning rate 0.0009445853861590646, Model learning rate 0.0009445853647775948\n",
      "\n",
      " Optimizer iteration 2246, batch 291\n",
      "\n",
      " Learning rate 0.0009442162155434534, Model learning rate 0.0009442162117920816\n",
      "292/391 [=====================>........] - ETA: 7s - loss: 1.0241 - acc: 0.6895\n",
      " Optimizer iteration 2247, batch 292\n",
      "\n",
      " Learning rate 0.0009438458919397112, Model learning rate 0.0009438458946533501\n",
      "\n",
      " Optimizer iteration 2248, batch 293\n",
      "\n",
      " Learning rate 0.0009434744163090339, Model learning rate 0.0009434744133614004\n",
      "294/391 [=====================>........] - ETA: 7s - loss: 1.0242 - acc: 0.6897\n",
      " Optimizer iteration 2249, batch 294\n",
      "\n",
      " Learning rate 0.0009431017896156073, Model learning rate 0.0009431017679162323\n",
      "295/391 [=====================>........] - ETA: 7s - loss: 1.0241 - acc: 0.6896\n",
      " Optimizer iteration 2250, batch 295\n",
      "\n",
      " Learning rate 0.0009427280128266049, Model learning rate 0.000942728016525507\n",
      "296/391 [=====================>........] - ETA: 7s - loss: 1.0238 - acc: 0.6898\n",
      " Optimizer iteration 2251, batch 296\n",
      "\n",
      " Learning rate 0.0009423530869121855, Model learning rate 0.0009423531009815633\n",
      "\n",
      " Optimizer iteration 2252, batch 297\n",
      "\n",
      " Learning rate 0.0009419770128454898, Model learning rate 0.0009419770212844014\n",
      "298/391 [=====================>........] - ETA: 7s - loss: 1.0237 - acc: 0.6898\n",
      " Optimizer iteration 2253, batch 298\n",
      "\n",
      " Learning rate 0.00094159979160264, Model learning rate 0.0009415997774340212\n",
      "299/391 [=====================>........] - ETA: 7s - loss: 1.0237 - acc: 0.6900\n",
      " Optimizer iteration 2254, batch 299\n",
      "\n",
      " Learning rate 0.0009412214241627343, Model learning rate 0.0009412214276380837\n",
      "\n",
      " Optimizer iteration 2255, batch 300\n",
      "\n",
      " Learning rate 0.000940841911507847, Model learning rate 0.0009408419136889279\n",
      "301/391 [======================>.......] - ETA: 7s - loss: 1.0237 - acc: 0.6901\n",
      " Optimizer iteration 2256, batch 301\n",
      "\n",
      " Learning rate 0.0009404612546230243, Model learning rate 0.0009404612355865538\n",
      "302/391 [======================>.......] - ETA: 6s - loss: 1.0234 - acc: 0.6902\n",
      " Optimizer iteration 2257, batch 302\n",
      "\n",
      " Learning rate 0.0009400794544962828, Model learning rate 0.0009400794515386224\n",
      "303/391 [======================>.......] - ETA: 6s - loss: 1.0235 - acc: 0.6902\n",
      " Optimizer iteration 2258, batch 303\n",
      "\n",
      " Learning rate 0.0009396965121186058, Model learning rate 0.0009396965033374727\n",
      "304/391 [======================>.......] - ETA: 6s - loss: 1.0233 - acc: 0.6901\n",
      " Optimizer iteration 2259, batch 304\n",
      "\n",
      " Learning rate 0.0009393124284839423, Model learning rate 0.0009393124491907656\n",
      "305/391 [======================>.......] - ETA: 6s - loss: 1.0233 - acc: 0.6900\n",
      " Optimizer iteration 2260, batch 305\n",
      "\n",
      " Learning rate 0.0009389272045892023, Model learning rate 0.0009389272308908403\n",
      "306/391 [======================>.......] - ETA: 6s - loss: 1.0229 - acc: 0.6902\n",
      " Optimizer iteration 2261, batch 306\n",
      "\n",
      " Learning rate 0.0009385408414342564, Model learning rate 0.0009385408484376967\n",
      "\n",
      " Optimizer iteration 2262, batch 307\n",
      "\n",
      " Learning rate 0.0009381533400219318, Model learning rate 0.0009381533600389957\n",
      "308/391 [======================>.......] - ETA: 6s - loss: 1.0229 - acc: 0.6900\n",
      " Optimizer iteration 2263, batch 308\n",
      "\n",
      " Learning rate 0.0009377647013580102, Model learning rate 0.0009377647074870765\n",
      "309/391 [======================>.......] - ETA: 6s - loss: 1.0230 - acc: 0.6899\n",
      " Optimizer iteration 2264, batch 309\n",
      "\n",
      " Learning rate 0.000937374926451225, Model learning rate 0.0009373749489895999\n",
      "310/391 [======================>.......] - ETA: 6s - loss: 1.0228 - acc: 0.6900\n",
      " Optimizer iteration 2265, batch 310\n",
      "\n",
      " Learning rate 0.000936984016313259, Model learning rate 0.0009369840263389051\n",
      "\n",
      " Optimizer iteration 2266, batch 311\n",
      "\n",
      " Learning rate 0.0009365919719587412, Model learning rate 0.0009365919977426529\n",
      "312/391 [======================>.......] - ETA: 6s - loss: 1.0228 - acc: 0.6901\n",
      " Optimizer iteration 2267, batch 312\n",
      "\n",
      " Learning rate 0.000936198794405245, Model learning rate 0.0009361988049931824\n",
      "313/391 [=======================>......] - ETA: 6s - loss: 1.0224 - acc: 0.6902\n",
      " Optimizer iteration 2268, batch 313\n",
      "\n",
      " Learning rate 0.0009358044846732847, Model learning rate 0.0009358045062981546\n",
      "314/391 [=======================>......] - ETA: 6s - loss: 1.0222 - acc: 0.6902\n",
      " Optimizer iteration 2269, batch 314\n",
      "\n",
      " Learning rate 0.0009354090437863132, Model learning rate 0.0009354090434499085\n",
      "315/391 [=======================>......] - ETA: 5s - loss: 1.0219 - acc: 0.6902\n",
      " Optimizer iteration 2270, batch 315\n",
      "\n",
      " Learning rate 0.0009350124727707196, Model learning rate 0.000935012474656105\n",
      "316/391 [=======================>......] - ETA: 5s - loss: 1.0221 - acc: 0.6902\n",
      " Optimizer iteration 2271, batch 316\n",
      "\n",
      " Learning rate 0.0009346147726558265, Model learning rate 0.0009346147999167442\n",
      "\n",
      " Optimizer iteration 2272, batch 317\n",
      "\n",
      " Learning rate 0.0009342159444738864, Model learning rate 0.0009342159610241652\n",
      "318/391 [=======================>......] - ETA: 5s - loss: 1.0217 - acc: 0.6903\n",
      " Optimizer iteration 2273, batch 318\n",
      "\n",
      " Learning rate 0.0009338159892600807, Model learning rate 0.0009338160161860287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319/391 [=======================>......] - ETA: 5s - loss: 1.0216 - acc: 0.6905\n",
      " Optimizer iteration 2274, batch 319\n",
      "\n",
      " Learning rate 0.0009334149080525154, Model learning rate 0.000933414907194674\n",
      "320/391 [=======================>......] - ETA: 5s - loss: 1.0213 - acc: 0.6906\n",
      " Optimizer iteration 2275, batch 320\n",
      "\n",
      " Learning rate 0.0009330127018922195, Model learning rate 0.000933012692257762\n",
      "\n",
      " Optimizer iteration 2276, batch 321\n",
      "\n",
      " Learning rate 0.0009326093718231412, Model learning rate 0.0009326093713752925\n",
      "322/391 [=======================>......] - ETA: 5s - loss: 1.0207 - acc: 0.6908\n",
      " Optimizer iteration 2277, batch 322\n",
      "\n",
      " Learning rate 0.0009322049188921467, Model learning rate 0.0009322049445472658\n",
      "323/391 [=======================>......] - ETA: 5s - loss: 1.0205 - acc: 0.6908\n",
      " Optimizer iteration 2278, batch 323\n",
      "\n",
      " Learning rate 0.0009317993441490162, Model learning rate 0.0009317993535660207\n",
      "\n",
      " Optimizer iteration 2279, batch 324\n",
      "\n",
      " Learning rate 0.0009313926486464419, Model learning rate 0.0009313926566392183\n",
      "325/391 [=======================>......] - ETA: 5s - loss: 1.0212 - acc: 0.6905\n",
      " Optimizer iteration 2280, batch 325\n",
      "\n",
      " Learning rate 0.0009309848334400246, Model learning rate 0.0009309848537668586\n",
      "326/391 [========================>.....] - ETA: 5s - loss: 1.0206 - acc: 0.6907\n",
      " Optimizer iteration 2281, batch 326\n",
      "\n",
      " Learning rate 0.0009305758995882716, Model learning rate 0.0009305758867412806\n",
      "327/391 [========================>.....] - ETA: 4s - loss: 1.0206 - acc: 0.6908\n",
      " Optimizer iteration 2282, batch 327\n",
      "\n",
      " Learning rate 0.0009301658481525939, Model learning rate 0.0009301658719778061\n",
      "328/391 [========================>.....] - ETA: 4s - loss: 1.0208 - acc: 0.6908\n",
      " Optimizer iteration 2283, batch 328\n",
      "\n",
      " Learning rate 0.0009297546801973025, Model learning rate 0.0009297546930611134\n",
      "\n",
      " Optimizer iteration 2284, batch 329\n",
      "\n",
      " Learning rate 0.0009293423967896076, Model learning rate 0.0009293424081988633\n",
      "330/391 [========================>.....] - ETA: 4s - loss: 1.0206 - acc: 0.6911\n",
      " Optimizer iteration 2285, batch 330\n",
      "\n",
      " Learning rate 0.0009289289989996133, Model learning rate 0.0009289290173910558\n",
      "331/391 [========================>.....] - ETA: 4s - loss: 1.0203 - acc: 0.6912\n",
      " Optimizer iteration 2286, batch 331\n",
      "\n",
      " Learning rate 0.0009285144879003172, Model learning rate 0.0009285144624300301\n",
      "332/391 [========================>.....] - ETA: 4s - loss: 1.0197 - acc: 0.6915\n",
      " Optimizer iteration 2287, batch 332\n",
      "\n",
      " Learning rate 0.0009280988645676059, Model learning rate 0.000928098859731108\n",
      "\n",
      " Optimizer iteration 2288, batch 333\n",
      "\n",
      " Learning rate 0.0009276821300802534, Model learning rate 0.0009276821510866284\n",
      "334/391 [========================>.....] - ETA: 4s - loss: 1.0199 - acc: 0.6914\n",
      " Optimizer iteration 2289, batch 334\n",
      "\n",
      " Learning rate 0.0009272642855199171, Model learning rate 0.0009272642782889307\n",
      "335/391 [========================>.....] - ETA: 4s - loss: 1.0198 - acc: 0.6914\n",
      " Optimizer iteration 2290, batch 335\n",
      "\n",
      " Learning rate 0.0009268453319711362, Model learning rate 0.0009268453577533364\n",
      "336/391 [========================>.....] - ETA: 4s - loss: 1.0196 - acc: 0.6916\n",
      " Optimizer iteration 2291, batch 336\n",
      "\n",
      " Learning rate 0.000926425270521328, Model learning rate 0.0009264252730645239\n",
      "\n",
      " Optimizer iteration 2292, batch 337\n",
      "\n",
      " Learning rate 0.0009260041022607859, Model learning rate 0.0009260040824301541\n",
      "338/391 [========================>.....] - ETA: 4s - loss: 1.0194 - acc: 0.6916\n",
      " Optimizer iteration 2293, batch 338\n",
      "\n",
      " Learning rate 0.0009255818282826756, Model learning rate 0.0009255818440578878\n",
      "339/391 [=========================>....] - ETA: 4s - loss: 1.0187 - acc: 0.6918\n",
      " Optimizer iteration 2294, batch 339\n",
      "\n",
      " Learning rate 0.0009251584496830327, Model learning rate 0.0009251584415324032\n",
      "340/391 [=========================>....] - ETA: 3s - loss: 1.0186 - acc: 0.6917\n",
      " Optimizer iteration 2295, batch 340\n",
      "\n",
      " Learning rate 0.0009247339675607605, Model learning rate 0.0009247339912690222\n",
      "341/391 [=========================>....] - ETA: 3s - loss: 1.0185 - acc: 0.6917\n",
      " Optimizer iteration 2296, batch 341\n",
      "\n",
      " Learning rate 0.000924308383017626, Model learning rate 0.000924308376852423\n",
      "342/391 [=========================>....] - ETA: 3s - loss: 1.0181 - acc: 0.6919\n",
      " Optimizer iteration 2297, batch 342\n",
      "\n",
      " Learning rate 0.0009238816971582578, Model learning rate 0.0009238817146979272\n",
      "343/391 [=========================>....] - ETA: 3s - loss: 1.0178 - acc: 0.6920\n",
      " Optimizer iteration 2298, batch 343\n",
      "\n",
      " Learning rate 0.000923453911090143, Model learning rate 0.0009234538883902133\n",
      "344/391 [=========================>....] - ETA: 3s - loss: 1.0183 - acc: 0.6917\n",
      " Optimizer iteration 2299, batch 344\n",
      "\n",
      " Learning rate 0.0009230250259236243, Model learning rate 0.0009230250143446028\n",
      "345/391 [=========================>....] - ETA: 3s - loss: 1.0188 - acc: 0.6916\n",
      " Optimizer iteration 2300, batch 345\n",
      "\n",
      " Learning rate 0.0009225950427718975, Model learning rate 0.000922595034353435\n",
      "346/391 [=========================>....] - ETA: 3s - loss: 1.0185 - acc: 0.6916\n",
      " Optimizer iteration 2301, batch 346\n",
      "\n",
      " Learning rate 0.0009221639627510075, Model learning rate 0.0009221639484167099\n",
      "347/391 [=========================>....] - ETA: 3s - loss: 1.0185 - acc: 0.6916\n",
      " Optimizer iteration 2302, batch 347\n",
      "\n",
      " Learning rate 0.0009217317869798471, Model learning rate 0.0009217318147420883\n",
      "348/391 [=========================>....] - ETA: 3s - loss: 1.0185 - acc: 0.6916\n",
      " Optimizer iteration 2303, batch 348\n",
      "\n",
      " Learning rate 0.0009212985165801529, Model learning rate 0.0009212985169142485\n",
      "349/391 [=========================>....] - ETA: 3s - loss: 1.0184 - acc: 0.6915\n",
      " Optimizer iteration 2304, batch 349\n",
      "\n",
      " Learning rate 0.0009208641526765023, Model learning rate 0.0009208641713485122\n",
      "350/391 [=========================>....] - ETA: 3s - loss: 1.0185 - acc: 0.6914\n",
      " Optimizer iteration 2305, batch 350\n",
      "\n",
      " Learning rate 0.0009204286963963111, Model learning rate 0.0009204287198372185\n",
      "351/391 [=========================>....] - ETA: 3s - loss: 1.0182 - acc: 0.6914\n",
      " Optimizer iteration 2306, batch 351\n",
      "\n",
      " Learning rate 0.0009199921488698308, Model learning rate 0.0009199921623803675\n",
      "352/391 [==========================>...] - ETA: 3s - loss: 1.0183 - acc: 0.6915\n",
      " Optimizer iteration 2307, batch 352\n",
      "\n",
      " Learning rate 0.0009195545112301446, Model learning rate 0.0009195544989779592\n",
      "353/391 [==========================>...] - ETA: 2s - loss: 1.0184 - acc: 0.6915\n",
      " Optimizer iteration 2308, batch 353\n",
      "\n",
      " Learning rate 0.0009191157846131662, Model learning rate 0.0009191157878376544\n",
      "354/391 [==========================>...] - ETA: 2s - loss: 1.0183 - acc: 0.6916\n",
      " Optimizer iteration 2309, batch 354\n",
      "\n",
      " Learning rate 0.0009186759701576349, Model learning rate 0.0009186759707517922\n",
      "355/391 [==========================>...] - ETA: 2s - loss: 1.0182 - acc: 0.6917\n",
      " Optimizer iteration 2310, batch 355\n",
      "\n",
      " Learning rate 0.0009182350690051134, Model learning rate 0.0009182350477203727\n",
      "\n",
      " Optimizer iteration 2311, batch 356\n",
      "\n",
      " Learning rate 0.0009177930822999859, Model learning rate 0.0009177930769510567\n",
      "357/391 [==========================>...] - ETA: 2s - loss: 1.0173 - acc: 0.6921\n",
      " Optimizer iteration 2312, batch 357\n",
      "\n",
      " Learning rate 0.0009173500111894535, Model learning rate 0.0009173500002361834\n",
      "358/391 [==========================>...] - ETA: 2s - loss: 1.0170 - acc: 0.6922\n",
      " Optimizer iteration 2313, batch 358\n",
      "\n",
      " Learning rate 0.0009169058568235323, Model learning rate 0.0009169058757834136\n",
      "359/391 [==========================>...] - ETA: 2s - loss: 1.0171 - acc: 0.6922\n",
      " Optimizer iteration 2314, batch 359\n",
      "\n",
      " Learning rate 0.0009164606203550497, Model learning rate 0.0009164606453850865\n",
      "\n",
      " Optimizer iteration 2315, batch 360\n",
      "\n",
      " Learning rate 0.0009160143029396422, Model learning rate 0.0009160143090412021\n",
      "361/391 [==========================>...] - ETA: 2s - loss: 1.0169 - acc: 0.6925\n",
      " Optimizer iteration 2316, batch 361\n",
      "\n",
      " Learning rate 0.0009155669057357514, Model learning rate 0.0009155669249594212\n",
      "362/391 [==========================>...] - ETA: 2s - loss: 1.0165 - acc: 0.6926\n",
      " Optimizer iteration 2317, batch 362\n",
      "\n",
      " Learning rate 0.0009151184299046221, Model learning rate 0.0009151184349320829\n",
      "363/391 [==========================>...] - ETA: 2s - loss: 1.0166 - acc: 0.6926\n",
      " Optimizer iteration 2318, batch 363\n",
      "\n",
      " Learning rate 0.0009146688766102984, Model learning rate 0.0009146688971668482\n",
      "\n",
      " Optimizer iteration 2319, batch 364\n",
      "\n",
      " Learning rate 0.0009142182470196212, Model learning rate 0.0009142182534560561\n",
      "365/391 [===========================>..] - ETA: 2s - loss: 1.0163 - acc: 0.6927\n",
      " Optimizer iteration 2320, batch 365\n",
      "\n",
      " Learning rate 0.000913766542302225, Model learning rate 0.0009137665620073676\n",
      "366/391 [===========================>..] - ETA: 1s - loss: 1.0160 - acc: 0.6928\n",
      " Optimizer iteration 2321, batch 366\n",
      "\n",
      " Learning rate 0.0009133137636305345, Model learning rate 0.0009133137646131217\n",
      "367/391 [===========================>..] - ETA: 1s - loss: 1.0158 - acc: 0.6929\n",
      " Optimizer iteration 2322, batch 367\n",
      "\n",
      " Learning rate 0.0009128599121797621, Model learning rate 0.0009128599194809794\n",
      "\n",
      " Optimizer iteration 2323, batch 368\n",
      "\n",
      " Learning rate 0.000912404989127905, Model learning rate 0.0009124049684032798\n",
      "369/391 [===========================>..] - ETA: 1s - loss: 1.0149 - acc: 0.6934\n",
      " Optimizer iteration 2324, batch 369\n",
      "\n",
      " Learning rate 0.0009119489956557415, Model learning rate 0.0009119489695876837\n",
      "370/391 [===========================>..] - ETA: 1s - loss: 1.0148 - acc: 0.6934\n",
      " Optimizer iteration 2325, batch 370\n",
      "\n",
      " Learning rate 0.0009114919329468282, Model learning rate 0.0009114919230341911\n",
      "371/391 [===========================>..] - ETA: 1s - loss: 1.0146 - acc: 0.6936\n",
      " Optimizer iteration 2326, batch 371\n",
      "\n",
      " Learning rate 0.000911033802187497, Model learning rate 0.0009110338287428021\n",
      "372/391 [===========================>..] - ETA: 1s - loss: 1.0145 - acc: 0.6936\n",
      " Optimizer iteration 2327, batch 372\n",
      "\n",
      " Learning rate 0.000910574604566852, Model learning rate 0.0009105746285058558\n",
      "\n",
      " Optimizer iteration 2328, batch 373\n",
      "\n",
      " Learning rate 0.0009101143412767665, Model learning rate 0.0009101143223233521\n",
      "374/391 [===========================>..] - ETA: 1s - loss: 1.0148 - acc: 0.6935\n",
      " Optimizer iteration 2329, batch 374\n",
      "\n",
      " Learning rate 0.0009096530135118797, Model learning rate 0.0009096530266106129\n",
      "375/391 [===========================>..] - ETA: 1s - loss: 1.0146 - acc: 0.6935\n",
      " Optimizer iteration 2330, batch 375\n",
      "\n",
      " Learning rate 0.0009091906224695935, Model learning rate 0.0009091906249523163\n",
      "\n",
      " Optimizer iteration 2331, batch 376\n",
      "\n",
      " Learning rate 0.00090872716935007, Model learning rate 0.0009087271755561233\n",
      "377/391 [===========================>..] - ETA: 1s - loss: 1.0147 - acc: 0.6935\n",
      " Optimizer iteration 2332, batch 377\n",
      "\n",
      " Learning rate 0.000908262655356228, Model learning rate 0.0009082626784220338\n",
      "378/391 [============================>.] - ETA: 1s - loss: 1.0148 - acc: 0.6934\n",
      " Optimizer iteration 2333, batch 378\n",
      "\n",
      " Learning rate 0.0009077970816937393, Model learning rate 0.000907797075342387\n",
      "379/391 [============================>.] - ETA: 0s - loss: 1.0149 - acc: 0.6933\n",
      " Optimizer iteration 2334, batch 379\n",
      "\n",
      " Learning rate 0.0009073304495710266, Model learning rate 0.0009073304245248437\n",
      "380/391 [============================>.] - ETA: 0s - loss: 1.0150 - acc: 0.6933\n",
      " Optimizer iteration 2335, batch 380\n",
      "\n",
      " Learning rate 0.0009068627601992598, Model learning rate 0.0009068627841770649\n",
      "381/391 [============================>.] - ETA: 0s - loss: 1.0149 - acc: 0.6933\n",
      " Optimizer iteration 2336, batch 381\n",
      "\n",
      " Learning rate 0.0009063940147923528, Model learning rate 0.0009063940378837287\n",
      "382/391 [============================>.] - ETA: 0s - loss: 1.0147 - acc: 0.6934\n",
      " Optimizer iteration 2337, batch 382\n",
      "\n",
      " Learning rate 0.000905924214566961, Model learning rate 0.0009059241856448352\n",
      "383/391 [============================>.] - ETA: 0s - loss: 1.0145 - acc: 0.6936\n",
      " Optimizer iteration 2338, batch 383\n",
      "\n",
      " Learning rate 0.0009054533607424769, Model learning rate 0.0009054533438757062\n",
      "384/391 [============================>.] - ETA: 0s - loss: 1.0141 - acc: 0.6938\n",
      " Optimizer iteration 2339, batch 384\n",
      "\n",
      " Learning rate 0.0009049814545410281, Model learning rate 0.0009049814543686807\n",
      "385/391 [============================>.] - ETA: 0s - loss: 1.0139 - acc: 0.6938\n",
      " Optimizer iteration 2340, batch 385\n",
      "\n",
      " Learning rate 0.0009045084971874737, Model learning rate 0.0009045085171237588\n",
      "386/391 [============================>.] - ETA: 0s - loss: 1.0136 - acc: 0.6939\n",
      " Optimizer iteration 2341, batch 386\n",
      "\n",
      " Learning rate 0.0009040344899094011, Model learning rate 0.0009040344739332795\n",
      "387/391 [============================>.] - ETA: 0s - loss: 1.0139 - acc: 0.6938\n",
      " Optimizer iteration 2342, batch 387\n",
      "\n",
      " Learning rate 0.0009035594339371228, Model learning rate 0.0009035594412125647\n",
      "\n",
      " Optimizer iteration 2343, batch 388\n",
      "\n",
      " Learning rate 0.0009030833305036732, Model learning rate 0.0009030833025462925\n",
      "389/391 [============================>.] - ETA: 0s - loss: 1.0140 - acc: 0.6939\n",
      " Optimizer iteration 2344, batch 389\n",
      "\n",
      " Learning rate 0.0009026061808448055, Model learning rate 0.0009026061743497849\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.0134 - acc: 0.6941\n",
      " Optimizer iteration 2345, batch 390\n",
      "\n",
      " Learning rate 0.0009021279861989884, Model learning rate 0.0009021279984153807\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 1.0139 - acc: 0.6940 - val_loss: 1.2218 - val_acc: 0.6251\n",
      "\n",
      "Epoch 00006: saving model to /home/ubuntu/Projects/hybrid-ensemble/model/run_200/cifar10_ResNet20v1_model-0006.h5\n",
      "Epoch 7/10\n",
      "\n",
      " Optimizer iteration 2346, batch 0\n",
      "\n",
      " Learning rate 0.0009016487478074031, Model learning rate 0.0009016487747430801\n",
      "  1/391 [..............................] - ETA: 19s - loss: 1.0260 - acc: 0.6875\n",
      " Optimizer iteration 2347, batch 1\n",
      "\n",
      " Learning rate 0.0009011684669139397, Model learning rate 0.0009011684451252222\n",
      "  2/391 [..............................] - ETA: 19s - loss: 0.9902 - acc: 0.6992\n",
      " Optimizer iteration 2348, batch 2\n",
      "\n",
      " Learning rate 0.0009006871447651941, Model learning rate 0.0009006871259771287\n",
      "  3/391 [..............................] - ETA: 21s - loss: 0.9535 - acc: 0.7109\n",
      " Optimizer iteration 2349, batch 3\n",
      "\n",
      " Learning rate 0.0009002047826104651, Model learning rate 0.0009002047590911388\n",
      "  4/391 [..............................] - ETA: 21s - loss: 0.9459 - acc: 0.7168\n",
      " Optimizer iteration 2350, batch 4\n",
      "\n",
      " Learning rate 0.0008997213817017506, Model learning rate 0.0008997214026749134\n",
      "  5/391 [..............................] - ETA: 21s - loss: 0.9530 - acc: 0.7047\n",
      " Optimizer iteration 2351, batch 5\n",
      "\n",
      " Learning rate 0.0008992369432937451, Model learning rate 0.0008992369403131306\n",
      "  6/391 [..............................] - ETA: 22s - loss: 0.9330 - acc: 0.7122\n",
      " Optimizer iteration 2352, batch 6\n",
      "\n",
      " Learning rate 0.0008987514686438353, Model learning rate 0.0008987514884211123\n",
      "  7/391 [..............................] - ETA: 22s - loss: 0.9232 - acc: 0.7165\n",
      " Optimizer iteration 2353, batch 7\n",
      "\n",
      " Learning rate 0.0008982649590120981, Model learning rate 0.0008982649305835366\n",
      "  8/391 [..............................] - ETA: 22s - loss: 0.9446 - acc: 0.7070\n",
      " Optimizer iteration 2354, batch 8\n",
      "\n",
      " Learning rate 0.0008977774156612968, Model learning rate 0.0008977774414233863\n",
      "\n",
      " Optimizer iteration 2355, batch 9\n",
      "\n",
      " Learning rate 0.0008972888398568772, Model learning rate 0.0008972888463176787\n",
      " 10/391 [..............................] - ETA: 21s - loss: 0.9399 - acc: 0.7125\n",
      " Optimizer iteration 2356, batch 10\n",
      "\n",
      " Learning rate 0.0008967992328669654, Model learning rate 0.0008967992616817355\n",
      "\n",
      " Optimizer iteration 2357, batch 11\n",
      "\n",
      " Learning rate 0.0008963085959623637, Model learning rate 0.000896308571100235\n",
      " 12/391 [..............................] - ETA: 21s - loss: 0.9550 - acc: 0.7051\n",
      " Optimizer iteration 2358, batch 12\n",
      "\n",
      " Learning rate 0.0008958169304165479, Model learning rate 0.0008958169491961598\n",
      "\n",
      " Optimizer iteration 2359, batch 13\n",
      "\n",
      " Learning rate 0.0008953242375056634, Model learning rate 0.0008953242213465273\n",
      " 14/391 [>.............................] - ETA: 20s - loss: 0.9374 - acc: 0.7132\n",
      " Optimizer iteration 2360, batch 14\n",
      "\n",
      " Learning rate 0.0008948305185085225, Model learning rate 0.0008948305039666593\n",
      " 15/391 [>.............................] - ETA: 20s - loss: 0.9416 - acc: 0.7104\n",
      " Optimizer iteration 2361, batch 15\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learning rate 0.0008943357747066003, Model learning rate 0.0008943357970565557\n",
      "\n",
      " Optimizer iteration 2362, batch 16\n",
      "\n",
      " Learning rate 0.0008938400073840325, Model learning rate 0.0008938399842008948\n",
      " 17/391 [>.............................] - ETA: 20s - loss: 0.9466 - acc: 0.7068\n",
      " Optimizer iteration 2363, batch 17\n",
      "\n",
      " Learning rate 0.0008933432178276107, Model learning rate 0.0008933432400226593\n",
      "\n",
      " Optimizer iteration 2364, batch 18\n",
      "\n",
      " Learning rate 0.0008928454073267801, Model learning rate 0.0008928453898988664\n",
      " 19/391 [>.............................] - ETA: 19s - loss: 0.9390 - acc: 0.7076\n",
      " Optimizer iteration 2365, batch 19\n",
      "\n",
      " Learning rate 0.000892346577173636, Model learning rate 0.000892346550244838\n",
      " 20/391 [>.............................] - ETA: 19s - loss: 0.9450 - acc: 0.7066\n",
      " Optimizer iteration 2366, batch 20\n",
      "\n",
      " Learning rate 0.0008918467286629199, Model learning rate 0.0008918467210605741\n",
      " 21/391 [>.............................] - ETA: 20s - loss: 0.9480 - acc: 0.7076\n",
      " Optimizer iteration 2367, batch 21\n",
      "\n",
      " Learning rate 0.0008913458630920168, Model learning rate 0.0008913458441384137\n",
      " 22/391 [>.............................] - ETA: 20s - loss: 0.9485 - acc: 0.7085\n",
      " Optimizer iteration 2368, batch 22\n",
      "\n",
      " Learning rate 0.0008908439817609514, Model learning rate 0.0008908439776860178\n",
      " 23/391 [>.............................] - ETA: 21s - loss: 0.9484 - acc: 0.7069\n",
      " Optimizer iteration 2369, batch 23\n",
      "\n",
      " Learning rate 0.0008903410859723847, Model learning rate 0.0008903410634957254\n",
      " 24/391 [>.............................] - ETA: 21s - loss: 0.9504 - acc: 0.7067\n",
      " Optimizer iteration 2370, batch 24\n",
      "\n",
      " Learning rate 0.0008898371770316111, Model learning rate 0.0008898371597751975\n",
      " 25/391 [>.............................] - ETA: 21s - loss: 0.9538 - acc: 0.7044\n",
      " Optimizer iteration 2371, batch 25\n",
      "\n",
      " Learning rate 0.0008893322562465546, Model learning rate 0.0008893322665244341\n",
      " 26/391 [>.............................] - ETA: 21s - loss: 0.9571 - acc: 0.7022\n",
      " Optimizer iteration 2372, batch 26\n",
      "\n",
      " Learning rate 0.0008888263249277656, Model learning rate 0.0008888263255357742\n",
      " 27/391 [=>............................] - ETA: 22s - loss: 0.9610 - acc: 0.7023\n",
      " Optimizer iteration 2373, batch 27\n",
      "\n",
      " Learning rate 0.0008883193843884169, Model learning rate 0.0008883193950168788\n",
      " 28/391 [=>............................] - ETA: 22s - loss: 0.9580 - acc: 0.7031\n",
      " Optimizer iteration 2374, batch 28\n",
      "\n",
      " Learning rate 0.0008878114359443012, Model learning rate 0.000887811416760087\n",
      " 29/391 [=>............................] - ETA: 22s - loss: 0.9525 - acc: 0.7055\n",
      " Optimizer iteration 2375, batch 29\n",
      "\n",
      " Learning rate 0.0008873024809138273, Model learning rate 0.0008873025071807206\n",
      " 30/391 [=>............................] - ETA: 22s - loss: 0.9554 - acc: 0.7044\n",
      " Optimizer iteration 2376, batch 30\n",
      "\n",
      " Learning rate 0.0008867925206180165, Model learning rate 0.0008867924916557968\n",
      " 31/391 [=>............................] - ETA: 22s - loss: 0.9565 - acc: 0.7051\n",
      " Optimizer iteration 2377, batch 31\n",
      "\n",
      " Learning rate 0.0008862815563804996, Model learning rate 0.0008862815448082983\n",
      " 32/391 [=>............................] - ETA: 22s - loss: 0.9529 - acc: 0.7070\n",
      " Optimizer iteration 2378, batch 32\n",
      "\n",
      " Learning rate 0.0008857695895275126, Model learning rate 0.0008857696084305644\n",
      " 33/391 [=>............................] - ETA: 22s - loss: 0.9491 - acc: 0.7095\n",
      " Optimizer iteration 2379, batch 33\n",
      "\n",
      " Learning rate 0.0008852566213878947, Model learning rate 0.000885256624314934\n",
      " 34/391 [=>............................] - ETA: 23s - loss: 0.9482 - acc: 0.7096\n",
      " Optimizer iteration 2380, batch 34\n",
      "\n",
      " Learning rate 0.000884742653293083, Model learning rate 0.0008847426506690681\n",
      " 35/391 [=>............................] - ETA: 23s - loss: 0.9509 - acc: 0.7078\n",
      " Optimizer iteration 2381, batch 35\n",
      "\n",
      " Learning rate 0.0008842276865771108, Model learning rate 0.0008842276874929667\n",
      " 36/391 [=>............................] - ETA: 23s - loss: 0.9484 - acc: 0.7094\n",
      " Optimizer iteration 2382, batch 36\n",
      "\n",
      " Learning rate 0.0008837117225766032, Model learning rate 0.0008837117347866297\n",
      " 37/391 [=>............................] - ETA: 23s - loss: 0.9433 - acc: 0.7124\n",
      " Optimizer iteration 2383, batch 37\n",
      "\n",
      " Learning rate 0.0008831947626307734, Model learning rate 0.0008831947343423963\n",
      " 38/391 [=>............................] - ETA: 23s - loss: 0.9422 - acc: 0.7124\n",
      " Optimizer iteration 2384, batch 38\n",
      "\n",
      " Learning rate 0.0008826768080814205, Model learning rate 0.0008826768025755882\n",
      " 39/391 [=>............................] - ETA: 23s - loss: 0.9394 - acc: 0.7141\n",
      " Optimizer iteration 2385, batch 39\n",
      "\n",
      " Learning rate 0.0008821578602729241, Model learning rate 0.0008821578812785447\n",
      " 40/391 [==>...........................] - ETA: 23s - loss: 0.9371 - acc: 0.7162\n",
      " Optimizer iteration 2386, batch 40\n",
      "\n",
      " Learning rate 0.0008816379205522428, Model learning rate 0.0008816379122436047\n",
      " 41/391 [==>...........................] - ETA: 23s - loss: 0.9389 - acc: 0.7157\n",
      " Optimizer iteration 2387, batch 41\n",
      "\n",
      " Learning rate 0.000881116990268909, Model learning rate 0.00088111701188609\n",
      " 42/391 [==>...........................] - ETA: 23s - loss: 0.9414 - acc: 0.7154\n",
      " Optimizer iteration 2388, batch 42\n",
      "\n",
      " Learning rate 0.0008805950707750268, Model learning rate 0.000880595063790679\n",
      " 43/391 [==>...........................] - ETA: 23s - loss: 0.9400 - acc: 0.7166\n",
      " Optimizer iteration 2389, batch 43\n",
      "\n",
      " Learning rate 0.0008800721634252671, Model learning rate 0.0008800721843726933\n",
      " 44/391 [==>...........................] - ETA: 23s - loss: 0.9419 - acc: 0.7154\n",
      " Optimizer iteration 2390, batch 44\n",
      "\n",
      " Learning rate 0.0008795482695768658, Model learning rate 0.0008795482572168112\n",
      " 45/391 [==>...........................] - ETA: 23s - loss: 0.9404 - acc: 0.7160\n",
      " Optimizer iteration 2391, batch 45\n",
      "\n",
      " Learning rate 0.0008790233905896185, Model learning rate 0.0008790233987383544\n",
      " 46/391 [==>...........................] - ETA: 23s - loss: 0.9441 - acc: 0.7145\n",
      " Optimizer iteration 2392, batch 46\n",
      "\n",
      " Learning rate 0.0008784975278258782, Model learning rate 0.0008784975507296622\n",
      " 47/391 [==>...........................] - ETA: 23s - loss: 0.9431 - acc: 0.7144\n",
      " Optimizer iteration 2393, batch 47\n",
      "\n",
      " Learning rate 0.0008779706826505513, Model learning rate 0.0008779706549830735\n",
      " 48/391 [==>...........................] - ETA: 23s - loss: 0.9447 - acc: 0.7140\n",
      " Optimizer iteration 2394, batch 48\n",
      "\n",
      " Learning rate 0.0008774428564310938, Model learning rate 0.0008774428279139102\n",
      " 49/391 [==>...........................] - ETA: 23s - loss: 0.9462 - acc: 0.7136\n",
      " Optimizer iteration 2395, batch 49\n",
      "\n",
      " Learning rate 0.0008769140505375084, Model learning rate 0.0008769140695221722\n",
      " 50/391 [==>...........................] - ETA: 23s - loss: 0.9445 - acc: 0.7137\n",
      " Optimizer iteration 2396, batch 50\n",
      "\n",
      " Learning rate 0.0008763842663423407, Model learning rate 0.0008763842633925378\n",
      " 51/391 [==>...........................] - ETA: 23s - loss: 0.9456 - acc: 0.7134\n",
      " Optimizer iteration 2397, batch 51\n",
      "\n",
      " Learning rate 0.0008758535052206749, Model learning rate 0.0008758535259403288\n",
      " 52/391 [==>...........................] - ETA: 23s - loss: 0.9456 - acc: 0.7126\n",
      " Optimizer iteration 2398, batch 52\n",
      "\n",
      " Learning rate 0.0008753217685501317, Model learning rate 0.0008753217407502234\n",
      " 53/391 [===>..........................] - ETA: 23s - loss: 0.9445 - acc: 0.7136\n",
      " Optimizer iteration 2399, batch 53\n",
      "\n",
      " Learning rate 0.0008747890577108633, Model learning rate 0.0008747890824452043\n",
      " 54/391 [===>..........................] - ETA: 23s - loss: 0.9425 - acc: 0.7147\n",
      " Optimizer iteration 2400, batch 54\n",
      "\n",
      " Learning rate 0.0008742553740855505, Model learning rate 0.0008742553764022887\n",
      " 55/391 [===>..........................] - ETA: 23s - loss: 0.9419 - acc: 0.7148\n",
      " Optimizer iteration 2401, batch 55\n",
      "\n",
      " Learning rate 0.0008737207190593994, Model learning rate 0.0008737207390367985\n",
      " 56/391 [===>..........................] - ETA: 23s - loss: 0.9414 - acc: 0.7148\n",
      " Optimizer iteration 2402, batch 56\n",
      "\n",
      " Learning rate 0.0008731850940201369, Model learning rate 0.0008731851121410728\n",
      " 57/391 [===>..........................] - ETA: 23s - loss: 0.9410 - acc: 0.7148\n",
      " Optimizer iteration 2403, batch 57\n",
      "\n",
      " Learning rate 0.000872648500358008, Model learning rate 0.0008726484957151115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizer iteration 2404, batch 58\n",
      "\n",
      " Learning rate 0.0008721109394657716, Model learning rate 0.0008721109479665756\n",
      " 59/391 [===>..........................] - ETA: 23s - loss: 0.9408 - acc: 0.7148\n",
      " Optimizer iteration 2405, batch 59\n",
      "\n",
      " Learning rate 0.0008715724127386971, Model learning rate 0.0008715724106878042\n",
      " 60/391 [===>..........................] - ETA: 23s - loss: 0.9400 - acc: 0.7148\n",
      " Optimizer iteration 2406, batch 60\n",
      "\n",
      " Learning rate 0.0008710329215745611, Model learning rate 0.0008710329420864582\n",
      " 61/391 [===>..........................] - ETA: 23s - loss: 0.9377 - acc: 0.7155\n",
      " Optimizer iteration 2407, batch 61\n",
      "\n",
      " Learning rate 0.000870492467373643, Model learning rate 0.0008704924839548767\n",
      " 62/391 [===>..........................] - ETA: 23s - loss: 0.9387 - acc: 0.7153\n",
      " Optimizer iteration 2408, batch 62\n",
      "\n",
      " Learning rate 0.0008699510515387221, Model learning rate 0.0008699510362930596\n",
      " 63/391 [===>..........................] - ETA: 23s - loss: 0.9397 - acc: 0.7152\n",
      " Optimizer iteration 2409, batch 63\n",
      "\n",
      " Learning rate 0.0008694086754750737, Model learning rate 0.0008694086573086679\n",
      " 64/391 [===>..........................] - ETA: 23s - loss: 0.9403 - acc: 0.7153\n",
      " Optimizer iteration 2410, batch 64\n",
      "\n",
      " Learning rate 0.0008688653405904651, Model learning rate 0.0008688653470017016\n",
      " 65/391 [===>..........................] - ETA: 23s - loss: 0.9402 - acc: 0.7156\n",
      " Optimizer iteration 2411, batch 65\n",
      "\n",
      " Learning rate 0.0008683210482951527, Model learning rate 0.0008683210471644998\n",
      "\n",
      " Optimizer iteration 2412, batch 66\n",
      "\n",
      " Learning rate 0.0008677758000018776, Model learning rate 0.0008677758160047233\n",
      " 67/391 [====>.........................] - ETA: 23s - loss: 0.9416 - acc: 0.7151\n",
      " Optimizer iteration 2413, batch 67\n",
      "\n",
      " Learning rate 0.0008672295971258625, Model learning rate 0.0008672295953147113\n",
      " 68/391 [====>.........................] - ETA: 23s - loss: 0.9434 - acc: 0.7150\n",
      " Optimizer iteration 2414, batch 68\n",
      "\n",
      " Learning rate 0.0008666824410848075, Model learning rate 0.0008666824433021247\n",
      "\n",
      " Optimizer iteration 2415, batch 69\n",
      "\n",
      " Learning rate 0.0008661343332988868, Model learning rate 0.0008661343599669635\n",
      " 70/391 [====>.........................] - ETA: 22s - loss: 0.9441 - acc: 0.7150\n",
      " Optimizer iteration 2416, batch 70\n",
      "\n",
      " Learning rate 0.0008655852751907451, Model learning rate 0.0008655852871015668\n",
      " 71/391 [====>.........................] - ETA: 22s - loss: 0.9454 - acc: 0.7141\n",
      " Optimizer iteration 2417, batch 71\n",
      "\n",
      " Learning rate 0.0008650352681854933, Model learning rate 0.0008650352829135954\n",
      " 72/391 [====>.........................] - ETA: 22s - loss: 0.9461 - acc: 0.7140\n",
      " Optimizer iteration 2418, batch 72\n",
      "\n",
      " Learning rate 0.0008644843137107057, Model learning rate 0.0008644842891953886\n",
      " 73/391 [====>.........................] - ETA: 22s - loss: 0.9449 - acc: 0.7146\n",
      " Optimizer iteration 2419, batch 73\n",
      "\n",
      " Learning rate 0.0008639324131964155, Model learning rate 0.000863932422362268\n",
      " 74/391 [====>.........................] - ETA: 22s - loss: 0.9453 - acc: 0.7143\n",
      " Optimizer iteration 2420, batch 74\n",
      "\n",
      " Learning rate 0.0008633795680751116, Model learning rate 0.0008633795659989119\n",
      " 75/391 [====>.........................] - ETA: 22s - loss: 0.9428 - acc: 0.7152\n",
      " Optimizer iteration 2421, batch 75\n",
      "\n",
      " Learning rate 0.0008628257797817344, Model learning rate 0.0008628257783129811\n",
      " 76/391 [====>.........................] - ETA: 22s - loss: 0.9410 - acc: 0.7159\n",
      " Optimizer iteration 2422, batch 76\n",
      "\n",
      " Learning rate 0.0008622710497536725, Model learning rate 0.0008622710593044758\n",
      " 77/391 [====>.........................] - ETA: 22s - loss: 0.9398 - acc: 0.7165\n",
      " Optimizer iteration 2423, batch 77\n",
      "\n",
      " Learning rate 0.0008617153794307588, Model learning rate 0.0008617153507657349\n",
      "\n",
      " Optimizer iteration 2424, batch 78\n",
      "\n",
      " Learning rate 0.000861158770255267, Model learning rate 0.0008611587691120803\n",
      " 79/391 [=====>........................] - ETA: 22s - loss: 0.9391 - acc: 0.7167\n",
      " Optimizer iteration 2425, batch 79\n",
      "\n",
      " Learning rate 0.0008606012236719073, Model learning rate 0.0008606011979281902\n",
      "\n",
      " Optimizer iteration 2426, batch 80\n",
      "\n",
      " Learning rate 0.0008600427411278233, Model learning rate 0.0008600427536293864\n",
      " 81/391 [=====>........................] - ETA: 22s - loss: 0.9403 - acc: 0.7164\n",
      " Optimizer iteration 2427, batch 81\n",
      "\n",
      " Learning rate 0.0008594833240725876, Model learning rate 0.0008594833198003471\n",
      " 82/391 [=====>........................] - ETA: 22s - loss: 0.9409 - acc: 0.7166\n",
      " Optimizer iteration 2428, batch 82\n",
      "\n",
      " Learning rate 0.0008589229739581988, Model learning rate 0.0008589229546487331\n",
      " 83/391 [=====>........................] - ETA: 22s - loss: 0.9412 - acc: 0.7160\n",
      " Optimizer iteration 2429, batch 83\n",
      "\n",
      " Learning rate 0.0008583616922390771, Model learning rate 0.0008583617163822055\n",
      " 84/391 [=====>........................] - ETA: 22s - loss: 0.9415 - acc: 0.7158\n",
      " Optimizer iteration 2430, batch 84\n",
      "\n",
      " Learning rate 0.0008577994803720606, Model learning rate 0.0008577994885854423\n",
      " 85/391 [=====>........................] - ETA: 22s - loss: 0.9415 - acc: 0.7165\n",
      " Optimizer iteration 2431, batch 85\n",
      "\n",
      " Learning rate 0.0008572363398164017, Model learning rate 0.0008572363294661045\n",
      " 86/391 [=====>........................] - ETA: 21s - loss: 0.9407 - acc: 0.7167\n",
      " Optimizer iteration 2432, batch 86\n",
      "\n",
      " Learning rate 0.0008566722720337634, Model learning rate 0.000856672297231853\n",
      " 87/391 [=====>........................] - ETA: 22s - loss: 0.9410 - acc: 0.7167\n",
      " Optimizer iteration 2433, batch 87\n",
      "\n",
      " Learning rate 0.0008561072784882155, Model learning rate 0.000856107275467366\n",
      " 88/391 [=====>........................] - ETA: 22s - loss: 0.9403 - acc: 0.7173\n",
      " Optimizer iteration 2434, batch 88\n",
      "\n",
      " Learning rate 0.0008555413606462301, Model learning rate 0.0008555413805879653\n",
      " 89/391 [=====>........................] - ETA: 21s - loss: 0.9405 - acc: 0.7173\n",
      " Optimizer iteration 2435, batch 89\n",
      "\n",
      " Learning rate 0.0008549745199766792, Model learning rate 0.000854974496178329\n",
      " 90/391 [=====>........................] - ETA: 21s - loss: 0.9406 - acc: 0.7174\n",
      " Optimizer iteration 2436, batch 90\n",
      "\n",
      " Learning rate 0.0008544067579508291, Model learning rate 0.000854406738653779\n",
      " 91/391 [=====>........................] - ETA: 22s - loss: 0.9406 - acc: 0.7176\n",
      " Optimizer iteration 2437, batch 91\n",
      "\n",
      " Learning rate 0.0008538380760423383, Model learning rate 0.0008538380498066545\n",
      " 92/391 [======>.......................] - ETA: 21s - loss: 0.9404 - acc: 0.7180\n",
      " Optimizer iteration 2438, batch 92\n",
      "\n",
      " Learning rate 0.0008532684757272526, Model learning rate 0.0008532684878446162\n",
      " 93/391 [======>.......................] - ETA: 21s - loss: 0.9400 - acc: 0.7182\n",
      " Optimizer iteration 2439, batch 93\n",
      "\n",
      " Learning rate 0.0008526979584840015, Model learning rate 0.0008526979363523424\n",
      " 94/391 [======>.......................] - ETA: 21s - loss: 0.9373 - acc: 0.7192\n",
      " Optimizer iteration 2440, batch 94\n",
      "\n",
      " Learning rate 0.0008521265257933948, Model learning rate 0.0008521265117451549\n",
      " 95/391 [======>.......................] - ETA: 21s - loss: 0.9366 - acc: 0.7192\n",
      " Optimizer iteration 2441, batch 95\n",
      "\n",
      " Learning rate 0.0008515541791386177, Model learning rate 0.0008515541558153927\n",
      " 96/391 [======>.......................] - ETA: 21s - loss: 0.9388 - acc: 0.7185\n",
      " Optimizer iteration 2442, batch 96\n",
      "\n",
      " Learning rate 0.0008509809200052286, Model learning rate 0.0008509809267707169\n",
      "\n",
      " Optimizer iteration 2443, batch 97\n",
      "\n",
      " Learning rate 0.0008504067498811532, Model learning rate 0.0008504067664034665\n",
      " 98/391 [======>.......................] - ETA: 21s - loss: 0.9379 - acc: 0.7184\n",
      " Optimizer iteration 2444, batch 98\n",
      "\n",
      " Learning rate 0.0008498316702566827, Model learning rate 0.0008498316747136414\n",
      " 99/391 [======>.......................] - ETA: 21s - loss: 0.9377 - acc: 0.7187\n",
      " Optimizer iteration 2445, batch 99\n",
      "\n",
      " Learning rate 0.0008492556826244686, Model learning rate 0.0008492557099089026\n",
      "100/391 [======>.......................] - ETA: 21s - loss: 0.9362 - acc: 0.7192\n",
      " Optimizer iteration 2446, batch 100\n",
      "\n",
      " Learning rate 0.0008486787884795188, Model learning rate 0.0008486788137815893\n",
      "101/391 [======>.......................] - ETA: 21s - loss: 0.9367 - acc: 0.7193\n",
      " Optimizer iteration 2447, batch 101\n",
      "\n",
      " Learning rate 0.0008481009893191947, Model learning rate 0.0008481009863317013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizer iteration 2448, batch 102\n",
      "\n",
      " Learning rate 0.0008475222866432064, Model learning rate 0.0008475222857668996\n",
      "103/391 [======>.......................] - ETA: 21s - loss: 0.9368 - acc: 0.7191\n",
      " Optimizer iteration 2449, batch 103\n",
      "\n",
      " Learning rate 0.0008469426819536092, Model learning rate 0.0008469426538795233\n",
      "104/391 [======>.......................] - ETA: 21s - loss: 0.9350 - acc: 0.7199\n",
      " Optimizer iteration 2450, batch 104\n",
      "\n",
      " Learning rate 0.0008463621767547997, Model learning rate 0.0008463621488772333\n",
      "105/391 [=======>......................] - ETA: 20s - loss: 0.9347 - acc: 0.7198\n",
      " Optimizer iteration 2451, batch 105\n",
      "\n",
      " Learning rate 0.0008457807725535116, Model learning rate 0.0008457807707600296\n",
      "\n",
      " Optimizer iteration 2452, batch 106\n",
      "\n",
      " Learning rate 0.0008451984708588121, Model learning rate 0.0008451984613202512\n",
      "107/391 [=======>......................] - ETA: 20s - loss: 0.9348 - acc: 0.7201\n",
      " Optimizer iteration 2453, batch 107\n",
      "\n",
      " Learning rate 0.0008446152731820983, Model learning rate 0.0008446152787655592\n",
      "108/391 [=======>......................] - ETA: 20s - loss: 0.9352 - acc: 0.7201\n",
      " Optimizer iteration 2454, batch 108\n",
      "\n",
      " Learning rate 0.0008440311810370921, Model learning rate 0.0008440311648882926\n",
      "109/391 [=======>......................] - ETA: 20s - loss: 0.9346 - acc: 0.7203\n",
      " Optimizer iteration 2455, batch 109\n",
      "\n",
      " Learning rate 0.0008434461959398376, Model learning rate 0.0008434461778961122\n",
      "110/391 [=======>......................] - ETA: 20s - loss: 0.9347 - acc: 0.7202\n",
      " Optimizer iteration 2456, batch 110\n",
      "\n",
      " Learning rate 0.0008428603194086966, Model learning rate 0.0008428603177890182\n",
      "111/391 [=======>......................] - ETA: 20s - loss: 0.9347 - acc: 0.7197\n",
      " Optimizer iteration 2457, batch 111\n",
      "\n",
      " Learning rate 0.0008422735529643444, Model learning rate 0.0008422735263593495\n",
      "112/391 [=======>......................] - ETA: 20s - loss: 0.9360 - acc: 0.7194\n",
      " Optimizer iteration 2458, batch 112\n",
      "\n",
      " Learning rate 0.0008416858981297663, Model learning rate 0.000841685920022428\n",
      "\n",
      " Optimizer iteration 2459, batch 113\n",
      "\n",
      " Learning rate 0.0008410973564302533, Model learning rate 0.000841097382362932\n",
      "114/391 [=======>......................] - ETA: 20s - loss: 0.9353 - acc: 0.7199\n",
      " Optimizer iteration 2460, batch 114\n",
      "\n",
      " Learning rate 0.0008405079293933986, Model learning rate 0.0008405079133808613\n",
      "115/391 [=======>......................] - ETA: 20s - loss: 0.9358 - acc: 0.7198\n",
      " Optimizer iteration 2461, batch 115\n",
      "\n",
      " Learning rate 0.000839917618549093, Model learning rate 0.0008399176294915378\n",
      "116/391 [=======>......................] - ETA: 20s - loss: 0.9355 - acc: 0.7202\n",
      " Optimizer iteration 2462, batch 116\n",
      "\n",
      " Learning rate 0.0008393264254295217, Model learning rate 0.0008393264142796397\n",
      "117/391 [=======>......................] - ETA: 20s - loss: 0.9346 - acc: 0.7206\n",
      " Optimizer iteration 2463, batch 117\n",
      "\n",
      " Learning rate 0.0008387343515691595, Model learning rate 0.0008387343259528279\n",
      "118/391 [========>.....................] - ETA: 20s - loss: 0.9339 - acc: 0.7211\n",
      " Optimizer iteration 2464, batch 118\n",
      "\n",
      " Learning rate 0.0008381413985047672, Model learning rate 0.0008381414227187634\n",
      "119/391 [========>.....................] - ETA: 20s - loss: 0.9323 - acc: 0.7216\n",
      " Optimizer iteration 2465, batch 119\n",
      "\n",
      " Learning rate 0.0008375475677753881, Model learning rate 0.0008375475881621242\n",
      "120/391 [========>.....................] - ETA: 20s - loss: 0.9336 - acc: 0.7209\n",
      " Optimizer iteration 2466, batch 120\n",
      "\n",
      " Learning rate 0.0008369528609223429, Model learning rate 0.0008369528804905713\n",
      "121/391 [========>.....................] - ETA: 19s - loss: 0.9326 - acc: 0.7216\n",
      " Optimizer iteration 2467, batch 121\n",
      "\n",
      " Learning rate 0.0008363572794892267, Model learning rate 0.0008363572997041047\n",
      "122/391 [========>.....................] - ETA: 19s - loss: 0.9339 - acc: 0.7211\n",
      " Optimizer iteration 2468, batch 122\n",
      "\n",
      " Learning rate 0.0008357608250219047, Model learning rate 0.0008357608458027244\n",
      "123/391 [========>.....................] - ETA: 19s - loss: 0.9330 - acc: 0.7213\n",
      " Optimizer iteration 2469, batch 123\n",
      "\n",
      " Learning rate 0.0008351634990685079, Model learning rate 0.0008351635187864304\n",
      "124/391 [========>.....................] - ETA: 19s - loss: 0.9322 - acc: 0.7215\n",
      " Optimizer iteration 2470, batch 124\n",
      "\n",
      " Learning rate 0.0008345653031794292, Model learning rate 0.0008345653186552227\n",
      "125/391 [========>.....................] - ETA: 19s - loss: 0.9321 - acc: 0.7214\n",
      " Optimizer iteration 2471, batch 125\n",
      "\n",
      " Learning rate 0.0008339662389073197, Model learning rate 0.0008339662454091012\n",
      "126/391 [========>.....................] - ETA: 19s - loss: 0.9323 - acc: 0.7215\n",
      " Optimizer iteration 2472, batch 126\n",
      "\n",
      " Learning rate 0.0008333663078070846, Model learning rate 0.0008333662990480661\n",
      "127/391 [========>.....................] - ETA: 19s - loss: 0.9321 - acc: 0.7217\n",
      " Optimizer iteration 2473, batch 127\n",
      "\n",
      " Learning rate 0.0008327655114358782, Model learning rate 0.0008327655377797782\n",
      "\n",
      " Optimizer iteration 2474, batch 128\n",
      "\n",
      " Learning rate 0.0008321638513531018, Model learning rate 0.0008321638451889157\n",
      "129/391 [========>.....................] - ETA: 19s - loss: 0.9318 - acc: 0.7214\n",
      " Optimizer iteration 2475, batch 129\n",
      "\n",
      " Learning rate 0.0008315613291203976, Model learning rate 0.0008315613376908004\n",
      "\n",
      " Optimizer iteration 2476, batch 130\n",
      "\n",
      " Learning rate 0.000830957946301646, Model learning rate 0.0008309579570777714\n",
      "131/391 [=========>....................] - ETA: 19s - loss: 0.9317 - acc: 0.7211\n",
      " Optimizer iteration 2477, batch 131\n",
      "\n",
      " Learning rate 0.0008303537044629611, Model learning rate 0.0008303537033498287\n",
      "132/391 [=========>....................] - ETA: 19s - loss: 0.9308 - acc: 0.7212\n",
      " Optimizer iteration 2478, batch 132\n",
      "\n",
      " Learning rate 0.0008297486051726862, Model learning rate 0.0008297485765069723\n",
      "133/391 [=========>....................] - ETA: 19s - loss: 0.9320 - acc: 0.7206\n",
      " Optimizer iteration 2479, batch 133\n",
      "\n",
      " Learning rate 0.0008291426500013908, Model learning rate 0.0008291426347568631\n",
      "134/391 [=========>....................] - ETA: 19s - loss: 0.9319 - acc: 0.7206\n",
      " Optimizer iteration 2480, batch 134\n",
      "\n",
      " Learning rate 0.0008285358405218655, Model learning rate 0.0008285358198918402\n",
      "135/391 [=========>....................] - ETA: 19s - loss: 0.9316 - acc: 0.7205\n",
      " Optimizer iteration 2481, batch 135\n",
      "\n",
      " Learning rate 0.0008279281783091181, Model learning rate 0.0008279281901195645\n",
      "136/391 [=========>....................] - ETA: 19s - loss: 0.9321 - acc: 0.7205\n",
      " Optimizer iteration 2482, batch 136\n",
      "\n",
      " Learning rate 0.0008273196649403702, Model learning rate 0.0008273196872323751\n",
      "137/391 [=========>....................] - ETA: 18s - loss: 0.9329 - acc: 0.7202\n",
      " Optimizer iteration 2483, batch 137\n",
      "\n",
      " Learning rate 0.0008267103019950528, Model learning rate 0.0008267103112302721\n",
      "\n",
      " Optimizer iteration 2484, batch 138\n",
      "\n",
      " Learning rate 0.000826100091054801, Model learning rate 0.0008261000621132553\n",
      "139/391 [=========>....................] - ETA: 18s - loss: 0.9324 - acc: 0.7205\n",
      " Optimizer iteration 2485, batch 139\n",
      "\n",
      " Learning rate 0.0008254890337034519, Model learning rate 0.0008254890562966466\n",
      "\n",
      " Optimizer iteration 2486, batch 140\n",
      "\n",
      " Learning rate 0.0008248771315270392, Model learning rate 0.0008248771191574633\n",
      "141/391 [=========>....................] - ETA: 18s - loss: 0.9349 - acc: 0.7197\n",
      " Optimizer iteration 2487, batch 141\n",
      "\n",
      " Learning rate 0.0008242643861137891, Model learning rate 0.0008242643671110272\n",
      "142/391 [=========>....................] - ETA: 18s - loss: 0.9350 - acc: 0.7196\n",
      " Optimizer iteration 2488, batch 142\n",
      "\n",
      " Learning rate 0.0008236507990541169, Model learning rate 0.0008236508001573384\n",
      "143/391 [=========>....................] - ETA: 18s - loss: 0.9345 - acc: 0.7195\n",
      " Optimizer iteration 2489, batch 143\n",
      "\n",
      " Learning rate 0.0008230363719406223, Model learning rate 0.0008230363600887358\n",
      "144/391 [==========>...................] - ETA: 18s - loss: 0.9352 - acc: 0.7192\n",
      " Optimizer iteration 2490, batch 144\n",
      "\n",
      " Learning rate 0.0008224211063680853, Model learning rate 0.0008224211051128805\n",
      "145/391 [==========>...................] - ETA: 18s - loss: 0.9354 - acc: 0.7189\n",
      " Optimizer iteration 2491, batch 145\n",
      "\n",
      " Learning rate 0.0008218050039334624, Model learning rate 0.0008218049770221114\n",
      "146/391 [==========>...................] - ETA: 18s - loss: 0.9357 - acc: 0.7190\n",
      " Optimizer iteration 2492, batch 146\n",
      "\n",
      " Learning rate 0.0008211880662358817, Model learning rate 0.0008211880922317505\n",
      "147/391 [==========>...................] - ETA: 18s - loss: 0.9345 - acc: 0.7197\n",
      " Optimizer iteration 2493, batch 147\n",
      "\n",
      " Learning rate 0.00082057029487664, Model learning rate 0.0008205702761188149\n",
      "\n",
      " Optimizer iteration 2494, batch 148\n",
      "\n",
      " Learning rate 0.0008199516914591976, Model learning rate 0.0008199517033062875\n",
      "149/391 [==========>...................] - ETA: 18s - loss: 0.9342 - acc: 0.7193\n",
      " Optimizer iteration 2495, batch 149\n",
      "\n",
      " Learning rate 0.0008193322575891739, Model learning rate 0.0008193322573788464\n",
      "\n",
      " Optimizer iteration 2496, batch 150\n",
      "\n",
      " Learning rate 0.0008187119948743449, Model learning rate 0.0008187119965441525\n",
      "151/391 [==========>...................] - ETA: 17s - loss: 0.9341 - acc: 0.7191\n",
      " Optimizer iteration 2497, batch 151\n",
      "\n",
      " Learning rate 0.000818090904924637, Model learning rate 0.0008180909208022058\n",
      "\n",
      " Optimizer iteration 2498, batch 152\n",
      "\n",
      " Learning rate 0.0008174689893521239, Model learning rate 0.0008174689719453454\n",
      "153/391 [==========>...................] - ETA: 17s - loss: 0.9333 - acc: 0.7190\n",
      " Optimizer iteration 2499, batch 153\n",
      "\n",
      " Learning rate 0.0008168462497710226, Model learning rate 0.0008168462663888931\n",
      "154/391 [==========>...................] - ETA: 17s - loss: 0.9336 - acc: 0.7188\n",
      " Optimizer iteration 2500, batch 154\n",
      "\n",
      " Learning rate 0.0008162226877976886, Model learning rate 0.0008162226877175272\n",
      "155/391 [==========>...................] - ETA: 17s - loss: 0.9345 - acc: 0.7189\n",
      " Optimizer iteration 2501, batch 155\n",
      "\n",
      " Learning rate 0.0008155983050506122, Model learning rate 0.0008155982941389084\n",
      "156/391 [==========>...................] - ETA: 17s - loss: 0.9341 - acc: 0.7192\n",
      " Optimizer iteration 2502, batch 156\n",
      "\n",
      " Learning rate 0.0008149731031504135, Model learning rate 0.0008149730856530368\n",
      "157/391 [===========>..................] - ETA: 17s - loss: 0.9342 - acc: 0.7192\n",
      " Optimizer iteration 2503, batch 157\n",
      "\n",
      " Learning rate 0.0008143470837198393, Model learning rate 0.0008143470622599125\n",
      "158/391 [===========>..................] - ETA: 17s - loss: 0.9329 - acc: 0.7198\n",
      " Optimizer iteration 2504, batch 158\n",
      "\n",
      " Learning rate 0.0008137202483837583, Model learning rate 0.0008137202239595354\n",
      "159/391 [===========>..................] - ETA: 17s - loss: 0.9329 - acc: 0.7200\n",
      " Optimizer iteration 2505, batch 159\n",
      "\n",
      " Learning rate 0.0008130925987691568, Model learning rate 0.0008130925707519054\n",
      "160/391 [===========>..................] - ETA: 17s - loss: 0.9340 - acc: 0.7194\n",
      " Optimizer iteration 2506, batch 160\n",
      "\n",
      " Learning rate 0.0008124641365051346, Model learning rate 0.0008124641608446836\n",
      "161/391 [===========>..................] - ETA: 17s - loss: 0.9336 - acc: 0.7196\n",
      " Optimizer iteration 2507, batch 161\n",
      "\n",
      " Learning rate 0.0008118348632229007, Model learning rate 0.0008118348778225482\n",
      "\n",
      " Optimizer iteration 2508, batch 162\n",
      "\n",
      " Learning rate 0.0008112047805557692, Model learning rate 0.0008112047798931599\n",
      "163/391 [===========>..................] - ETA: 17s - loss: 0.9340 - acc: 0.7199\n",
      " Optimizer iteration 2509, batch 163\n",
      "\n",
      " Learning rate 0.0008105738901391552, Model learning rate 0.0008105738670565188\n",
      "164/391 [===========>..................] - ETA: 17s - loss: 0.9352 - acc: 0.7197\n",
      " Optimizer iteration 2510, batch 164\n",
      "\n",
      " Learning rate 0.0008099421936105702, Model learning rate 0.0008099421975202858\n",
      "165/391 [===========>..................] - ETA: 16s - loss: 0.9347 - acc: 0.7201\n",
      " Optimizer iteration 2511, batch 165\n",
      "\n",
      " Learning rate 0.0008093096926096177, Model learning rate 0.0008093097130768001\n",
      "166/391 [===========>..................] - ETA: 16s - loss: 0.9333 - acc: 0.7206\n",
      " Optimizer iteration 2512, batch 166\n",
      "\n",
      " Learning rate 0.00080867638877799, Model learning rate 0.0008086764137260616\n",
      "167/391 [===========>..................] - ETA: 16s - loss: 0.9328 - acc: 0.7211\n",
      " Optimizer iteration 2513, batch 167\n",
      "\n",
      " Learning rate 0.0008080422837594627, Model learning rate 0.0008080422994680703\n",
      "\n",
      " Optimizer iteration 2514, batch 168\n",
      "\n",
      " Learning rate 0.0008074073791998906, Model learning rate 0.0008074073703028262\n",
      "169/391 [===========>..................] - ETA: 16s - loss: 0.9325 - acc: 0.7211\n",
      " Optimizer iteration 2515, batch 169\n",
      "\n",
      " Learning rate 0.0008067716767472045, Model learning rate 0.0008067716844379902\n",
      "170/391 [============>.................] - ETA: 16s - loss: 0.9322 - acc: 0.7211\n",
      " Optimizer iteration 2516, batch 170\n",
      "\n",
      " Learning rate 0.0008061351780514057, Model learning rate 0.0008061351836659014\n",
      "171/391 [============>.................] - ETA: 16s - loss: 0.9321 - acc: 0.7213\n",
      " Optimizer iteration 2517, batch 171\n",
      "\n",
      " Learning rate 0.0008054978847645622, Model learning rate 0.0008054978679865599\n",
      "172/391 [============>.................] - ETA: 16s - loss: 0.9329 - acc: 0.7210\n",
      " Optimizer iteration 2518, batch 172\n",
      "\n",
      " Learning rate 0.0008048597985408047, Model learning rate 0.0008048597956076264\n",
      "173/391 [============>.................] - ETA: 16s - loss: 0.9331 - acc: 0.7210\n",
      " Optimizer iteration 2519, batch 173\n",
      "\n",
      " Learning rate 0.0008042209210363216, Model learning rate 0.0008042209083214402\n",
      "174/391 [============>.................] - ETA: 16s - loss: 0.9326 - acc: 0.7214\n",
      " Optimizer iteration 2520, batch 174\n",
      "\n",
      " Learning rate 0.0008035812539093556, Model learning rate 0.0008035812643356621\n",
      "175/391 [============>.................] - ETA: 16s - loss: 0.9328 - acc: 0.7211\n",
      " Optimizer iteration 2521, batch 175\n",
      "\n",
      " Learning rate 0.0008029407988201985, Model learning rate 0.0008029408054426312\n",
      "176/391 [============>.................] - ETA: 16s - loss: 0.9332 - acc: 0.7209\n",
      " Optimizer iteration 2522, batch 176\n",
      "\n",
      " Learning rate 0.0008022995574311875, Model learning rate 0.0008022995316423476\n",
      "177/391 [============>.................] - ETA: 16s - loss: 0.9343 - acc: 0.7206\n",
      " Optimizer iteration 2523, batch 177\n",
      "\n",
      " Learning rate 0.0008016575314067005, Model learning rate 0.0008016575593501329\n",
      "178/391 [============>.................] - ETA: 15s - loss: 0.9341 - acc: 0.7207\n",
      " Optimizer iteration 2524, batch 178\n",
      "\n",
      " Learning rate 0.0008010147224131523, Model learning rate 0.0008010147139430046\n",
      "179/391 [============>.................] - ETA: 15s - loss: 0.9343 - acc: 0.7208\n",
      " Optimizer iteration 2525, batch 179\n",
      "\n",
      " Learning rate 0.0008003711321189895, Model learning rate 0.0008003711118362844\n",
      "180/391 [============>.................] - ETA: 15s - loss: 0.9344 - acc: 0.7207\n",
      " Optimizer iteration 2526, batch 180\n",
      "\n",
      " Learning rate 0.000799726762194687, Model learning rate 0.0007997267530299723\n",
      "181/391 [============>.................] - ETA: 15s - loss: 0.9337 - acc: 0.7210\n",
      " Optimizer iteration 2527, batch 181\n",
      "\n",
      " Learning rate 0.0007990816143127431, Model learning rate 0.0007990816375240684\n",
      "182/391 [============>.................] - ETA: 15s - loss: 0.9330 - acc: 0.7212\n",
      " Optimizer iteration 2528, batch 182\n",
      "\n",
      " Learning rate 0.0007984356901476755, Model learning rate 0.0007984357071109116\n",
      "183/391 [=============>................] - ETA: 15s - loss: 0.9336 - acc: 0.7212\n",
      " Optimizer iteration 2529, batch 183\n",
      "\n",
      " Learning rate 0.0007977889913760163, Model learning rate 0.000797789019998163\n",
      "\n",
      " Optimizer iteration 2530, batch 184\n",
      "\n",
      " Learning rate 0.0007971415196763087, Model learning rate 0.0007971415179781616\n",
      "185/391 [=============>................] - ETA: 15s - loss: 0.9334 - acc: 0.7212\n",
      " Optimizer iteration 2531, batch 185\n",
      "\n",
      " Learning rate 0.0007964932767291019, Model learning rate 0.0007964932592585683\n",
      "186/391 [=============>................] - ETA: 15s - loss: 0.9337 - acc: 0.7211\n",
      " Optimizer iteration 2532, batch 186\n",
      "\n",
      " Learning rate 0.0007958442642169468, Model learning rate 0.0007958442438393831\n",
      "187/391 [=============>................] - ETA: 15s - loss: 0.9339 - acc: 0.7211\n",
      " Optimizer iteration 2533, batch 187\n",
      "\n",
      " Learning rate 0.0007951944838243916, Model learning rate 0.0007951944717206061\n",
      "188/391 [=============>................] - ETA: 15s - loss: 0.9334 - acc: 0.7214\n",
      " Optimizer iteration 2534, batch 188\n",
      "\n",
      " Learning rate 0.0007945439372379782, Model learning rate 0.0007945439429022372\n",
      "189/391 [=============>................] - ETA: 15s - loss: 0.9338 - acc: 0.7212\n",
      " Optimizer iteration 2535, batch 189\n",
      "\n",
      " Learning rate 0.0007938926261462366, Model learning rate 0.0007938925991766155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190/391 [=============>................] - ETA: 15s - loss: 0.9336 - acc: 0.7213\n",
      " Optimizer iteration 2536, batch 190\n",
      "\n",
      " Learning rate 0.0007932405522396812, Model learning rate 0.0007932405569590628\n",
      "191/391 [=============>................] - ETA: 15s - loss: 0.9345 - acc: 0.7209\n",
      " Optimizer iteration 2537, batch 191\n",
      "\n",
      " Learning rate 0.0007925877172108067, Model learning rate 0.0007925876998342574\n",
      "192/391 [=============>................] - ETA: 15s - loss: 0.9337 - acc: 0.7212\n",
      " Optimizer iteration 2538, batch 192\n",
      "\n",
      " Learning rate 0.0007919341227540828, Model learning rate 0.000791934144217521\n",
      "193/391 [=============>................] - ETA: 14s - loss: 0.9346 - acc: 0.7208\n",
      " Optimizer iteration 2539, batch 193\n",
      "\n",
      " Learning rate 0.0007912797705659507, Model learning rate 0.0007912797736935318\n",
      "194/391 [=============>................] - ETA: 14s - loss: 0.9342 - acc: 0.7210\n",
      " Optimizer iteration 2540, batch 194\n",
      "\n",
      " Learning rate 0.0007906246623448183, Model learning rate 0.0007906246464699507\n",
      "195/391 [=============>................] - ETA: 14s - loss: 0.9344 - acc: 0.7209\n",
      " Optimizer iteration 2541, batch 195\n",
      "\n",
      " Learning rate 0.0007899687997910558, Model learning rate 0.0007899688207544386\n",
      "196/391 [==============>...............] - ETA: 14s - loss: 0.9338 - acc: 0.7212\n",
      " Optimizer iteration 2542, batch 196\n",
      "\n",
      " Learning rate 0.0007893121846069913, Model learning rate 0.0007893121801316738\n",
      "\n",
      " Optimizer iteration 2543, batch 197\n",
      "\n",
      " Learning rate 0.0007886548184969063, Model learning rate 0.000788654841016978\n",
      "198/391 [==============>...............] - ETA: 14s - loss: 0.9335 - acc: 0.7214\n",
      " Optimizer iteration 2544, batch 198\n",
      "\n",
      " Learning rate 0.0007879967031670313, Model learning rate 0.0007879966869950294\n",
      "199/391 [==============>...............] - ETA: 14s - loss: 0.9330 - acc: 0.7216\n",
      " Optimizer iteration 2545, batch 199\n",
      "\n",
      " Learning rate 0.0007873378403255419, Model learning rate 0.0007873378344811499\n",
      "200/391 [==============>...............] - ETA: 14s - loss: 0.9318 - acc: 0.7223\n",
      " Optimizer iteration 2546, batch 200\n",
      "\n",
      " Learning rate 0.0007866782316825535, Model learning rate 0.0007866782252676785\n",
      "\n",
      " Optimizer iteration 2547, batch 201\n",
      "\n",
      " Learning rate 0.0007860178789501172, Model learning rate 0.0007860178593546152\n",
      "202/391 [==============>...............] - ETA: 14s - loss: 0.9312 - acc: 0.7224\n",
      " Optimizer iteration 2548, batch 202\n",
      "\n",
      " Learning rate 0.000785356783842216, Model learning rate 0.000785356794949621\n",
      "203/391 [==============>...............] - ETA: 14s - loss: 0.9308 - acc: 0.7226\n",
      " Optimizer iteration 2549, batch 203\n",
      "\n",
      " Learning rate 0.0007846949480747588, Model learning rate 0.0007846949738450348\n",
      "204/391 [==============>...............] - ETA: 14s - loss: 0.9309 - acc: 0.7223\n",
      " Optimizer iteration 2550, batch 204\n",
      "\n",
      " Learning rate 0.0007840323733655779, Model learning rate 0.0007840323960408568\n",
      "\n",
      " Optimizer iteration 2551, batch 205\n",
      "\n",
      " Learning rate 0.0007833690614344231, Model learning rate 0.000783369061537087\n",
      "206/391 [==============>...............] - ETA: 13s - loss: 0.9302 - acc: 0.7225\n",
      " Optimizer iteration 2552, batch 206\n",
      "\n",
      " Learning rate 0.0007827050140029577, Model learning rate 0.0007827050285413861\n",
      "207/391 [==============>...............] - ETA: 13s - loss: 0.9294 - acc: 0.7228\n",
      " Optimizer iteration 2553, batch 207\n",
      "\n",
      " Learning rate 0.0007820402327947542, Model learning rate 0.0007820402388460934\n",
      "208/391 [==============>...............] - ETA: 13s - loss: 0.9291 - acc: 0.7228\n",
      " Optimizer iteration 2554, batch 208\n",
      "\n",
      " Learning rate 0.0007813747195352895, Model learning rate 0.0007813746924512088\n",
      "209/391 [===============>..............] - ETA: 13s - loss: 0.9293 - acc: 0.7227\n",
      " Optimizer iteration 2555, batch 209\n",
      "\n",
      " Learning rate 0.0007807084759519405, Model learning rate 0.0007807084475643933\n",
      "210/391 [===============>..............] - ETA: 13s - loss: 0.9285 - acc: 0.7231\n",
      " Optimizer iteration 2556, batch 210\n",
      "\n",
      " Learning rate 0.0007800415037739801, Model learning rate 0.0007800415041856468\n",
      "211/391 [===============>..............] - ETA: 13s - loss: 0.9289 - acc: 0.7231\n",
      " Optimizer iteration 2557, batch 211\n",
      "\n",
      " Learning rate 0.0007793738047325717, Model learning rate 0.0007793738041073084\n",
      "212/391 [===============>..............] - ETA: 13s - loss: 0.9289 - acc: 0.7231\n",
      " Optimizer iteration 2558, batch 212\n",
      "\n",
      " Learning rate 0.0007787053805607659, Model learning rate 0.000778705405537039\n",
      "213/391 [===============>..............] - ETA: 13s - loss: 0.9286 - acc: 0.7233\n",
      " Optimizer iteration 2559, batch 213\n",
      "\n",
      " Learning rate 0.0007780362329934951, Model learning rate 0.0007780362502671778\n",
      "214/391 [===============>..............] - ETA: 13s - loss: 0.9280 - acc: 0.7237\n",
      " Optimizer iteration 2560, batch 214\n",
      "\n",
      " Learning rate 0.0007773663637675694, Model learning rate 0.0007773663382977247\n",
      "215/391 [===============>..............] - ETA: 13s - loss: 0.9277 - acc: 0.7238\n",
      " Optimizer iteration 2561, batch 215\n",
      "\n",
      " Learning rate 0.000776695774621672, Model learning rate 0.0007766957860440016\n",
      "216/391 [===============>..............] - ETA: 13s - loss: 0.9279 - acc: 0.7237\n",
      " Optimizer iteration 2562, batch 216\n",
      "\n",
      " Learning rate 0.0007760244672963548, Model learning rate 0.0007760244770906866\n",
      "217/391 [===============>..............] - ETA: 13s - loss: 0.9280 - acc: 0.7236\n",
      " Optimizer iteration 2563, batch 217\n",
      "\n",
      " Learning rate 0.0007753524435340334, Model learning rate 0.0007753524696454406\n",
      "218/391 [===============>..............] - ETA: 13s - loss: 0.9270 - acc: 0.7241\n",
      " Optimizer iteration 2564, batch 218\n",
      "\n",
      " Learning rate 0.0007746797050789834, Model learning rate 0.0007746797055006027\n",
      "219/391 [===============>..............] - ETA: 13s - loss: 0.9262 - acc: 0.7241\n",
      " Optimizer iteration 2565, batch 219\n",
      "\n",
      " Learning rate 0.0007740062536773351, Model learning rate 0.0007740062428638339\n",
      "220/391 [===============>..............] - ETA: 12s - loss: 0.9260 - acc: 0.7240\n",
      " Optimizer iteration 2566, batch 220\n",
      "\n",
      " Learning rate 0.0007733320910770693, Model learning rate 0.0007733320817351341\n",
      "\n",
      " Optimizer iteration 2567, batch 221\n",
      "\n",
      " Learning rate 0.0007726572190280134, Model learning rate 0.0007726572221145034\n",
      "222/391 [================>.............] - ETA: 12s - loss: 0.9256 - acc: 0.7243\n",
      " Optimizer iteration 2568, batch 222\n",
      "\n",
      " Learning rate 0.0007719816392818353, Model learning rate 0.0007719816640019417\n",
      "223/391 [================>.............] - ETA: 12s - loss: 0.9261 - acc: 0.7240\n",
      " Optimizer iteration 2569, batch 223\n",
      "\n",
      " Learning rate 0.0007713053535920402, Model learning rate 0.0007713053491897881\n",
      "224/391 [================>.............] - ETA: 12s - loss: 0.9259 - acc: 0.7241\n",
      " Optimizer iteration 2570, batch 224\n",
      "\n",
      " Learning rate 0.0007706283637139657, Model learning rate 0.0007706283358857036\n",
      "\n",
      " Optimizer iteration 2571, batch 225\n",
      "\n",
      " Learning rate 0.000769950671404777, Model learning rate 0.000769950682297349\n",
      "226/391 [================>.............] - ETA: 12s - loss: 0.9255 - acc: 0.7241\n",
      " Optimizer iteration 2572, batch 226\n",
      "\n",
      " Learning rate 0.0007692722784234624, Model learning rate 0.0007692722720094025\n",
      "227/391 [================>.............] - ETA: 12s - loss: 0.9255 - acc: 0.7242\n",
      " Optimizer iteration 2573, batch 227\n",
      "\n",
      " Learning rate 0.0007685931865308292, Model learning rate 0.0007685931632295251\n",
      "228/391 [================>.............] - ETA: 12s - loss: 0.9252 - acc: 0.7244\n",
      " Optimizer iteration 2574, batch 228\n",
      "\n",
      " Learning rate 0.0007679133974894983, Model learning rate 0.0007679134141653776\n",
      "229/391 [================>.............] - ETA: 12s - loss: 0.9252 - acc: 0.7244\n",
      " Optimizer iteration 2575, batch 229\n",
      "\n",
      " Learning rate 0.0007672329130639005, Model learning rate 0.0007672329084016383\n",
      "230/391 [================>.............] - ETA: 12s - loss: 0.9250 - acc: 0.7246\n",
      " Optimizer iteration 2576, batch 230\n",
      "\n",
      " Learning rate 0.0007665517350202715, Model learning rate 0.0007665517623536289\n",
      "231/391 [================>.............] - ETA: 12s - loss: 0.9246 - acc: 0.7248\n",
      " Optimizer iteration 2577, batch 231\n",
      "\n",
      " Learning rate 0.0007658698651266467, Model learning rate 0.0007658698596060276\n",
      "232/391 [================>.............] - ETA: 12s - loss: 0.9239 - acc: 0.7251\n",
      " Optimizer iteration 2578, batch 232\n",
      "\n",
      " Learning rate 0.000765187305152858, Model learning rate 0.0007651873165741563\n",
      "\n",
      " Optimizer iteration 2579, batch 233\n",
      "\n",
      " Learning rate 0.0007645040568705282, Model learning rate 0.000764504075050354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234/391 [================>.............] - ETA: 11s - loss: 0.9237 - acc: 0.7253\n",
      " Optimizer iteration 2580, batch 234\n",
      "\n",
      " Learning rate 0.0007638201220530663, Model learning rate 0.0007638201350346208\n",
      "235/391 [=================>............] - ETA: 11s - loss: 0.9236 - acc: 0.7255\n",
      " Optimizer iteration 2581, batch 235\n",
      "\n",
      " Learning rate 0.0007631355024756639, Model learning rate 0.0007631354965269566\n",
      "236/391 [=================>............] - ETA: 11s - loss: 0.9239 - acc: 0.7254\n",
      " Optimizer iteration 2582, batch 236\n",
      "\n",
      " Learning rate 0.0007624501999152893, Model learning rate 0.0007624502177350223\n",
      "237/391 [=================>............] - ETA: 11s - loss: 0.9229 - acc: 0.7257\n",
      " Optimizer iteration 2583, batch 237\n",
      "\n",
      " Learning rate 0.0007617642161506837, Model learning rate 0.0007617642404511571\n",
      "238/391 [=================>............] - ETA: 11s - loss: 0.9230 - acc: 0.7256\n",
      " Optimizer iteration 2584, batch 238\n",
      "\n",
      " Learning rate 0.0007610775529623568, Model learning rate 0.0007610775646753609\n",
      "\n",
      " Optimizer iteration 2585, batch 239\n",
      "\n",
      " Learning rate 0.0007603902121325811, Model learning rate 0.0007603901904076338\n",
      "240/391 [=================>............] - ETA: 11s - loss: 0.9224 - acc: 0.7258\n",
      " Optimizer iteration 2586, batch 240\n",
      "\n",
      " Learning rate 0.0007597021954453886, Model learning rate 0.0007597021758556366\n",
      "241/391 [=================>............] - ETA: 11s - loss: 0.9222 - acc: 0.7261\n",
      " Optimizer iteration 2587, batch 241\n",
      "\n",
      " Learning rate 0.0007590135046865651, Model learning rate 0.0007590135210193694\n",
      "\n",
      " Optimizer iteration 2588, batch 242\n",
      "\n",
      " Learning rate 0.0007583241416436461, Model learning rate 0.0007583241676911712\n",
      "243/391 [=================>............] - ETA: 11s - loss: 0.9219 - acc: 0.7260\n",
      " Optimizer iteration 2589, batch 243\n",
      "\n",
      " Learning rate 0.0007576341081059123, Model learning rate 0.000757634115871042\n",
      "244/391 [=================>............] - ETA: 11s - loss: 0.9217 - acc: 0.7260\n",
      " Optimizer iteration 2590, batch 244\n",
      "\n",
      " Learning rate 0.0007569434058643844, Model learning rate 0.0007569434237666428\n",
      "245/391 [=================>............] - ETA: 11s - loss: 0.9221 - acc: 0.7259\n",
      " Optimizer iteration 2591, batch 245\n",
      "\n",
      " Learning rate 0.0007562520367118186, Model learning rate 0.0007562520331703126\n",
      "246/391 [=================>............] - ETA: 11s - loss: 0.9223 - acc: 0.7260\n",
      " Optimizer iteration 2592, batch 246\n",
      "\n",
      " Learning rate 0.0007555600024427027, Model learning rate 0.0007555600022897124\n",
      "247/391 [=================>............] - ETA: 10s - loss: 0.9224 - acc: 0.7260\n",
      " Optimizer iteration 2593, batch 247\n",
      "\n",
      " Learning rate 0.0007548673048532504, Model learning rate 0.0007548673311248422\n",
      "248/391 [==================>...........] - ETA: 10s - loss: 0.9228 - acc: 0.7259\n",
      " Optimizer iteration 2594, batch 248\n",
      "\n",
      " Learning rate 0.000754173945741397, Model learning rate 0.0007541739614680409\n",
      "249/391 [==================>...........] - ETA: 10s - loss: 0.9222 - acc: 0.7262\n",
      " Optimizer iteration 2595, batch 249\n",
      "\n",
      " Learning rate 0.0007534799269067953, Model learning rate 0.0007534799515269697\n",
      "250/391 [==================>...........] - ETA: 10s - loss: 0.9216 - acc: 0.7265\n",
      " Optimizer iteration 2596, batch 250\n",
      "\n",
      " Learning rate 0.0007527852501508099, Model learning rate 0.0007527852430939674\n",
      "251/391 [==================>...........] - ETA: 10s - loss: 0.9215 - acc: 0.7266\n",
      " Optimizer iteration 2597, batch 251\n",
      "\n",
      " Learning rate 0.0007520899172765136, Model learning rate 0.0007520898943766952\n",
      "252/391 [==================>...........] - ETA: 10s - loss: 0.9217 - acc: 0.7265\n",
      " Optimizer iteration 2598, batch 252\n",
      "\n",
      " Learning rate 0.0007513939300886816, Model learning rate 0.0007513939053751528\n",
      "253/391 [==================>...........] - ETA: 10s - loss: 0.9214 - acc: 0.7267\n",
      " Optimizer iteration 2599, batch 253\n",
      "\n",
      " Learning rate 0.0007506972903937878, Model learning rate 0.0007506972760893404\n",
      "254/391 [==================>...........] - ETA: 10s - loss: 0.9209 - acc: 0.7268\n",
      " Optimizer iteration 2600, batch 254\n",
      "\n",
      " Learning rate 0.00075, Model learning rate 0.000750000006519258\n",
      "255/391 [==================>...........] - ETA: 10s - loss: 0.9212 - acc: 0.7269\n",
      " Optimizer iteration 2601, batch 255\n",
      "\n",
      " Learning rate 0.0007493020607171743, Model learning rate 0.0007493020384572446\n",
      "256/391 [==================>...........] - ETA: 10s - loss: 0.9214 - acc: 0.7268\n",
      " Optimizer iteration 2602, batch 256\n",
      "\n",
      " Learning rate 0.0007486034743568511, Model learning rate 0.0007486034883186221\n",
      "257/391 [==================>...........] - ETA: 10s - loss: 0.9209 - acc: 0.7269\n",
      " Optimizer iteration 2603, batch 257\n",
      "\n",
      " Learning rate 0.0007479042427322508, Model learning rate 0.0007479042396880686\n",
      "258/391 [==================>...........] - ETA: 10s - loss: 0.9209 - acc: 0.7270\n",
      " Optimizer iteration 2604, batch 258\n",
      "\n",
      " Learning rate 0.0007472043676582685, Model learning rate 0.0007472043507732451\n",
      "259/391 [==================>...........] - ETA: 10s - loss: 0.9206 - acc: 0.7270\n",
      " Optimizer iteration 2605, batch 259\n",
      "\n",
      " Learning rate 0.0007465038509514688, Model learning rate 0.0007465038797818124\n",
      "260/391 [==================>...........] - ETA: 9s - loss: 0.9202 - acc: 0.7270 \n",
      " Optimizer iteration 2606, batch 260\n",
      "\n",
      " Learning rate 0.0007458026944300824, Model learning rate 0.0007458027102984488\n",
      "261/391 [===================>..........] - ETA: 9s - loss: 0.9200 - acc: 0.7272\n",
      " Optimizer iteration 2607, batch 261\n",
      "\n",
      " Learning rate 0.0007451008999140005, Model learning rate 0.0007451009005308151\n",
      "262/391 [===================>..........] - ETA: 9s - loss: 0.9200 - acc: 0.7271\n",
      " Optimizer iteration 2608, batch 262\n",
      "\n",
      " Learning rate 0.0007443984692247701, Model learning rate 0.0007443984504789114\n",
      "263/391 [===================>..........] - ETA: 9s - loss: 0.9200 - acc: 0.7269\n",
      " Optimizer iteration 2609, batch 263\n",
      "\n",
      " Learning rate 0.0007436954041855892, Model learning rate 0.0007436954183503985\n",
      "264/391 [===================>..........] - ETA: 9s - loss: 0.9203 - acc: 0.7268\n",
      " Optimizer iteration 2610, batch 264\n",
      "\n",
      " Learning rate 0.000742991706621303, Model learning rate 0.0007429916877299547\n",
      "265/391 [===================>..........] - ETA: 9s - loss: 0.9206 - acc: 0.7267\n",
      " Optimizer iteration 2611, batch 265\n",
      "\n",
      " Learning rate 0.0007422873783583981, Model learning rate 0.0007422873750329018\n",
      "\n",
      " Optimizer iteration 2612, batch 266\n",
      "\n",
      " Learning rate 0.0007415824212249977, Model learning rate 0.0007415824220515788\n",
      "267/391 [===================>..........] - ETA: 9s - loss: 0.9206 - acc: 0.7267\n",
      " Optimizer iteration 2613, batch 267\n",
      "\n",
      " Learning rate 0.0007408768370508576, Model learning rate 0.0007408768287859857\n",
      "268/391 [===================>..........] - ETA: 9s - loss: 0.9204 - acc: 0.7269\n",
      " Optimizer iteration 2614, batch 268\n",
      "\n",
      " Learning rate 0.0007401706276673615, Model learning rate 0.0007401706534437835\n",
      "269/391 [===================>..........] - ETA: 9s - loss: 0.9207 - acc: 0.7267\n",
      " Optimizer iteration 2615, batch 269\n",
      "\n",
      " Learning rate 0.0007394637949075153, Model learning rate 0.0007394637796096504\n",
      "270/391 [===================>..........] - ETA: 9s - loss: 0.9201 - acc: 0.7269\n",
      " Optimizer iteration 2616, batch 270\n",
      "\n",
      " Learning rate 0.0007387563406059432, Model learning rate 0.0007387563236989081\n",
      "271/391 [===================>..........] - ETA: 9s - loss: 0.9202 - acc: 0.7268\n",
      " Optimizer iteration 2617, batch 271\n",
      "\n",
      " Learning rate 0.0007380482665988826, Model learning rate 0.0007380482857115567\n",
      "272/391 [===================>..........] - ETA: 9s - loss: 0.9198 - acc: 0.7269\n",
      " Optimizer iteration 2618, batch 272\n",
      "\n",
      " Learning rate 0.0007373395747241791, Model learning rate 0.0007373395492322743\n",
      "273/391 [===================>..........] - ETA: 8s - loss: 0.9199 - acc: 0.7269\n",
      " Optimizer iteration 2619, batch 273\n",
      "\n",
      " Learning rate 0.0007366302668212826, Model learning rate 0.0007366302888840437\n",
      "\n",
      " Optimizer iteration 2620, batch 274\n",
      "\n",
      " Learning rate 0.000735920344731241, Model learning rate 0.0007359203300438821\n",
      "275/391 [====================>.........] - ETA: 8s - loss: 0.9200 - acc: 0.7270\n",
      " Optimizer iteration 2621, batch 275\n",
      "\n",
      " Learning rate 0.0007352098102966978, Model learning rate 0.0007352097891271114\n",
      "276/391 [====================>.........] - ETA: 8s - loss: 0.9199 - acc: 0.7271\n",
      " Optimizer iteration 2622, batch 276\n",
      "\n",
      " Learning rate 0.0007344986653618844, Model learning rate 0.0007344986661337316\n",
      "\n",
      " Optimizer iteration 2623, batch 277\n",
      "\n",
      " Learning rate 0.0007337869117726176, Model learning rate 0.0007337869028560817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/391 [====================>.........] - ETA: 8s - loss: 0.9192 - acc: 0.7274\n",
      " Optimizer iteration 2624, batch 278\n",
      "\n",
      " Learning rate 0.0007330745513762936, Model learning rate 0.0007330745575018227\n",
      "279/391 [====================>.........] - ETA: 8s - loss: 0.9194 - acc: 0.7274\n",
      " Optimizer iteration 2625, batch 279\n",
      "\n",
      " Learning rate 0.0007323615860218843, Model learning rate 0.0007323615718632936\n",
      "280/391 [====================>.........] - ETA: 8s - loss: 0.9193 - acc: 0.7274\n",
      " Optimizer iteration 2626, batch 280\n",
      "\n",
      " Learning rate 0.0007316480175599309, Model learning rate 0.0007316480041481555\n",
      "\n",
      " Optimizer iteration 2627, batch 281\n",
      "\n",
      " Learning rate 0.0007309338478425404, Model learning rate 0.0007309338543564081\n",
      "282/391 [====================>.........] - ETA: 8s - loss: 0.9179 - acc: 0.7278\n",
      " Optimizer iteration 2628, batch 282\n",
      "\n",
      " Learning rate 0.0007302190787233807, Model learning rate 0.0007302190642803907\n",
      "\n",
      " Optimizer iteration 2629, batch 283\n",
      "\n",
      " Learning rate 0.0007295037120576749, Model learning rate 0.0007295036921277642\n",
      "284/391 [====================>.........] - ETA: 8s - loss: 0.9176 - acc: 0.7280\n",
      " Optimizer iteration 2630, batch 284\n",
      "\n",
      " Learning rate 0.0007287877497021977, Model learning rate 0.0007287877378985286\n",
      "\n",
      " Optimizer iteration 2631, batch 285\n",
      "\n",
      " Learning rate 0.0007280711935152691, Model learning rate 0.0007280712015926838\n",
      "286/391 [====================>.........] - ETA: 7s - loss: 0.9180 - acc: 0.7280\n",
      " Optimizer iteration 2632, batch 286\n",
      "\n",
      " Learning rate 0.0007273540453567512, Model learning rate 0.000727354025002569\n",
      "\n",
      " Optimizer iteration 2633, batch 287\n",
      "\n",
      " Learning rate 0.0007266363070880424, Model learning rate 0.0007266363245435059\n",
      "288/391 [=====================>........] - ETA: 7s - loss: 0.9176 - acc: 0.7281\n",
      " Optimizer iteration 2634, batch 288\n",
      "\n",
      " Learning rate 0.0007259179805720726, Model learning rate 0.0007259179838001728\n",
      "289/391 [=====================>........] - ETA: 7s - loss: 0.9175 - acc: 0.7282\n",
      " Optimizer iteration 2635, batch 289\n",
      "\n",
      " Learning rate 0.0007251990676732984, Model learning rate 0.0007251990609802306\n",
      "\n",
      " Optimizer iteration 2636, batch 290\n",
      "\n",
      " Learning rate 0.0007244795702576989, Model learning rate 0.0007244795560836792\n",
      "291/391 [=====================>........] - ETA: 7s - loss: 0.9168 - acc: 0.7284\n",
      " Optimizer iteration 2637, batch 291\n",
      "\n",
      " Learning rate 0.0007237594901927699, Model learning rate 0.0007237594691105187\n",
      "292/391 [=====================>........] - ETA: 7s - loss: 0.9173 - acc: 0.7282\n",
      " Optimizer iteration 2638, batch 292\n",
      "\n",
      " Learning rate 0.0007230388293475197, Model learning rate 0.00072303885826841\n",
      "293/391 [=====================>........] - ETA: 7s - loss: 0.9172 - acc: 0.7282\n",
      " Optimizer iteration 2639, batch 293\n",
      "\n",
      " Learning rate 0.0007223175895924637, Model learning rate 0.0007223176071420312\n",
      "294/391 [=====================>........] - ETA: 7s - loss: 0.9175 - acc: 0.7280\n",
      " Optimizer iteration 2640, batch 294\n",
      "\n",
      " Learning rate 0.0007215957727996207, Model learning rate 0.0007215957739390433\n",
      "295/391 [=====================>........] - ETA: 7s - loss: 0.9168 - acc: 0.7283\n",
      " Optimizer iteration 2641, batch 295\n",
      "\n",
      " Learning rate 0.0007208733808425063, Model learning rate 0.0007208733586594462\n",
      "296/391 [=====================>........] - ETA: 7s - loss: 0.9166 - acc: 0.7283\n",
      " Optimizer iteration 2642, batch 296\n",
      "\n",
      " Learning rate 0.0007201504155961296, Model learning rate 0.000720150419510901\n",
      "297/391 [=====================>........] - ETA: 7s - loss: 0.9165 - acc: 0.7284\n",
      " Optimizer iteration 2643, batch 297\n",
      "\n",
      " Learning rate 0.0007194268789369874, Model learning rate 0.0007194268982857466\n",
      "298/391 [=====================>........] - ETA: 7s - loss: 0.9163 - acc: 0.7283\n",
      " Optimizer iteration 2644, batch 298\n",
      "\n",
      " Learning rate 0.00071870277274306, Model learning rate 0.000718702794983983\n",
      "299/391 [=====================>........] - ETA: 7s - loss: 0.9165 - acc: 0.7282\n",
      " Optimizer iteration 2645, batch 299\n",
      "\n",
      " Learning rate 0.000717978098893805, Model learning rate 0.0007179781096056104\n",
      "300/391 [======================>.......] - ETA: 6s - loss: 0.9161 - acc: 0.7284\n",
      " Optimizer iteration 2646, batch 300\n",
      "\n",
      " Learning rate 0.000717252859270155, Model learning rate 0.0007172528421506286\n",
      "\n",
      " Optimizer iteration 2647, batch 301\n",
      "\n",
      " Learning rate 0.0007165270557545094, Model learning rate 0.0007165270508266985\n",
      "302/391 [======================>.......] - ETA: 6s - loss: 0.9162 - acc: 0.7284\n",
      " Optimizer iteration 2648, batch 302\n",
      "\n",
      " Learning rate 0.0007158006902307321, Model learning rate 0.0007158006774261594\n",
      "303/391 [======================>.......] - ETA: 6s - loss: 0.9162 - acc: 0.7284\n",
      " Optimizer iteration 2649, batch 303\n",
      "\n",
      " Learning rate 0.000715073764584146, Model learning rate 0.000715073780156672\n",
      "304/391 [======================>.......] - ETA: 6s - loss: 0.9161 - acc: 0.7285\n",
      " Optimizer iteration 2650, batch 304\n",
      "\n",
      " Learning rate 0.000714346280701527, Model learning rate 0.0007143463008105755\n",
      "\n",
      " Optimizer iteration 2651, batch 305\n",
      "\n",
      " Learning rate 0.0007136182404711008, Model learning rate 0.0007136182393878698\n",
      "306/391 [======================>.......] - ETA: 6s - loss: 0.9154 - acc: 0.7288\n",
      " Optimizer iteration 2652, batch 306\n",
      "\n",
      " Learning rate 0.0007128896457825364, Model learning rate 0.000712889654096216\n",
      "307/391 [======================>.......] - ETA: 6s - loss: 0.9154 - acc: 0.7288\n",
      " Optimizer iteration 2653, batch 307\n",
      "\n",
      " Learning rate 0.0007121604985269422, Model learning rate 0.000712160486727953\n",
      "308/391 [======================>.......] - ETA: 6s - loss: 0.9146 - acc: 0.7291\n",
      " Optimizer iteration 2654, batch 308\n",
      "\n",
      " Learning rate 0.0007114308005968609, Model learning rate 0.0007114307954907417\n",
      "\n",
      " Optimizer iteration 2655, batch 309\n",
      "\n",
      " Learning rate 0.0007107005538862646, Model learning rate 0.0007107005803845823\n",
      "310/391 [======================>.......] - ETA: 6s - loss: 0.9137 - acc: 0.7295\n",
      " Optimizer iteration 2656, batch 310\n",
      "\n",
      " Learning rate 0.0007099697602905493, Model learning rate 0.0007099697832018137\n",
      "\n",
      " Optimizer iteration 2657, batch 311\n",
      "\n",
      " Learning rate 0.0007092384217065314, Model learning rate 0.000709238403942436\n",
      "312/391 [======================>.......] - ETA: 6s - loss: 0.9135 - acc: 0.7297\n",
      " Optimizer iteration 2658, batch 312\n",
      "\n",
      " Learning rate 0.0007085065400324407, Model learning rate 0.000708506559021771\n",
      "313/391 [=======================>......] - ETA: 5s - loss: 0.9138 - acc: 0.7294\n",
      " Optimizer iteration 2659, batch 313\n",
      "\n",
      " Learning rate 0.0007077741171679173, Model learning rate 0.0007077741320244968\n",
      "\n",
      " Optimizer iteration 2660, batch 314\n",
      "\n",
      " Learning rate 0.000707041155014006, Model learning rate 0.0007070411811582744\n",
      "315/391 [=======================>......] - ETA: 5s - loss: 0.9133 - acc: 0.7297\n",
      " Optimizer iteration 2661, batch 315\n",
      "\n",
      " Learning rate 0.0007063076554731512, Model learning rate 0.0007063076482154429\n",
      "316/391 [=======================>......] - ETA: 5s - loss: 0.9131 - acc: 0.7299\n",
      " Optimizer iteration 2662, batch 316\n",
      "\n",
      " Learning rate 0.0007055736204491922, Model learning rate 0.0007055735914036632\n",
      "\n",
      " Optimizer iteration 2663, batch 317\n",
      "\n",
      " Learning rate 0.0007048390518473579, Model learning rate 0.0007048390689305961\n",
      "318/391 [=======================>......] - ETA: 5s - loss: 0.9126 - acc: 0.7301\n",
      " Optimizer iteration 2664, batch 318\n",
      "\n",
      " Learning rate 0.0007041039515742625, Model learning rate 0.0007041039643809199\n",
      "\n",
      " Optimizer iteration 2665, batch 319\n",
      "\n",
      " Learning rate 0.0007033683215379002, Model learning rate 0.0007033683359622955\n",
      "320/391 [=======================>......] - ETA: 5s - loss: 0.9129 - acc: 0.7301\n",
      " Optimizer iteration 2666, batch 320\n",
      "\n",
      " Learning rate 0.0007026321636476397, Model learning rate 0.0007026321836747229\n",
      "321/391 [=======================>......] - ETA: 5s - loss: 0.9129 - acc: 0.7301\n",
      " Optimizer iteration 2667, batch 321\n",
      "\n",
      " Learning rate 0.0007018954798142204, Model learning rate 0.0007018955075182021\n",
      "\n",
      " Optimizer iteration 2668, batch 322\n",
      "\n",
      " Learning rate 0.0007011582719497466, Model learning rate 0.0007011582492850721\n",
      "323/391 [=======================>......] - ETA: 5s - loss: 0.9135 - acc: 0.7299\n",
      " Optimizer iteration 2669, batch 323\n",
      "\n",
      " Learning rate 0.0007004205419676825, Model learning rate 0.0007004205253906548\n",
      "324/391 [=======================>......] - ETA: 5s - loss: 0.9135 - acc: 0.7299\n",
      " Optimizer iteration 2670, batch 324\n",
      "\n",
      " Learning rate 0.0006996822917828477, Model learning rate 0.0006996822776272893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/391 [=======================>......] - ETA: 5s - loss: 0.9133 - acc: 0.7300\n",
      " Optimizer iteration 2671, batch 325\n",
      "\n",
      " Learning rate 0.0006989435233114123, Model learning rate 0.0006989435059949756\n",
      "326/391 [========================>.....] - ETA: 4s - loss: 0.9131 - acc: 0.7301\n",
      " Optimizer iteration 2672, batch 326\n",
      "\n",
      " Learning rate 0.000698204238470891, Model learning rate 0.0006982042104937136\n",
      "\n",
      " Optimizer iteration 2673, batch 327\n",
      "\n",
      " Learning rate 0.0006974644391801395, Model learning rate 0.0006974644493311644\n",
      "328/391 [========================>.....] - ETA: 4s - loss: 0.9134 - acc: 0.7301\n",
      " Optimizer iteration 2674, batch 328\n",
      "\n",
      " Learning rate 0.000696724127359348, Model learning rate 0.000696724106092006\n",
      "329/391 [========================>.....] - ETA: 4s - loss: 0.9138 - acc: 0.7300\n",
      " Optimizer iteration 2675, batch 329\n",
      "\n",
      " Learning rate 0.0006959833049300376, Model learning rate 0.0006959832971915603\n",
      "\n",
      " Optimizer iteration 2676, batch 330\n",
      "\n",
      " Learning rate 0.0006952419738150545, Model learning rate 0.0006952419644221663\n",
      "331/391 [========================>.....] - ETA: 4s - loss: 0.9134 - acc: 0.7300\n",
      " Optimizer iteration 2677, batch 331\n",
      "\n",
      " Learning rate 0.0006945001359385651, Model learning rate 0.0006945001077838242\n",
      "332/391 [========================>.....] - ETA: 4s - loss: 0.9133 - acc: 0.7300\n",
      " Optimizer iteration 2678, batch 332\n",
      "\n",
      " Learning rate 0.0006937577932260515, Model learning rate 0.0006937577854841948\n",
      "333/391 [========================>.....] - ETA: 4s - loss: 0.9131 - acc: 0.7301\n",
      " Optimizer iteration 2679, batch 333\n",
      "\n",
      " Learning rate 0.0006930149476043058, Model learning rate 0.0006930149393156171\n",
      "334/391 [========================>.....] - ETA: 4s - loss: 0.9130 - acc: 0.7301\n",
      " Optimizer iteration 2680, batch 334\n",
      "\n",
      " Learning rate 0.0006922716010014255, Model learning rate 0.0006922716274857521\n",
      "335/391 [========================>.....] - ETA: 4s - loss: 0.9132 - acc: 0.7300\n",
      " Optimizer iteration 2681, batch 335\n",
      "\n",
      " Learning rate 0.0006915277553468083, Model learning rate 0.000691527733579278\n",
      "336/391 [========================>.....] - ETA: 4s - loss: 0.9131 - acc: 0.7301\n",
      " Optimizer iteration 2682, batch 336\n",
      "\n",
      " Learning rate 0.0006907834125711476, Model learning rate 0.0006907834322191775\n",
      "\n",
      " Optimizer iteration 2683, batch 337\n",
      "\n",
      " Learning rate 0.0006900385746064268, Model learning rate 0.0006900385487824678\n",
      "338/391 [========================>.....] - ETA: 4s - loss: 0.9127 - acc: 0.7304\n",
      " Optimizer iteration 2684, batch 338\n",
      "\n",
      " Learning rate 0.0006892932433859147, Model learning rate 0.0006892932578921318\n",
      "339/391 [=========================>....] - ETA: 3s - loss: 0.9126 - acc: 0.7304\n",
      " Optimizer iteration 2685, batch 339\n",
      "\n",
      " Learning rate 0.0006885474208441603, Model learning rate 0.0006885474431328475\n",
      "340/391 [=========================>....] - ETA: 3s - loss: 0.9130 - acc: 0.7303\n",
      " Optimizer iteration 2686, batch 340\n",
      "\n",
      " Learning rate 0.0006878011089169878, Model learning rate 0.0006878011045046151\n",
      "341/391 [=========================>....] - ETA: 3s - loss: 0.9127 - acc: 0.7305\n",
      " Optimizer iteration 2687, batch 341\n",
      "\n",
      " Learning rate 0.0006870543095414918, Model learning rate 0.0006870543002150953\n",
      "342/391 [=========================>....] - ETA: 3s - loss: 0.9130 - acc: 0.7304\n",
      " Optimizer iteration 2688, batch 342\n",
      "\n",
      " Learning rate 0.0006863070246560319, Model learning rate 0.0006863070302642882\n",
      "\n",
      " Optimizer iteration 2689, batch 343\n",
      "\n",
      " Learning rate 0.0006855592562002281, Model learning rate 0.0006855592364445329\n",
      "344/391 [=========================>....] - ETA: 3s - loss: 0.9126 - acc: 0.7306\n",
      " Optimizer iteration 2690, batch 344\n",
      "\n",
      " Learning rate 0.0006848110061149555, Model learning rate 0.0006848110351711512\n",
      "\n",
      " Optimizer iteration 2691, batch 345\n",
      "\n",
      " Learning rate 0.0006840622763423391, Model learning rate 0.0006840622518211603\n",
      "346/391 [=========================>....] - ETA: 3s - loss: 0.9127 - acc: 0.7306\n",
      " Optimizer iteration 2692, batch 346\n",
      "\n",
      " Learning rate 0.0006833130688257489, Model learning rate 0.0006833130610175431\n",
      "\n",
      " Optimizer iteration 2693, batch 347\n",
      "\n",
      " Learning rate 0.0006825633855097954, Model learning rate 0.0006825634045526385\n",
      "348/391 [=========================>....] - ETA: 3s - loss: 0.9125 - acc: 0.7306\n",
      " Optimizer iteration 2694, batch 348\n",
      "\n",
      " Learning rate 0.0006818132283403235, Model learning rate 0.0006818132242187858\n",
      "349/391 [=========================>....] - ETA: 3s - loss: 0.9125 - acc: 0.7305\n",
      " Optimizer iteration 2695, batch 349\n",
      "\n",
      " Learning rate 0.0006810625992644084, Model learning rate 0.0006810625782236457\n",
      "350/391 [=========================>....] - ETA: 3s - loss: 0.9121 - acc: 0.7308\n",
      " Optimizer iteration 2696, batch 350\n",
      "\n",
      " Learning rate 0.0006803115002303499, Model learning rate 0.0006803115247748792\n",
      "\n",
      " Optimizer iteration 2697, batch 351\n",
      "\n",
      " Learning rate 0.0006795599331876678, Model learning rate 0.0006795599474571645\n",
      "352/391 [==========================>...] - ETA: 2s - loss: 0.9122 - acc: 0.7309\n",
      " Optimizer iteration 2698, batch 352\n",
      "\n",
      " Learning rate 0.0006788079000870966, Model learning rate 0.0006788079044781625\n",
      "353/391 [==========================>...] - ETA: 2s - loss: 0.9120 - acc: 0.7309\n",
      " Optimizer iteration 2699, batch 353\n",
      "\n",
      " Learning rate 0.0006780554028805803, Model learning rate 0.0006780553958378732\n",
      "354/391 [==========================>...] - ETA: 2s - loss: 0.9116 - acc: 0.7311\n",
      " Optimizer iteration 2700, batch 354\n",
      "\n",
      " Learning rate 0.0006773024435212678, Model learning rate 0.0006773024215362966\n",
      "\n",
      " Optimizer iteration 2701, batch 355\n",
      "\n",
      " Learning rate 0.0006765490239635075, Model learning rate 0.0006765490397810936\n",
      "356/391 [==========================>...] - ETA: 2s - loss: 0.9109 - acc: 0.7315\n",
      " Optimizer iteration 2702, batch 356\n",
      "\n",
      " Learning rate 0.0006757951461628416, Model learning rate 0.0006757951341569424\n",
      "357/391 [==========================>...] - ETA: 2s - loss: 0.9108 - acc: 0.7315\n",
      " Optimizer iteration 2703, batch 357\n",
      "\n",
      " Learning rate 0.0006750408120760029, Model learning rate 0.0006750408210791647\n",
      "358/391 [==========================>...] - ETA: 2s - loss: 0.9103 - acc: 0.7315\n",
      " Optimizer iteration 2704, batch 358\n",
      "\n",
      " Learning rate 0.0006742860236609076, Model learning rate 0.0006742860423400998\n",
      "\n",
      " Optimizer iteration 2705, batch 359\n",
      "\n",
      " Learning rate 0.0006735307828766515, Model learning rate 0.0006735307979397476\n",
      "360/391 [==========================>...] - ETA: 2s - loss: 0.9098 - acc: 0.7317\n",
      " Optimizer iteration 2706, batch 360\n",
      "\n",
      " Learning rate 0.0006727750916835043, Model learning rate 0.000672775087878108\n",
      "361/391 [==========================>...] - ETA: 2s - loss: 0.9096 - acc: 0.7317\n",
      " Optimizer iteration 2707, batch 361\n",
      "\n",
      " Learning rate 0.0006720189520429052, Model learning rate 0.0006720189703628421\n",
      "362/391 [==========================>...] - ETA: 2s - loss: 0.9099 - acc: 0.7316\n",
      " Optimizer iteration 2708, batch 362\n",
      "\n",
      " Learning rate 0.0006712623659174569, Model learning rate 0.0006712623871862888\n",
      "363/391 [==========================>...] - ETA: 2s - loss: 0.9099 - acc: 0.7316\n",
      " Optimizer iteration 2709, batch 363\n",
      "\n",
      " Learning rate 0.0006705053352709212, Model learning rate 0.0006705053383484483\n",
      "364/391 [==========================>...] - ETA: 2s - loss: 0.9097 - acc: 0.7316\n",
      " Optimizer iteration 2710, batch 364\n",
      "\n",
      " Learning rate 0.0006697478620682136, Model learning rate 0.0006697478820569813\n",
      "365/391 [===========================>..] - ETA: 1s - loss: 0.9094 - acc: 0.7317\n",
      " Optimizer iteration 2711, batch 365\n",
      "\n",
      " Learning rate 0.0006689899482753984, Model learning rate 0.0006689899601042271\n",
      "366/391 [===========================>..] - ETA: 1s - loss: 0.9093 - acc: 0.7317\n",
      " Optimizer iteration 2712, batch 366\n",
      "\n",
      " Learning rate 0.0006682315958596836, Model learning rate 0.0006682315724901855\n",
      "\n",
      " Optimizer iteration 2713, batch 367\n",
      "\n",
      " Learning rate 0.0006674728067894149, Model learning rate 0.0006674728356301785\n",
      "368/391 [===========================>..] - ETA: 1s - loss: 0.9088 - acc: 0.7320\n",
      " Optimizer iteration 2714, batch 368\n",
      "\n",
      " Learning rate 0.0006667135830340727, Model learning rate 0.0006667135749012232\n",
      "369/391 [===========================>..] - ETA: 1s - loss: 0.9086 - acc: 0.7322\n",
      " Optimizer iteration 2715, batch 369\n",
      "\n",
      " Learning rate 0.0006659539265642643, Model learning rate 0.0006659539067186415\n",
      "370/391 [===========================>..] - ETA: 1s - loss: 0.9083 - acc: 0.7321\n",
      " Optimizer iteration 2716, batch 370\n",
      "\n",
      " Learning rate 0.000665193839351721, Model learning rate 0.0006651938310824335\n",
      "\n",
      " Optimizer iteration 2717, batch 371\n",
      "\n",
      " Learning rate 0.0006644333233692916, Model learning rate 0.000664433347992599\n",
      "372/391 [===========================>..] - ETA: 1s - loss: 0.9082 - acc: 0.7321\n",
      " Optimizer iteration 2718, batch 372\n",
      "\n",
      " Learning rate 0.0006636723805909383, Model learning rate 0.0006636723992414773\n",
      "373/391 [===========================>..] - ETA: 1s - loss: 0.9078 - acc: 0.7323\n",
      " Optimizer iteration 2719, batch 373\n",
      "\n",
      " Learning rate 0.0006629110129917308, Model learning rate 0.0006629109848290682\n",
      "374/391 [===========================>..] - ETA: 1s - loss: 0.9076 - acc: 0.7323\n",
      " Optimizer iteration 2720, batch 374\n",
      "\n",
      " Learning rate 0.0006621492225478414, Model learning rate 0.0006621492211706936\n",
      "\n",
      " Optimizer iteration 2721, batch 375\n",
      "\n",
      " Learning rate 0.0006613870112365398, Model learning rate 0.0006613869918510318\n",
      "376/391 [===========================>..] - ETA: 1s - loss: 0.9073 - acc: 0.7325\n",
      " Optimizer iteration 2722, batch 376\n",
      "\n",
      " Learning rate 0.0006606243810361885, Model learning rate 0.0006606243550777435\n",
      "377/391 [===========================>..] - ETA: 1s - loss: 0.9070 - acc: 0.7325\n",
      " Optimizer iteration 2723, batch 377\n",
      "\n",
      " Learning rate 0.0006598613339262369, Model learning rate 0.0006598613108508289\n",
      "\n",
      " Optimizer iteration 2724, batch 378\n",
      "\n",
      " Learning rate 0.0006590978718872166, Model learning rate 0.0006590978591702878\n",
      "379/391 [============================>.] - ETA: 0s - loss: 0.9068 - acc: 0.7325\n",
      " Optimizer iteration 2725, batch 379\n",
      "\n",
      " Learning rate 0.0006583339969007363, Model learning rate 0.0006583340000361204\n",
      "380/391 [============================>.] - ETA: 0s - loss: 0.9073 - acc: 0.7324\n",
      " Optimizer iteration 2726, batch 380\n",
      "\n",
      " Learning rate 0.0006575697109494763, Model learning rate 0.0006575697334483266\n",
      "381/391 [============================>.] - ETA: 0s - loss: 0.9068 - acc: 0.7325\n",
      " Optimizer iteration 2727, batch 381\n",
      "\n",
      " Learning rate 0.0006568050160171837, Model learning rate 0.0006568050011992455\n",
      "382/391 [============================>.] - ETA: 0s - loss: 0.9070 - acc: 0.7325\n",
      " Optimizer iteration 2728, batch 382\n",
      "\n",
      " Learning rate 0.0006560399140886673, Model learning rate 0.0006560399197041988\n",
      "383/391 [============================>.] - ETA: 0s - loss: 0.9068 - acc: 0.7326\n",
      " Optimizer iteration 2729, batch 383\n",
      "\n",
      " Learning rate 0.0006552744071497918, Model learning rate 0.0006552744307555258\n",
      "384/391 [============================>.] - ETA: 0s - loss: 0.9070 - acc: 0.7325\n",
      " Optimizer iteration 2730, batch 384\n",
      "\n",
      " Learning rate 0.0006545084971874737, Model learning rate 0.0006545084761455655\n",
      "385/391 [============================>.] - ETA: 0s - loss: 0.9069 - acc: 0.7327\n",
      " Optimizer iteration 2731, batch 385\n",
      "\n",
      " Learning rate 0.0006537421861896752, Model learning rate 0.0006537421722896397\n",
      "\n",
      " Optimizer iteration 2732, batch 386\n",
      "\n",
      " Learning rate 0.0006529754761453999, Model learning rate 0.0006529754609800875\n",
      "387/391 [============================>.] - ETA: 0s - loss: 0.9066 - acc: 0.7328\n",
      " Optimizer iteration 2733, batch 387\n",
      "\n",
      " Learning rate 0.0006522083690446862, Model learning rate 0.0006522083422169089\n",
      "388/391 [============================>.] - ETA: 0s - loss: 0.9062 - acc: 0.7329\n",
      " Optimizer iteration 2734, batch 388\n",
      "\n",
      " Learning rate 0.0006514408668786038, Model learning rate 0.0006514408742077649\n",
      "389/391 [============================>.] - ETA: 0s - loss: 0.9062 - acc: 0.7330\n",
      " Optimizer iteration 2735, batch 389\n",
      "\n",
      " Learning rate 0.000650672971639248, Model learning rate 0.0006506729987449944\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.9058 - acc: 0.7331\n",
      " Optimizer iteration 2736, batch 390\n",
      "\n",
      " Learning rate 0.0006499046853197338, Model learning rate 0.0006499046576209366\n",
      "391/391 [==============================] - 30s 78ms/step - loss: 0.9059 - acc: 0.7330 - val_loss: 1.1331 - val_acc: 0.6638\n",
      "\n",
      "Epoch 00007: saving model to /home/ubuntu/Projects/hybrid-ensemble/model/run_200/cifar10_ResNet20v1_model-0007.h5\n",
      "Epoch 8/10\n",
      "\n",
      " Optimizer iteration 2737, batch 0\n",
      "\n",
      " Learning rate 0.0006491360099141913, Model learning rate 0.0006491360254585743\n",
      "  1/391 [..............................] - ETA: 17s - loss: 0.7378 - acc: 0.8203\n",
      " Optimizer iteration 2738, batch 1\n",
      "\n",
      " Learning rate 0.0006483669474177608, Model learning rate 0.0006483669276349247\n",
      "  2/391 [..............................] - ETA: 18s - loss: 0.8032 - acc: 0.7578\n",
      " Optimizer iteration 2739, batch 2\n",
      "\n",
      " Learning rate 0.0006475974998265874, Model learning rate 0.0006475974805653095\n",
      "  3/391 [..............................] - ETA: 19s - loss: 0.8237 - acc: 0.7526\n",
      " Optimizer iteration 2740, batch 3\n",
      "\n",
      " Learning rate 0.0006468276691378155, Model learning rate 0.0006468276842497289\n",
      "\n",
      " Optimizer iteration 2741, batch 4\n",
      "\n",
      " Learning rate 0.0006460574573495835, Model learning rate 0.0006460574804805219\n",
      "  5/391 [..............................] - ETA: 19s - loss: 0.8578 - acc: 0.7406\n",
      " Optimizer iteration 2742, batch 5\n",
      "\n",
      " Learning rate 0.0006452868664610196, Model learning rate 0.0006452868692576885\n",
      "\n",
      " Optimizer iteration 2743, batch 6\n",
      "\n",
      " Learning rate 0.0006445158984722358, Model learning rate 0.0006445159087888896\n",
      "  7/391 [..............................] - ETA: 19s - loss: 0.8632 - acc: 0.7344\n",
      " Optimizer iteration 2744, batch 7\n",
      "\n",
      " Learning rate 0.0006437445553843229, Model learning rate 0.0006437445408664644\n",
      "  8/391 [..............................] - ETA: 19s - loss: 0.8507 - acc: 0.7441\n",
      " Optimizer iteration 2745, batch 8\n",
      "\n",
      " Learning rate 0.0006429728391993446, Model learning rate 0.0006429728236980736\n",
      "\n",
      " Optimizer iteration 2746, batch 9\n",
      "\n",
      " Learning rate 0.0006422007519203343, Model learning rate 0.0006422007572837174\n",
      " 10/391 [..............................] - ETA: 18s - loss: 0.8457 - acc: 0.7492\n",
      " Optimizer iteration 2747, batch 10\n",
      "\n",
      " Learning rate 0.0006414282955512875, Model learning rate 0.0006414282834157348\n",
      " 11/391 [..............................] - ETA: 19s - loss: 0.8323 - acc: 0.7543\n",
      " Optimizer iteration 2748, batch 11\n",
      "\n",
      " Learning rate 0.0006406554720971582, Model learning rate 0.0006406554603017867\n",
      "\n",
      " Optimizer iteration 2749, batch 12\n",
      "\n",
      " Learning rate 0.000639882283563853, Model learning rate 0.0006398822879418731\n",
      " 13/391 [..............................] - ETA: 18s - loss: 0.8270 - acc: 0.7530\n",
      " Optimizer iteration 2750, batch 13\n",
      "\n",
      " Learning rate 0.0006391087319582263, Model learning rate 0.0006391087081283331\n",
      " 14/391 [>.............................] - ETA: 18s - loss: 0.8270 - acc: 0.7533\n",
      " Optimizer iteration 2751, batch 14\n",
      "\n",
      " Learning rate 0.0006383348192880747, Model learning rate 0.0006383348372764885\n",
      " 15/391 [>.............................] - ETA: 18s - loss: 0.8269 - acc: 0.7536\n",
      " Optimizer iteration 2752, batch 15\n",
      "\n",
      " Learning rate 0.0006375605475621318, Model learning rate 0.0006375605589710176\n",
      " 16/391 [>.............................] - ETA: 18s - loss: 0.8315 - acc: 0.7524\n",
      " Optimizer iteration 2753, batch 16\n",
      "\n",
      " Learning rate 0.0006367859187900634, Model learning rate 0.0006367859314195812\n",
      "\n",
      " Optimizer iteration 2754, batch 17\n",
      "\n",
      " Learning rate 0.0006360109349824621, Model learning rate 0.0006360109546221793\n",
      " 18/391 [>.............................] - ETA: 19s - loss: 0.8329 - acc: 0.7526\n",
      " Optimizer iteration 2755, batch 18\n",
      "\n",
      " Learning rate 0.000635235598150842, Model learning rate 0.000635235570371151\n",
      " 19/391 [>.............................] - ETA: 19s - loss: 0.8292 - acc: 0.7549\n",
      " Optimizer iteration 2756, batch 19\n",
      "\n",
      " Learning rate 0.0006344599103076329, Model learning rate 0.0006344598950818181\n",
      " 20/391 [>.............................] - ETA: 20s - loss: 0.8309 - acc: 0.7531\n",
      " Optimizer iteration 2757, batch 20\n",
      "\n",
      " Learning rate 0.0006336838734661765, Model learning rate 0.0006336838705465198\n",
      " 21/391 [>.............................] - ETA: 20s - loss: 0.8370 - acc: 0.7500\n",
      " Optimizer iteration 2758, batch 21\n",
      "\n",
      " Learning rate 0.0006329074896407202, Model learning rate 0.0006329074967652559\n",
      " 22/391 [>.............................] - ETA: 21s - loss: 0.8421 - acc: 0.7489\n",
      " Optimizer iteration 2759, batch 22\n",
      "\n",
      " Learning rate 0.0006321307608464113, Model learning rate 0.0006321307737380266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23/391 [>.............................] - ETA: 21s - loss: 0.8462 - acc: 0.7469\n",
      " Optimizer iteration 2760, batch 23\n",
      "\n",
      " Learning rate 0.0006313536890992934, Model learning rate 0.0006313537014648318\n",
      " 24/391 [>.............................] - ETA: 21s - loss: 0.8444 - acc: 0.7467\n",
      " Optimizer iteration 2761, batch 24\n",
      "\n",
      " Learning rate 0.0006305762764162999, Model learning rate 0.0006305762799456716\n",
      " 25/391 [>.............................] - ETA: 21s - loss: 0.8468 - acc: 0.7472\n",
      " Optimizer iteration 2762, batch 25\n",
      "\n",
      " Learning rate 0.0006297985248152486, Model learning rate 0.0006297985091805458\n",
      " 26/391 [>.............................] - ETA: 22s - loss: 0.8459 - acc: 0.7485\n",
      " Optimizer iteration 2763, batch 26\n",
      "\n",
      " Learning rate 0.000629020436314838, Model learning rate 0.0006290204473771155\n",
      " 27/391 [=>............................] - ETA: 22s - loss: 0.8398 - acc: 0.7512\n",
      " Optimizer iteration 2764, batch 27\n",
      "\n",
      " Learning rate 0.0006282420129346401, Model learning rate 0.0006282420363277197\n",
      " 28/391 [=>............................] - ETA: 22s - loss: 0.8364 - acc: 0.7522\n",
      " Optimizer iteration 2765, batch 28\n",
      "\n",
      " Learning rate 0.0006274632566950966, Model learning rate 0.0006274632760323584\n",
      " 29/391 [=>............................] - ETA: 22s - loss: 0.8432 - acc: 0.7505\n",
      " Optimizer iteration 2766, batch 29\n",
      "\n",
      " Learning rate 0.0006266841696175131, Model learning rate 0.0006266841664910316\n",
      " 30/391 [=>............................] - ETA: 22s - loss: 0.8415 - acc: 0.7513\n",
      " Optimizer iteration 2767, batch 30\n",
      "\n",
      " Learning rate 0.000625904753724054, Model learning rate 0.0006259047659114003\n",
      "\n",
      " Optimizer iteration 2768, batch 31\n",
      "\n",
      " Learning rate 0.0006251250110377367, Model learning rate 0.0006251250160858035\n",
      " 32/391 [=>............................] - ETA: 23s - loss: 0.8397 - acc: 0.7505\n",
      " Optimizer iteration 2769, batch 32\n",
      "\n",
      " Learning rate 0.0006243449435824276, Model learning rate 0.0006243449170142412\n",
      "\n",
      " Optimizer iteration 2770, batch 33\n",
      "\n",
      " Learning rate 0.0006235645533828348, Model learning rate 0.0006235645269043744\n",
      " 34/391 [=>............................] - ETA: 23s - loss: 0.8407 - acc: 0.7516\n",
      " Optimizer iteration 2771, batch 34\n",
      "\n",
      " Learning rate 0.0006227838424645056, Model learning rate 0.0006227838457562029\n",
      " 35/391 [=>............................] - ETA: 23s - loss: 0.8389 - acc: 0.7525\n",
      " Optimizer iteration 2772, batch 35\n",
      "\n",
      " Learning rate 0.0006220028128538187, Model learning rate 0.000622002815362066\n",
      " 36/391 [=>............................] - ETA: 23s - loss: 0.8369 - acc: 0.7528\n",
      " Optimizer iteration 2773, batch 36\n",
      "\n",
      " Learning rate 0.0006212214665779805, Model learning rate 0.0006212214939296246\n",
      " 37/391 [=>............................] - ETA: 23s - loss: 0.8408 - acc: 0.7508\n",
      " Optimizer iteration 2774, batch 37\n",
      "\n",
      " Learning rate 0.000620439805665019, Model learning rate 0.0006204398232512176\n",
      " 38/391 [=>............................] - ETA: 23s - loss: 0.8416 - acc: 0.7512\n",
      " Optimizer iteration 2775, batch 38\n",
      "\n",
      " Learning rate 0.0006196578321437789, Model learning rate 0.0006196578033268452\n",
      " 39/391 [=>............................] - ETA: 23s - loss: 0.8451 - acc: 0.7500\n",
      " Optimizer iteration 2776, batch 39\n",
      "\n",
      " Learning rate 0.0006188755480439165, Model learning rate 0.0006188755505718291\n",
      " 40/391 [==>...........................] - ETA: 23s - loss: 0.8408 - acc: 0.7516\n",
      " Optimizer iteration 2777, batch 40\n",
      "\n",
      " Learning rate 0.0006180929553958942, Model learning rate 0.0006180929485708475\n",
      "\n",
      " Optimizer iteration 2778, batch 41\n",
      "\n",
      " Learning rate 0.000617310056230975, Model learning rate 0.0006173100555315614\n",
      " 42/391 [==>...........................] - ETA: 23s - loss: 0.8395 - acc: 0.7533\n",
      " Optimizer iteration 2779, batch 42\n",
      "\n",
      " Learning rate 0.0006165268525812178, Model learning rate 0.0006165268714539707\n",
      " 43/391 [==>...........................] - ETA: 23s - loss: 0.8381 - acc: 0.7540\n",
      " Optimizer iteration 2780, batch 43\n",
      "\n",
      " Learning rate 0.0006157433464794716, Model learning rate 0.0006157433381304145\n",
      " 44/391 [==>...........................] - ETA: 23s - loss: 0.8380 - acc: 0.7543\n",
      " Optimizer iteration 2781, batch 44\n",
      "\n",
      " Learning rate 0.0006149595399593703, Model learning rate 0.0006149595137685537\n",
      " 45/391 [==>...........................] - ETA: 23s - loss: 0.8378 - acc: 0.7543\n",
      " Optimizer iteration 2782, batch 45\n",
      "\n",
      " Learning rate 0.0006141754350553279, Model learning rate 0.0006141754565760493\n",
      " 46/391 [==>...........................] - ETA: 23s - loss: 0.8379 - acc: 0.7554\n",
      " Optimizer iteration 2783, batch 46\n",
      "\n",
      " Learning rate 0.0006133910338025328, Model learning rate 0.0006133910501375794\n",
      " 47/391 [==>...........................] - ETA: 23s - loss: 0.8359 - acc: 0.7562\n",
      " Optimizer iteration 2784, batch 47\n",
      "\n",
      " Learning rate 0.0006126063382369423, Model learning rate 0.000612606352660805\n",
      " 48/391 [==>...........................] - ETA: 23s - loss: 0.8356 - acc: 0.7565\n",
      " Optimizer iteration 2785, batch 48\n",
      "\n",
      " Learning rate 0.0006118213503952778, Model learning rate 0.000611821364145726\n",
      " 49/391 [==>...........................] - ETA: 23s - loss: 0.8373 - acc: 0.7559\n",
      " Optimizer iteration 2786, batch 49\n",
      "\n",
      " Learning rate 0.0006110360723150194, Model learning rate 0.0006110360845923424\n",
      " 50/391 [==>...........................] - ETA: 23s - loss: 0.8373 - acc: 0.7559\n",
      " Optimizer iteration 2787, batch 50\n",
      "\n",
      " Learning rate 0.0006102505060344006, Model learning rate 0.0006102505140006542\n",
      " 51/391 [==>...........................] - ETA: 23s - loss: 0.8360 - acc: 0.7561\n",
      " Optimizer iteration 2788, batch 51\n",
      "\n",
      " Learning rate 0.0006094646535924025, Model learning rate 0.0006094646523706615\n",
      " 52/391 [==>...........................] - ETA: 23s - loss: 0.8376 - acc: 0.7553\n",
      " Optimizer iteration 2789, batch 52\n",
      "\n",
      " Learning rate 0.0006086785170287495, Model learning rate 0.0006086784997023642\n",
      " 53/391 [===>..........................] - ETA: 23s - loss: 0.8385 - acc: 0.7552\n",
      " Optimizer iteration 2790, batch 53\n",
      "\n",
      " Learning rate 0.0006078920983839031, Model learning rate 0.0006078921142034233\n",
      " 54/391 [===>..........................] - ETA: 23s - loss: 0.8390 - acc: 0.7555\n",
      " Optimizer iteration 2791, batch 54\n",
      "\n",
      " Learning rate 0.000607105399699057, Model learning rate 0.0006071053794585168\n",
      "\n",
      " Optimizer iteration 2792, batch 55\n",
      "\n",
      " Learning rate 0.0006063184230161318, Model learning rate 0.0006063184118829668\n",
      " 56/391 [===>..........................] - ETA: 23s - loss: 0.8395 - acc: 0.7560\n",
      " Optimizer iteration 2793, batch 56\n",
      "\n",
      " Learning rate 0.0006055311703777698, Model learning rate 0.0006055311532691121\n",
      " 57/391 [===>..........................] - ETA: 23s - loss: 0.8387 - acc: 0.7562\n",
      " Optimizer iteration 2794, batch 57\n",
      "\n",
      " Learning rate 0.0006047436438273293, Model learning rate 0.0006047436618246138\n",
      " 58/391 [===>..........................] - ETA: 23s - loss: 0.8384 - acc: 0.7563\n",
      " Optimizer iteration 2795, batch 58\n",
      "\n",
      " Learning rate 0.0006039558454088796, Model learning rate 0.00060395582113415\n",
      "\n",
      " Optimizer iteration 2796, batch 59\n",
      "\n",
      " Learning rate 0.0006031677771671962, Model learning rate 0.0006031678058207035\n",
      " 60/391 [===>..........................] - ETA: 23s - loss: 0.8378 - acc: 0.7579\n",
      " Optimizer iteration 2797, batch 60\n",
      "\n",
      " Learning rate 0.0006023794411477537, Model learning rate 0.0006023794412612915\n",
      " 61/391 [===>..........................] - ETA: 23s - loss: 0.8376 - acc: 0.7581\n",
      " Optimizer iteration 2798, batch 61\n",
      "\n",
      " Learning rate 0.0006015908393967232, Model learning rate 0.0006015908438712358\n",
      " 62/391 [===>..........................] - ETA: 23s - loss: 0.8374 - acc: 0.7577\n",
      " Optimizer iteration 2799, batch 62\n",
      "\n",
      " Learning rate 0.0006008019739609646, Model learning rate 0.0006008019554428756\n",
      " 63/391 [===>..........................] - ETA: 23s - loss: 0.8373 - acc: 0.7577\n",
      " Optimizer iteration 2800, batch 63\n",
      "\n",
      " Learning rate 0.0006000128468880223, Model learning rate 0.0006000128341838717\n",
      " 64/391 [===>..........................] - ETA: 23s - loss: 0.8338 - acc: 0.7592\n",
      " Optimizer iteration 2801, batch 64\n",
      "\n",
      " Learning rate 0.00059922346022612, Model learning rate 0.0005992234800942242\n",
      "\n",
      " Optimizer iteration 2802, batch 65\n",
      "\n",
      " Learning rate 0.0005984338160241551, Model learning rate 0.0005984338349662721\n",
      " 66/391 [====>.........................] - ETA: 23s - loss: 0.8368 - acc: 0.7579\n",
      " Optimizer iteration 2803, batch 66\n",
      "\n",
      " Learning rate 0.0005976439163316936, Model learning rate 0.0005976438988000154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizer iteration 2804, batch 67\n",
      "\n",
      " Learning rate 0.0005968537631989645, Model learning rate 0.000596853788010776\n",
      " 68/391 [====>.........................] - ETA: 23s - loss: 0.8353 - acc: 0.7583\n",
      " Optimizer iteration 2805, batch 68\n",
      "\n",
      " Learning rate 0.0005960633586768543, Model learning rate 0.0005960633861832321\n",
      "\n",
      " Optimizer iteration 2806, batch 69\n",
      "\n",
      " Learning rate 0.0005952727048169024, Model learning rate 0.0005952726933173835\n",
      " 70/391 [====>.........................] - ETA: 22s - loss: 0.8347 - acc: 0.7581\n",
      " Optimizer iteration 2807, batch 70\n",
      "\n",
      " Learning rate 0.0005944818036712959, Model learning rate 0.0005944818258285522\n",
      " 71/391 [====>.........................] - ETA: 22s - loss: 0.8349 - acc: 0.7577\n",
      " Optimizer iteration 2808, batch 71\n",
      "\n",
      " Learning rate 0.0005936906572928624, Model learning rate 0.0005936906673014164\n",
      " 72/391 [====>.........................] - ETA: 22s - loss: 0.8352 - acc: 0.7574\n",
      " Optimizer iteration 2809, batch 72\n",
      "\n",
      " Learning rate 0.000592899267735067, Model learning rate 0.0005928992759436369\n",
      "\n",
      " Optimizer iteration 2810, batch 73\n",
      "\n",
      " Learning rate 0.0005921076370520058, Model learning rate 0.0005921076517552137\n",
      " 74/391 [====>.........................] - ETA: 22s - loss: 0.8361 - acc: 0.7577\n",
      " Optimizer iteration 2811, batch 74\n",
      "\n",
      " Learning rate 0.0005913157672984006, Model learning rate 0.0005913157947361469\n",
      " 75/391 [====>.........................] - ETA: 22s - loss: 0.8361 - acc: 0.7573\n",
      " Optimizer iteration 2812, batch 75\n",
      "\n",
      " Learning rate 0.000590523660529594, Model learning rate 0.0005905236466787755\n",
      " 76/391 [====>.........................] - ETA: 22s - loss: 0.8336 - acc: 0.7583\n",
      " Optimizer iteration 2813, batch 76\n",
      "\n",
      " Learning rate 0.0005897313188015433, Model learning rate 0.0005897313239984214\n",
      "\n",
      " Optimizer iteration 2814, batch 77\n",
      "\n",
      " Learning rate 0.0005889387441708161, Model learning rate 0.0005889387684874237\n",
      " 78/391 [====>.........................] - ETA: 22s - loss: 0.8323 - acc: 0.7589\n",
      " Optimizer iteration 2815, batch 78\n",
      "\n",
      " Learning rate 0.0005881459386945845, Model learning rate 0.0005881459219381213\n",
      "\n",
      " Optimizer iteration 2816, batch 79\n",
      "\n",
      " Learning rate 0.0005873529044306193, Model learning rate 0.0005873529007658362\n",
      " 80/391 [=====>........................] - ETA: 22s - loss: 0.8336 - acc: 0.7592\n",
      " Optimizer iteration 2817, batch 80\n",
      "\n",
      " Learning rate 0.0005865596434372857, Model learning rate 0.0005865596467629075\n",
      " 81/391 [=====>........................] - ETA: 22s - loss: 0.8337 - acc: 0.7591\n",
      " Optimizer iteration 2818, batch 81\n",
      "\n",
      " Learning rate 0.0005857661577735372, Model learning rate 0.0005857661599293351\n",
      " 82/391 [=====>........................] - ETA: 22s - loss: 0.8343 - acc: 0.7591\n",
      " Optimizer iteration 2819, batch 82\n",
      "\n",
      " Learning rate 0.0005849724494989103, Model learning rate 0.0005849724402651191\n",
      " 83/391 [=====>........................] - ETA: 22s - loss: 0.8346 - acc: 0.7589\n",
      " Optimizer iteration 2820, batch 83\n",
      "\n",
      " Learning rate 0.0005841785206735191, Model learning rate 0.0005841785459779203\n",
      " 84/391 [=====>........................] - ETA: 22s - loss: 0.8363 - acc: 0.7581\n",
      " Optimizer iteration 2821, batch 84\n",
      "\n",
      " Learning rate 0.0005833843733580511, Model learning rate 0.0005833843606524169\n",
      " 85/391 [=====>........................] - ETA: 22s - loss: 0.8375 - acc: 0.7574\n",
      " Optimizer iteration 2822, batch 85\n",
      "\n",
      " Learning rate 0.0005825900096137599, Model learning rate 0.0005825900007039309\n",
      " 86/391 [=====>........................] - ETA: 22s - loss: 0.8355 - acc: 0.7582\n",
      " Optimizer iteration 2823, batch 86\n",
      "\n",
      " Learning rate 0.0005817954315024609, Model learning rate 0.0005817954079248011\n",
      " 87/391 [=====>........................] - ETA: 22s - loss: 0.8352 - acc: 0.7584\n",
      " Optimizer iteration 2824, batch 87\n",
      "\n",
      " Learning rate 0.0005810006410865267, Model learning rate 0.0005810006405226886\n",
      " 88/391 [=====>........................] - ETA: 22s - loss: 0.8381 - acc: 0.7574\n",
      " Optimizer iteration 2825, batch 88\n",
      "\n",
      " Learning rate 0.0005802056404288802, Model learning rate 0.0005802056402899325\n",
      " 89/391 [=====>........................] - ETA: 22s - loss: 0.8390 - acc: 0.7568\n",
      " Optimizer iteration 2826, batch 89\n",
      "\n",
      " Learning rate 0.0005794104315929903, Model learning rate 0.0005794104072265327\n",
      " 90/391 [=====>........................] - ETA: 22s - loss: 0.8387 - acc: 0.7575\n",
      " Optimizer iteration 2827, batch 90\n",
      "\n",
      " Learning rate 0.0005786150166428661, Model learning rate 0.0005786149995401502\n",
      " 91/391 [=====>........................] - ETA: 21s - loss: 0.8384 - acc: 0.7577\n",
      " Optimizer iteration 2828, batch 91\n",
      "\n",
      " Learning rate 0.0005778193976430518, Model learning rate 0.0005778194172307849\n",
      " 92/391 [======>.......................] - ETA: 22s - loss: 0.8387 - acc: 0.7575\n",
      " Optimizer iteration 2829, batch 92\n",
      "\n",
      " Learning rate 0.0005770235766586215, Model learning rate 0.000577023602090776\n",
      " 93/391 [======>.......................] - ETA: 22s - loss: 0.8394 - acc: 0.7572\n",
      " Optimizer iteration 2830, batch 93\n",
      "\n",
      " Learning rate 0.0005762275557551727, Model learning rate 0.0005762275541201234\n",
      " 94/391 [======>.......................] - ETA: 21s - loss: 0.8412 - acc: 0.7566\n",
      " Optimizer iteration 2831, batch 94\n",
      "\n",
      " Learning rate 0.0005754313369988227, Model learning rate 0.0005754313315264881\n",
      " 95/391 [======>.......................] - ETA: 21s - loss: 0.8400 - acc: 0.7566\n",
      " Optimizer iteration 2832, batch 95\n",
      "\n",
      " Learning rate 0.0005746349224562021, Model learning rate 0.00057463493430987\n",
      " 96/391 [======>.......................] - ETA: 21s - loss: 0.8404 - acc: 0.7566\n",
      " Optimizer iteration 2833, batch 96\n",
      "\n",
      " Learning rate 0.0005738383141944493, Model learning rate 0.0005738383042626083\n",
      " 97/391 [======>.......................] - ETA: 21s - loss: 0.8404 - acc: 0.7564\n",
      " Optimizer iteration 2834, batch 97\n",
      "\n",
      " Learning rate 0.0005730415142812059, Model learning rate 0.0005730414995923638\n",
      " 98/391 [======>.......................] - ETA: 21s - loss: 0.8394 - acc: 0.7568\n",
      " Optimizer iteration 2835, batch 98\n",
      "\n",
      " Learning rate 0.0005722445247846107, Model learning rate 0.0005722445202991366\n",
      " 99/391 [======>.......................] - ETA: 21s - loss: 0.8397 - acc: 0.7565\n",
      " Optimizer iteration 2836, batch 99\n",
      "\n",
      " Learning rate 0.0005714473477732947, Model learning rate 0.0005714473663829267\n",
      "100/391 [======>.......................] - ETA: 21s - loss: 0.8394 - acc: 0.7565\n",
      " Optimizer iteration 2837, batch 100\n",
      "\n",
      " Learning rate 0.000570649985316376, Model learning rate 0.0005706499796360731\n",
      "101/391 [======>.......................] - ETA: 21s - loss: 0.8397 - acc: 0.7563\n",
      " Optimizer iteration 2838, batch 101\n",
      "\n",
      " Learning rate 0.000569852439483453, Model learning rate 0.0005698524182662368\n",
      "102/391 [======>.......................] - ETA: 21s - loss: 0.8405 - acc: 0.7564\n",
      " Optimizer iteration 2839, batch 102\n",
      "\n",
      " Learning rate 0.000569054712344601, Model learning rate 0.0005690547404810786\n",
      "103/391 [======>.......................] - ETA: 21s - loss: 0.8404 - acc: 0.7566\n",
      " Optimizer iteration 2840, batch 103\n",
      "\n",
      " Learning rate 0.0005682568059703659, Model learning rate 0.0005682568298652768\n",
      "104/391 [======>.......................] - ETA: 21s - loss: 0.8420 - acc: 0.7562\n",
      " Optimizer iteration 2841, batch 104\n",
      "\n",
      " Learning rate 0.0005674587224317579, Model learning rate 0.0005674587446264923\n",
      "105/391 [=======>......................] - ETA: 21s - loss: 0.8422 - acc: 0.7565\n",
      " Optimizer iteration 2842, batch 105\n",
      "\n",
      " Learning rate 0.000566660463800248, Model learning rate 0.000566660484764725\n",
      "106/391 [=======>......................] - ETA: 21s - loss: 0.8422 - acc: 0.7566\n",
      " Optimizer iteration 2843, batch 106\n",
      "\n",
      " Learning rate 0.0005658620321477612, Model learning rate 0.0005658620502799749\n",
      "107/391 [=======>......................] - ETA: 21s - loss: 0.8424 - acc: 0.7563\n",
      " Optimizer iteration 2844, batch 107\n",
      "\n",
      " Learning rate 0.0005650634295466716, Model learning rate 0.0005650634411722422\n",
      "108/391 [=======>......................] - ETA: 21s - loss: 0.8420 - acc: 0.7561\n",
      " Optimizer iteration 2845, batch 108\n",
      "\n",
      " Learning rate 0.0005642646580697973, Model learning rate 0.0005642646574415267\n",
      "109/391 [=======>......................] - ETA: 21s - loss: 0.8428 - acc: 0.7559\n",
      " Optimizer iteration 2846, batch 109\n",
      "\n",
      " Learning rate 0.0005634657197903944, Model learning rate 0.0005634656990878284\n",
      "110/391 [=======>......................] - ETA: 21s - loss: 0.8426 - acc: 0.7558\n",
      " Optimizer iteration 2847, batch 110\n",
      "\n",
      " Learning rate 0.0005626666167821521, Model learning rate 0.0005626666243188083\n",
      "111/391 [=======>......................] - ETA: 21s - loss: 0.8414 - acc: 0.7562\n",
      " Optimizer iteration 2848, batch 111\n",
      "\n",
      " Learning rate 0.0005618673511191873, Model learning rate 0.0005618673749268055\n",
      "112/391 [=======>......................] - ETA: 21s - loss: 0.8420 - acc: 0.7564\n",
      " Optimizer iteration 2849, batch 112\n",
      "\n",
      " Learning rate 0.0005610679248760384, Model learning rate 0.0005610679509118199\n",
      "113/391 [=======>......................] - ETA: 21s - loss: 0.8414 - acc: 0.7568\n",
      " Optimizer iteration 2850, batch 113\n",
      "\n",
      " Learning rate 0.0005602683401276614, Model learning rate 0.0005602683522738516\n",
      "114/391 [=======>......................] - ETA: 21s - loss: 0.8423 - acc: 0.7566\n",
      " Optimizer iteration 2851, batch 114\n",
      "\n",
      " Learning rate 0.0005594685989494238, Model learning rate 0.0005594685790129006\n",
      "\n",
      " Optimizer iteration 2852, batch 115\n",
      "\n",
      " Learning rate 0.0005586687034170981, Model learning rate 0.0005586686893366277\n",
      "116/391 [=======>......................] - ETA: 20s - loss: 0.8422 - acc: 0.7566\n",
      " Optimizer iteration 2853, batch 116\n",
      "\n",
      " Learning rate 0.0005578686556068585, Model learning rate 0.000557868683245033\n",
      "117/391 [=======>......................] - ETA: 20s - loss: 0.8431 - acc: 0.7564\n",
      " Optimizer iteration 2854, batch 117\n",
      "\n",
      " Learning rate 0.0005570684575952737, Model learning rate 0.0005570684443227947\n",
      "118/391 [========>.....................] - ETA: 20s - loss: 0.8427 - acc: 0.7563\n",
      " Optimizer iteration 2855, batch 118\n",
      "\n",
      " Learning rate 0.0005562681114593028, Model learning rate 0.0005562680889852345\n",
      "119/391 [========>.....................] - ETA: 20s - loss: 0.8420 - acc: 0.7564\n",
      " Optimizer iteration 2856, batch 119\n",
      "\n",
      " Learning rate 0.0005554676192762891, Model learning rate 0.0005554676172323525\n",
      "120/391 [========>.....................] - ETA: 20s - loss: 0.8427 - acc: 0.7563\n",
      " Optimizer iteration 2857, batch 120\n",
      "\n",
      " Learning rate 0.000554666983123955, Model learning rate 0.0005546669708564878\n",
      "121/391 [========>.....................] - ETA: 20s - loss: 0.8430 - acc: 0.7560\n",
      " Optimizer iteration 2858, batch 121\n",
      "\n",
      " Learning rate 0.0005538662050803964, Model learning rate 0.0005538662080653012\n",
      "\n",
      " Optimizer iteration 2859, batch 122\n",
      "\n",
      " Learning rate 0.0005530652872240779, Model learning rate 0.0005530652706511319\n",
      "123/391 [========>.....................] - ETA: 20s - loss: 0.8433 - acc: 0.7558\n",
      " Optimizer iteration 2860, batch 123\n",
      "\n",
      " Learning rate 0.0005522642316338268, Model learning rate 0.0005522642168216407\n",
      "124/391 [========>.....................] - ETA: 20s - loss: 0.8423 - acc: 0.7562\n",
      " Optimizer iteration 2861, batch 124\n",
      "\n",
      " Learning rate 0.0005514630403888278, Model learning rate 0.0005514630465768278\n",
      "125/391 [========>.....................] - ETA: 20s - loss: 0.8415 - acc: 0.7569\n",
      " Optimizer iteration 2862, batch 125\n",
      "\n",
      " Learning rate 0.0005506617155686176, Model learning rate 0.0005506617017090321\n",
      "\n",
      " Optimizer iteration 2863, batch 126\n",
      "\n",
      " Learning rate 0.0005498602592530799, Model learning rate 0.0005498602404259145\n",
      "127/391 [========>.....................] - ETA: 20s - loss: 0.8421 - acc: 0.7570\n",
      " Optimizer iteration 2864, batch 127\n",
      "\n",
      " Learning rate 0.0005490586735224398, Model learning rate 0.0005490586627274752\n",
      "128/391 [========>.....................] - ETA: 19s - loss: 0.8418 - acc: 0.7571\n",
      " Optimizer iteration 2865, batch 128\n",
      "\n",
      " Learning rate 0.0005482569604572576, Model learning rate 0.000548256968613714\n",
      "129/391 [========>.....................] - ETA: 19s - loss: 0.8404 - acc: 0.7576\n",
      " Optimizer iteration 2866, batch 129\n",
      "\n",
      " Learning rate 0.000547455122138425, Model learning rate 0.00054745509987697\n",
      "130/391 [========>.....................] - ETA: 19s - loss: 0.8403 - acc: 0.7576\n",
      " Optimizer iteration 2867, batch 130\n",
      "\n",
      " Learning rate 0.000546653160647158, Model learning rate 0.0005466531729325652\n",
      "131/391 [=========>....................] - ETA: 19s - loss: 0.8389 - acc: 0.7582\n",
      " Optimizer iteration 2868, batch 131\n",
      "\n",
      " Learning rate 0.0005458510780649931, Model learning rate 0.0005458510713651776\n",
      "132/391 [=========>....................] - ETA: 19s - loss: 0.8379 - acc: 0.7583\n",
      " Optimizer iteration 2869, batch 132\n",
      "\n",
      " Learning rate 0.0005450488764737804, Model learning rate 0.0005450488533824682\n",
      "133/391 [=========>....................] - ETA: 19s - loss: 0.8376 - acc: 0.7585\n",
      " Optimizer iteration 2870, batch 133\n",
      "\n",
      " Learning rate 0.0005442465579556792, Model learning rate 0.0005442465771920979\n",
      "134/391 [=========>....................] - ETA: 19s - loss: 0.8378 - acc: 0.7585\n",
      " Optimizer iteration 2871, batch 134\n",
      "\n",
      " Learning rate 0.0005434441245931524, Model learning rate 0.0005434441263787448\n",
      "135/391 [=========>....................] - ETA: 19s - loss: 0.8367 - acc: 0.7587\n",
      " Optimizer iteration 2872, batch 135\n",
      "\n",
      " Learning rate 0.0005426415784689607, Model learning rate 0.00054264155915007\n",
      "136/391 [=========>....................] - ETA: 19s - loss: 0.8360 - acc: 0.7588\n",
      " Optimizer iteration 2873, batch 136\n",
      "\n",
      " Learning rate 0.0005418389216661579, Model learning rate 0.0005418389337137341\n",
      "137/391 [=========>....................] - ETA: 19s - loss: 0.8355 - acc: 0.7590\n",
      " Optimizer iteration 2874, batch 137\n",
      "\n",
      " Learning rate 0.0005410361562680842, Model learning rate 0.0005410361336544156\n",
      "138/391 [=========>....................] - ETA: 19s - loss: 0.8351 - acc: 0.7592\n",
      " Optimizer iteration 2875, batch 138\n",
      "\n",
      " Learning rate 0.000540233284358363, Model learning rate 0.0005402332753874362\n",
      "139/391 [=========>....................] - ETA: 19s - loss: 0.8346 - acc: 0.7594\n",
      " Optimizer iteration 2876, batch 139\n",
      "\n",
      " Learning rate 0.000539430308020893, Model learning rate 0.0005394303007051349\n",
      "140/391 [=========>....................] - ETA: 19s - loss: 0.8352 - acc: 0.7594\n",
      " Optimizer iteration 2877, batch 140\n",
      "\n",
      " Learning rate 0.0005386272293398444, Model learning rate 0.0005386272096075118\n",
      "141/391 [=========>....................] - ETA: 19s - loss: 0.8346 - acc: 0.7595\n",
      " Optimizer iteration 2878, batch 141\n",
      "\n",
      " Learning rate 0.0005378240503996531, Model learning rate 0.0005378240603022277\n",
      "142/391 [=========>....................] - ETA: 19s - loss: 0.8351 - acc: 0.7595\n",
      " Optimizer iteration 2879, batch 142\n",
      "\n",
      " Learning rate 0.000537020773285015, Model learning rate 0.0005370207945816219\n",
      "143/391 [=========>....................] - ETA: 18s - loss: 0.8338 - acc: 0.7601\n",
      " Optimizer iteration 2880, batch 143\n",
      "\n",
      " Learning rate 0.0005362174000808813, Model learning rate 0.0005362174124456942\n",
      "\n",
      " Optimizer iteration 2881, batch 144\n",
      "\n",
      " Learning rate 0.0005354139328724518, Model learning rate 0.0005354139138944447\n",
      "145/391 [==========>...................] - ETA: 18s - loss: 0.8329 - acc: 0.7605\n",
      " Optimizer iteration 2882, batch 145\n",
      "\n",
      " Learning rate 0.000534610373745171, Model learning rate 0.0005346103571355343\n",
      "146/391 [==========>...................] - ETA: 18s - loss: 0.8321 - acc: 0.7607\n",
      " Optimizer iteration 2883, batch 146\n",
      "\n",
      " Learning rate 0.0005338067247847219, Model learning rate 0.000533806742168963\n",
      "147/391 [==========>...................] - ETA: 18s - loss: 0.8322 - acc: 0.7606\n",
      " Optimizer iteration 2884, batch 147\n",
      "\n",
      " Learning rate 0.0005330029880770201, Model learning rate 0.0005330030107870698\n",
      "148/391 [==========>...................] - ETA: 18s - loss: 0.8314 - acc: 0.7608\n",
      " Optimizer iteration 2885, batch 148\n",
      "\n",
      " Learning rate 0.0005321991657082097, Model learning rate 0.0005321991629898548\n",
      "149/391 [==========>...................] - ETA: 18s - loss: 0.8301 - acc: 0.7612\n",
      " Optimizer iteration 2886, batch 149\n",
      "\n",
      " Learning rate 0.0005313952597646568, Model learning rate 0.0005313952569849789\n",
      "150/391 [==========>...................] - ETA: 18s - loss: 0.8296 - acc: 0.7614\n",
      " Optimizer iteration 2887, batch 150\n",
      "\n",
      " Learning rate 0.0005305912723329441, Model learning rate 0.0005305912927724421\n",
      "151/391 [==========>...................] - ETA: 18s - loss: 0.8298 - acc: 0.7610\n",
      " Optimizer iteration 2888, batch 151\n",
      "\n",
      " Learning rate 0.0005297872054998662, Model learning rate 0.0005297872121445835\n",
      "152/391 [==========>...................] - ETA: 18s - loss: 0.8298 - acc: 0.7610\n",
      " Optimizer iteration 2889, batch 152\n",
      "\n",
      " Learning rate 0.0005289830613524241, Model learning rate 0.0005289830733090639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/391 [==========>...................] - ETA: 18s - loss: 0.8293 - acc: 0.7612\n",
      " Optimizer iteration 2890, batch 153\n",
      "\n",
      " Learning rate 0.0005281788419778188, Model learning rate 0.0005281788180582225\n",
      "154/391 [==========>...................] - ETA: 18s - loss: 0.8293 - acc: 0.7611\n",
      " Optimizer iteration 2891, batch 154\n",
      "\n",
      " Learning rate 0.0005273745494634467, Model learning rate 0.0005273745628073812\n",
      "155/391 [==========>...................] - ETA: 18s - loss: 0.8300 - acc: 0.7607\n",
      " Optimizer iteration 2892, batch 155\n",
      "\n",
      " Learning rate 0.0005265701858968943, Model learning rate 0.000526570191141218\n",
      "156/391 [==========>...................] - ETA: 17s - loss: 0.8295 - acc: 0.7610\n",
      " Optimizer iteration 2893, batch 156\n",
      "\n",
      " Learning rate 0.0005257657533659325, Model learning rate 0.0005257657612673938\n",
      "157/391 [===========>..................] - ETA: 17s - loss: 0.8298 - acc: 0.7609\n",
      " Optimizer iteration 2894, batch 157\n",
      "\n",
      " Learning rate 0.0005249612539585113, Model learning rate 0.0005249612731859088\n",
      "158/391 [===========>..................] - ETA: 17s - loss: 0.8291 - acc: 0.7612\n",
      " Optimizer iteration 2895, batch 158\n",
      "\n",
      " Learning rate 0.0005241566897627535, Model learning rate 0.0005241566686891019\n",
      "159/391 [===========>..................] - ETA: 17s - loss: 0.8283 - acc: 0.7614\n",
      " Optimizer iteration 2896, batch 159\n",
      "\n",
      " Learning rate 0.0005233520628669512, Model learning rate 0.0005233520641922951\n",
      "160/391 [===========>..................] - ETA: 17s - loss: 0.8280 - acc: 0.7614\n",
      " Optimizer iteration 2897, batch 160\n",
      "\n",
      " Learning rate 0.0005225473753595585, Model learning rate 0.0005225474014878273\n",
      "161/391 [===========>..................] - ETA: 17s - loss: 0.8285 - acc: 0.7612\n",
      " Optimizer iteration 2898, batch 161\n",
      "\n",
      " Learning rate 0.0005217426293291868, Model learning rate 0.0005217426223680377\n",
      "162/391 [===========>..................] - ETA: 17s - loss: 0.8293 - acc: 0.7609\n",
      " Optimizer iteration 2899, batch 162\n",
      "\n",
      " Learning rate 0.0005209378268645998, Model learning rate 0.0005209378432482481\n",
      "163/391 [===========>..................] - ETA: 17s - loss: 0.8291 - acc: 0.7608\n",
      " Optimizer iteration 2900, batch 163\n",
      "\n",
      " Learning rate 0.0005201329700547076, Model learning rate 0.0005201329477131367\n",
      "164/391 [===========>..................] - ETA: 17s - loss: 0.8283 - acc: 0.7610\n",
      " Optimizer iteration 2901, batch 164\n",
      "\n",
      " Learning rate 0.0005193280609885611, Model learning rate 0.0005193280521780252\n",
      "165/391 [===========>..................] - ETA: 17s - loss: 0.8274 - acc: 0.7613\n",
      " Optimizer iteration 2902, batch 165\n",
      "\n",
      " Learning rate 0.000518523101755347, Model learning rate 0.0005185230984352529\n",
      "166/391 [===========>..................] - ETA: 17s - loss: 0.8271 - acc: 0.7614\n",
      " Optimizer iteration 2903, batch 166\n",
      "\n",
      " Learning rate 0.0005177180944443821, Model learning rate 0.0005177180864848197\n",
      "167/391 [===========>..................] - ETA: 17s - loss: 0.8277 - acc: 0.7612\n",
      " Optimizer iteration 2904, batch 167\n",
      "\n",
      " Learning rate 0.0005169130411451083, Model learning rate 0.0005169130163267255\n",
      "168/391 [===========>..................] - ETA: 17s - loss: 0.8271 - acc: 0.7617\n",
      " Optimizer iteration 2905, batch 168\n",
      "\n",
      " Learning rate 0.0005161079439470866, Model learning rate 0.0005161079461686313\n",
      "169/391 [===========>..................] - ETA: 17s - loss: 0.8272 - acc: 0.7616\n",
      " Optimizer iteration 2906, batch 169\n",
      "\n",
      " Learning rate 0.0005153028049399916, Model learning rate 0.0005153028178028762\n",
      "170/391 [============>.................] - ETA: 16s - loss: 0.8262 - acc: 0.7619\n",
      " Optimizer iteration 2907, batch 170\n",
      "\n",
      " Learning rate 0.0005144976262136073, Model learning rate 0.0005144976312294602\n",
      "171/391 [============>.................] - ETA: 16s - loss: 0.8259 - acc: 0.7618\n",
      " Optimizer iteration 2908, batch 171\n",
      "\n",
      " Learning rate 0.00051369240985782, Model learning rate 0.0005136923864483833\n",
      "172/391 [============>.................] - ETA: 16s - loss: 0.8261 - acc: 0.7617\n",
      " Optimizer iteration 2909, batch 172\n",
      "\n",
      " Learning rate 0.0005128871579626142, Model learning rate 0.0005128871416673064\n",
      "173/391 [============>.................] - ETA: 16s - loss: 0.8265 - acc: 0.7613\n",
      " Optimizer iteration 2910, batch 173\n",
      "\n",
      " Learning rate 0.0005120818726180662, Model learning rate 0.0005120818968862295\n",
      "174/391 [============>.................] - ETA: 16s - loss: 0.8258 - acc: 0.7616\n",
      " Optimizer iteration 2911, batch 174\n",
      "\n",
      " Learning rate 0.0005112765559143394, Model learning rate 0.0005112765356898308\n",
      "175/391 [============>.................] - ETA: 16s - loss: 0.8260 - acc: 0.7614\n",
      " Optimizer iteration 2912, batch 175\n",
      "\n",
      " Learning rate 0.0005104712099416785, Model learning rate 0.000510471232701093\n",
      "176/391 [============>.................] - ETA: 16s - loss: 0.8271 - acc: 0.7613\n",
      " Optimizer iteration 2913, batch 176\n",
      "\n",
      " Learning rate 0.0005096658367904042, Model learning rate 0.0005096658132970333\n",
      "177/391 [============>.................] - ETA: 16s - loss: 0.8276 - acc: 0.7613\n",
      " Optimizer iteration 2914, batch 177\n",
      "\n",
      " Learning rate 0.0005088604385509079, Model learning rate 0.0005088604521006346\n",
      "178/391 [============>.................] - ETA: 16s - loss: 0.8268 - acc: 0.7615\n",
      " Optimizer iteration 2915, batch 178\n",
      "\n",
      " Learning rate 0.0005080550173136456, Model learning rate 0.0005080550326965749\n",
      "179/391 [============>.................] - ETA: 16s - loss: 0.8265 - acc: 0.7615\n",
      " Optimizer iteration 2916, batch 179\n",
      "\n",
      " Learning rate 0.0005072495751691338, Model learning rate 0.0005072495550848544\n",
      "180/391 [============>.................] - ETA: 16s - loss: 0.8263 - acc: 0.7615\n",
      " Optimizer iteration 2917, batch 180\n",
      "\n",
      " Learning rate 0.0005064441142079425, Model learning rate 0.0005064441356807947\n",
      "181/391 [============>.................] - ETA: 16s - loss: 0.8268 - acc: 0.7611\n",
      " Optimizer iteration 2918, batch 181\n",
      "\n",
      " Learning rate 0.0005056386365206907, Model learning rate 0.0005056386580690742\n",
      "182/391 [============>.................] - ETA: 16s - loss: 0.8269 - acc: 0.7611\n",
      " Optimizer iteration 2919, batch 182\n",
      "\n",
      " Learning rate 0.0005048331441980416, Model learning rate 0.0005048331222496927\n",
      "\n",
      " Optimizer iteration 2920, batch 183\n",
      "\n",
      " Learning rate 0.0005040276393306949, Model learning rate 0.0005040276446379721\n",
      "184/391 [=============>................] - ETA: 15s - loss: 0.8255 - acc: 0.7614\n",
      " Optimizer iteration 2921, batch 184\n",
      "\n",
      " Learning rate 0.0005032221240093846, Model learning rate 0.0005032221088185906\n",
      "185/391 [=============>................] - ETA: 15s - loss: 0.8258 - acc: 0.7615\n",
      " Optimizer iteration 2922, batch 185\n",
      "\n",
      " Learning rate 0.0005024166003248702, Model learning rate 0.0005024165729992092\n",
      "\n",
      " Optimizer iteration 2923, batch 186\n",
      "\n",
      " Learning rate 0.000501611070367934, Model learning rate 0.0005016110953874886\n",
      "187/391 [=============>................] - ETA: 15s - loss: 0.8264 - acc: 0.7614\n",
      " Optimizer iteration 2924, batch 187\n",
      "\n",
      " Learning rate 0.0005008055362293743, Model learning rate 0.0005008055595681071\n",
      "188/391 [=============>................] - ETA: 15s - loss: 0.8263 - acc: 0.7612\n",
      " Optimizer iteration 2925, batch 188\n",
      "\n",
      " Learning rate 0.0005, Model learning rate 0.0005000000237487257\n",
      "189/391 [=============>................] - ETA: 15s - loss: 0.8265 - acc: 0.7611\n",
      " Optimizer iteration 2926, batch 189\n",
      "\n",
      " Learning rate 0.0004991944637706257, Model learning rate 0.0004991944879293442\n",
      "190/391 [=============>................] - ETA: 15s - loss: 0.8271 - acc: 0.7611\n",
      " Optimizer iteration 2927, batch 190\n",
      "\n",
      " Learning rate 0.000498388929632066, Model learning rate 0.0004983889521099627\n",
      "\n",
      " Optimizer iteration 2928, batch 191\n",
      "\n",
      " Learning rate 0.0004975833996751299, Model learning rate 0.0004975834162905812\n",
      "192/391 [=============>................] - ETA: 15s - loss: 0.8267 - acc: 0.7614\n",
      " Optimizer iteration 2929, batch 192\n",
      "\n",
      " Learning rate 0.0004967778759906157, Model learning rate 0.0004967778804711998\n",
      "193/391 [=============>................] - ETA: 15s - loss: 0.8265 - acc: 0.7613\n",
      " Optimizer iteration 2930, batch 193\n",
      "\n",
      " Learning rate 0.0004959723606693051, Model learning rate 0.0004959723446518183\n",
      "194/391 [=============>................] - ETA: 15s - loss: 0.8261 - acc: 0.7614\n",
      " Optimizer iteration 2931, batch 194\n",
      "\n",
      " Learning rate 0.0004951668558019585, Model learning rate 0.0004951668670400977\n",
      "195/391 [=============>................] - ETA: 15s - loss: 0.8258 - acc: 0.7615\n",
      " Optimizer iteration 2932, batch 195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Learning rate 0.0004943613634793092, Model learning rate 0.0004943613894283772\n",
      "196/391 [==============>...............] - ETA: 14s - loss: 0.8252 - acc: 0.7617\n",
      " Optimizer iteration 2933, batch 196\n",
      "\n",
      " Learning rate 0.0004935558857920576, Model learning rate 0.0004935559118166566\n",
      "197/391 [==============>...............] - ETA: 14s - loss: 0.8248 - acc: 0.7618\n",
      " Optimizer iteration 2934, batch 197\n",
      "\n",
      " Learning rate 0.0004927504248308663, Model learning rate 0.000492750434204936\n",
      "\n",
      " Optimizer iteration 2935, batch 198\n",
      "\n",
      " Learning rate 0.0004919449826863544, Model learning rate 0.0004919449565932155\n",
      "199/391 [==============>...............] - ETA: 14s - loss: 0.8248 - acc: 0.7619\n",
      " Optimizer iteration 2936, batch 199\n",
      "\n",
      " Learning rate 0.0004911395614490922, Model learning rate 0.0004911395371891558\n",
      "200/391 [==============>...............] - ETA: 14s - loss: 0.8241 - acc: 0.7621\n",
      " Optimizer iteration 2937, batch 200\n",
      "\n",
      " Learning rate 0.0004903341632095958, Model learning rate 0.0004903341759927571\n",
      "201/391 [==============>...............] - ETA: 14s - loss: 0.8242 - acc: 0.7621\n",
      " Optimizer iteration 2938, batch 201\n",
      "\n",
      " Learning rate 0.0004895287900583215, Model learning rate 0.0004895288147963583\n",
      "\n",
      " Optimizer iteration 2939, batch 202\n",
      "\n",
      " Learning rate 0.0004887234440856608, Model learning rate 0.0004887234535999596\n",
      "203/391 [==============>...............] - ETA: 14s - loss: 0.8233 - acc: 0.7625\n",
      " Optimizer iteration 2940, batch 203\n",
      "\n",
      " Learning rate 0.000487918127381934, Model learning rate 0.00048791812150739133\n",
      "204/391 [==============>...............] - ETA: 14s - loss: 0.8234 - acc: 0.7624\n",
      " Optimizer iteration 2941, batch 204\n",
      "\n",
      " Learning rate 0.0004871128420373859, Model learning rate 0.00048711284762248397\n",
      "205/391 [==============>...............] - ETA: 14s - loss: 0.8231 - acc: 0.7625\n",
      " Optimizer iteration 2942, batch 205\n",
      "\n",
      " Learning rate 0.00048630759014218, Model learning rate 0.00048630760284140706\n",
      "206/391 [==============>...............] - ETA: 14s - loss: 0.8235 - acc: 0.7621\n",
      " Optimizer iteration 2943, batch 206\n",
      "\n",
      " Learning rate 0.00048550237378639275, Model learning rate 0.0004855023871641606\n",
      "\n",
      " Optimizer iteration 2944, batch 207\n",
      "\n",
      " Learning rate 0.00048469719506000837, Model learning rate 0.0004846972005907446\n",
      "208/391 [==============>...............] - ETA: 14s - loss: 0.8234 - acc: 0.7622\n",
      " Optimizer iteration 2945, batch 208\n",
      "\n",
      " Learning rate 0.00048389205605291365, Model learning rate 0.0004838920431211591\n",
      "209/391 [===============>..............] - ETA: 14s - loss: 0.8229 - acc: 0.7624\n",
      " Optimizer iteration 2946, batch 209\n",
      "\n",
      " Learning rate 0.0004830869588548918, Model learning rate 0.0004830869729630649\n",
      "210/391 [===============>..............] - ETA: 13s - loss: 0.8237 - acc: 0.7623\n",
      " Optimizer iteration 2947, batch 210\n",
      "\n",
      " Learning rate 0.0004822819055556179, Model learning rate 0.00048228190280497074\n",
      "211/391 [===============>..............] - ETA: 13s - loss: 0.8238 - acc: 0.7622\n",
      " Optimizer iteration 2948, batch 211\n",
      "\n",
      " Learning rate 0.00048147689824465313, Model learning rate 0.0004814768908545375\n",
      "212/391 [===============>..............] - ETA: 13s - loss: 0.8237 - acc: 0.7622\n",
      " Optimizer iteration 2949, batch 212\n",
      "\n",
      " Learning rate 0.00048067193901143887, Model learning rate 0.00048067193711176515\n",
      "213/391 [===============>..............] - ETA: 13s - loss: 0.8234 - acc: 0.7623\n",
      " Optimizer iteration 2950, batch 213\n",
      "\n",
      " Learning rate 0.0004798670299452926, Model learning rate 0.0004798670415766537\n",
      "\n",
      " Optimizer iteration 2951, batch 214\n",
      "\n",
      " Learning rate 0.0004790621731354003, Model learning rate 0.00047906217514537275\n",
      "215/391 [===============>..............] - ETA: 13s - loss: 0.8230 - acc: 0.7624\n",
      " Optimizer iteration 2952, batch 215\n",
      "\n",
      " Learning rate 0.00047825737067081327, Model learning rate 0.0004782573669217527\n",
      "216/391 [===============>..............] - ETA: 13s - loss: 0.8238 - acc: 0.7623\n",
      " Optimizer iteration 2953, batch 216\n",
      "\n",
      " Learning rate 0.00047745262464044165, Model learning rate 0.00047745261690579355\n",
      "217/391 [===============>..............] - ETA: 13s - loss: 0.8237 - acc: 0.7623\n",
      " Optimizer iteration 2954, batch 217\n",
      "\n",
      " Learning rate 0.0004766479371330488, Model learning rate 0.0004766479250974953\n",
      "\n",
      " Optimizer iteration 2955, batch 218\n",
      "\n",
      " Learning rate 0.00047584331023724653, Model learning rate 0.00047584332060068846\n",
      "219/391 [===============>..............] - ETA: 13s - loss: 0.8235 - acc: 0.7626\n",
      " Optimizer iteration 2956, batch 219\n",
      "\n",
      " Learning rate 0.0004750387460414889, Model learning rate 0.00047503874520771205\n",
      "220/391 [===============>..............] - ETA: 13s - loss: 0.8238 - acc: 0.7623\n",
      " Optimizer iteration 2957, batch 220\n",
      "\n",
      " Learning rate 0.00047423424663406747, Model learning rate 0.000474234257126227\n",
      "221/391 [===============>..............] - ETA: 13s - loss: 0.8238 - acc: 0.7623\n",
      " Optimizer iteration 2958, batch 221\n",
      "\n",
      " Learning rate 0.0004734298141031057, Model learning rate 0.0004734298272524029\n",
      "222/391 [================>.............] - ETA: 13s - loss: 0.8239 - acc: 0.7624\n",
      " Optimizer iteration 2959, batch 222\n",
      "\n",
      " Learning rate 0.00047262545053655344, Model learning rate 0.0004726254555862397\n",
      "\n",
      " Optimizer iteration 2960, batch 223\n",
      "\n",
      " Learning rate 0.0004718211580221812, Model learning rate 0.00047182117123156786\n",
      "224/391 [================>.............] - ETA: 12s - loss: 0.8240 - acc: 0.7626\n",
      " Optimizer iteration 2961, batch 224\n",
      "\n",
      " Learning rate 0.00047101693864757605, Model learning rate 0.00047101694508455694\n",
      "225/391 [================>.............] - ETA: 12s - loss: 0.8240 - acc: 0.7625\n",
      " Optimizer iteration 2962, batch 225\n",
      "\n",
      " Learning rate 0.00047021279450013383, Model learning rate 0.0004702128062490374\n",
      "226/391 [================>.............] - ETA: 12s - loss: 0.8241 - acc: 0.7624\n",
      " Optimizer iteration 2963, batch 226\n",
      "\n",
      " Learning rate 0.000469408727667056, Model learning rate 0.00046940872562117875\n",
      "227/391 [================>.............] - ETA: 12s - loss: 0.8233 - acc: 0.7626\n",
      " Optimizer iteration 2964, batch 227\n",
      "\n",
      " Learning rate 0.0004686047402353433, Model learning rate 0.0004686047323048115\n",
      "228/391 [================>.............] - ETA: 12s - loss: 0.8233 - acc: 0.7627\n",
      " Optimizer iteration 2965, batch 228\n",
      "\n",
      " Learning rate 0.00046780083429179026, Model learning rate 0.0004678008262999356\n",
      "229/391 [================>.............] - ETA: 12s - loss: 0.8233 - acc: 0.7626\n",
      " Optimizer iteration 2966, batch 229\n",
      "\n",
      " Learning rate 0.00046699701192297994, Model learning rate 0.00046699700760655105\n",
      "230/391 [================>.............] - ETA: 12s - loss: 0.8236 - acc: 0.7625\n",
      " Optimizer iteration 2967, batch 230\n",
      "\n",
      " Learning rate 0.00046619327521527825, Model learning rate 0.0004661932762246579\n",
      "\n",
      " Optimizer iteration 2968, batch 231\n",
      "\n",
      " Learning rate 0.00046538962625482905, Model learning rate 0.0004653896321542561\n",
      "232/391 [================>.............] - ETA: 12s - loss: 0.8237 - acc: 0.7627\n",
      " Optimizer iteration 2969, batch 232\n",
      "\n",
      " Learning rate 0.0004645860671275483, Model learning rate 0.0004645860753953457\n",
      "233/391 [================>.............] - ETA: 12s - loss: 0.8235 - acc: 0.7627\n",
      " Optimizer iteration 2970, batch 233\n",
      "\n",
      " Learning rate 0.00046378259991911887, Model learning rate 0.00046378260594792664\n",
      "234/391 [================>.............] - ETA: 12s - loss: 0.8240 - acc: 0.7626\n",
      " Optimizer iteration 2971, batch 234\n",
      "\n",
      " Learning rate 0.0004629792267149849, Model learning rate 0.00046297922381199896\n",
      "235/391 [=================>............] - ETA: 11s - loss: 0.8238 - acc: 0.7626\n",
      " Optimizer iteration 2972, batch 235\n",
      "\n",
      " Learning rate 0.00046217594960034714, Model learning rate 0.0004621759580913931\n",
      "236/391 [=================>............] - ETA: 11s - loss: 0.8250 - acc: 0.7621\n",
      " Optimizer iteration 2973, batch 236\n",
      "\n",
      " Learning rate 0.0004613727706601557, Model learning rate 0.00046137277968227863\n",
      "237/391 [=================>............] - ETA: 11s - loss: 0.8256 - acc: 0.7620\n",
      " Optimizer iteration 2974, batch 237\n",
      "\n",
      " Learning rate 0.00046056969197910707, Model learning rate 0.0004605696885846555\n",
      "238/391 [=================>............] - ETA: 11s - loss: 0.8258 - acc: 0.7619\n",
      " Optimizer iteration 2975, batch 238\n",
      "\n",
      " Learning rate 0.00045976671564163706, Model learning rate 0.00045976671390235424\n",
      "239/391 [=================>............] - ETA: 11s - loss: 0.8257 - acc: 0.7620\n",
      " Optimizer iteration 2976, batch 239\n",
      "\n",
      " Learning rate 0.0004589638437319157, Model learning rate 0.0004589638556353748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/391 [=================>............] - ETA: 11s - loss: 0.8259 - acc: 0.7620\n",
      " Optimizer iteration 2977, batch 240\n",
      "\n",
      " Learning rate 0.00045816107833384235, Model learning rate 0.0004581610846798867\n",
      "241/391 [=================>............] - ETA: 11s - loss: 0.8257 - acc: 0.7620\n",
      " Optimizer iteration 2978, batch 241\n",
      "\n",
      " Learning rate 0.00045735842153103934, Model learning rate 0.00045735843013972044\n",
      "242/391 [=================>............] - ETA: 11s - loss: 0.8256 - acc: 0.7621\n",
      " Optimizer iteration 2979, batch 242\n",
      "\n",
      " Learning rate 0.0004565558754068477, Model learning rate 0.00045655586291104555\n",
      "243/391 [=================>............] - ETA: 11s - loss: 0.8258 - acc: 0.7619\n",
      " Optimizer iteration 2980, batch 243\n",
      "\n",
      " Learning rate 0.00045575344204432084, Model learning rate 0.00045575344120152295\n",
      "244/391 [=================>............] - ETA: 11s - loss: 0.8256 - acc: 0.7619\n",
      " Optimizer iteration 2981, batch 244\n",
      "\n",
      " Learning rate 0.00045495112352621957, Model learning rate 0.00045495113590732217\n",
      "245/391 [=================>............] - ETA: 11s - loss: 0.8253 - acc: 0.7618\n",
      " Optimizer iteration 2982, batch 245\n",
      "\n",
      " Learning rate 0.0004541489219350069, Model learning rate 0.00045414891792461276\n",
      "246/391 [=================>............] - ETA: 11s - loss: 0.8250 - acc: 0.7620\n",
      " Optimizer iteration 2983, batch 246\n",
      "\n",
      " Learning rate 0.0004533468393528421, Model learning rate 0.00045334684546105564\n",
      "247/391 [=================>............] - ETA: 11s - loss: 0.8246 - acc: 0.7621\n",
      " Optimizer iteration 2984, batch 247\n",
      "\n",
      " Learning rate 0.0004525448778615752, Model learning rate 0.00045254488941282034\n",
      "248/391 [==================>...........] - ETA: 11s - loss: 0.8252 - acc: 0.7619\n",
      " Optimizer iteration 2985, batch 248\n",
      "\n",
      " Learning rate 0.00045174303954274245, Model learning rate 0.00045174304977990687\n",
      "\n",
      " Optimizer iteration 2986, batch 249\n",
      "\n",
      " Learning rate 0.0004509413264775604, Model learning rate 0.0004509413265623152\n",
      "250/391 [==================>...........] - ETA: 10s - loss: 0.8245 - acc: 0.7623\n",
      " Optimizer iteration 2987, batch 250\n",
      "\n",
      " Learning rate 0.0004501397407469201, Model learning rate 0.00045013974886387587\n",
      "251/391 [==================>...........] - ETA: 10s - loss: 0.8244 - acc: 0.7624\n",
      " Optimizer iteration 2988, batch 251\n",
      "\n",
      " Learning rate 0.0004493382844313826, Model learning rate 0.00044933828758075833\n",
      "252/391 [==================>...........] - ETA: 10s - loss: 0.8243 - acc: 0.7625\n",
      " Optimizer iteration 2989, batch 252\n",
      "\n",
      " Learning rate 0.0004485369596111724, Model learning rate 0.0004485369718167931\n",
      "\n",
      " Optimizer iteration 2990, batch 253\n",
      "\n",
      " Learning rate 0.00044773576836617336, Model learning rate 0.00044773577246814966\n",
      "254/391 [==================>...........] - ETA: 10s - loss: 0.8247 - acc: 0.7625\n",
      " Optimizer iteration 2991, batch 254\n",
      "\n",
      " Learning rate 0.0004469347127759222, Model learning rate 0.0004469347186386585\n",
      "255/391 [==================>...........] - ETA: 10s - loss: 0.8250 - acc: 0.7623\n",
      " Optimizer iteration 2992, batch 255\n",
      "\n",
      " Learning rate 0.0004461337949196036, Model learning rate 0.0004461337812244892\n",
      "256/391 [==================>...........] - ETA: 10s - loss: 0.8248 - acc: 0.7623\n",
      " Optimizer iteration 2993, batch 256\n",
      "\n",
      " Learning rate 0.0004453330168760451, Model learning rate 0.00044533301843330264\n",
      "257/391 [==================>...........] - ETA: 10s - loss: 0.8246 - acc: 0.7624\n",
      " Optimizer iteration 2994, batch 257\n",
      "\n",
      " Learning rate 0.00044453238072371116, Model learning rate 0.0004445323720574379\n",
      "258/391 [==================>...........] - ETA: 10s - loss: 0.8242 - acc: 0.7626\n",
      " Optimizer iteration 2995, batch 258\n",
      "\n",
      " Learning rate 0.0004437318885406973, Model learning rate 0.0004437319003045559\n",
      "259/391 [==================>...........] - ETA: 10s - loss: 0.8243 - acc: 0.7627\n",
      " Optimizer iteration 2996, batch 259\n",
      "\n",
      " Learning rate 0.0004429315424047263, Model learning rate 0.0004429315449669957\n",
      "260/391 [==================>...........] - ETA: 10s - loss: 0.8243 - acc: 0.7628\n",
      " Optimizer iteration 2997, batch 260\n",
      "\n",
      " Learning rate 0.0004421313443931416, Model learning rate 0.0004421313351485878\n",
      "261/391 [===================>..........] - ETA: 10s - loss: 0.8241 - acc: 0.7628\n",
      " Optimizer iteration 2998, batch 261\n",
      "\n",
      " Learning rate 0.00044133129658290195, Model learning rate 0.00044133129995316267\n",
      "262/391 [===================>..........] - ETA: 9s - loss: 0.8241 - acc: 0.7628 \n",
      " Optimizer iteration 2999, batch 262\n",
      "\n",
      " Learning rate 0.00044053140105057635, Model learning rate 0.0004405314102768898\n",
      "263/391 [===================>..........] - ETA: 9s - loss: 0.8240 - acc: 0.7628\n",
      " Optimizer iteration 3000, batch 263\n",
      "\n",
      " Learning rate 0.00043973165987233853, Model learning rate 0.0004397316661197692\n",
      "264/391 [===================>..........] - ETA: 9s - loss: 0.8242 - acc: 0.7628\n",
      " Optimizer iteration 3001, batch 264\n",
      "\n",
      " Learning rate 0.0004389320751239617, Model learning rate 0.0004389320674818009\n",
      "265/391 [===================>..........] - ETA: 9s - loss: 0.8243 - acc: 0.7629\n",
      " Optimizer iteration 3002, batch 265\n",
      "\n",
      " Learning rate 0.00043813264888081284, Model learning rate 0.00043813264346681535\n",
      "266/391 [===================>..........] - ETA: 9s - loss: 0.8241 - acc: 0.7629\n",
      " Optimizer iteration 3003, batch 266\n",
      "\n",
      " Learning rate 0.00043733338321784784, Model learning rate 0.00043733339407481253\n",
      "267/391 [===================>..........] - ETA: 9s - loss: 0.8235 - acc: 0.7632\n",
      " Optimizer iteration 3004, batch 267\n",
      "\n",
      " Learning rate 0.0004365342802096057, Model learning rate 0.000436534290201962\n",
      "268/391 [===================>..........] - ETA: 9s - loss: 0.8230 - acc: 0.7633\n",
      " Optimizer iteration 3005, batch 268\n",
      "\n",
      " Learning rate 0.00043573534193020277, Model learning rate 0.00043573533184826374\n",
      "269/391 [===================>..........] - ETA: 9s - loss: 0.8224 - acc: 0.7635\n",
      " Optimizer iteration 3006, batch 269\n",
      "\n",
      " Learning rate 0.0004349365704533284, Model learning rate 0.0004349365772213787\n",
      "270/391 [===================>..........] - ETA: 9s - loss: 0.8224 - acc: 0.7633\n",
      " Optimizer iteration 3007, batch 270\n",
      "\n",
      " Learning rate 0.0004341379678522389, Model learning rate 0.0004341379681136459\n",
      "271/391 [===================>..........] - ETA: 9s - loss: 0.8223 - acc: 0.7635\n",
      " Optimizer iteration 3008, batch 271\n",
      "\n",
      " Learning rate 0.00043333953619975206, Model learning rate 0.0004333395336288959\n",
      "272/391 [===================>..........] - ETA: 9s - loss: 0.8229 - acc: 0.7633\n",
      " Optimizer iteration 3009, batch 272\n",
      "\n",
      " Learning rate 0.00043254127756824214, Model learning rate 0.0004325412737671286\n",
      "273/391 [===================>..........] - ETA: 9s - loss: 0.8227 - acc: 0.7634\n",
      " Optimizer iteration 3010, batch 273\n",
      "\n",
      " Learning rate 0.0004317431940296343, Model learning rate 0.00043174318852834404\n",
      "274/391 [====================>.........] - ETA: 9s - loss: 0.8224 - acc: 0.7635\n",
      " Optimizer iteration 3011, batch 274\n",
      "\n",
      " Learning rate 0.000430945287655399, Model learning rate 0.0004309452779125422\n",
      "275/391 [====================>.........] - ETA: 8s - loss: 0.8226 - acc: 0.7635\n",
      " Optimizer iteration 3012, batch 275\n",
      "\n",
      " Learning rate 0.00043014756051654705, Model learning rate 0.0004301475710235536\n",
      "276/391 [====================>.........] - ETA: 8s - loss: 0.8224 - acc: 0.7634\n",
      " Optimizer iteration 3013, batch 276\n",
      "\n",
      " Learning rate 0.00042935001468362405, Model learning rate 0.0004293500096537173\n",
      "277/391 [====================>.........] - ETA: 8s - loss: 0.8221 - acc: 0.7635\n",
      " Optimizer iteration 3014, batch 277\n",
      "\n",
      " Learning rate 0.00042855265222670517, Model learning rate 0.00042855265201069415\n",
      "278/391 [====================>.........] - ETA: 8s - loss: 0.8219 - acc: 0.7635\n",
      " Optimizer iteration 3015, batch 278\n",
      "\n",
      " Learning rate 0.0004277554752153895, Model learning rate 0.00042775546899065375\n",
      "279/391 [====================>.........] - ETA: 8s - loss: 0.8220 - acc: 0.7634\n",
      " Optimizer iteration 3016, batch 279\n",
      "\n",
      " Learning rate 0.00042695848571879425, Model learning rate 0.00042695848969742656\n",
      "280/391 [====================>.........] - ETA: 8s - loss: 0.8218 - acc: 0.7634\n",
      " Optimizer iteration 3017, batch 280\n",
      "\n",
      " Learning rate 0.0004261616858055508, Model learning rate 0.0004261616850271821\n",
      "281/391 [====================>.........] - ETA: 8s - loss: 0.8221 - acc: 0.7633\n",
      " Optimizer iteration 3018, batch 281\n",
      "\n",
      " Learning rate 0.000425365077543798, Model learning rate 0.00042536508408375084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/391 [====================>.........] - ETA: 8s - loss: 0.8219 - acc: 0.7633\n",
      " Optimizer iteration 3019, batch 282\n",
      "\n",
      " Learning rate 0.00042456866300117724, Model learning rate 0.0004245686577633023\n",
      "\n",
      " Optimizer iteration 3020, batch 283\n",
      "\n",
      " Learning rate 0.0004237724442448273, Model learning rate 0.000423772435169667\n",
      "284/391 [====================>.........] - ETA: 8s - loss: 0.8222 - acc: 0.7631\n",
      " Optimizer iteration 3021, batch 284\n",
      "\n",
      " Learning rate 0.00042297642334137875, Model learning rate 0.0004229764163028449\n",
      "285/391 [====================>.........] - ETA: 8s - loss: 0.8222 - acc: 0.7631\n",
      " Optimizer iteration 3022, batch 285\n",
      "\n",
      " Learning rate 0.00042218060235694826, Model learning rate 0.00042218060116283596\n",
      "286/391 [====================>.........] - ETA: 8s - loss: 0.8223 - acc: 0.7631\n",
      " Optimizer iteration 3023, batch 286\n",
      "\n",
      " Learning rate 0.000421384983357134, Model learning rate 0.0004213849897496402\n",
      "287/391 [=====================>........] - ETA: 8s - loss: 0.8230 - acc: 0.7629\n",
      " Optimizer iteration 3024, batch 287\n",
      "\n",
      " Learning rate 0.0004205895684070098, Model learning rate 0.0004205895820632577\n",
      "288/391 [=====================>........] - ETA: 7s - loss: 0.8228 - acc: 0.7630\n",
      " Optimizer iteration 3025, batch 288\n",
      "\n",
      " Learning rate 0.0004197943595711198, Model learning rate 0.0004197943489998579\n",
      "289/391 [=====================>........] - ETA: 7s - loss: 0.8230 - acc: 0.7628\n",
      " Optimizer iteration 3026, batch 289\n",
      "\n",
      " Learning rate 0.0004189993589134735, Model learning rate 0.00041899934876710176\n",
      "290/391 [=====================>........] - ETA: 7s - loss: 0.8224 - acc: 0.7630\n",
      " Optimizer iteration 3027, batch 290\n",
      "\n",
      " Learning rate 0.0004182045684975391, Model learning rate 0.0004182045813649893\n",
      "291/391 [=====================>........] - ETA: 7s - loss: 0.8225 - acc: 0.7629\n",
      " Optimizer iteration 3028, batch 291\n",
      "\n",
      " Learning rate 0.0004174099903862403, Model learning rate 0.00041740998858585954\n",
      "292/391 [=====================>........] - ETA: 7s - loss: 0.8227 - acc: 0.7627\n",
      " Optimizer iteration 3029, batch 292\n",
      "\n",
      " Learning rate 0.0004166156266419489, Model learning rate 0.00041661562863737345\n",
      "293/391 [=====================>........] - ETA: 7s - loss: 0.8227 - acc: 0.7627\n",
      " Optimizer iteration 3030, batch 293\n",
      "\n",
      " Learning rate 0.0004158214793264807, Model learning rate 0.00041582147241570055\n",
      "294/391 [=====================>........] - ETA: 7s - loss: 0.8227 - acc: 0.7627\n",
      " Optimizer iteration 3031, batch 294\n",
      "\n",
      " Learning rate 0.00041502755050108975, Model learning rate 0.0004150275490246713\n",
      "\n",
      " Optimizer iteration 3032, batch 295\n",
      "\n",
      " Learning rate 0.00041423384222646285, Model learning rate 0.0004142338293604553\n",
      "296/391 [=====================>........] - ETA: 7s - loss: 0.8232 - acc: 0.7627\n",
      " Optimizer iteration 3033, batch 296\n",
      "\n",
      " Learning rate 0.00041344035656271436, Model learning rate 0.0004134403425268829\n",
      "\n",
      " Optimizer iteration 3034, batch 297\n",
      "\n",
      " Learning rate 0.0004126470955693806, Model learning rate 0.00041264708852395415\n",
      "298/391 [=====================>........] - ETA: 7s - loss: 0.8233 - acc: 0.7626\n",
      " Optimizer iteration 3035, batch 298\n",
      "\n",
      " Learning rate 0.0004118540613054155, Model learning rate 0.0004118540673516691\n",
      "299/391 [=====================>........] - ETA: 7s - loss: 0.8237 - acc: 0.7625\n",
      " Optimizer iteration 3036, batch 299\n",
      "\n",
      " Learning rate 0.00041106125582918385, Model learning rate 0.0004110612499061972\n",
      "300/391 [======================>.......] - ETA: 7s - loss: 0.8239 - acc: 0.7625\n",
      " Optimizer iteration 3037, batch 300\n",
      "\n",
      " Learning rate 0.0004102686811984568, Model learning rate 0.0004102686943951994\n",
      "301/391 [======================>.......] - ETA: 6s - loss: 0.8242 - acc: 0.7623\n",
      " Optimizer iteration 3038, batch 301\n",
      "\n",
      " Learning rate 0.00040947633947040614, Model learning rate 0.00040947634261101484\n",
      "302/391 [======================>.......] - ETA: 6s - loss: 0.8242 - acc: 0.7623\n",
      " Optimizer iteration 3039, batch 302\n",
      "\n",
      " Learning rate 0.00040868423270159945, Model learning rate 0.0004086842236574739\n",
      "\n",
      " Optimizer iteration 3040, batch 303\n",
      "\n",
      " Learning rate 0.0004078923629479943, Model learning rate 0.0004078923666384071\n",
      "304/391 [======================>.......] - ETA: 6s - loss: 0.8241 - acc: 0.7624\n",
      " Optimizer iteration 3041, batch 304\n",
      "\n",
      " Learning rate 0.00040710073226493307, Model learning rate 0.00040710074244998395\n",
      "\n",
      " Optimizer iteration 3042, batch 305\n",
      "\n",
      " Learning rate 0.0004063093427071376, Model learning rate 0.00040630935109220445\n",
      "306/391 [======================>.......] - ETA: 6s - loss: 0.8240 - acc: 0.7625\n",
      " Optimizer iteration 3043, batch 306\n",
      "\n",
      " Learning rate 0.00040551819632870433, Model learning rate 0.0004055181925650686\n",
      "\n",
      " Optimizer iteration 3044, batch 307\n",
      "\n",
      " Learning rate 0.0004047272951830976, Model learning rate 0.00040472729597240686\n",
      "308/391 [======================>.......] - ETA: 6s - loss: 0.8239 - acc: 0.7627\n",
      " Optimizer iteration 3045, batch 308\n",
      "\n",
      " Learning rate 0.00040393664132314577, Model learning rate 0.0004039366322103888\n",
      "309/391 [======================>.......] - ETA: 6s - loss: 0.8239 - acc: 0.7626\n",
      " Optimizer iteration 3046, batch 309\n",
      "\n",
      " Learning rate 0.0004031462368010357, Model learning rate 0.0004031462303828448\n",
      "310/391 [======================>.......] - ETA: 6s - loss: 0.8234 - acc: 0.7628\n",
      " Optimizer iteration 3047, batch 310\n",
      "\n",
      " Learning rate 0.0004023560836683065, Model learning rate 0.00040235609048977494\n",
      "311/391 [======================>.......] - ETA: 6s - loss: 0.8241 - acc: 0.7627\n",
      " Optimizer iteration 3048, batch 311\n",
      "\n",
      " Learning rate 0.000401566183975845, Model learning rate 0.00040156618342734873\n",
      "312/391 [======================>.......] - ETA: 6s - loss: 0.8239 - acc: 0.7627\n",
      " Optimizer iteration 3049, batch 312\n",
      "\n",
      " Learning rate 0.00040077653977388015, Model learning rate 0.00040077653829939663\n",
      "313/391 [=======================>......] - ETA: 6s - loss: 0.8235 - acc: 0.7629\n",
      " Optimizer iteration 3050, batch 313\n",
      "\n",
      " Learning rate 0.0003999871531119779, Model learning rate 0.00039998715510591865\n",
      "314/391 [=======================>......] - ETA: 5s - loss: 0.8234 - acc: 0.7627\n",
      " Optimizer iteration 3051, batch 314\n",
      "\n",
      " Learning rate 0.00039919802603903553, Model learning rate 0.00039919803384691477\n",
      "\n",
      " Optimizer iteration 3052, batch 315\n",
      "\n",
      " Learning rate 0.0003984091606032768, Model learning rate 0.000398409174522385\n",
      "316/391 [=======================>......] - ETA: 5s - loss: 0.8239 - acc: 0.7627\n",
      " Optimizer iteration 3053, batch 316\n",
      "\n",
      " Learning rate 0.0003976205588522461, Model learning rate 0.0003976205480284989\n",
      "\n",
      " Optimizer iteration 3054, batch 317\n",
      "\n",
      " Learning rate 0.0003968322228328041, Model learning rate 0.00039683221257291734\n",
      "318/391 [=======================>......] - ETA: 5s - loss: 0.8248 - acc: 0.7624\n",
      " Optimizer iteration 3055, batch 318\n",
      "\n",
      " Learning rate 0.0003960441545911204, Model learning rate 0.00039604416815564036\n",
      "\n",
      " Optimizer iteration 3056, batch 319\n",
      "\n",
      " Learning rate 0.00039525635617267075, Model learning rate 0.00039525635656900704\n",
      "320/391 [=======================>......] - ETA: 5s - loss: 0.8250 - acc: 0.7623\n",
      " Optimizer iteration 3057, batch 320\n",
      "\n",
      " Learning rate 0.00039446882962223027, Model learning rate 0.0003944688360206783\n",
      "\n",
      " Optimizer iteration 3058, batch 321\n",
      "\n",
      " Learning rate 0.0003936815769838682, Model learning rate 0.00039368157740682364\n",
      "322/391 [=======================>......] - ETA: 5s - loss: 0.8252 - acc: 0.7620\n",
      " Optimizer iteration 3059, batch 322\n",
      "\n",
      " Learning rate 0.0003928946003009431, Model learning rate 0.00039289460983127356\n",
      "\n",
      " Optimizer iteration 3060, batch 323\n",
      "\n",
      " Learning rate 0.000392107901616097, Model learning rate 0.0003921079041901976\n",
      "324/391 [=======================>......] - ETA: 5s - loss: 0.8261 - acc: 0.7617\n",
      " Optimizer iteration 3061, batch 324\n",
      "\n",
      " Learning rate 0.00039132148297125053, Model learning rate 0.0003913214895874262\n",
      "325/391 [=======================>......] - ETA: 5s - loss: 0.8261 - acc: 0.7616\n",
      " Optimizer iteration 3062, batch 325\n",
      "\n",
      " Learning rate 0.0003905353464075975, Model learning rate 0.0003905353369191289\n",
      "326/391 [========================>.....] - ETA: 5s - loss: 0.8258 - acc: 0.7618\n",
      " Optimizer iteration 3063, batch 326\n",
      "\n",
      " Learning rate 0.0003897494939655995, Model learning rate 0.00038974950439296663\n",
      "\n",
      " Optimizer iteration 3064, batch 327\n",
      "\n",
      " Learning rate 0.00038896392768498074, Model learning rate 0.00038896393380127847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/391 [========================>.....] - ETA: 4s - loss: 0.8252 - acc: 0.7621\n",
      " Optimizer iteration 3065, batch 328\n",
      "\n",
      " Learning rate 0.00038817864960472237, Model learning rate 0.0003881786542478949\n",
      "\n",
      " Optimizer iteration 3066, batch 329\n",
      "\n",
      " Learning rate 0.00038739366176305785, Model learning rate 0.00038739366573281586\n",
      "330/391 [========================>.....] - ETA: 4s - loss: 0.8259 - acc: 0.7620\n",
      " Optimizer iteration 3067, batch 330\n",
      "\n",
      " Learning rate 0.00038660896619746734, Model learning rate 0.0003866089682560414\n",
      "331/391 [========================>.....] - ETA: 4s - loss: 0.8254 - acc: 0.7621\n",
      " Optimizer iteration 3068, batch 331\n",
      "\n",
      " Learning rate 0.0003858245649446721, Model learning rate 0.0003858245618175715\n",
      "332/391 [========================>.....] - ETA: 4s - loss: 0.8252 - acc: 0.7621\n",
      " Optimizer iteration 3069, batch 332\n",
      "\n",
      " Learning rate 0.00038504046004062974, Model learning rate 0.0003850404464174062\n",
      "333/391 [========================>.....] - ETA: 4s - loss: 0.8254 - acc: 0.7619\n",
      " Optimizer iteration 3070, batch 333\n",
      "\n",
      " Learning rate 0.0003842566535205286, Model learning rate 0.0003842566511593759\n",
      "\n",
      " Optimizer iteration 3071, batch 334\n",
      "\n",
      " Learning rate 0.00038347314741878227, Model learning rate 0.0003834731469396502\n",
      "335/391 [========================>.....] - ETA: 4s - loss: 0.8257 - acc: 0.7617\n",
      " Optimizer iteration 3072, batch 335\n",
      "\n",
      " Learning rate 0.000382689943769025, Model learning rate 0.000382689933758229\n",
      "336/391 [========================>.....] - ETA: 4s - loss: 0.8256 - acc: 0.7618\n",
      " Optimizer iteration 3073, batch 336\n",
      "\n",
      " Learning rate 0.00038190704460410585, Model learning rate 0.0003819070407189429\n",
      "337/391 [========================>.....] - ETA: 4s - loss: 0.8259 - acc: 0.7618\n",
      " Optimizer iteration 3074, batch 337\n",
      "\n",
      " Learning rate 0.00038112445195608334, Model learning rate 0.0003811244387179613\n",
      "\n",
      " Optimizer iteration 3075, batch 338\n",
      "\n",
      " Learning rate 0.00038034216785622126, Model learning rate 0.00038034215685911477\n",
      "339/391 [=========================>....] - ETA: 4s - loss: 0.8259 - acc: 0.7618\n",
      " Optimizer iteration 3076, batch 339\n",
      "\n",
      " Learning rate 0.00037956019433498124, Model learning rate 0.00037956019514240324\n",
      "340/391 [=========================>....] - ETA: 3s - loss: 0.8256 - acc: 0.7619\n",
      " Optimizer iteration 3077, batch 340\n",
      "\n",
      " Learning rate 0.0003787785334220196, Model learning rate 0.0003787785244639963\n",
      "\n",
      " Optimizer iteration 3078, batch 341\n",
      "\n",
      " Learning rate 0.00037799718714618124, Model learning rate 0.00037799717392772436\n",
      "342/391 [=========================>....] - ETA: 3s - loss: 0.8256 - acc: 0.7618\n",
      " Optimizer iteration 3079, batch 342\n",
      "\n",
      " Learning rate 0.00037721615753549446, Model learning rate 0.00037721614353358746\n",
      "343/391 [=========================>....] - ETA: 3s - loss: 0.8253 - acc: 0.7619\n",
      " Optimizer iteration 3080, batch 343\n",
      "\n",
      " Learning rate 0.0003764354466171652, Model learning rate 0.0003764354332815856\n",
      "344/391 [=========================>....] - ETA: 3s - loss: 0.8249 - acc: 0.7620\n",
      " Optimizer iteration 3081, batch 344\n",
      "\n",
      " Learning rate 0.0003756550564175727, Model learning rate 0.0003756550431717187\n",
      "345/391 [=========================>....] - ETA: 3s - loss: 0.8245 - acc: 0.7622\n",
      " Optimizer iteration 3082, batch 345\n",
      "\n",
      " Learning rate 0.00037487498896226335, Model learning rate 0.00037487500230781734\n",
      "\n",
      " Optimizer iteration 3083, batch 346\n",
      "\n",
      " Learning rate 0.00037409524627594605, Model learning rate 0.00037409525248222053\n",
      "347/391 [=========================>....] - ETA: 3s - loss: 0.8246 - acc: 0.7623\n",
      " Optimizer iteration 3084, batch 347\n",
      "\n",
      " Learning rate 0.0003733158303824868, Model learning rate 0.00037331582279875875\n",
      "348/391 [=========================>....] - ETA: 3s - loss: 0.8246 - acc: 0.7623\n",
      " Optimizer iteration 3085, batch 348\n",
      "\n",
      " Learning rate 0.0003725367433049033, Model learning rate 0.00037253674236126244\n",
      "349/391 [=========================>....] - ETA: 3s - loss: 0.8244 - acc: 0.7624\n",
      " Optimizer iteration 3086, batch 349\n",
      "\n",
      " Learning rate 0.0003717579870653601, Model learning rate 0.00037175798206590116\n",
      "\n",
      " Optimizer iteration 3087, batch 350\n",
      "\n",
      " Learning rate 0.0003709795636851622, Model learning rate 0.00037097957101650536\n",
      "351/391 [=========================>....] - ETA: 3s - loss: 0.8245 - acc: 0.7624\n",
      " Optimizer iteration 3088, batch 351\n",
      "\n",
      " Learning rate 0.00037020147518475137, Model learning rate 0.0003702014801092446\n",
      "352/391 [==========================>...] - ETA: 3s - loss: 0.8242 - acc: 0.7625\n",
      " Optimizer iteration 3089, batch 352\n",
      "\n",
      " Learning rate 0.00036942372358370024, Model learning rate 0.00036942370934411883\n",
      "353/391 [==========================>...] - ETA: 2s - loss: 0.8239 - acc: 0.7626\n",
      " Optimizer iteration 3090, batch 353\n",
      "\n",
      " Learning rate 0.0003686463109007065, Model learning rate 0.000368646316928789\n",
      "354/391 [==========================>...] - ETA: 2s - loss: 0.8240 - acc: 0.7626\n",
      " Optimizer iteration 3091, batch 354\n",
      "\n",
      " Learning rate 0.0003678692391535886, Model learning rate 0.00036786924465559423\n",
      "355/391 [==========================>...] - ETA: 2s - loss: 0.8241 - acc: 0.7625\n",
      " Optimizer iteration 3092, batch 355\n",
      "\n",
      " Learning rate 0.00036709251035927997, Model learning rate 0.0003670925216283649\n",
      "356/391 [==========================>...] - ETA: 2s - loss: 0.8240 - acc: 0.7625\n",
      " Optimizer iteration 3093, batch 356\n",
      "\n",
      " Learning rate 0.0003663161265338235, Model learning rate 0.00036631611874327064\n",
      "357/391 [==========================>...] - ETA: 2s - loss: 0.8241 - acc: 0.7624\n",
      " Optimizer iteration 3094, batch 357\n",
      "\n",
      " Learning rate 0.00036554008969236717, Model learning rate 0.0003655400942079723\n",
      "358/391 [==========================>...] - ETA: 2s - loss: 0.8242 - acc: 0.7624\n",
      " Optimizer iteration 3095, batch 358\n",
      "\n",
      " Learning rate 0.0003647644018491582, Model learning rate 0.00036476438981480896\n",
      "359/391 [==========================>...] - ETA: 2s - loss: 0.8241 - acc: 0.7623\n",
      " Optimizer iteration 3096, batch 359\n",
      "\n",
      " Learning rate 0.00036398906501753785, Model learning rate 0.0003639890637714416\n",
      "360/391 [==========================>...] - ETA: 2s - loss: 0.8241 - acc: 0.7624\n",
      " Optimizer iteration 3097, batch 360\n",
      "\n",
      " Learning rate 0.0003632140812099368, Model learning rate 0.0003632140869740397\n",
      "\n",
      " Optimizer iteration 3098, batch 361\n",
      "\n",
      " Learning rate 0.00036243945243786836, Model learning rate 0.00036243945942260325\n",
      "362/391 [==========================>...] - ETA: 2s - loss: 0.8233 - acc: 0.7628\n",
      " Optimizer iteration 3099, batch 362\n",
      "\n",
      " Learning rate 0.00036166518071192546, Model learning rate 0.0003616651811171323\n",
      "363/391 [==========================>...] - ETA: 2s - loss: 0.8228 - acc: 0.7630\n",
      " Optimizer iteration 3100, batch 363\n",
      "\n",
      " Learning rate 0.0003608912680417737, Model learning rate 0.0003608912811614573\n",
      "364/391 [==========================>...] - ETA: 2s - loss: 0.8231 - acc: 0.7629\n",
      " Optimizer iteration 3101, batch 364\n",
      "\n",
      " Learning rate 0.0003601177164361469, Model learning rate 0.0003601177304517478\n",
      "365/391 [===========================>..] - ETA: 2s - loss: 0.8229 - acc: 0.7631\n",
      " Optimizer iteration 3102, batch 365\n",
      "\n",
      " Learning rate 0.00035934452790284177, Model learning rate 0.00035934452898800373\n",
      "366/391 [===========================>..] - ETA: 1s - loss: 0.8229 - acc: 0.7632\n",
      " Optimizer iteration 3103, batch 366\n",
      "\n",
      " Learning rate 0.00035857170444871254, Model learning rate 0.0003585717058740556\n",
      "367/391 [===========================>..] - ETA: 1s - loss: 0.8233 - acc: 0.7631\n",
      " Optimizer iteration 3104, batch 367\n",
      "\n",
      " Learning rate 0.0003577992480796658, Model learning rate 0.00035779926110990345\n",
      "368/391 [===========================>..] - ETA: 1s - loss: 0.8231 - acc: 0.7631\n",
      " Optimizer iteration 3105, batch 368\n",
      "\n",
      " Learning rate 0.00035702716080065545, Model learning rate 0.00035702716559171677\n",
      "\n",
      " Optimizer iteration 3106, batch 369\n",
      "\n",
      " Learning rate 0.00035625544461567727, Model learning rate 0.000356255448423326\n",
      "370/391 [===========================>..] - ETA: 1s - loss: 0.8226 - acc: 0.7633\n",
      " Optimizer iteration 3107, batch 370\n",
      "\n",
      " Learning rate 0.0003554841015277641, Model learning rate 0.0003554841096047312\n",
      "371/391 [===========================>..] - ETA: 1s - loss: 0.8230 - acc: 0.7632\n",
      " Optimizer iteration 3108, batch 371\n",
      "\n",
      " Learning rate 0.00035471313353898054, Model learning rate 0.00035471312003210187\n",
      "372/391 [===========================>..] - ETA: 1s - loss: 0.8230 - acc: 0.7631\n",
      " Optimizer iteration 3109, batch 372\n",
      "\n",
      " Learning rate 0.00035394254265041657, Model learning rate 0.00035394253791309893\n",
      "373/391 [===========================>..] - ETA: 1s - loss: 0.8230 - acc: 0.7630\n",
      " Optimizer iteration 3110, batch 373\n",
      "\n",
      " Learning rate 0.0003531723308621847, Model learning rate 0.00035317233414389193\n",
      "\n",
      " Optimizer iteration 3111, batch 374\n",
      "\n",
      " Learning rate 0.0003524025001734126, Model learning rate 0.00035240250872448087\n",
      "375/391 [===========================>..] - ETA: 1s - loss: 0.8231 - acc: 0.7630\n",
      " Optimizer iteration 3112, batch 375\n",
      "\n",
      " Learning rate 0.0003516330525822391, Model learning rate 0.00035163306165486574\n",
      "376/391 [===========================>..] - ETA: 1s - loss: 0.8231 - acc: 0.7630\n",
      " Optimizer iteration 3113, batch 376\n",
      "\n",
      " Learning rate 0.00035086399008580884, Model learning rate 0.00035086399293504655\n",
      "377/391 [===========================>..] - ETA: 1s - loss: 0.8230 - acc: 0.7628\n",
      " Optimizer iteration 3114, batch 377\n",
      "\n",
      " Learning rate 0.00035009531468026647, Model learning rate 0.0003500953025650233\n",
      "378/391 [============================>.] - ETA: 1s - loss: 0.8228 - acc: 0.7629\n",
      " Optimizer iteration 3115, batch 378\n",
      "\n",
      " Learning rate 0.00034932702836075214, Model learning rate 0.00034932701964862645\n",
      "379/391 [============================>.] - ETA: 0s - loss: 0.8226 - acc: 0.7630\n",
      " Optimizer iteration 3116, batch 379\n",
      "\n",
      " Learning rate 0.0003485591331213962, Model learning rate 0.000348559144185856\n",
      "380/391 [============================>.] - ETA: 0s - loss: 0.8228 - acc: 0.7630\n",
      " Optimizer iteration 3117, batch 380\n",
      "\n",
      " Learning rate 0.00034779163095531384, Model learning rate 0.000347791617969051\n",
      "381/391 [============================>.] - ETA: 0s - loss: 0.8229 - acc: 0.7629\n",
      " Optimizer iteration 3118, batch 381\n",
      "\n",
      " Learning rate 0.00034702452385460014, Model learning rate 0.0003470245283097029\n",
      "382/391 [============================>.] - ETA: 0s - loss: 0.8228 - acc: 0.7630\n",
      " Optimizer iteration 3119, batch 382\n",
      "\n",
      " Learning rate 0.00034625781381032484, Model learning rate 0.0003462578170001507\n",
      "\n",
      " Optimizer iteration 3120, batch 383\n",
      "\n",
      " Learning rate 0.00034549150281252633, Model learning rate 0.0003454915131442249\n",
      "384/391 [============================>.] - ETA: 0s - loss: 0.8229 - acc: 0.7630\n",
      " Optimizer iteration 3121, batch 384\n",
      "\n",
      " Learning rate 0.00034472559285020826, Model learning rate 0.000344725587638095\n",
      "385/391 [============================>.] - ETA: 0s - loss: 0.8229 - acc: 0.7630\n",
      " Optimizer iteration 3122, batch 385\n",
      "\n",
      " Learning rate 0.0003439600859113329, Model learning rate 0.000343960098689422\n",
      "386/391 [============================>.] - ETA: 0s - loss: 0.8224 - acc: 0.7632\n",
      " Optimizer iteration 3123, batch 386\n",
      "\n",
      " Learning rate 0.00034319498398281635, Model learning rate 0.00034319498809054494\n",
      "\n",
      " Optimizer iteration 3124, batch 387\n",
      "\n",
      " Learning rate 0.00034243028905052387, Model learning rate 0.00034243028494529426\n",
      "388/391 [============================>.] - ETA: 0s - loss: 0.8223 - acc: 0.7633\n",
      " Optimizer iteration 3125, batch 388\n",
      "\n",
      " Learning rate 0.00034166600309926387, Model learning rate 0.00034166598925367\n",
      "389/391 [============================>.] - ETA: 0s - loss: 0.8219 - acc: 0.7635\n",
      " Optimizer iteration 3126, batch 389\n",
      "\n",
      " Learning rate 0.0003409021281127835, Model learning rate 0.00034090213011950254\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.8216 - acc: 0.7635\n",
      " Optimizer iteration 3127, batch 390\n",
      "\n",
      " Learning rate 0.00034013866607376307, Model learning rate 0.0003401386784389615\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.8214 - acc: 0.7636 - val_loss: 0.9432 - val_acc: 0.7244\n",
      "\n",
      "Epoch 00008: saving model to /home/ubuntu/Projects/hybrid-ensemble/model/run_200/cifar10_ResNet20v1_model-0008.h5\n",
      "Epoch 9/10\n",
      "\n",
      " Optimizer iteration 3128, batch 0\n",
      "\n",
      " Learning rate 0.00033937561896381146, Model learning rate 0.0003393756051082164\n",
      "  1/391 [..............................] - ETA: 17s - loss: 0.7394 - acc: 0.8203\n",
      " Optimizer iteration 3129, batch 1\n",
      "\n",
      " Learning rate 0.0003386129887634601, Model learning rate 0.0003386129974387586\n",
      "  2/391 [..............................] - ETA: 19s - loss: 0.7456 - acc: 0.8008\n",
      " Optimizer iteration 3130, batch 2\n",
      "\n",
      " Learning rate 0.0003378507774521587, Model learning rate 0.00033785076811909676\n",
      "  3/391 [..............................] - ETA: 19s - loss: 0.7596 - acc: 0.7969\n",
      " Optimizer iteration 3131, batch 3\n",
      "\n",
      " Learning rate 0.0003370889870082692, Model learning rate 0.00033708897535689175\n",
      "  4/391 [..............................] - ETA: 19s - loss: 0.7779 - acc: 0.7930\n",
      " Optimizer iteration 3132, batch 4\n",
      "\n",
      " Learning rate 0.0003363276194090617, Model learning rate 0.0003363276191521436\n",
      "\n",
      " Optimizer iteration 3133, batch 5\n",
      "\n",
      " Learning rate 0.00033556667663070836, Model learning rate 0.00033556667040102184\n",
      "  6/391 [..............................] - ETA: 19s - loss: 0.7792 - acc: 0.7904\n",
      " Optimizer iteration 3134, batch 6\n",
      "\n",
      " Learning rate 0.0003348061606482791, Model learning rate 0.00033480615820735693\n",
      "  7/391 [..............................] - ETA: 19s - loss: 0.7945 - acc: 0.7812\n",
      " Optimizer iteration 3135, batch 7\n",
      "\n",
      " Learning rate 0.0003340460734357359, Model learning rate 0.00033404608257114887\n",
      "  8/391 [..............................] - ETA: 20s - loss: 0.7958 - acc: 0.7842\n",
      " Optimizer iteration 3136, batch 8\n",
      "\n",
      " Learning rate 0.0003332864169659275, Model learning rate 0.0003332864143885672\n",
      "  9/391 [..............................] - ETA: 20s - loss: 0.7972 - acc: 0.7804\n",
      " Optimizer iteration 3137, batch 9\n",
      "\n",
      " Learning rate 0.0003325271932105851, Model learning rate 0.0003325271827634424\n",
      " 10/391 [..............................] - ETA: 20s - loss: 0.7990 - acc: 0.7773\n",
      " Optimizer iteration 3138, batch 10\n",
      "\n",
      " Learning rate 0.0003317684041403165, Model learning rate 0.0003317684167996049\n",
      " 11/391 [..............................] - ETA: 21s - loss: 0.7960 - acc: 0.7784\n",
      " Optimizer iteration 3139, batch 11\n",
      "\n",
      " Learning rate 0.00033101005172460156, Model learning rate 0.0003310100582893938\n",
      " 12/391 [..............................] - ETA: 21s - loss: 0.7907 - acc: 0.7786\n",
      " Optimizer iteration 3140, batch 12\n",
      "\n",
      " Learning rate 0.00033025213793178644, Model learning rate 0.0003302521363366395\n",
      " 13/391 [..............................] - ETA: 21s - loss: 0.7836 - acc: 0.7825\n",
      " Optimizer iteration 3141, batch 13\n",
      "\n",
      " Learning rate 0.00032949466472907896, Model learning rate 0.0003294946509413421\n",
      " 14/391 [>.............................] - ETA: 21s - loss: 0.7803 - acc: 0.7824\n",
      " Optimizer iteration 3142, batch 14\n",
      "\n",
      " Learning rate 0.0003287376340825432, Model learning rate 0.000328737631207332\n",
      " 15/391 [>.............................] - ETA: 21s - loss: 0.7774 - acc: 0.7839\n",
      " Optimizer iteration 3143, batch 15\n",
      "\n",
      " Learning rate 0.0003279810479570948, Model learning rate 0.00032798104803077877\n",
      " 16/391 [>.............................] - ETA: 21s - loss: 0.7798 - acc: 0.7832\n",
      " Optimizer iteration 3144, batch 16\n",
      "\n",
      " Learning rate 0.0003272249083164957, Model learning rate 0.00032722490141168237\n",
      " 17/391 [>.............................] - ETA: 22s - loss: 0.7816 - acc: 0.7790\n",
      " Optimizer iteration 3145, batch 17\n",
      "\n",
      " Learning rate 0.00032646921712334856, Model learning rate 0.0003264692204538733\n",
      " 18/391 [>.............................] - ETA: 22s - loss: 0.7823 - acc: 0.7769\n",
      " Optimizer iteration 3146, batch 18\n",
      "\n",
      " Learning rate 0.0003257139763390925, Model learning rate 0.00032571397605352104\n",
      " 19/391 [>.............................] - ETA: 22s - loss: 0.7833 - acc: 0.7767\n",
      " Optimizer iteration 3147, batch 19\n",
      "\n",
      " Learning rate 0.0003249591879239972, Model learning rate 0.0003249591973144561\n",
      " 20/391 [>.............................] - ETA: 22s - loss: 0.7809 - acc: 0.7773\n",
      " Optimizer iteration 3148, batch 20\n",
      "\n",
      " Learning rate 0.00032420485383715845, Model learning rate 0.000324204855132848\n",
      " 21/391 [>.............................] - ETA: 23s - loss: 0.7886 - acc: 0.7764\n",
      " Optimizer iteration 3149, batch 21\n",
      "\n",
      " Learning rate 0.00032345097603649265, Model learning rate 0.00032345097861252725\n",
      " 22/391 [>.............................] - ETA: 23s - loss: 0.7863 - acc: 0.7773\n",
      " Optimizer iteration 3150, batch 22\n",
      "\n",
      " Learning rate 0.00032269755647873217, Model learning rate 0.0003226975677534938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23/391 [>.............................] - ETA: 23s - loss: 0.7848 - acc: 0.7782\n",
      " Optimizer iteration 3151, batch 23\n",
      "\n",
      " Learning rate 0.00032194459711941967, Model learning rate 0.00032194459345191717\n",
      " 24/391 [>.............................] - ETA: 23s - loss: 0.7843 - acc: 0.7790\n",
      " Optimizer iteration 3152, batch 24\n",
      "\n",
      " Learning rate 0.00032119209991290346, Model learning rate 0.0003211921139154583\n",
      " 25/391 [>.............................] - ETA: 24s - loss: 0.7741 - acc: 0.7828\n",
      " Optimizer iteration 3153, batch 25\n",
      "\n",
      " Learning rate 0.0003204400668123322, Model learning rate 0.0003204400709364563\n",
      " 26/391 [>.............................] - ETA: 24s - loss: 0.7700 - acc: 0.7846\n",
      " Optimizer iteration 3154, batch 26\n",
      "\n",
      " Learning rate 0.00031968849976965014, Model learning rate 0.00031968849361874163\n",
      " 27/391 [=>............................] - ETA: 24s - loss: 0.7721 - acc: 0.7839\n",
      " Optimizer iteration 3155, batch 27\n",
      "\n",
      " Learning rate 0.00031893740073559167, Model learning rate 0.0003189374110661447\n",
      "\n",
      " Optimizer iteration 3156, batch 28\n",
      "\n",
      " Learning rate 0.00031818677165967646, Model learning rate 0.00031818676507100463\n",
      " 29/391 [=>............................] - ETA: 24s - loss: 0.7722 - acc: 0.7837\n",
      " Optimizer iteration 3157, batch 29\n",
      "\n",
      " Learning rate 0.0003174366144902048, Model learning rate 0.0003174366138409823\n",
      "\n",
      " Optimizer iteration 3158, batch 30\n",
      "\n",
      " Learning rate 0.00031668693117425126, Model learning rate 0.0003166869282722473\n",
      " 31/391 [=>............................] - ETA: 24s - loss: 0.7716 - acc: 0.7825\n",
      " Optimizer iteration 3159, batch 31\n",
      "\n",
      " Learning rate 0.00031593772365766105, Model learning rate 0.0003159377374686301\n",
      "\n",
      " Optimizer iteration 3160, batch 32\n",
      "\n",
      " Learning rate 0.0003151889938850445, Model learning rate 0.0003151889832224697\n",
      " 33/391 [=>............................] - ETA: 24s - loss: 0.7700 - acc: 0.7843\n",
      " Optimizer iteration 3161, batch 33\n",
      "\n",
      " Learning rate 0.00031444074379977186, Model learning rate 0.0003144407528452575\n",
      " 34/391 [=>............................] - ETA: 24s - loss: 0.7680 - acc: 0.7861\n",
      " Optimizer iteration 3162, batch 34\n",
      "\n",
      " Learning rate 0.00031369297534396826, Model learning rate 0.00031369298812933266\n",
      " 35/391 [=>............................] - ETA: 24s - loss: 0.7665 - acc: 0.7879\n",
      " Optimizer iteration 3163, batch 35\n",
      "\n",
      " Learning rate 0.0003129456904585084, Model learning rate 0.0003129456890746951\n",
      " 36/391 [=>............................] - ETA: 24s - loss: 0.7657 - acc: 0.7886\n",
      " Optimizer iteration 3164, batch 36\n",
      "\n",
      " Learning rate 0.00031219889108301236, Model learning rate 0.0003121988847851753\n",
      " 37/391 [=>............................] - ETA: 24s - loss: 0.7672 - acc: 0.7880\n",
      " Optimizer iteration 3165, batch 37\n",
      "\n",
      " Learning rate 0.0003114525791558398, Model learning rate 0.0003114525752607733\n",
      " 38/391 [=>............................] - ETA: 24s - loss: 0.7698 - acc: 0.7870\n",
      " Optimizer iteration 3166, batch 38\n",
      "\n",
      " Learning rate 0.0003107067566140853, Model learning rate 0.00031070676050148904\n",
      " 39/391 [=>............................] - ETA: 24s - loss: 0.7729 - acc: 0.7859\n",
      " Optimizer iteration 3167, batch 39\n",
      "\n",
      " Learning rate 0.0003099614253935731, Model learning rate 0.0003099614114034921\n",
      "\n",
      " Optimizer iteration 3168, batch 40\n",
      "\n",
      " Learning rate 0.0003092165874288525, Model learning rate 0.00030921658617444336\n",
      " 41/391 [==>...........................] - ETA: 24s - loss: 0.7718 - acc: 0.7849\n",
      " Optimizer iteration 3169, batch 41\n",
      "\n",
      " Learning rate 0.0003084722446531918, Model learning rate 0.0003084722557105124\n",
      " 42/391 [==>...........................] - ETA: 24s - loss: 0.7718 - acc: 0.7848\n",
      " Optimizer iteration 3170, batch 42\n",
      "\n",
      " Learning rate 0.00030772839899857464, Model learning rate 0.00030772839090786874\n",
      " 43/391 [==>...........................] - ETA: 24s - loss: 0.7704 - acc: 0.7852\n",
      " Optimizer iteration 3171, batch 43\n",
      "\n",
      " Learning rate 0.00030698505239569424, Model learning rate 0.0003069850499741733\n",
      " 44/391 [==>...........................] - ETA: 24s - loss: 0.7699 - acc: 0.7852\n",
      " Optimizer iteration 3172, batch 44\n",
      "\n",
      " Learning rate 0.0003062422067739485, Model learning rate 0.00030624220380559564\n",
      " 45/391 [==>...........................] - ETA: 24s - loss: 0.7691 - acc: 0.7847\n",
      " Optimizer iteration 3173, batch 45\n",
      "\n",
      " Learning rate 0.00030549986406143496, Model learning rate 0.00030549985240213573\n",
      " 46/391 [==>...........................] - ETA: 24s - loss: 0.7698 - acc: 0.7843\n",
      " Optimizer iteration 3174, batch 46\n",
      "\n",
      " Learning rate 0.0003047580261849456, Model learning rate 0.00030475802486762404\n",
      " 47/391 [==>...........................] - ETA: 24s - loss: 0.7680 - acc: 0.7849\n",
      " Optimizer iteration 3175, batch 47\n",
      "\n",
      " Learning rate 0.0003040166950699625, Model learning rate 0.0003040166920982301\n",
      " 48/391 [==>...........................] - ETA: 24s - loss: 0.7659 - acc: 0.7861\n",
      " Optimizer iteration 3176, batch 48\n",
      "\n",
      " Learning rate 0.0003032758726406521, Model learning rate 0.0003032758831977844\n",
      " 49/391 [==>...........................] - ETA: 24s - loss: 0.7671 - acc: 0.7851\n",
      " Optimizer iteration 3177, batch 49\n",
      "\n",
      " Learning rate 0.0003025355608198606, Model learning rate 0.0003025355690624565\n",
      " 50/391 [==>...........................] - ETA: 24s - loss: 0.7650 - acc: 0.7856\n",
      " Optimizer iteration 3178, batch 50\n",
      "\n",
      " Learning rate 0.0003017957615291088, Model learning rate 0.0003017957496922463\n",
      "\n",
      " Optimizer iteration 3179, batch 51\n",
      "\n",
      " Learning rate 0.0003010564766885878, Model learning rate 0.0003010564832948148\n",
      " 52/391 [==>...........................] - ETA: 23s - loss: 0.7662 - acc: 0.7846\n",
      " Optimizer iteration 3180, batch 52\n",
      "\n",
      " Learning rate 0.0003003177082171523, Model learning rate 0.0003003177116625011\n",
      " 53/391 [===>..........................] - ETA: 24s - loss: 0.7635 - acc: 0.7849\n",
      " Optimizer iteration 3181, batch 53\n",
      "\n",
      " Learning rate 0.00029957945803231754, Model learning rate 0.0002995794638991356\n",
      " 54/391 [===>..........................] - ETA: 24s - loss: 0.7657 - acc: 0.7847\n",
      " Optimizer iteration 3182, batch 54\n",
      "\n",
      " Learning rate 0.00029884172805025343, Model learning rate 0.0002988417400047183\n",
      " 55/391 [===>..........................] - ETA: 23s - loss: 0.7635 - acc: 0.7852\n",
      " Optimizer iteration 3183, batch 55\n",
      "\n",
      " Learning rate 0.0002981045201857796, Model learning rate 0.0002981045108754188\n",
      " 56/391 [===>..........................] - ETA: 23s - loss: 0.7653 - acc: 0.7857\n",
      " Optimizer iteration 3184, batch 56\n",
      "\n",
      " Learning rate 0.0002973678363523604, Model learning rate 0.00029736783471889794\n",
      " 57/391 [===>..........................] - ETA: 24s - loss: 0.7672 - acc: 0.7845\n",
      " Optimizer iteration 3185, batch 57\n",
      "\n",
      " Learning rate 0.0002966316784621, Model learning rate 0.0002966316824313253\n",
      " 58/391 [===>..........................] - ETA: 24s - loss: 0.7668 - acc: 0.7850\n",
      " Optimizer iteration 3186, batch 58\n",
      "\n",
      " Learning rate 0.00029589604842573757, Model learning rate 0.0002958960540127009\n",
      " 59/391 [===>..........................] - ETA: 24s - loss: 0.7693 - acc: 0.7844\n",
      " Optimizer iteration 3187, batch 59\n",
      "\n",
      " Learning rate 0.00029516094815264216, Model learning rate 0.00029516094946302474\n",
      " 60/391 [===>..........................] - ETA: 23s - loss: 0.7685 - acc: 0.7850\n",
      " Optimizer iteration 3188, batch 60\n",
      "\n",
      " Learning rate 0.00029442637955080786, Model learning rate 0.0002944263687822968\n",
      " 61/391 [===>..........................] - ETA: 24s - loss: 0.7684 - acc: 0.7848\n",
      " Optimizer iteration 3189, batch 61\n",
      "\n",
      " Learning rate 0.0002936923445268488, Model learning rate 0.0002936923410743475\n",
      " 62/391 [===>..........................] - ETA: 24s - loss: 0.7689 - acc: 0.7848\n",
      " Optimizer iteration 3190, batch 62\n",
      "\n",
      " Learning rate 0.0002929588449859941, Model learning rate 0.00029295883723534644\n",
      " 63/391 [===>..........................] - ETA: 23s - loss: 0.7673 - acc: 0.7851\n",
      " Optimizer iteration 3191, batch 63\n",
      "\n",
      " Learning rate 0.00029222588283208274, Model learning rate 0.00029222588636912405\n",
      " 64/391 [===>..........................] - ETA: 23s - loss: 0.7684 - acc: 0.7848\n",
      " Optimizer iteration 3192, batch 64\n",
      "\n",
      " Learning rate 0.00029149345996755936, Model learning rate 0.0002914934593718499\n",
      " 65/391 [===>..........................] - ETA: 24s - loss: 0.7693 - acc: 0.7843\n",
      " Optimizer iteration 3193, batch 65\n",
      "\n",
      " Learning rate 0.00029076157829346883, Model learning rate 0.0002907615853473544\n",
      " 66/391 [====>.........................] - ETA: 23s - loss: 0.7691 - acc: 0.7844\n",
      " Optimizer iteration 3194, batch 66\n",
      "\n",
      " Learning rate 0.00029003023970945057, Model learning rate 0.00029003023519180715\n",
      "\n",
      " Optimizer iteration 3195, batch 67\n",
      "\n",
      " Learning rate 0.00028929944611373555, Model learning rate 0.00028929943800903857\n",
      " 68/391 [====>.........................] - ETA: 23s - loss: 0.7675 - acc: 0.7845\n",
      " Optimizer iteration 3196, batch 68\n",
      "\n",
      " Learning rate 0.0002885691994031393, Model learning rate 0.00028856919379904866\n",
      " 69/391 [====>.........................] - ETA: 23s - loss: 0.7675 - acc: 0.7849\n",
      " Optimizer iteration 3197, batch 69\n",
      "\n",
      " Learning rate 0.0002878395014730579, Model learning rate 0.00028783950256183743\n",
      " 70/391 [====>.........................] - ETA: 23s - loss: 0.7687 - acc: 0.7848\n",
      " Optimizer iteration 3198, batch 70\n",
      "\n",
      " Learning rate 0.00028711035421746366, Model learning rate 0.0002871103642974049\n",
      "\n",
      " Optimizer iteration 3199, batch 71\n",
      "\n",
      " Learning rate 0.0002863817595288993, Model learning rate 0.00028638174990192056\n",
      " 72/391 [====>.........................] - ETA: 23s - loss: 0.7688 - acc: 0.7844\n",
      " Optimizer iteration 3200, batch 72\n",
      "\n",
      " Learning rate 0.00028565371929847286, Model learning rate 0.00028565371758304536\n",
      " 73/391 [====>.........................] - ETA: 23s - loss: 0.7692 - acc: 0.7844\n",
      " Optimizer iteration 3201, batch 73\n",
      "\n",
      " Learning rate 0.00028492623541585404, Model learning rate 0.00028492623823694885\n",
      " 74/391 [====>.........................] - ETA: 23s - loss: 0.7704 - acc: 0.7839\n",
      " Optimizer iteration 3202, batch 74\n",
      "\n",
      " Learning rate 0.000284199309769268, Model learning rate 0.000284199311863631\n",
      " 75/391 [====>.........................] - ETA: 23s - loss: 0.7706 - acc: 0.7835\n",
      " Optimizer iteration 3203, batch 75\n",
      "\n",
      " Learning rate 0.00028347294424549077, Model learning rate 0.00028347293846309185\n",
      " 76/391 [====>.........................] - ETA: 23s - loss: 0.7694 - acc: 0.7833\n",
      " Optimizer iteration 3204, batch 76\n",
      "\n",
      " Learning rate 0.00028274714072984506, Model learning rate 0.0002827471471391618\n",
      " 77/391 [====>.........................] - ETA: 23s - loss: 0.7712 - acc: 0.7826\n",
      " Optimizer iteration 3205, batch 77\n",
      "\n",
      " Learning rate 0.0002820219011061949, Model learning rate 0.0002820219087880105\n",
      " 78/391 [====>.........................] - ETA: 23s - loss: 0.7712 - acc: 0.7826\n",
      " Optimizer iteration 3206, batch 78\n",
      "\n",
      " Learning rate 0.0002812972272569402, Model learning rate 0.0002812972234096378\n",
      "\n",
      " Optimizer iteration 3207, batch 79\n",
      "\n",
      " Learning rate 0.00028057312106301253, Model learning rate 0.0002805731201078743\n",
      " 80/391 [=====>........................] - ETA: 22s - loss: 0.7692 - acc: 0.7837\n",
      " Optimizer iteration 3208, batch 80\n",
      "\n",
      " Learning rate 0.00027984958440387044, Model learning rate 0.0002798495988827199\n",
      " 81/391 [=====>........................] - ETA: 23s - loss: 0.7686 - acc: 0.7837\n",
      " Optimizer iteration 3209, batch 81\n",
      "\n",
      " Learning rate 0.0002791266191574936, Model learning rate 0.00027912663063034415\n",
      " 82/391 [=====>........................] - ETA: 22s - loss: 0.7676 - acc: 0.7837\n",
      " Optimizer iteration 3210, batch 82\n",
      "\n",
      " Learning rate 0.0002784042272003794, Model learning rate 0.0002784042153507471\n",
      " 83/391 [=====>........................] - ETA: 22s - loss: 0.7666 - acc: 0.7841\n",
      " Optimizer iteration 3211, batch 83\n",
      "\n",
      " Learning rate 0.00027768241040753637, Model learning rate 0.00027768241125158966\n",
      " 84/391 [=====>........................] - ETA: 22s - loss: 0.7675 - acc: 0.7833\n",
      " Optimizer iteration 3212, batch 84\n",
      "\n",
      " Learning rate 0.0002769611706524805, Model learning rate 0.0002769611601252109\n",
      " 85/391 [=====>........................] - ETA: 22s - loss: 0.7686 - acc: 0.7830\n",
      " Optimizer iteration 3213, batch 85\n",
      "\n",
      " Learning rate 0.0002762405098072303, Model learning rate 0.0002762405201792717\n",
      " 86/391 [=====>........................] - ETA: 22s - loss: 0.7694 - acc: 0.7825\n",
      " Optimizer iteration 3214, batch 86\n",
      "\n",
      " Learning rate 0.00027552042974230117, Model learning rate 0.0002755204332061112\n",
      "\n",
      " Optimizer iteration 3215, batch 87\n",
      "\n",
      " Learning rate 0.0002748009323267016, Model learning rate 0.0002748009283095598\n",
      " 88/391 [=====>........................] - ETA: 22s - loss: 0.7702 - acc: 0.7820\n",
      " Optimizer iteration 3216, batch 88\n",
      "\n",
      " Learning rate 0.00027408201942792756, Model learning rate 0.0002740820054896176\n",
      " 89/391 [=====>........................] - ETA: 22s - loss: 0.7700 - acc: 0.7825\n",
      " Optimizer iteration 3217, batch 89\n",
      "\n",
      " Learning rate 0.00027336369291195773, Model learning rate 0.00027336369385011494\n",
      " 90/391 [=====>........................] - ETA: 22s - loss: 0.7713 - acc: 0.7820\n",
      " Optimizer iteration 3218, batch 90\n",
      "\n",
      " Learning rate 0.00027264595464324875, Model learning rate 0.00027264596428722143\n",
      " 91/391 [=====>........................] - ETA: 22s - loss: 0.7725 - acc: 0.7817\n",
      " Optimizer iteration 3219, batch 91\n",
      "\n",
      " Learning rate 0.000271928806484731, Model learning rate 0.00027192881680093706\n",
      " 92/391 [======>.......................] - ETA: 22s - loss: 0.7708 - acc: 0.7824\n",
      " Optimizer iteration 3220, batch 92\n",
      "\n",
      " Learning rate 0.0002712122502978024, Model learning rate 0.0002712122513912618\n",
      " 93/391 [======>.......................] - ETA: 22s - loss: 0.7700 - acc: 0.7830\n",
      " Optimizer iteration 3221, batch 93\n",
      "\n",
      " Learning rate 0.00027049628794232505, Model learning rate 0.00027049629716202617\n",
      "\n",
      " Optimizer iteration 3222, batch 94\n",
      "\n",
      " Learning rate 0.00026978092127661945, Model learning rate 0.00026978092500939965\n",
      " 95/391 [======>.......................] - ETA: 22s - loss: 0.7688 - acc: 0.7835\n",
      " Optimizer iteration 3223, batch 95\n",
      "\n",
      " Learning rate 0.0002690661521574596, Model learning rate 0.00026906616403721273\n",
      " 96/391 [======>.......................] - ETA: 21s - loss: 0.7673 - acc: 0.7842\n",
      " Optimizer iteration 3224, batch 96\n",
      "\n",
      " Learning rate 0.00026835198244006924, Model learning rate 0.00026835198514163494\n",
      " 97/391 [======>.......................] - ETA: 22s - loss: 0.7670 - acc: 0.7846\n",
      " Optimizer iteration 3225, batch 97\n",
      "\n",
      " Learning rate 0.00026763841397811573, Model learning rate 0.00026763841742649674\n",
      " 98/391 [======>.......................] - ETA: 21s - loss: 0.7683 - acc: 0.7845\n",
      " Optimizer iteration 3226, batch 98\n",
      "\n",
      " Learning rate 0.0002669254486237062, Model learning rate 0.00026692546089179814\n",
      " 99/391 [======>.......................] - ETA: 21s - loss: 0.7671 - acc: 0.7851\n",
      " Optimizer iteration 3227, batch 99\n",
      "\n",
      " Learning rate 0.0002662130882273825, Model learning rate 0.00026621308643370867\n",
      "100/391 [======>.......................] - ETA: 21s - loss: 0.7663 - acc: 0.7854\n",
      " Optimizer iteration 3228, batch 100\n",
      "\n",
      " Learning rate 0.0002655013346381158, Model learning rate 0.0002655013231560588\n",
      "101/391 [======>.......................] - ETA: 21s - loss: 0.7646 - acc: 0.7859\n",
      " Optimizer iteration 3229, batch 101\n",
      "\n",
      " Learning rate 0.00026479018970330227, Model learning rate 0.00026479020016267896\n",
      "\n",
      " Optimizer iteration 3230, batch 102\n",
      "\n",
      " Learning rate 0.000264079655268759, Model learning rate 0.00026407965924590826\n",
      "103/391 [======>.......................] - ETA: 21s - loss: 0.7624 - acc: 0.7866\n",
      " Optimizer iteration 3231, batch 103\n",
      "\n",
      " Learning rate 0.00026336973317871756, Model learning rate 0.00026336972950957716\n",
      "104/391 [======>.......................] - ETA: 21s - loss: 0.7638 - acc: 0.7863\n",
      " Optimizer iteration 3232, batch 104\n",
      "\n",
      " Learning rate 0.000262660425275821, Model learning rate 0.00026266041095368564\n",
      "105/391 [=======>......................] - ETA: 21s - loss: 0.7634 - acc: 0.7864\n",
      " Optimizer iteration 3233, batch 105\n",
      "\n",
      " Learning rate 0.0002619517334011177, Model learning rate 0.0002619517326820642\n",
      "106/391 [=======>......................] - ETA: 21s - loss: 0.7633 - acc: 0.7863\n",
      " Optimizer iteration 3234, batch 106\n",
      "\n",
      " Learning rate 0.0002612436593940568, Model learning rate 0.0002612436655908823\n",
      "107/391 [=======>......................] - ETA: 21s - loss: 0.7633 - acc: 0.7864\n",
      " Optimizer iteration 3235, batch 107\n",
      "\n",
      " Learning rate 0.0002605362050924848, Model learning rate 0.00026053620968014\n",
      "108/391 [=======>......................] - ETA: 21s - loss: 0.7624 - acc: 0.7866\n",
      " Optimizer iteration 3236, batch 108\n",
      "\n",
      " Learning rate 0.00025982937233263846, Model learning rate 0.0002598293649498373\n",
      "109/391 [=======>......................] - ETA: 21s - loss: 0.7627 - acc: 0.7866\n",
      " Optimizer iteration 3237, batch 109\n",
      "\n",
      " Learning rate 0.0002591231629491423, Model learning rate 0.0002591231605038047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/391 [=======>......................] - ETA: 21s - loss: 0.7629 - acc: 0.7865\n",
      " Optimizer iteration 3238, batch 110\n",
      "\n",
      " Learning rate 0.0002584175787750024, Model learning rate 0.00025841756723821163\n",
      "111/391 [=======>......................] - ETA: 21s - loss: 0.7622 - acc: 0.7865\n",
      " Optimizer iteration 3239, batch 111\n",
      "\n",
      " Learning rate 0.00025771262164160213, Model learning rate 0.00025771261425688863\n",
      "112/391 [=======>......................] - ETA: 20s - loss: 0.7623 - acc: 0.7863\n",
      " Optimizer iteration 3240, batch 112\n",
      "\n",
      " Learning rate 0.00025700829337869696, Model learning rate 0.00025700830155983567\n",
      "113/391 [=======>......................] - ETA: 21s - loss: 0.7634 - acc: 0.7860\n",
      " Optimizer iteration 3241, batch 113\n",
      "\n",
      " Learning rate 0.0002563045958144109, Model learning rate 0.0002563046000432223\n",
      "114/391 [=======>......................] - ETA: 20s - loss: 0.7632 - acc: 0.7864\n",
      " Optimizer iteration 3242, batch 114\n",
      "\n",
      " Learning rate 0.0002556015307752301, Model learning rate 0.000255601538810879\n",
      "115/391 [=======>......................] - ETA: 20s - loss: 0.7631 - acc: 0.7869\n",
      " Optimizer iteration 3243, batch 115\n",
      "\n",
      " Learning rate 0.00025489910008599966, Model learning rate 0.00025489908875897527\n",
      "116/391 [=======>......................] - ETA: 20s - loss: 0.7632 - acc: 0.7868\n",
      " Optimizer iteration 3244, batch 116\n",
      "\n",
      " Learning rate 0.0002541973055699178, Model learning rate 0.00025419730809517205\n",
      "117/391 [=======>......................] - ETA: 20s - loss: 0.7632 - acc: 0.7867\n",
      " Optimizer iteration 3245, batch 117\n",
      "\n",
      " Learning rate 0.0002534961490485313, Model learning rate 0.0002534961386118084\n",
      "118/391 [========>.....................] - ETA: 20s - loss: 0.7632 - acc: 0.7865\n",
      " Optimizer iteration 3246, batch 118\n",
      "\n",
      " Learning rate 0.00025279563234173176, Model learning rate 0.0002527956385165453\n",
      "\n",
      " Optimizer iteration 3247, batch 119\n",
      "\n",
      " Learning rate 0.00025209575726774914, Model learning rate 0.00025209574960172176\n",
      "120/391 [========>.....................] - ETA: 20s - loss: 0.7636 - acc: 0.7861\n",
      " Optimizer iteration 3248, batch 120\n",
      "\n",
      " Learning rate 0.0002513965256431488, Model learning rate 0.00025139653007499874\n",
      "121/391 [========>.....................] - ETA: 20s - loss: 0.7627 - acc: 0.7865\n",
      " Optimizer iteration 3249, batch 121\n",
      "\n",
      " Learning rate 0.0002506979392828258, Model learning rate 0.00025069795083254576\n",
      "122/391 [========>.....................] - ETA: 20s - loss: 0.7623 - acc: 0.7866\n",
      " Optimizer iteration 3250, batch 122\n",
      "\n",
      " Learning rate 0.0002500000000000001, Model learning rate 0.0002500000118743628\n",
      "123/391 [========>.....................] - ETA: 20s - loss: 0.7621 - acc: 0.7865\n",
      " Optimizer iteration 3251, batch 123\n",
      "\n",
      " Learning rate 0.0002493027096062121, Model learning rate 0.00024930271320044994\n",
      "124/391 [========>.....................] - ETA: 20s - loss: 0.7629 - acc: 0.7861\n",
      " Optimizer iteration 3252, batch 124\n",
      "\n",
      " Learning rate 0.00024860606991131855, Model learning rate 0.00024860608391463757\n",
      "125/391 [========>.....................] - ETA: 20s - loss: 0.7638 - acc: 0.7858\n",
      " Optimizer iteration 3253, batch 125\n",
      "\n",
      " Learning rate 0.00024791008272348654, Model learning rate 0.00024791009491309524\n",
      "126/391 [========>.....................] - ETA: 20s - loss: 0.7628 - acc: 0.7860\n",
      " Optimizer iteration 3254, batch 126\n",
      "\n",
      " Learning rate 0.0002472147498491902, Model learning rate 0.00024721474619582295\n",
      "127/391 [========>.....................] - ETA: 19s - loss: 0.7622 - acc: 0.7863\n",
      " Optimizer iteration 3255, batch 127\n",
      "\n",
      " Learning rate 0.00024652007309320496, Model learning rate 0.0002465200668666512\n",
      "128/391 [========>.....................] - ETA: 19s - loss: 0.7621 - acc: 0.7862\n",
      " Optimizer iteration 3256, batch 128\n",
      "\n",
      " Learning rate 0.00024582605425860313, Model learning rate 0.0002458260569255799\n",
      "129/391 [========>.....................] - ETA: 19s - loss: 0.7618 - acc: 0.7861\n",
      " Optimizer iteration 3257, batch 129\n",
      "\n",
      " Learning rate 0.00024513269514674985, Model learning rate 0.0002451326872687787\n",
      "\n",
      " Optimizer iteration 3258, batch 130\n",
      "\n",
      " Learning rate 0.0002444399975572974, Model learning rate 0.00024443998700007796\n",
      "131/391 [=========>....................] - ETA: 19s - loss: 0.7608 - acc: 0.7865\n",
      " Optimizer iteration 3259, batch 131\n",
      "\n",
      " Learning rate 0.00024374796328818134, Model learning rate 0.00024374795611947775\n",
      "132/391 [=========>....................] - ETA: 19s - loss: 0.7600 - acc: 0.7868\n",
      " Optimizer iteration 3260, batch 132\n",
      "\n",
      " Learning rate 0.00024305659413561572, Model learning rate 0.00024305659462697804\n",
      "133/391 [=========>....................] - ETA: 19s - loss: 0.7594 - acc: 0.7868\n",
      " Optimizer iteration 3261, batch 133\n",
      "\n",
      " Learning rate 0.0002423658918940878, Model learning rate 0.0002423658879706636\n",
      "\n",
      " Optimizer iteration 3262, batch 134\n",
      "\n",
      " Learning rate 0.0002416758583563538, Model learning rate 0.0002416758652543649\n",
      "135/391 [=========>....................] - ETA: 19s - loss: 0.7582 - acc: 0.7870\n",
      " Optimizer iteration 3263, batch 135\n",
      "\n",
      " Learning rate 0.00024098649531343496, Model learning rate 0.00024098649737425148\n",
      "\n",
      " Optimizer iteration 3264, batch 136\n",
      "\n",
      " Learning rate 0.00024029780455461138, Model learning rate 0.00024029779888223857\n",
      "137/391 [=========>....................] - ETA: 19s - loss: 0.7569 - acc: 0.7878\n",
      " Optimizer iteration 3265, batch 137\n",
      "\n",
      " Learning rate 0.00023960978786741877, Model learning rate 0.00023960978433024138\n",
      "138/391 [=========>....................] - ETA: 19s - loss: 0.7573 - acc: 0.7878\n",
      " Optimizer iteration 3266, batch 138\n",
      "\n",
      " Learning rate 0.00023892244703764342, Model learning rate 0.00023892245371825993\n",
      "139/391 [=========>....................] - ETA: 19s - loss: 0.7580 - acc: 0.7878\n",
      " Optimizer iteration 3267, batch 139\n",
      "\n",
      " Learning rate 0.00023823578384931632, Model learning rate 0.00023823577794246376\n",
      "\n",
      " Optimizer iteration 3268, batch 140\n",
      "\n",
      " Learning rate 0.0002375498000847107, Model learning rate 0.00023754980065859854\n",
      "141/391 [=========>....................] - ETA: 18s - loss: 0.7566 - acc: 0.7886\n",
      " Optimizer iteration 3269, batch 141\n",
      "\n",
      " Learning rate 0.00023686449752433614, Model learning rate 0.00023686449276283383\n",
      "142/391 [=========>....................] - ETA: 18s - loss: 0.7560 - acc: 0.7890\n",
      " Optimizer iteration 3270, batch 142\n",
      "\n",
      " Learning rate 0.00023617987794693357, Model learning rate 0.0002361798833590001\n",
      "143/391 [=========>....................] - ETA: 18s - loss: 0.7564 - acc: 0.7886\n",
      " Optimizer iteration 3271, batch 143\n",
      "\n",
      " Learning rate 0.00023549594312947188, Model learning rate 0.00023549594334326684\n",
      "144/391 [==========>...................] - ETA: 18s - loss: 0.7569 - acc: 0.7886\n",
      " Optimizer iteration 3272, batch 144\n",
      "\n",
      " Learning rate 0.00023481269484714208, Model learning rate 0.00023481270181946456\n",
      "145/391 [==========>...................] - ETA: 18s - loss: 0.7569 - acc: 0.7889\n",
      " Optimizer iteration 3273, batch 145\n",
      "\n",
      " Learning rate 0.00023413013487335333, Model learning rate 0.0002341301296837628\n",
      "146/391 [==========>...................] - ETA: 18s - loss: 0.7567 - acc: 0.7892\n",
      " Optimizer iteration 3274, batch 146\n",
      "\n",
      " Learning rate 0.0002334482649797287, Model learning rate 0.0002334482705919072\n",
      "147/391 [==========>...................] - ETA: 18s - loss: 0.7579 - acc: 0.7884\n",
      " Optimizer iteration 3275, batch 147\n",
      "\n",
      " Learning rate 0.00023276708693609945, Model learning rate 0.00023276708088815212\n",
      "\n",
      " Optimizer iteration 3276, batch 148\n",
      "\n",
      " Learning rate 0.00023208660251050156, Model learning rate 0.00023208660422824323\n",
      "149/391 [==========>...................] - ETA: 18s - loss: 0.7576 - acc: 0.7885\n",
      " Optimizer iteration 3277, batch 149\n",
      "\n",
      " Learning rate 0.00023140681346917104, Model learning rate 0.00023140681150835007\n",
      "150/391 [==========>...................] - ETA: 18s - loss: 0.7576 - acc: 0.7886\n",
      " Optimizer iteration 3278, batch 150\n",
      "\n",
      " Learning rate 0.00023072772157653766, Model learning rate 0.00023072771728038788\n",
      "151/391 [==========>...................] - ETA: 18s - loss: 0.7577 - acc: 0.7885\n",
      " Optimizer iteration 3279, batch 151\n",
      "\n",
      " Learning rate 0.00023004932859522305, Model learning rate 0.00023004932154435664\n",
      "152/391 [==========>...................] - ETA: 18s - loss: 0.7574 - acc: 0.7885\n",
      " Optimizer iteration 3280, batch 152\n",
      "\n",
      " Learning rate 0.00022937163628603436, Model learning rate 0.0002293716388521716\n",
      "153/391 [==========>...................] - ETA: 18s - loss: 0.7588 - acc: 0.7877\n",
      " Optimizer iteration 3281, batch 153\n",
      "\n",
      " Learning rate 0.00022869464640795973, Model learning rate 0.0002286946401000023\n",
      "154/391 [==========>...................] - ETA: 18s - loss: 0.7578 - acc: 0.7880\n",
      " Optimizer iteration 3282, batch 154\n",
      "\n",
      " Learning rate 0.00022801836071816473, Model learning rate 0.00022801835439167917\n",
      "155/391 [==========>...................] - ETA: 17s - loss: 0.7580 - acc: 0.7878\n",
      " Optimizer iteration 3283, batch 155\n",
      "\n",
      " Learning rate 0.00022734278097198669, Model learning rate 0.00022734278172720224\n",
      "156/391 [==========>...................] - ETA: 17s - loss: 0.7582 - acc: 0.7877\n",
      " Optimizer iteration 3284, batch 156\n",
      "\n",
      " Learning rate 0.0002266679089229306, Model learning rate 0.00022666790755465627\n",
      "157/391 [===========>..................] - ETA: 17s - loss: 0.7583 - acc: 0.7878\n",
      " Optimizer iteration 3285, batch 157\n",
      "\n",
      " Learning rate 0.00022599374632266512, Model learning rate 0.0002259937464259565\n",
      "158/391 [===========>..................] - ETA: 17s - loss: 0.7589 - acc: 0.7876\n",
      " Optimizer iteration 3286, batch 158\n",
      "\n",
      " Learning rate 0.00022532029492101674, Model learning rate 0.0002253202983411029\n",
      "159/391 [===========>..................] - ETA: 17s - loss: 0.7589 - acc: 0.7875\n",
      " Optimizer iteration 3287, batch 159\n",
      "\n",
      " Learning rate 0.0002246475564659666, Model learning rate 0.0002246475633000955\n",
      "\n",
      " Optimizer iteration 3288, batch 160\n",
      "\n",
      " Learning rate 0.00022397553270364545, Model learning rate 0.00022397552675101906\n",
      "161/391 [===========>..................] - ETA: 17s - loss: 0.7596 - acc: 0.7875\n",
      " Optimizer iteration 3289, batch 161\n",
      "\n",
      " Learning rate 0.000223304225378328, Model learning rate 0.00022330423234961927\n",
      "162/391 [===========>..................] - ETA: 17s - loss: 0.7594 - acc: 0.7877\n",
      " Optimizer iteration 3290, batch 162\n",
      "\n",
      " Learning rate 0.00022263363623243056, Model learning rate 0.00022263363644015044\n",
      "163/391 [===========>..................] - ETA: 17s - loss: 0.7590 - acc: 0.7878\n",
      " Optimizer iteration 3291, batch 163\n",
      "\n",
      " Learning rate 0.00022196376700650496, Model learning rate 0.00022196376812644303\n",
      "164/391 [===========>..................] - ETA: 17s - loss: 0.7590 - acc: 0.7877\n",
      " Optimizer iteration 3292, batch 164\n",
      "\n",
      " Learning rate 0.00022129461943923406, Model learning rate 0.0002212946128565818\n",
      "165/391 [===========>..................] - ETA: 17s - loss: 0.7589 - acc: 0.7876\n",
      " Optimizer iteration 3293, batch 165\n",
      "\n",
      " Learning rate 0.0002206261952674284, Model learning rate 0.00022062619973439723\n",
      "166/391 [===========>..................] - ETA: 17s - loss: 0.7594 - acc: 0.7876\n",
      " Optimizer iteration 3294, batch 166\n",
      "\n",
      " Learning rate 0.00021995849622602015, Model learning rate 0.00021995849965605885\n",
      "\n",
      " Optimizer iteration 3295, batch 167\n",
      "\n",
      " Learning rate 0.00021929152404805957, Model learning rate 0.00021929152717348188\n",
      "168/391 [===========>..................] - ETA: 16s - loss: 0.7604 - acc: 0.7871\n",
      " Optimizer iteration 3296, batch 168\n",
      "\n",
      " Learning rate 0.0002186252804647107, Model learning rate 0.00021862528228666633\n",
      "169/391 [===========>..................] - ETA: 16s - loss: 0.7611 - acc: 0.7869\n",
      " Optimizer iteration 3297, batch 169\n",
      "\n",
      " Learning rate 0.0002179597672052458, Model learning rate 0.0002179597649956122\n",
      "\n",
      " Optimizer iteration 3298, batch 170\n",
      "\n",
      " Learning rate 0.00021729498599704216, Model learning rate 0.00021729498985223472\n",
      "171/391 [============>.................] - ETA: 16s - loss: 0.7613 - acc: 0.7868\n",
      " Optimizer iteration 3299, batch 171\n",
      "\n",
      " Learning rate 0.00021663093856557708, Model learning rate 0.00021663094230461866\n",
      "172/391 [============>.................] - ETA: 16s - loss: 0.7610 - acc: 0.7868\n",
      " Optimizer iteration 3300, batch 172\n",
      "\n",
      " Learning rate 0.00021596762663442215, Model learning rate 0.000215967622352764\n",
      "173/391 [============>.................] - ETA: 16s - loss: 0.7610 - acc: 0.7871\n",
      " Optimizer iteration 3301, batch 173\n",
      "\n",
      " Learning rate 0.00021530505192524118, Model learning rate 0.00021530505910050124\n",
      "174/391 [============>.................] - ETA: 16s - loss: 0.7610 - acc: 0.7871\n",
      " Optimizer iteration 3302, batch 174\n",
      "\n",
      " Learning rate 0.0002146432161577842, Model learning rate 0.00021464320889208466\n",
      "175/391 [============>.................] - ETA: 16s - loss: 0.7608 - acc: 0.7875\n",
      " Optimizer iteration 3303, batch 175\n",
      "\n",
      " Learning rate 0.00021398212104988275, Model learning rate 0.00021398211538325995\n",
      "\n",
      " Optimizer iteration 3304, batch 176\n",
      "\n",
      " Learning rate 0.0002133217683174466, Model learning rate 0.0002133217640221119\n",
      "177/391 [============>.................] - ETA: 16s - loss: 0.7613 - acc: 0.7875\n",
      " Optimizer iteration 3305, batch 177\n",
      "\n",
      " Learning rate 0.00021266215967445824, Model learning rate 0.00021266215480864048\n",
      "178/391 [============>.................] - ETA: 16s - loss: 0.7605 - acc: 0.7878\n",
      " Optimizer iteration 3306, batch 178\n",
      "\n",
      " Learning rate 0.0002120032968329687, Model learning rate 0.00021200330229476094\n",
      "\n",
      " Optimizer iteration 3307, batch 179\n",
      "\n",
      " Learning rate 0.0002113451815030939, Model learning rate 0.00021134517737664282\n",
      "180/391 [============>.................] - ETA: 16s - loss: 0.7611 - acc: 0.7876\n",
      " Optimizer iteration 3308, batch 180\n",
      "\n",
      " Learning rate 0.00021068781539300874, Model learning rate 0.00021068780915811658\n",
      "181/391 [============>.................] - ETA: 16s - loss: 0.7611 - acc: 0.7876\n",
      " Optimizer iteration 3309, batch 181\n",
      "\n",
      " Learning rate 0.0002100312002089441, Model learning rate 0.0002100311976391822\n",
      "182/391 [============>.................] - ETA: 15s - loss: 0.7604 - acc: 0.7879\n",
      " Optimizer iteration 3310, batch 182\n",
      "\n",
      " Learning rate 0.00020937533765518184, Model learning rate 0.00020937534281983972\n",
      "183/391 [=============>................] - ETA: 15s - loss: 0.7604 - acc: 0.7880\n",
      " Optimizer iteration 3311, batch 183\n",
      "\n",
      " Learning rate 0.0002087202294340494, Model learning rate 0.00020872023014817387\n",
      "184/391 [=============>................] - ETA: 15s - loss: 0.7604 - acc: 0.7881\n",
      " Optimizer iteration 3312, batch 184\n",
      "\n",
      " Learning rate 0.00020806587724591725, Model learning rate 0.0002080658741760999\n",
      "185/391 [=============>................] - ETA: 15s - loss: 0.7601 - acc: 0.7879\n",
      " Optimizer iteration 3313, batch 185\n",
      "\n",
      " Learning rate 0.00020741228278919343, Model learning rate 0.00020741228945553303\n",
      "186/391 [=============>................] - ETA: 15s - loss: 0.7597 - acc: 0.7881\n",
      " Optimizer iteration 3314, batch 186\n",
      "\n",
      " Learning rate 0.00020675944776031875, Model learning rate 0.0002067594468826428\n",
      "\n",
      " Optimizer iteration 3315, batch 187\n",
      "\n",
      " Learning rate 0.00020610737385376348, Model learning rate 0.0002061073755612597\n",
      "188/391 [=============>................] - ETA: 15s - loss: 0.7591 - acc: 0.7882\n",
      " Optimizer iteration 3316, batch 188\n",
      "\n",
      " Learning rate 0.0002054560627620219, Model learning rate 0.00020545606093946844\n",
      "189/391 [=============>................] - ETA: 15s - loss: 0.7589 - acc: 0.7883\n",
      " Optimizer iteration 3317, batch 189\n",
      "\n",
      " Learning rate 0.00020480551617560832, Model learning rate 0.0002048055175691843\n",
      "\n",
      " Optimizer iteration 3318, batch 190\n",
      "\n",
      " Learning rate 0.0002041557357830534, Model learning rate 0.00020415573089849204\n",
      "191/391 [=============>................] - ETA: 15s - loss: 0.7590 - acc: 0.7882\n",
      " Optimizer iteration 3319, batch 191\n",
      "\n",
      " Learning rate 0.00020350672327089814, Model learning rate 0.0002035067300312221\n",
      "\n",
      " Optimizer iteration 3320, batch 192\n",
      "\n",
      " Learning rate 0.00020285848032369137, Model learning rate 0.00020285848586354405\n",
      "193/391 [=============>................] - ETA: 15s - loss: 0.7590 - acc: 0.7881\n",
      " Optimizer iteration 3321, batch 193\n",
      "\n",
      " Learning rate 0.00020221100862398374, Model learning rate 0.0002022110129473731\n",
      "194/391 [=============>................] - ETA: 15s - loss: 0.7588 - acc: 0.7880\n",
      " Optimizer iteration 3322, batch 194\n",
      "\n",
      " Learning rate 0.00020156430985232465, Model learning rate 0.00020156431128270924\n",
      "195/391 [=============>................] - ETA: 14s - loss: 0.7588 - acc: 0.7880\n",
      " Optimizer iteration 3323, batch 195\n",
      "\n",
      " Learning rate 0.00020091838568725683, Model learning rate 0.0002009183808695525\n",
      "196/391 [==============>...............] - ETA: 14s - loss: 0.7594 - acc: 0.7880\n",
      " Optimizer iteration 3324, batch 196\n",
      "\n",
      " Learning rate 0.0002002732378053131, Model learning rate 0.00020027323625981808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/391 [==============>...............] - ETA: 14s - loss: 0.7594 - acc: 0.7881\n",
      " Optimizer iteration 3325, batch 197\n",
      "\n",
      " Learning rate 0.00019962886788101047, Model learning rate 0.00019962886290159076\n",
      "198/391 [==============>...............] - ETA: 14s - loss: 0.7591 - acc: 0.7880\n",
      " Optimizer iteration 3326, batch 198\n",
      "\n",
      " Learning rate 0.00019898527758684787, Model learning rate 0.00019898527534678578\n",
      "199/391 [==============>...............] - ETA: 14s - loss: 0.7595 - acc: 0.7879\n",
      " Optimizer iteration 3327, batch 199\n",
      "\n",
      " Learning rate 0.00019834246859329964, Model learning rate 0.00019834247359540313\n",
      "200/391 [==============>...............] - ETA: 14s - loss: 0.7594 - acc: 0.7879\n",
      " Optimizer iteration 3328, batch 200\n",
      "\n",
      " Learning rate 0.00019770044256881258, Model learning rate 0.0001977004430955276\n",
      "201/391 [==============>...............] - ETA: 14s - loss: 0.7599 - acc: 0.7876\n",
      " Optimizer iteration 3329, batch 201\n",
      "\n",
      " Learning rate 0.00019705920117980147, Model learning rate 0.00019705919839907438\n",
      "202/391 [==============>...............] - ETA: 14s - loss: 0.7598 - acc: 0.7877\n",
      " Optimizer iteration 3330, batch 202\n",
      "\n",
      " Learning rate 0.00019641874609064441, Model learning rate 0.0001964187395060435\n",
      "203/391 [==============>...............] - ETA: 14s - loss: 0.7597 - acc: 0.7877\n",
      " Optimizer iteration 3331, batch 203\n",
      "\n",
      " Learning rate 0.00019577907896367846, Model learning rate 0.00019577908096835017\n",
      "204/391 [==============>...............] - ETA: 14s - loss: 0.7591 - acc: 0.7879\n",
      " Optimizer iteration 3332, batch 204\n",
      "\n",
      " Learning rate 0.00019514020145919536, Model learning rate 0.00019514020823407918\n",
      "205/391 [==============>...............] - ETA: 14s - loss: 0.7590 - acc: 0.7880\n",
      " Optimizer iteration 3333, batch 205\n",
      "\n",
      " Learning rate 0.00019450211523543793, Model learning rate 0.00019450212130323052\n",
      "206/391 [==============>...............] - ETA: 14s - loss: 0.7587 - acc: 0.7881\n",
      " Optimizer iteration 3334, batch 206\n",
      "\n",
      " Learning rate 0.0001938648219485944, Model learning rate 0.0001938648201758042\n",
      "207/391 [==============>...............] - ETA: 14s - loss: 0.7584 - acc: 0.7882\n",
      " Optimizer iteration 3335, batch 207\n",
      "\n",
      " Learning rate 0.0001932283232527956, Model learning rate 0.00019322831940371543\n",
      "208/391 [==============>...............] - ETA: 14s - loss: 0.7583 - acc: 0.7882\n",
      " Optimizer iteration 3336, batch 208\n",
      "\n",
      " Learning rate 0.00019259262080010937, Model learning rate 0.00019259261898696423\n",
      "209/391 [===============>..............] - ETA: 13s - loss: 0.7577 - acc: 0.7886\n",
      " Optimizer iteration 3337, batch 209\n",
      "\n",
      " Learning rate 0.00019195771624053743, Model learning rate 0.00019195771892555058\n",
      "210/391 [===============>..............] - ETA: 13s - loss: 0.7575 - acc: 0.7887\n",
      " Optimizer iteration 3338, batch 210\n",
      "\n",
      " Learning rate 0.0001913236112220101, Model learning rate 0.00019132360466755927\n",
      "211/391 [===============>..............] - ETA: 13s - loss: 0.7586 - acc: 0.7884\n",
      " Optimizer iteration 3339, batch 211\n",
      "\n",
      " Learning rate 0.00019069030739038222, Model learning rate 0.00019069030531682074\n",
      "212/391 [===============>..............] - ETA: 13s - loss: 0.7579 - acc: 0.7887\n",
      " Optimizer iteration 3340, batch 212\n",
      "\n",
      " Learning rate 0.00019005780638942983, Model learning rate 0.00019005780632141978\n",
      "213/391 [===============>..............] - ETA: 13s - loss: 0.7581 - acc: 0.7885\n",
      " Optimizer iteration 3341, batch 213\n",
      "\n",
      " Learning rate 0.00018942610986084484, Model learning rate 0.00018942610768135637\n",
      "214/391 [===============>..............] - ETA: 13s - loss: 0.7577 - acc: 0.7888\n",
      " Optimizer iteration 3342, batch 214\n",
      "\n",
      " Learning rate 0.0001887952194442309, Model learning rate 0.00018879522394854575\n",
      "215/391 [===============>..............] - ETA: 13s - loss: 0.7583 - acc: 0.7885\n",
      " Optimizer iteration 3343, batch 215\n",
      "\n",
      " Learning rate 0.00018816513677709934, Model learning rate 0.0001881651405710727\n",
      "216/391 [===============>..............] - ETA: 13s - loss: 0.7577 - acc: 0.7889\n",
      " Optimizer iteration 3344, batch 216\n",
      "\n",
      " Learning rate 0.00018753586349486552, Model learning rate 0.0001875358575489372\n",
      "217/391 [===============>..............] - ETA: 13s - loss: 0.7573 - acc: 0.7890\n",
      " Optimizer iteration 3345, batch 217\n",
      "\n",
      " Learning rate 0.00018690740123084316, Model learning rate 0.00018690740398596972\n",
      "218/391 [===============>..............] - ETA: 13s - loss: 0.7578 - acc: 0.7890\n",
      " Optimizer iteration 3346, batch 218\n",
      "\n",
      " Learning rate 0.00018627975161624165, Model learning rate 0.0001862797507783398\n",
      "219/391 [===============>..............] - ETA: 13s - loss: 0.7584 - acc: 0.7889\n",
      " Optimizer iteration 3347, batch 219\n",
      "\n",
      " Learning rate 0.00018565291628016062, Model learning rate 0.00018565291247796267\n",
      "220/391 [===============>..............] - ETA: 13s - loss: 0.7586 - acc: 0.7889\n",
      " Optimizer iteration 3348, batch 220\n",
      "\n",
      " Learning rate 0.00018502689684958662, Model learning rate 0.00018502690363675356\n",
      "221/391 [===============>..............] - ETA: 13s - loss: 0.7580 - acc: 0.7891\n",
      " Optimizer iteration 3349, batch 221\n",
      "\n",
      " Learning rate 0.00018440169494938802, Model learning rate 0.000184401695150882\n",
      "222/391 [================>.............] - ETA: 13s - loss: 0.7582 - acc: 0.7892\n",
      " Optimizer iteration 3350, batch 222\n",
      "\n",
      " Learning rate 0.0001837773122023114, Model learning rate 0.00018377731612417847\n",
      "223/391 [================>.............] - ETA: 12s - loss: 0.7581 - acc: 0.7894\n",
      " Optimizer iteration 3351, batch 223\n",
      "\n",
      " Learning rate 0.00018315375022897736, Model learning rate 0.00018315375200472772\n",
      "224/391 [================>.............] - ETA: 12s - loss: 0.7576 - acc: 0.7895\n",
      " Optimizer iteration 3352, batch 224\n",
      "\n",
      " Learning rate 0.0001825310106478762, Model learning rate 0.000182531017344445\n",
      "225/391 [================>.............] - ETA: 12s - loss: 0.7576 - acc: 0.7895\n",
      " Optimizer iteration 3353, batch 225\n",
      "\n",
      " Learning rate 0.00018190909507536323, Model learning rate 0.00018190909759141505\n",
      "226/391 [================>.............] - ETA: 12s - loss: 0.7578 - acc: 0.7894\n",
      " Optimizer iteration 3354, batch 226\n",
      "\n",
      " Learning rate 0.00018128800512565513, Model learning rate 0.00018128800729755312\n",
      "227/391 [================>.............] - ETA: 12s - loss: 0.7577 - acc: 0.7895\n",
      " Optimizer iteration 3355, batch 227\n",
      "\n",
      " Learning rate 0.00018066774241082612, Model learning rate 0.0001806677464628592\n",
      "228/391 [================>.............] - ETA: 12s - loss: 0.7577 - acc: 0.7894\n",
      " Optimizer iteration 3356, batch 228\n",
      "\n",
      " Learning rate 0.0001800483085408025, Model learning rate 0.00018004831508733332\n",
      "229/391 [================>.............] - ETA: 12s - loss: 0.7574 - acc: 0.7895\n",
      " Optimizer iteration 3357, batch 229\n",
      "\n",
      " Learning rate 0.00017942970512335998, Model learning rate 0.00017942969861906022\n",
      "230/391 [================>.............] - ETA: 12s - loss: 0.7570 - acc: 0.7897\n",
      " Optimizer iteration 3358, batch 230\n",
      "\n",
      " Learning rate 0.00017881193376411818, Model learning rate 0.0001788119407137856\n",
      "\n",
      " Optimizer iteration 3359, batch 231\n",
      "\n",
      " Learning rate 0.00017819499606653772, Model learning rate 0.00017819499771576375\n",
      "232/391 [================>.............] - ETA: 12s - loss: 0.7567 - acc: 0.7898\n",
      " Optimizer iteration 3360, batch 232\n",
      "\n",
      " Learning rate 0.00017757889363191482, Model learning rate 0.00017757889872882515\n",
      "233/391 [================>.............] - ETA: 12s - loss: 0.7561 - acc: 0.7900\n",
      " Optimizer iteration 3361, batch 233\n",
      "\n",
      " Learning rate 0.00017696362805937776, Model learning rate 0.00017696362920105457\n",
      "234/391 [================>.............] - ETA: 12s - loss: 0.7560 - acc: 0.7900\n",
      " Optimizer iteration 3362, batch 234\n",
      "\n",
      " Learning rate 0.00017634920094588308, Model learning rate 0.00017634920368436724\n",
      "235/391 [=================>............] - ETA: 12s - loss: 0.7558 - acc: 0.7901\n",
      " Optimizer iteration 3363, batch 235\n",
      "\n",
      " Learning rate 0.00017573561388621101, Model learning rate 0.00017573560762684792\n",
      "236/391 [=================>............] - ETA: 12s - loss: 0.7557 - acc: 0.7901\n",
      " Optimizer iteration 3364, batch 236\n",
      "\n",
      " Learning rate 0.00017512286847296105, Model learning rate 0.00017512287013232708\n",
      "237/391 [=================>............] - ETA: 11s - loss: 0.7560 - acc: 0.7899\n",
      " Optimizer iteration 3365, batch 237\n",
      "\n",
      " Learning rate 0.0001745109662965481, Model learning rate 0.00017451096209697425\n",
      "238/391 [=================>............] - ETA: 11s - loss: 0.7560 - acc: 0.7898\n",
      " Optimizer iteration 3366, batch 238\n",
      "\n",
      " Learning rate 0.0001738999089451991, Model learning rate 0.0001738999126246199\n",
      "239/391 [=================>............] - ETA: 11s - loss: 0.7560 - acc: 0.7899\n",
      " Optimizer iteration 3367, batch 239\n",
      "\n",
      " Learning rate 0.00017328969800494727, Model learning rate 0.00017328969261143357\n",
      "240/391 [=================>............] - ETA: 11s - loss: 0.7564 - acc: 0.7898\n",
      " Optimizer iteration 3368, batch 240\n",
      "\n",
      " Learning rate 0.0001726803350596297, Model learning rate 0.0001726803311612457\n",
      "\n",
      " Optimizer iteration 3369, batch 241\n",
      "\n",
      " Learning rate 0.00017207182169088204, Model learning rate 0.00017207182827405632\n",
      "242/391 [=================>............] - ETA: 11s - loss: 0.7572 - acc: 0.7895\n",
      " Optimizer iteration 3370, batch 242\n",
      "\n",
      " Learning rate 0.00017146415947813472, Model learning rate 0.00017146415484603494\n",
      "243/391 [=================>............] - ETA: 11s - loss: 0.7571 - acc: 0.7896\n",
      " Optimizer iteration 3371, batch 243\n",
      "\n",
      " Learning rate 0.00017085734999860937, Model learning rate 0.00017085735453292727\n",
      "244/391 [=================>............] - ETA: 11s - loss: 0.7566 - acc: 0.7897\n",
      " Optimizer iteration 3372, batch 244\n",
      "\n",
      " Learning rate 0.00017025139482731384, Model learning rate 0.00017025139823090285\n",
      "245/391 [=================>............] - ETA: 11s - loss: 0.7565 - acc: 0.7898\n",
      " Optimizer iteration 3373, batch 245\n",
      "\n",
      " Learning rate 0.00016964629553703893, Model learning rate 0.0001696463004918769\n",
      "246/391 [=================>............] - ETA: 11s - loss: 0.7562 - acc: 0.7899\n",
      " Optimizer iteration 3374, batch 246\n",
      "\n",
      " Learning rate 0.000169042053698354, Model learning rate 0.0001690420467639342\n",
      "247/391 [=================>............] - ETA: 11s - loss: 0.7558 - acc: 0.7899\n",
      " Optimizer iteration 3375, batch 247\n",
      "\n",
      " Learning rate 0.00016843867087960252, Model learning rate 0.0001684386661509052\n",
      "248/391 [==================>...........] - ETA: 11s - loss: 0.7561 - acc: 0.7896\n",
      " Optimizer iteration 3376, batch 248\n",
      "\n",
      " Learning rate 0.00016783614864689827, Model learning rate 0.00016783614410087466\n",
      "249/391 [==================>...........] - ETA: 11s - loss: 0.7559 - acc: 0.7896\n",
      " Optimizer iteration 3377, batch 249\n",
      "\n",
      " Learning rate 0.00016723448856412188, Model learning rate 0.00016723449516575783\n",
      "250/391 [==================>...........] - ETA: 11s - loss: 0.7558 - acc: 0.7897\n",
      " Optimizer iteration 3378, batch 250\n",
      "\n",
      " Learning rate 0.00016663369219291558, Model learning rate 0.00016663369024172425\n",
      "251/391 [==================>...........] - ETA: 10s - loss: 0.7552 - acc: 0.7898\n",
      " Optimizer iteration 3379, batch 251\n",
      "\n",
      " Learning rate 0.00016603376109268042, Model learning rate 0.00016603375843260437\n",
      "\n",
      " Optimizer iteration 3380, batch 252\n",
      "\n",
      " Learning rate 0.00016543469682057105, Model learning rate 0.0001654346997383982\n",
      "253/391 [==================>...........] - ETA: 10s - loss: 0.7565 - acc: 0.7895\n",
      " Optimizer iteration 3381, batch 253\n",
      "\n",
      " Learning rate 0.00016483650093149227, Model learning rate 0.0001648364996071905\n",
      "254/391 [==================>...........] - ETA: 10s - loss: 0.7571 - acc: 0.7892\n",
      " Optimizer iteration 3382, batch 254\n",
      "\n",
      " Learning rate 0.00016423917497809532, Model learning rate 0.0001642391725908965\n",
      "255/391 [==================>...........] - ETA: 10s - loss: 0.7567 - acc: 0.7894\n",
      " Optimizer iteration 3383, batch 255\n",
      "\n",
      " Learning rate 0.00016364272051077333, Model learning rate 0.0001636427186895162\n",
      "256/391 [==================>...........] - ETA: 10s - loss: 0.7564 - acc: 0.7894\n",
      " Optimizer iteration 3384, batch 256\n",
      "\n",
      " Learning rate 0.00016304713907765712, Model learning rate 0.0001630471379030496\n",
      "257/391 [==================>...........] - ETA: 10s - loss: 0.7565 - acc: 0.7895\n",
      " Optimizer iteration 3385, batch 257\n",
      "\n",
      " Learning rate 0.00016245243222461197, Model learning rate 0.0001624524302314967\n",
      "258/391 [==================>...........] - ETA: 10s - loss: 0.7564 - acc: 0.7895\n",
      " Optimizer iteration 3386, batch 258\n",
      "\n",
      " Learning rate 0.00016185860149523285, Model learning rate 0.0001618585956748575\n",
      "259/391 [==================>...........] - ETA: 10s - loss: 0.7564 - acc: 0.7895\n",
      " Optimizer iteration 3387, batch 259\n",
      "\n",
      " Learning rate 0.00016126564843084052, Model learning rate 0.00016126564878504723\n",
      "260/391 [==================>...........] - ETA: 10s - loss: 0.7569 - acc: 0.7893\n",
      " Optimizer iteration 3388, batch 260\n",
      "\n",
      " Learning rate 0.00016067357457047837, Model learning rate 0.00016067357501015067\n",
      "261/391 [===================>..........] - ETA: 10s - loss: 0.7572 - acc: 0.7892\n",
      " Optimizer iteration 3389, batch 261\n",
      "\n",
      " Learning rate 0.00016008238145090692, Model learning rate 0.0001600823743501678\n",
      "262/391 [===================>..........] - ETA: 10s - loss: 0.7572 - acc: 0.7892\n",
      " Optimizer iteration 3390, batch 262\n",
      "\n",
      " Learning rate 0.00015949207060660136, Model learning rate 0.0001594920759089291\n",
      "263/391 [===================>..........] - ETA: 10s - loss: 0.7570 - acc: 0.7893\n",
      " Optimizer iteration 3391, batch 263\n",
      "\n",
      " Learning rate 0.00015890264356974688, Model learning rate 0.0001589026505826041\n",
      "264/391 [===================>..........] - ETA: 9s - loss: 0.7574 - acc: 0.7892 \n",
      " Optimizer iteration 3392, batch 264\n",
      "\n",
      " Learning rate 0.00015831410187023388, Model learning rate 0.0001583140983711928\n",
      "265/391 [===================>..........] - ETA: 9s - loss: 0.7575 - acc: 0.7891\n",
      " Optimizer iteration 3393, batch 265\n",
      "\n",
      " Learning rate 0.00015772644703565563, Model learning rate 0.00015772644837852567\n",
      "266/391 [===================>..........] - ETA: 9s - loss: 0.7570 - acc: 0.7891\n",
      " Optimizer iteration 3394, batch 266\n",
      "\n",
      " Learning rate 0.00015713968059130346, Model learning rate 0.00015713968605268747\n",
      "267/391 [===================>..........] - ETA: 9s - loss: 0.7573 - acc: 0.7891\n",
      " Optimizer iteration 3395, batch 267\n",
      "\n",
      " Learning rate 0.00015655380406016234, Model learning rate 0.00015655379684176296\n",
      "\n",
      " Optimizer iteration 3396, batch 268\n",
      "\n",
      " Learning rate 0.000155968818962908, Model learning rate 0.00015596882440149784\n",
      "269/391 [===================>..........] - ETA: 9s - loss: 0.7573 - acc: 0.7890\n",
      " Optimizer iteration 3397, batch 269\n",
      "\n",
      " Learning rate 0.00015538472681790185, Model learning rate 0.00015538472507614642\n",
      "270/391 [===================>..........] - ETA: 9s - loss: 0.7571 - acc: 0.7891\n",
      " Optimizer iteration 3398, batch 270\n",
      "\n",
      " Learning rate 0.00015480152914118783, Model learning rate 0.00015480152796953917\n",
      "271/391 [===================>..........] - ETA: 9s - loss: 0.7573 - acc: 0.7890\n",
      " Optimizer iteration 3399, batch 271\n",
      "\n",
      " Learning rate 0.00015421922744648846, Model learning rate 0.00015421923308167607\n",
      "272/391 [===================>..........] - ETA: 9s - loss: 0.7572 - acc: 0.7890\n",
      " Optimizer iteration 3400, batch 272\n",
      "\n",
      " Learning rate 0.00015363782324520031, Model learning rate 0.0001536378258606419\n",
      "273/391 [===================>..........] - ETA: 9s - loss: 0.7577 - acc: 0.7888\n",
      " Optimizer iteration 3401, batch 273\n",
      "\n",
      " Learning rate 0.00015305731804639066, Model learning rate 0.00015305732085835189\n",
      "274/391 [====================>.........] - ETA: 9s - loss: 0.7576 - acc: 0.7886\n",
      " Optimizer iteration 3402, batch 274\n",
      "\n",
      " Learning rate 0.00015247771335679373, Model learning rate 0.00015247771807480603\n",
      "275/391 [====================>.........] - ETA: 9s - loss: 0.7576 - acc: 0.7885\n",
      " Optimizer iteration 3403, batch 275\n",
      "\n",
      " Learning rate 0.00015189901068080535, Model learning rate 0.00015189901751000434\n",
      "276/391 [====================>.........] - ETA: 9s - loss: 0.7580 - acc: 0.7882\n",
      " Optimizer iteration 3404, batch 276\n",
      "\n",
      " Learning rate 0.00015132121152048117, Model learning rate 0.00015132120461203158\n",
      "277/391 [====================>.........] - ETA: 8s - loss: 0.7582 - acc: 0.7882\n",
      " Optimizer iteration 3405, batch 277\n",
      "\n",
      " Learning rate 0.00015074431737553158, Model learning rate 0.00015074432303663343\n",
      "278/391 [====================>.........] - ETA: 8s - loss: 0.7580 - acc: 0.7882\n",
      " Optimizer iteration 3406, batch 278\n",
      "\n",
      " Learning rate 0.00015016832974331724, Model learning rate 0.00015016832912806422\n",
      "279/391 [====================>.........] - ETA: 8s - loss: 0.7582 - acc: 0.7883\n",
      " Optimizer iteration 3407, batch 279\n",
      "\n",
      " Learning rate 0.00014959325011884683, Model learning rate 0.00014959325199015439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/391 [====================>.........] - ETA: 8s - loss: 0.7581 - acc: 0.7883\n",
      " Optimizer iteration 3408, batch 280\n",
      "\n",
      " Learning rate 0.00014901907999477165, Model learning rate 0.00014901907707098871\n",
      "281/391 [====================>.........] - ETA: 8s - loss: 0.7580 - acc: 0.7882\n",
      " Optimizer iteration 3409, batch 281\n",
      "\n",
      " Learning rate 0.00014844582086138232, Model learning rate 0.00014844581892248243\n",
      "282/391 [====================>.........] - ETA: 8s - loss: 0.7583 - acc: 0.7881\n",
      " Optimizer iteration 3410, batch 282\n",
      "\n",
      " Learning rate 0.0001478734742066054, Model learning rate 0.00014787347754463553\n",
      "283/391 [====================>.........] - ETA: 8s - loss: 0.7583 - acc: 0.7882\n",
      " Optimizer iteration 3411, batch 283\n",
      "\n",
      " Learning rate 0.00014730204151599846, Model learning rate 0.0001473020383855328\n",
      "\n",
      " Optimizer iteration 3412, batch 284\n",
      "\n",
      " Learning rate 0.00014673152427274738, Model learning rate 0.00014673153054900467\n",
      "285/391 [====================>.........] - ETA: 8s - loss: 0.7581 - acc: 0.7882\n",
      " Optimizer iteration 3413, batch 285\n",
      "\n",
      " Learning rate 0.00014616192395766186, Model learning rate 0.0001461619249312207\n",
      "286/391 [====================>.........] - ETA: 8s - loss: 0.7581 - acc: 0.7882\n",
      " Optimizer iteration 3414, batch 286\n",
      "\n",
      " Learning rate 0.000145593242049171, Model learning rate 0.00014559323608409613\n",
      "287/391 [=====================>........] - ETA: 8s - loss: 0.7579 - acc: 0.7882\n",
      " Optimizer iteration 3415, batch 287\n",
      "\n",
      " Learning rate 0.00014502548002332088, Model learning rate 0.00014502547855954617\n",
      "\n",
      " Optimizer iteration 3416, batch 288\n",
      "\n",
      " Learning rate 0.0001444586393537699, Model learning rate 0.0001444586378056556\n",
      "289/391 [=====================>........] - ETA: 7s - loss: 0.7574 - acc: 0.7884\n",
      " Optimizer iteration 3417, batch 289\n",
      "\n",
      " Learning rate 0.00014389272151178452, Model learning rate 0.00014389272837433964\n",
      "290/391 [=====================>........] - ETA: 7s - loss: 0.7572 - acc: 0.7886\n",
      " Optimizer iteration 3418, batch 290\n",
      "\n",
      " Learning rate 0.00014332772796623656, Model learning rate 0.00014332772116176784\n",
      "291/391 [=====================>........] - ETA: 7s - loss: 0.7570 - acc: 0.7886\n",
      " Optimizer iteration 3419, batch 291\n",
      "\n",
      " Learning rate 0.00014276366018359842, Model learning rate 0.00014276365982368588\n",
      "292/391 [=====================>........] - ETA: 7s - loss: 0.7566 - acc: 0.7886\n",
      " Optimizer iteration 3420, batch 292\n",
      "\n",
      " Learning rate 0.00014220051962793951, Model learning rate 0.00014220051525626332\n",
      "\n",
      " Optimizer iteration 3421, batch 293\n",
      "\n",
      " Learning rate 0.000141638307760923, Model learning rate 0.00014163830201141536\n",
      "294/391 [=====================>........] - ETA: 7s - loss: 0.7566 - acc: 0.7887\n",
      " Optimizer iteration 3422, batch 294\n",
      "\n",
      " Learning rate 0.00014107702604180118, Model learning rate 0.00014107702008914202\n",
      "295/391 [=====================>........] - ETA: 7s - loss: 0.7570 - acc: 0.7887\n",
      " Optimizer iteration 3423, batch 295\n",
      "\n",
      " Learning rate 0.0001405166759274123, Model learning rate 0.0001405166694894433\n",
      "\n",
      " Optimizer iteration 3424, batch 296\n",
      "\n",
      " Learning rate 0.0001399572588721769, Model learning rate 0.00013995726476423442\n",
      "297/391 [=====================>........] - ETA: 7s - loss: 0.7568 - acc: 0.7886\n",
      " Optimizer iteration 3425, batch 297\n",
      "\n",
      " Learning rate 0.0001393987763280928, Model learning rate 0.00013939877680968493\n",
      "298/391 [=====================>........] - ETA: 7s - loss: 0.7563 - acc: 0.7889\n",
      " Optimizer iteration 3426, batch 298\n",
      "\n",
      " Learning rate 0.00013884122974473307, Model learning rate 0.00013884123472962528\n",
      "299/391 [=====================>........] - ETA: 7s - loss: 0.7557 - acc: 0.7891\n",
      " Optimizer iteration 3427, batch 299\n",
      "\n",
      " Learning rate 0.0001382846205692413, Model learning rate 0.00013828462397214025\n",
      "300/391 [======================>.......] - ETA: 7s - loss: 0.7558 - acc: 0.7890\n",
      " Optimizer iteration 3428, batch 300\n",
      "\n",
      " Learning rate 0.00013772895024632753, Model learning rate 0.00013772894453722984\n",
      "301/391 [======================>.......] - ETA: 7s - loss: 0.7556 - acc: 0.7889\n",
      " Optimizer iteration 3429, batch 301\n",
      "\n",
      " Learning rate 0.00013717422021826569, Model learning rate 0.0001371742255287245\n",
      "302/391 [======================>.......] - ETA: 6s - loss: 0.7556 - acc: 0.7889\n",
      " Optimizer iteration 3430, batch 302\n",
      "\n",
      " Learning rate 0.0001366204319248885, Model learning rate 0.00013662043784279376\n",
      "303/391 [======================>.......] - ETA: 6s - loss: 0.7558 - acc: 0.7890\n",
      " Optimizer iteration 3431, batch 303\n",
      "\n",
      " Learning rate 0.00013606758680358445, Model learning rate 0.00013606758147943765\n",
      "304/391 [======================>.......] - ETA: 6s - loss: 0.7557 - acc: 0.7891\n",
      " Optimizer iteration 3432, batch 304\n",
      "\n",
      " Learning rate 0.00013551568628929433, Model learning rate 0.0001355156855424866\n",
      "\n",
      " Optimizer iteration 3433, batch 305\n",
      "\n",
      " Learning rate 0.0001349647318145067, Model learning rate 0.0001349647354800254\n",
      "306/391 [======================>.......] - ETA: 6s - loss: 0.7556 - acc: 0.7889\n",
      " Optimizer iteration 3434, batch 306\n",
      "\n",
      " Learning rate 0.00013441472480925492, Model learning rate 0.00013441473129205406\n",
      "307/391 [======================>.......] - ETA: 6s - loss: 0.7556 - acc: 0.7889\n",
      " Optimizer iteration 3435, batch 307\n",
      "\n",
      " Learning rate 0.0001338656667011134, Model learning rate 0.00013386567297857255\n",
      "308/391 [======================>.......] - ETA: 6s - loss: 0.7557 - acc: 0.7888\n",
      " Optimizer iteration 3436, batch 308\n",
      "\n",
      " Learning rate 0.00013331755891519265, Model learning rate 0.00013331756053958088\n",
      "309/391 [======================>.......] - ETA: 6s - loss: 0.7553 - acc: 0.7889\n",
      " Optimizer iteration 3437, batch 309\n",
      "\n",
      " Learning rate 0.00013277040287413755, Model learning rate 0.0001327704085269943\n",
      "310/391 [======================>.......] - ETA: 6s - loss: 0.7552 - acc: 0.7889\n",
      " Optimizer iteration 3438, batch 310\n",
      "\n",
      " Learning rate 0.00013222419999812246, Model learning rate 0.00013222420238889754\n",
      "\n",
      " Optimizer iteration 3439, batch 311\n",
      "\n",
      " Learning rate 0.0001316789517048473, Model learning rate 0.00013167895667720586\n",
      "312/391 [======================>.......] - ETA: 6s - loss: 0.7553 - acc: 0.7887\n",
      " Optimizer iteration 3440, batch 312\n",
      "\n",
      " Learning rate 0.00013113465940953496, Model learning rate 0.00013113465684000403\n",
      "\n",
      " Optimizer iteration 3441, batch 313\n",
      "\n",
      " Learning rate 0.00013059132452492651, Model learning rate 0.00013059131742920727\n",
      "314/391 [=======================>......] - ETA: 6s - loss: 0.7555 - acc: 0.7888\n",
      " Optimizer iteration 3442, batch 314\n",
      "\n",
      " Learning rate 0.0001300489484612779, Model learning rate 0.0001300489529967308\n",
      "\n",
      " Optimizer iteration 3443, batch 315\n",
      "\n",
      " Learning rate 0.00012950753262635711, Model learning rate 0.0001295075344387442\n",
      "316/391 [=======================>......] - ETA: 5s - loss: 0.7553 - acc: 0.7888\n",
      " Optimizer iteration 3444, batch 316\n",
      "\n",
      " Learning rate 0.00012896707842543898, Model learning rate 0.00012896707630716264\n",
      "317/391 [=======================>......] - ETA: 5s - loss: 0.7555 - acc: 0.7888\n",
      " Optimizer iteration 3445, batch 317\n",
      "\n",
      " Learning rate 0.00012842758726130281, Model learning rate 0.0001284275931539014\n",
      "318/391 [=======================>......] - ETA: 5s - loss: 0.7554 - acc: 0.7887\n",
      " Optimizer iteration 3446, batch 318\n",
      "\n",
      " Learning rate 0.0001278890605342285, Model learning rate 0.00012788905587513\n",
      "319/391 [=======================>......] - ETA: 5s - loss: 0.7553 - acc: 0.7888\n",
      " Optimizer iteration 3447, batch 319\n",
      "\n",
      " Learning rate 0.0001273514996419921, Model learning rate 0.0001273514935746789\n",
      "320/391 [=======================>......] - ETA: 5s - loss: 0.7553 - acc: 0.7888\n",
      " Optimizer iteration 3448, batch 320\n",
      "\n",
      " Learning rate 0.00012681490597986312, Model learning rate 0.0001268149062525481\n",
      "321/391 [=======================>......] - ETA: 5s - loss: 0.7548 - acc: 0.7890\n",
      " Optimizer iteration 3449, batch 321\n",
      "\n",
      " Learning rate 0.00012627928094060065, Model learning rate 0.00012627927935682237\n",
      "322/391 [=======================>......] - ETA: 5s - loss: 0.7551 - acc: 0.7889\n",
      " Optimizer iteration 3450, batch 322\n",
      "\n",
      " Learning rate 0.0001257446259144494, Model learning rate 0.00012574462743941694\n",
      "323/391 [=======================>......] - ETA: 5s - loss: 0.7551 - acc: 0.7889\n",
      " Optimizer iteration 3451, batch 323\n",
      "\n",
      " Learning rate 0.00012521094228913683, Model learning rate 0.0001252109359484166\n",
      "\n",
      " Optimizer iteration 3452, batch 324\n",
      "\n",
      " Learning rate 0.00012467823144986844, Model learning rate 0.00012467823398765177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/391 [=======================>......] - ETA: 5s - loss: 0.7541 - acc: 0.7893\n",
      " Optimizer iteration 3453, batch 325\n",
      "\n",
      " Learning rate 0.0001241464947793251, Model learning rate 0.000124146492453292\n",
      "326/391 [========================>.....] - ETA: 5s - loss: 0.7541 - acc: 0.7893\n",
      " Optimizer iteration 3454, batch 326\n",
      "\n",
      " Learning rate 0.00012361573365765938, Model learning rate 0.0001236157404491678\n",
      "327/391 [========================>.....] - ETA: 5s - loss: 0.7540 - acc: 0.7893\n",
      " Optimizer iteration 3455, batch 327\n",
      "\n",
      " Learning rate 0.00012308594946249164, Model learning rate 0.00012308594887144864\n",
      "\n",
      " Optimizer iteration 3456, batch 328\n",
      "\n",
      " Learning rate 0.0001225571435689062, Model learning rate 0.000122557146823965\n",
      "329/391 [========================>.....] - ETA: 4s - loss: 0.7535 - acc: 0.7896\n",
      " Optimizer iteration 3457, batch 329\n",
      "\n",
      " Learning rate 0.00012202931734944878, Model learning rate 0.00012202931975480169\n",
      "330/391 [========================>.....] - ETA: 4s - loss: 0.7537 - acc: 0.7894\n",
      " Optimizer iteration 3458, batch 330\n",
      "\n",
      " Learning rate 0.00012150247217412185, Model learning rate 0.00012150247493991628\n",
      "\n",
      " Optimizer iteration 3459, batch 331\n",
      "\n",
      " Learning rate 0.00012097660941038147, Model learning rate 0.00012097661237930879\n",
      "332/391 [========================>.....] - ETA: 4s - loss: 0.7536 - acc: 0.7894\n",
      " Optimizer iteration 3460, batch 332\n",
      "\n",
      " Learning rate 0.00012045173042313429, Model learning rate 0.00012045173207297921\n",
      "\n",
      " Optimizer iteration 3461, batch 333\n",
      "\n",
      " Learning rate 0.00011992783657473289, Model learning rate 0.00011992783402092755\n",
      "334/391 [========================>.....] - ETA: 4s - loss: 0.7531 - acc: 0.7897\n",
      " Optimizer iteration 3462, batch 334\n",
      "\n",
      " Learning rate 0.00011940492922497337, Model learning rate 0.00011940493277506903\n",
      "\n",
      " Optimizer iteration 3463, batch 335\n",
      "\n",
      " Learning rate 0.00011888300973109112, Model learning rate 0.00011888300650753081\n",
      "336/391 [========================>.....] - ETA: 4s - loss: 0.7528 - acc: 0.7897\n",
      " Optimizer iteration 3464, batch 336\n",
      "\n",
      " Learning rate 0.00011836207944775728, Model learning rate 0.00011836207704618573\n",
      "337/391 [========================>.....] - ETA: 4s - loss: 0.7533 - acc: 0.7895\n",
      " Optimizer iteration 3465, batch 337\n",
      "\n",
      " Learning rate 0.0001178421397270758, Model learning rate 0.00011784213711507618\n",
      "338/391 [========================>.....] - ETA: 4s - loss: 0.7534 - acc: 0.7895\n",
      " Optimizer iteration 3466, batch 338\n",
      "\n",
      " Learning rate 0.00011732319191857954, Model learning rate 0.00011732319399015978\n",
      "\n",
      " Optimizer iteration 3467, batch 339\n",
      "\n",
      " Learning rate 0.0001168052373692266, Model learning rate 0.0001168052403954789\n",
      "340/391 [=========================>....] - ETA: 3s - loss: 0.7534 - acc: 0.7896\n",
      " Optimizer iteration 3468, batch 340\n",
      "\n",
      " Learning rate 0.00011628827742339687, Model learning rate 0.00011628827633103356\n",
      "341/391 [=========================>....] - ETA: 3s - loss: 0.7532 - acc: 0.7896\n",
      " Optimizer iteration 3469, batch 341\n",
      "\n",
      " Learning rate 0.0001157723134228893, Model learning rate 0.00011577231634873897\n",
      "342/391 [=========================>....] - ETA: 3s - loss: 0.7531 - acc: 0.7897\n",
      " Optimizer iteration 3470, batch 342\n",
      "\n",
      " Learning rate 0.00011525734670691701, Model learning rate 0.00011525734589667991\n",
      "\n",
      " Optimizer iteration 3471, batch 343\n",
      "\n",
      " Learning rate 0.00011474337861210544, Model learning rate 0.0001147433795267716\n",
      "344/391 [=========================>....] - ETA: 3s - loss: 0.7528 - acc: 0.7899\n",
      " Optimizer iteration 3472, batch 344\n",
      "\n",
      " Learning rate 0.00011423041047248728, Model learning rate 0.00011423040996305645\n",
      "345/391 [=========================>....] - ETA: 3s - loss: 0.7527 - acc: 0.7899\n",
      " Optimizer iteration 3473, batch 345\n",
      "\n",
      " Learning rate 0.00011371844361950045, Model learning rate 0.00011371844448149204\n",
      "346/391 [=========================>....] - ETA: 3s - loss: 0.7529 - acc: 0.7898\n",
      " Optimizer iteration 3474, batch 346\n",
      "\n",
      " Learning rate 0.00011320747938198356, Model learning rate 0.00011320747580612078\n",
      "\n",
      " Optimizer iteration 3475, batch 347\n",
      "\n",
      " Learning rate 0.00011269751908617276, Model learning rate 0.0001126975184888579\n",
      "348/391 [=========================>....] - ETA: 3s - loss: 0.7524 - acc: 0.7898\n",
      " Optimizer iteration 3476, batch 348\n",
      "\n",
      " Learning rate 0.00011218856405569883, Model learning rate 0.00011218856525374576\n",
      "\n",
      " Optimizer iteration 3477, batch 349\n",
      "\n",
      " Learning rate 0.00011168061561158321, Model learning rate 0.00011168061610078439\n",
      "350/391 [=========================>....] - ETA: 3s - loss: 0.7523 - acc: 0.7897\n",
      " Optimizer iteration 3478, batch 350\n",
      "\n",
      " Learning rate 0.00011117367507223452, Model learning rate 0.00011117367830593139\n",
      "351/391 [=========================>....] - ETA: 3s - loss: 0.7522 - acc: 0.7898\n",
      " Optimizer iteration 3479, batch 351\n",
      "\n",
      " Learning rate 0.0001106677437534453, Model learning rate 0.00011066774459322914\n",
      "\n",
      " Optimizer iteration 3480, batch 352\n",
      "\n",
      " Learning rate 0.00011016282296838886, Model learning rate 0.00011016282223863527\n",
      "353/391 [==========================>...] - ETA: 2s - loss: 0.7515 - acc: 0.7898\n",
      " Optimizer iteration 3481, batch 353\n",
      "\n",
      " Learning rate 0.0001096589140276153, Model learning rate 0.00010965891124214977\n",
      "354/391 [==========================>...] - ETA: 2s - loss: 0.7517 - acc: 0.7897\n",
      " Optimizer iteration 3482, batch 354\n",
      "\n",
      " Learning rate 0.00010915601823904875, Model learning rate 0.00010915601887973025\n",
      "355/391 [==========================>...] - ETA: 2s - loss: 0.7515 - acc: 0.7899\n",
      " Optimizer iteration 3483, batch 355\n",
      "\n",
      " Learning rate 0.00010865413690798321, Model learning rate 0.00010865413787541911\n",
      "\n",
      " Optimizer iteration 3484, batch 356\n",
      "\n",
      " Learning rate 0.00010815327133708014, Model learning rate 0.00010815326822921634\n",
      "357/391 [==========================>...] - ETA: 2s - loss: 0.7516 - acc: 0.7899\n",
      " Optimizer iteration 3485, batch 357\n",
      "\n",
      " Learning rate 0.00010765342282636414, Model learning rate 0.00010765342449303716\n",
      "358/391 [==========================>...] - ETA: 2s - loss: 0.7514 - acc: 0.7899\n",
      " Optimizer iteration 3486, batch 358\n",
      "\n",
      " Learning rate 0.00010715459267321997, Model learning rate 0.00010715459211496636\n",
      "359/391 [==========================>...] - ETA: 2s - loss: 0.7512 - acc: 0.7900\n",
      " Optimizer iteration 3487, batch 359\n",
      "\n",
      " Learning rate 0.00010665678217238933, Model learning rate 0.00010665678564691916\n",
      "\n",
      " Optimizer iteration 3488, batch 360\n",
      "\n",
      " Learning rate 0.0001061599926159676, Model learning rate 0.00010615999053698033\n",
      "361/391 [==========================>...] - ETA: 2s - loss: 0.7512 - acc: 0.7899\n",
      " Optimizer iteration 3489, batch 361\n",
      "\n",
      " Learning rate 0.0001056642252933997, Model learning rate 0.00010566422861302271\n",
      "362/391 [==========================>...] - ETA: 2s - loss: 0.7511 - acc: 0.7900\n",
      " Optimizer iteration 3490, batch 362\n",
      "\n",
      " Learning rate 0.00010516948149147753, Model learning rate 0.00010516947804717347\n",
      "363/391 [==========================>...] - ETA: 2s - loss: 0.7512 - acc: 0.7899\n",
      " Optimizer iteration 3491, batch 363\n",
      "\n",
      " Learning rate 0.00010467576249433663, Model learning rate 0.00010467576066730544\n",
      "\n",
      " Optimizer iteration 3492, batch 364\n",
      "\n",
      " Learning rate 0.00010418306958345213, Model learning rate 0.00010418306919746101\n",
      "365/391 [===========================>..] - ETA: 2s - loss: 0.7510 - acc: 0.7900\n",
      " Optimizer iteration 3493, batch 365\n",
      "\n",
      " Learning rate 0.00010369140403763638, Model learning rate 0.00010369140363764018\n",
      "366/391 [===========================>..] - ETA: 1s - loss: 0.7509 - acc: 0.7901\n",
      " Optimizer iteration 3494, batch 366\n",
      "\n",
      " Learning rate 0.00010320076713303467, Model learning rate 0.00010320076398784295\n",
      "367/391 [===========================>..] - ETA: 1s - loss: 0.7508 - acc: 0.7901\n",
      " Optimizer iteration 3495, batch 367\n",
      "\n",
      " Learning rate 0.00010271116014312292, Model learning rate 0.00010271115752402693\n",
      "368/391 [===========================>..] - ETA: 1s - loss: 0.7511 - acc: 0.7900\n",
      " Optimizer iteration 3496, batch 368\n",
      "\n",
      " Learning rate 0.00010222258433870341, Model learning rate 0.00010222258424619213\n",
      "\n",
      " Optimizer iteration 3497, batch 369\n",
      "\n",
      " Learning rate 0.00010173504098790188, Model learning rate 0.00010173504415433854\n",
      "370/391 [===========================>..] - ETA: 1s - loss: 0.7507 - acc: 0.7902\n",
      " Optimizer iteration 3498, batch 370\n",
      "\n",
      " Learning rate 0.00010124853135616475, Model learning rate 0.00010124852997250855\n",
      "\n",
      " Optimizer iteration 3499, batch 371\n",
      "\n",
      " Learning rate 0.00010076305670625507, Model learning rate 0.00010076305625261739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372/391 [===========================>..] - ETA: 1s - loss: 0.7503 - acc: 0.7902\n",
      " Optimizer iteration 3500, batch 372\n",
      "\n",
      " Learning rate 0.00010027861829824952, Model learning rate 0.00010027861571870744\n",
      "373/391 [===========================>..] - ETA: 1s - loss: 0.7501 - acc: 0.7902\n",
      " Optimizer iteration 3501, batch 373\n",
      "\n",
      " Learning rate 9.9795217389535e-05, Model learning rate 9.979521564673632e-05\n",
      "374/391 [===========================>..] - ETA: 1s - loss: 0.7504 - acc: 0.7901\n",
      " Optimizer iteration 3502, batch 374\n",
      "\n",
      " Learning rate 9.931285523480604e-05, Model learning rate 9.931285603670403e-05\n",
      "\n",
      " Optimizer iteration 3503, batch 375\n",
      "\n",
      " Learning rate 9.883153308606035e-05, Model learning rate 9.883152961265296e-05\n",
      "376/391 [===========================>..] - ETA: 1s - loss: 0.7500 - acc: 0.7901\n",
      " Optimizer iteration 3504, batch 376\n",
      "\n",
      " Learning rate 9.835125219259695e-05, Model learning rate 9.835125092649832e-05\n",
      "377/391 [===========================>..] - ETA: 1s - loss: 0.7501 - acc: 0.7901\n",
      " Optimizer iteration 3505, batch 377\n",
      "\n",
      " Learning rate 9.787201380101158e-05, Model learning rate 9.787201270228252e-05\n",
      "378/391 [============================>.] - ETA: 1s - loss: 0.7501 - acc: 0.7902\n",
      " Optimizer iteration 3506, batch 378\n",
      "\n",
      " Learning rate 9.739381915519457e-05, Model learning rate 9.739382221596316e-05\n",
      "379/391 [============================>.] - ETA: 0s - loss: 0.7501 - acc: 0.7902\n",
      " Optimizer iteration 3507, batch 379\n",
      "\n",
      " Learning rate 9.691666949632683e-05, Model learning rate 9.691667219158262e-05\n",
      "380/391 [============================>.] - ETA: 0s - loss: 0.7498 - acc: 0.7903\n",
      " Optimizer iteration 3508, batch 380\n",
      "\n",
      " Learning rate 9.644056606287727e-05, Model learning rate 9.644056262914091e-05\n",
      "381/391 [============================>.] - ETA: 0s - loss: 0.7496 - acc: 0.7904\n",
      " Optimizer iteration 3509, batch 381\n",
      "\n",
      " Learning rate 9.596551009059884e-05, Model learning rate 9.596550808055326e-05\n",
      "382/391 [============================>.] - ETA: 0s - loss: 0.7497 - acc: 0.7903\n",
      " Optimizer iteration 3510, batch 382\n",
      "\n",
      " Learning rate 9.549150281252633e-05, Model learning rate 9.549150126986206e-05\n",
      "383/391 [============================>.] - ETA: 0s - loss: 0.7496 - acc: 0.7903\n",
      " Optimizer iteration 3511, batch 383\n",
      "\n",
      " Learning rate 9.501854545897203e-05, Model learning rate 9.501854219706729e-05\n",
      "384/391 [============================>.] - ETA: 0s - loss: 0.7496 - acc: 0.7903\n",
      " Optimizer iteration 3512, batch 384\n",
      "\n",
      " Learning rate 9.454663925752316e-05, Model learning rate 9.454663813812658e-05\n",
      "385/391 [============================>.] - ETA: 0s - loss: 0.7495 - acc: 0.7904\n",
      " Optimizer iteration 3513, batch 385\n",
      "\n",
      " Learning rate 9.407578543303913e-05, Model learning rate 9.407578181708232e-05\n",
      "386/391 [============================>.] - ETA: 0s - loss: 0.7492 - acc: 0.7906\n",
      " Optimizer iteration 3514, batch 386\n",
      "\n",
      " Learning rate 9.360598520764712e-05, Model learning rate 9.360598778584972e-05\n",
      "387/391 [============================>.] - ETA: 0s - loss: 0.7490 - acc: 0.7907\n",
      " Optimizer iteration 3515, batch 387\n",
      "\n",
      " Learning rate 9.313723980074018e-05, Model learning rate 9.313724149251357e-05\n",
      "388/391 [============================>.] - ETA: 0s - loss: 0.7491 - acc: 0.7905\n",
      " Optimizer iteration 3516, batch 388\n",
      "\n",
      " Learning rate 9.266955042897358e-05, Model learning rate 9.266955021303147e-05\n",
      "389/391 [============================>.] - ETA: 0s - loss: 0.7489 - acc: 0.7905\n",
      " Optimizer iteration 3517, batch 389\n",
      "\n",
      " Learning rate 9.220291830626082e-05, Model learning rate 9.220292122336105e-05\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.7493 - acc: 0.7904\n",
      " Optimizer iteration 3518, batch 390\n",
      "\n",
      " Learning rate 9.173734464377204e-05, Model learning rate 9.173734724754468e-05\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.7493 - acc: 0.7904 - val_loss: 0.7866 - val_acc: 0.7733\n",
      "\n",
      "Epoch 00009: saving model to /home/ubuntu/Projects/hybrid-ensemble/model/run_200/cifar10_ResNet20v1_model-0009.h5\n",
      "Epoch 10/10\n",
      "\n",
      " Optimizer iteration 3519, batch 0\n",
      "\n",
      " Learning rate 9.127283064992997e-05, Model learning rate 9.127282828558236e-05\n",
      "  1/391 [..............................] - ETA: 16s - loss: 0.8102 - acc: 0.7188\n",
      " Optimizer iteration 3520, batch 1\n",
      "\n",
      " Learning rate 9.080937753040646e-05, Model learning rate 9.080937888938934e-05\n",
      "  2/391 [..............................] - ETA: 18s - loss: 0.8287 - acc: 0.7422\n",
      " Optimizer iteration 3521, batch 2\n",
      "\n",
      " Learning rate 9.034698648812046e-05, Model learning rate 9.034698450705037e-05\n",
      "\n",
      " Optimizer iteration 3522, batch 3\n",
      "\n",
      " Learning rate 8.988565872323362e-05, Model learning rate 8.988565969048068e-05\n",
      "  4/391 [..............................] - ETA: 18s - loss: 0.7481 - acc: 0.7734\n",
      " Optimizer iteration 3523, batch 4\n",
      "\n",
      " Learning rate 8.942539543314798e-05, Model learning rate 8.942539716372266e-05\n",
      "  5/391 [..............................] - ETA: 18s - loss: 0.7276 - acc: 0.7906\n",
      " Optimizer iteration 3524, batch 5\n",
      "\n",
      " Learning rate 8.896619781250309e-05, Model learning rate 8.896619692677632e-05\n",
      "  6/391 [..............................] - ETA: 19s - loss: 0.7214 - acc: 0.7956\n",
      " Optimizer iteration 3525, batch 6\n",
      "\n",
      " Learning rate 8.850806705317183e-05, Model learning rate 8.850806625559926e-05\n",
      "\n",
      " Optimizer iteration 3526, batch 7\n",
      "\n",
      " Learning rate 8.805100434425845e-05, Model learning rate 8.805100515019149e-05\n",
      "  8/391 [..............................] - ETA: 19s - loss: 0.7190 - acc: 0.8008\n",
      " Optimizer iteration 3527, batch 8\n",
      "\n",
      " Learning rate 8.75950108720951e-05, Model learning rate 8.7595013610553e-05\n",
      "\n",
      " Optimizer iteration 3528, batch 9\n",
      "\n",
      " Learning rate 8.714008782023796e-05, Model learning rate 8.714008436072618e-05\n",
      " 10/391 [..............................] - ETA: 18s - loss: 0.7172 - acc: 0.7977\n",
      " Optimizer iteration 3529, batch 10\n",
      "\n",
      " Learning rate 8.668623636946566e-05, Model learning rate 8.668623922858387e-05\n",
      " 11/391 [..............................] - ETA: 19s - loss: 0.7174 - acc: 0.7983\n",
      " Optimizer iteration 3530, batch 11\n",
      "\n",
      " Learning rate 8.623345769777513e-05, Model learning rate 8.623345638625324e-05\n",
      " 12/391 [..............................] - ETA: 19s - loss: 0.7220 - acc: 0.7969\n",
      " Optimizer iteration 3531, batch 12\n",
      "\n",
      " Learning rate 8.578175298037872e-05, Model learning rate 8.57817503856495e-05\n",
      "\n",
      " Optimizer iteration 3532, batch 13\n",
      "\n",
      " Learning rate 8.533112338970156e-05, Model learning rate 8.533112122677267e-05\n",
      " 14/391 [>.............................] - ETA: 18s - loss: 0.7270 - acc: 0.7930\n",
      " Optimizer iteration 3533, batch 14\n",
      "\n",
      " Learning rate 8.488157009537794e-05, Model learning rate 8.488156890962273e-05\n",
      " 15/391 [>.............................] - ETA: 18s - loss: 0.7357 - acc: 0.7896\n",
      " Optimizer iteration 3534, batch 15\n",
      "\n",
      " Learning rate 8.443309426424861e-05, Model learning rate 8.443309343419969e-05\n",
      "\n",
      " Optimizer iteration 3535, batch 16\n",
      "\n",
      " Learning rate 8.398569706035791e-05, Model learning rate 8.398569480050355e-05\n",
      " 17/391 [>.............................] - ETA: 18s - loss: 0.7292 - acc: 0.7946\n",
      " Optimizer iteration 3536, batch 17\n",
      "\n",
      " Learning rate 8.353937964495028e-05, Model learning rate 8.353938028449193e-05\n",
      "\n",
      " Optimizer iteration 3537, batch 18\n",
      "\n",
      " Learning rate 8.309414317646769e-05, Model learning rate 8.30941426102072e-05\n",
      " 19/391 [>.............................] - ETA: 18s - loss: 0.7320 - acc: 0.7915\n",
      " Optimizer iteration 3538, batch 19\n",
      "\n",
      " Learning rate 8.264998881054659e-05, Model learning rate 8.264998905360699e-05\n",
      " 20/391 [>.............................] - ETA: 19s - loss: 0.7282 - acc: 0.7945\n",
      " Optimizer iteration 3539, batch 20\n",
      "\n",
      " Learning rate 8.220691770001421e-05, Model learning rate 8.220691961469129e-05\n",
      " 21/391 [>.............................] - ETA: 19s - loss: 0.7211 - acc: 0.7965\n",
      " Optimizer iteration 3540, batch 21\n",
      "\n",
      " Learning rate 8.176493099488664e-05, Model learning rate 8.17649342934601e-05\n",
      " 22/391 [>.............................] - ETA: 19s - loss: 0.7181 - acc: 0.7976\n",
      " Optimizer iteration 3541, batch 22\n",
      "\n",
      " Learning rate 8.132402984236531e-05, Model learning rate 8.132403308991343e-05\n",
      " 23/391 [>.............................] - ETA: 19s - loss: 0.7140 - acc: 0.7989\n",
      " Optimizer iteration 3542, batch 23\n",
      "\n",
      " Learning rate 8.088421538683377e-05, Model learning rate 8.088421600405127e-05\n",
      " 24/391 [>.............................] - ETA: 20s - loss: 0.7185 - acc: 0.7979\n",
      " Optimizer iteration 3543, batch 24\n",
      "\n",
      " Learning rate 8.04454887698553e-05, Model learning rate 8.044549031183124e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25/391 [>.............................] - ETA: 21s - loss: 0.7155 - acc: 0.7981\n",
      " Optimizer iteration 3544, batch 25\n",
      "\n",
      " Learning rate 8.000785113016939e-05, Model learning rate 8.000784873729572e-05\n",
      " 26/391 [>.............................] - ETA: 21s - loss: 0.7199 - acc: 0.7975\n",
      " Optimizer iteration 3545, batch 26\n",
      "\n",
      " Learning rate 7.957130360368897e-05, Model learning rate 7.957130583235994e-05\n",
      " 27/391 [=>............................] - ETA: 21s - loss: 0.7189 - acc: 0.7980\n",
      " Optimizer iteration 3546, batch 27\n",
      "\n",
      " Learning rate 7.913584732349787e-05, Model learning rate 7.913584704510868e-05\n",
      " 28/391 [=>............................] - ETA: 21s - loss: 0.7155 - acc: 0.7994\n",
      " Optimizer iteration 3547, batch 28\n",
      "\n",
      " Learning rate 7.870148341984713e-05, Model learning rate 7.870148692745715e-05\n",
      " 29/391 [=>............................] - ETA: 22s - loss: 0.7233 - acc: 0.7974\n",
      " Optimizer iteration 3548, batch 29\n",
      "\n",
      " Learning rate 7.826821302015275e-05, Model learning rate 7.826821092749014e-05\n",
      " 30/391 [=>............................] - ETA: 22s - loss: 0.7211 - acc: 0.7992\n",
      " Optimizer iteration 3549, batch 30\n",
      "\n",
      " Learning rate 7.783603724899258e-05, Model learning rate 7.783604087308049e-05\n",
      " 31/391 [=>............................] - ETA: 21s - loss: 0.7199 - acc: 0.7991\n",
      " Optimizer iteration 3550, batch 31\n",
      "\n",
      " Learning rate 7.74049572281027e-05, Model learning rate 7.740495493635535e-05\n",
      " 32/391 [=>............................] - ETA: 22s - loss: 0.7265 - acc: 0.7974\n",
      " Optimizer iteration 3551, batch 32\n",
      "\n",
      " Learning rate 7.697497407637566e-05, Model learning rate 7.697497494518757e-05\n",
      " 33/391 [=>............................] - ETA: 22s - loss: 0.7234 - acc: 0.7995\n",
      " Optimizer iteration 3552, batch 33\n",
      "\n",
      " Learning rate 7.654608890985709e-05, Model learning rate 7.654608634766191e-05\n",
      " 34/391 [=>............................] - ETA: 22s - loss: 0.7179 - acc: 0.8017\n",
      " Optimizer iteration 3553, batch 34\n",
      "\n",
      " Learning rate 7.611830284174221e-05, Model learning rate 7.611830369569361e-05\n",
      " 35/391 [=>............................] - ETA: 22s - loss: 0.7225 - acc: 0.8009\n",
      " Optimizer iteration 3554, batch 35\n",
      "\n",
      " Learning rate 7.569161698237404e-05, Model learning rate 7.569161971332505e-05\n",
      " 36/391 [=>............................] - ETA: 23s - loss: 0.7235 - acc: 0.8014\n",
      " Optimizer iteration 3555, batch 36\n",
      "\n",
      " Learning rate 7.526603243923958e-05, Model learning rate 7.526603440055624e-05\n",
      " 37/391 [=>............................] - ETA: 23s - loss: 0.7229 - acc: 0.8022\n",
      " Optimizer iteration 3556, batch 37\n",
      "\n",
      " Learning rate 7.484155031696727e-05, Model learning rate 7.484154775738716e-05\n",
      " 38/391 [=>............................] - ETA: 23s - loss: 0.7246 - acc: 0.8016\n",
      " Optimizer iteration 3557, batch 38\n",
      "\n",
      " Learning rate 7.441817171732457e-05, Model learning rate 7.441817433573306e-05\n",
      " 39/391 [=>............................] - ETA: 23s - loss: 0.7212 - acc: 0.8033\n",
      " Optimizer iteration 3558, batch 39\n",
      "\n",
      " Learning rate 7.399589773921412e-05, Model learning rate 7.399589958367869e-05\n",
      " 40/391 [==>...........................] - ETA: 23s - loss: 0.7202 - acc: 0.8035\n",
      " Optimizer iteration 3559, batch 40\n",
      "\n",
      " Learning rate 7.357472947867188e-05, Model learning rate 7.357473077718168e-05\n",
      " 41/391 [==>...........................] - ETA: 23s - loss: 0.7188 - acc: 0.8035\n",
      " Optimizer iteration 3560, batch 41\n",
      "\n",
      " Learning rate 7.315466802886401e-05, Model learning rate 7.315466791624203e-05\n",
      " 42/391 [==>...........................] - ETA: 23s - loss: 0.7179 - acc: 0.8041\n",
      " Optimizer iteration 3561, batch 42\n",
      "\n",
      " Learning rate 7.273571448008304e-05, Model learning rate 7.273571100085974e-05\n",
      "\n",
      " Optimizer iteration 3562, batch 43\n",
      "\n",
      " Learning rate 7.23178699197467e-05, Model learning rate 7.231786730699241e-05\n",
      " 44/391 [==>...........................] - ETA: 23s - loss: 0.7141 - acc: 0.8047\n",
      " Optimizer iteration 3563, batch 44\n",
      "\n",
      " Learning rate 7.190113543239407e-05, Model learning rate 7.190113683464006e-05\n",
      " 45/391 [==>...........................] - ETA: 23s - loss: 0.7139 - acc: 0.8047\n",
      " Optimizer iteration 3564, batch 45\n",
      "\n",
      " Learning rate 7.148551209968279e-05, Model learning rate 7.148551230784506e-05\n",
      "\n",
      " Optimizer iteration 3565, batch 46\n",
      "\n",
      " Learning rate 7.107100100038672e-05, Model learning rate 7.107100100256503e-05\n",
      " 47/391 [==>...........................] - ETA: 23s - loss: 0.7148 - acc: 0.8044\n",
      " Optimizer iteration 3566, batch 47\n",
      "\n",
      " Learning rate 7.06576032103926e-05, Model learning rate 7.065760291879997e-05\n",
      " 48/391 [==>...........................] - ETA: 23s - loss: 0.7139 - acc: 0.8047\n",
      " Optimizer iteration 3567, batch 48\n",
      "\n",
      " Learning rate 7.024531980269744e-05, Model learning rate 7.024531805654988e-05\n",
      " 49/391 [==>...........................] - ETA: 23s - loss: 0.7135 - acc: 0.8047\n",
      " Optimizer iteration 3568, batch 49\n",
      "\n",
      " Learning rate 6.983415184740616e-05, Model learning rate 6.983415369177237e-05\n",
      "\n",
      " Optimizer iteration 3569, batch 50\n",
      "\n",
      " Learning rate 6.942410041172836e-05, Model learning rate 6.942410254850984e-05\n",
      " 51/391 [==>...........................] - ETA: 23s - loss: 0.7127 - acc: 0.8041\n",
      " Optimizer iteration 3570, batch 51\n",
      "\n",
      " Learning rate 6.901516655997537e-05, Model learning rate 6.901516462676227e-05\n",
      " 52/391 [==>...........................] - ETA: 23s - loss: 0.7141 - acc: 0.8039\n",
      " Optimizer iteration 3571, batch 52\n",
      "\n",
      " Learning rate 6.860735135355811e-05, Model learning rate 6.86073544784449e-05\n",
      " 53/391 [===>..........................] - ETA: 23s - loss: 0.7127 - acc: 0.8042\n",
      " Optimizer iteration 3572, batch 53\n",
      "\n",
      " Learning rate 6.820065585098378e-05, Model learning rate 6.820065755164251e-05\n",
      "\n",
      " Optimizer iteration 3573, batch 54\n",
      "\n",
      " Learning rate 6.779508110785331e-05, Model learning rate 6.77950811223127e-05\n",
      " 55/391 [===>..........................] - ETA: 23s - loss: 0.7192 - acc: 0.8030\n",
      " Optimizer iteration 3574, batch 55\n",
      "\n",
      " Learning rate 6.739062817685892e-05, Model learning rate 6.739062519045547e-05\n",
      " 56/391 [===>..........................] - ETA: 23s - loss: 0.7194 - acc: 0.8029\n",
      " Optimizer iteration 3575, batch 56\n",
      "\n",
      " Learning rate 6.698729810778065e-05, Model learning rate 6.698729703202844e-05\n",
      " 57/391 [===>..........................] - ETA: 23s - loss: 0.7222 - acc: 0.8019\n",
      " Optimizer iteration 3576, batch 57\n",
      "\n",
      " Learning rate 6.658509194748463e-05, Model learning rate 6.658508937107399e-05\n",
      " 58/391 [===>..........................] - ETA: 23s - loss: 0.7257 - acc: 0.8005\n",
      " Optimizer iteration 3577, batch 58\n",
      "\n",
      " Learning rate 6.618401073991936e-05, Model learning rate 6.618400948354974e-05\n",
      " 59/391 [===>..........................] - ETA: 23s - loss: 0.7239 - acc: 0.8012\n",
      " Optimizer iteration 3578, batch 59\n",
      "\n",
      " Learning rate 6.57840555261136e-05, Model learning rate 6.57840573694557e-05\n",
      " 60/391 [===>..........................] - ETA: 23s - loss: 0.7275 - acc: 0.7995\n",
      " Optimizer iteration 3579, batch 60\n",
      "\n",
      " Learning rate 6.538522734417357e-05, Model learning rate 6.538522575283423e-05\n",
      " 61/391 [===>..........................] - ETA: 23s - loss: 0.7273 - acc: 0.7997\n",
      " Optimizer iteration 3580, batch 61\n",
      "\n",
      " Learning rate 6.498752722928042e-05, Model learning rate 6.498752918560058e-05\n",
      " 62/391 [===>..........................] - ETA: 23s - loss: 0.7265 - acc: 0.7998\n",
      " Optimizer iteration 3581, batch 62\n",
      "\n",
      " Learning rate 6.459095621368682e-05, Model learning rate 6.459095311583951e-05\n",
      " 63/391 [===>..........................] - ETA: 23s - loss: 0.7245 - acc: 0.8003\n",
      " Optimizer iteration 3582, batch 63\n",
      "\n",
      " Learning rate 6.419551532671541e-05, Model learning rate 6.419551209546626e-05\n",
      " 64/391 [===>..........................] - ETA: 23s - loss: 0.7253 - acc: 0.7999\n",
      " Optimizer iteration 3583, batch 64\n",
      "\n",
      " Learning rate 6.380120559475506e-05, Model learning rate 6.380120612448081e-05\n",
      " 65/391 [===>..........................] - ETA: 23s - loss: 0.7259 - acc: 0.7994\n",
      " Optimizer iteration 3584, batch 65\n",
      "\n",
      " Learning rate 6.340802804125873e-05, Model learning rate 6.340802792692557e-05\n",
      "\n",
      " Optimizer iteration 3585, batch 66\n",
      "\n",
      " Learning rate 6.301598368674105e-05, Model learning rate 6.301598477875814e-05\n",
      " 67/391 [====>.........................] - ETA: 22s - loss: 0.7263 - acc: 0.7992\n",
      " Optimizer iteration 3586, batch 67\n",
      "\n",
      " Learning rate 6.262507354877494e-05, Model learning rate 6.262507667997852e-05\n",
      " 68/391 [====>.........................] - ETA: 23s - loss: 0.7275 - acc: 0.7984\n",
      " Optimizer iteration 3587, batch 68\n",
      "\n",
      " Learning rate 6.223529864198984e-05, Model learning rate 6.22352963546291e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 69/391 [====>.........................] - ETA: 23s - loss: 0.7270 - acc: 0.7986\n",
      " Optimizer iteration 3588, batch 69\n",
      "\n",
      " Learning rate 6.184665997806832e-05, Model learning rate 6.18466583546251e-05\n",
      " 70/391 [====>.........................] - ETA: 22s - loss: 0.7263 - acc: 0.7989\n",
      " Optimizer iteration 3589, batch 70\n",
      "\n",
      " Learning rate 6.145915856574363e-05, Model learning rate 6.145915540400892e-05\n",
      " 71/391 [====>.........................] - ETA: 22s - loss: 0.7260 - acc: 0.7992\n",
      " Optimizer iteration 3590, batch 71\n",
      "\n",
      " Learning rate 6.10727954107977e-05, Model learning rate 6.107279477873817e-05\n",
      " 72/391 [====>.........................] - ETA: 22s - loss: 0.7266 - acc: 0.7990\n",
      " Optimizer iteration 3591, batch 72\n",
      "\n",
      " Learning rate 6.0687571516057803e-05, Model learning rate 6.0687572840834036e-05\n",
      " 73/391 [====>.........................] - ETA: 22s - loss: 0.7297 - acc: 0.7977\n",
      " Optimizer iteration 3592, batch 73\n",
      "\n",
      " Learning rate 6.030348788139406e-05, Model learning rate 6.030348959029652e-05\n",
      "\n",
      " Optimizer iteration 3593, batch 74\n",
      "\n",
      " Learning rate 5.9920545503717226e-05, Model learning rate 5.992054502712563e-05\n",
      " 75/391 [====>.........................] - ETA: 22s - loss: 0.7321 - acc: 0.7970\n",
      " Optimizer iteration 3594, batch 75\n",
      "\n",
      " Learning rate 5.9538745376975736e-05, Model learning rate 5.9538746427278966e-05\n",
      " 76/391 [====>.........................] - ETA: 22s - loss: 0.7320 - acc: 0.7967\n",
      " Optimizer iteration 3595, batch 76\n",
      "\n",
      " Learning rate 5.9158088492153036e-05, Model learning rate 5.915809015277773e-05\n",
      " 77/391 [====>.........................] - ETA: 22s - loss: 0.7319 - acc: 0.7969\n",
      " Optimizer iteration 3596, batch 77\n",
      "\n",
      " Learning rate 5.8778575837265754e-05, Model learning rate 5.8778576203621924e-05\n",
      " 78/391 [====>.........................] - ETA: 22s - loss: 0.7309 - acc: 0.7966\n",
      " Optimizer iteration 3597, batch 78\n",
      "\n",
      " Learning rate 5.84002083973601e-05, Model learning rate 5.840020821779035e-05\n",
      " 79/391 [=====>........................] - ETA: 22s - loss: 0.7311 - acc: 0.7967\n",
      " Optimizer iteration 3598, batch 79\n",
      "\n",
      " Learning rate 5.8022987154510154e-05, Model learning rate 5.802298619528301e-05\n",
      " 80/391 [=====>........................] - ETA: 22s - loss: 0.7316 - acc: 0.7964\n",
      " Optimizer iteration 3599, batch 80\n",
      "\n",
      " Learning rate 5.7646913087814725e-05, Model learning rate 5.764691377407871e-05\n",
      " 81/391 [=====>........................] - ETA: 22s - loss: 0.7325 - acc: 0.7959\n",
      " Optimizer iteration 3600, batch 81\n",
      "\n",
      " Learning rate 5.72719871733951e-05, Model learning rate 5.727198731619865e-05\n",
      "\n",
      " Optimizer iteration 3601, batch 82\n",
      "\n",
      " Learning rate 5.6898210384392636e-05, Model learning rate 5.689821045962162e-05\n",
      " 83/391 [=====>........................] - ETA: 22s - loss: 0.7325 - acc: 0.7957\n",
      " Optimizer iteration 3602, batch 83\n",
      "\n",
      " Learning rate 5.6525583690966056e-05, Model learning rate 5.652558320434764e-05\n",
      " 84/391 [=====>........................] - ETA: 22s - loss: 0.7326 - acc: 0.7957\n",
      " Optimizer iteration 3603, batch 84\n",
      "\n",
      " Learning rate 5.6154108060288756e-05, Model learning rate 5.6154109188355505e-05\n",
      " 85/391 [=====>........................] - ETA: 22s - loss: 0.7341 - acc: 0.7949\n",
      " Optimizer iteration 3604, batch 85\n",
      "\n",
      " Learning rate 5.578378445654664e-05, Model learning rate 5.578378477366641e-05\n",
      " 86/391 [=====>........................] - ETA: 22s - loss: 0.7340 - acc: 0.7946\n",
      " Optimizer iteration 3605, batch 86\n",
      "\n",
      " Learning rate 5.541461384093549e-05, Model learning rate 5.5414613598259166e-05\n",
      " 87/391 [=====>........................] - ETA: 21s - loss: 0.7349 - acc: 0.7939\n",
      " Optimizer iteration 3606, batch 87\n",
      "\n",
      " Learning rate 5.5046597171658106e-05, Model learning rate 5.504659566213377e-05\n",
      " 88/391 [=====>........................] - ETA: 22s - loss: 0.7347 - acc: 0.7939\n",
      " Optimizer iteration 3607, batch 88\n",
      "\n",
      " Learning rate 5.467973540392274e-05, Model learning rate 5.4679734603269026e-05\n",
      " 89/391 [=====>........................] - ETA: 22s - loss: 0.7347 - acc: 0.7938\n",
      " Optimizer iteration 3608, batch 89\n",
      "\n",
      " Learning rate 5.4314029489939464e-05, Model learning rate 5.431403042166494e-05\n",
      " 90/391 [=====>........................] - ETA: 21s - loss: 0.7364 - acc: 0.7930\n",
      " Optimizer iteration 3609, batch 90\n",
      "\n",
      " Learning rate 5.394948037891867e-05, Model learning rate 5.39494794793427e-05\n",
      " 91/391 [=====>........................] - ETA: 21s - loss: 0.7355 - acc: 0.7933\n",
      " Optimizer iteration 3610, batch 91\n",
      "\n",
      " Learning rate 5.358608901706802e-05, Model learning rate 5.358608905225992e-05\n",
      " 92/391 [======>.......................] - ETA: 21s - loss: 0.7358 - acc: 0.7932\n",
      " Optimizer iteration 3611, batch 92\n",
      "\n",
      " Learning rate 5.3223856347590084e-05, Model learning rate 5.32238555024378e-05\n",
      " 93/391 [======>.......................] - ETA: 21s - loss: 0.7372 - acc: 0.7928\n",
      " Optimizer iteration 3612, batch 93\n",
      "\n",
      " Learning rate 5.286278331068017e-05, Model learning rate 5.286278246785514e-05\n",
      "\n",
      " Optimizer iteration 3613, batch 94\n",
      "\n",
      " Learning rate 5.250287084352373e-05, Model learning rate 5.250286994851194e-05\n",
      " 95/391 [======>.......................] - ETA: 21s - loss: 0.7378 - acc: 0.7924\n",
      " Optimizer iteration 3614, batch 95\n",
      "\n",
      " Learning rate 5.214411988029355e-05, Model learning rate 5.2144121582387015e-05\n",
      " 96/391 [======>.......................] - ETA: 21s - loss: 0.7353 - acc: 0.7934\n",
      " Optimizer iteration 3615, batch 96\n",
      "\n",
      " Learning rate 5.1786531352148115e-05, Model learning rate 5.178653009352274e-05\n",
      " 97/391 [======>.......................] - ETA: 21s - loss: 0.7346 - acc: 0.7939\n",
      " Optimizer iteration 3616, batch 97\n",
      "\n",
      " Learning rate 5.1430106187228485e-05, Model learning rate 5.1430106395855546e-05\n",
      " 98/391 [======>.......................] - ETA: 21s - loss: 0.7331 - acc: 0.7946\n",
      " Optimizer iteration 3617, batch 98\n",
      "\n",
      " Learning rate 5.107484531065604e-05, Model learning rate 5.107484685140662e-05\n",
      " 99/391 [======>.......................] - ETA: 21s - loss: 0.7320 - acc: 0.7950\n",
      " Optimizer iteration 3618, batch 99\n",
      "\n",
      " Learning rate 5.072074964453055e-05, Model learning rate 5.072075146017596e-05\n",
      "100/391 [======>.......................] - ETA: 21s - loss: 0.7315 - acc: 0.7951\n",
      " Optimizer iteration 3619, batch 100\n",
      "\n",
      " Learning rate 5.0367820107926957e-05, Model learning rate 5.036782022216357e-05\n",
      "101/391 [======>.......................] - ETA: 21s - loss: 0.7317 - acc: 0.7949\n",
      " Optimizer iteration 3620, batch 101\n",
      "\n",
      " Learning rate 5.0016057616893987e-05, Model learning rate 5.001605677534826e-05\n",
      "\n",
      " Optimizer iteration 3621, batch 102\n",
      "\n",
      " Learning rate 4.966546308445074e-05, Model learning rate 4.966546475770883e-05\n",
      "103/391 [======>.......................] - ETA: 21s - loss: 0.7308 - acc: 0.7948\n",
      " Optimizer iteration 3622, batch 103\n",
      "\n",
      " Learning rate 4.9316037420584935e-05, Model learning rate 4.9316036893287674e-05\n",
      "104/391 [======>.......................] - ETA: 21s - loss: 0.7306 - acc: 0.7947\n",
      " Optimizer iteration 3623, batch 104\n",
      "\n",
      " Learning rate 4.896778153225062e-05, Model learning rate 4.89677804580424e-05\n",
      "105/391 [=======>......................] - ETA: 21s - loss: 0.7311 - acc: 0.7946\n",
      " Optimizer iteration 3624, batch 105\n",
      "\n",
      " Learning rate 4.862069632336558e-05, Model learning rate 4.8620695451973006e-05\n",
      "\n",
      " Optimizer iteration 3625, batch 106\n",
      "\n",
      " Learning rate 4.827478269480895e-05, Model learning rate 4.82747818750795e-05\n",
      "107/391 [=======>......................] - ETA: 20s - loss: 0.7313 - acc: 0.7945\n",
      " Optimizer iteration 3626, batch 107\n",
      "\n",
      " Learning rate 4.793004154441877e-05, Model learning rate 4.793003972736187e-05\n",
      "108/391 [=======>......................] - ETA: 20s - loss: 0.7306 - acc: 0.7948\n",
      " Optimizer iteration 3627, batch 108\n",
      "\n",
      " Learning rate 4.758647376699032e-05, Model learning rate 4.758647264679894e-05\n",
      "109/391 [=======>......................] - ETA: 20s - loss: 0.7310 - acc: 0.7947\n",
      " Optimizer iteration 3628, batch 109\n",
      "\n",
      " Learning rate 4.7244080254272795e-05, Model learning rate 4.7244080633390695e-05\n",
      "110/391 [=======>......................] - ETA: 20s - loss: 0.7316 - acc: 0.7947\n",
      " Optimizer iteration 3629, batch 110\n",
      "\n",
      " Learning rate 4.690286189496795e-05, Model learning rate 4.690286368713714e-05\n",
      "\n",
      " Optimizer iteration 3630, batch 111\n",
      "\n",
      " Learning rate 4.65628195747273e-05, Model learning rate 4.656281817005947e-05\n",
      "112/391 [=======>......................] - ETA: 20s - loss: 0.7321 - acc: 0.7945\n",
      " Optimizer iteration 3631, batch 112\n",
      "\n",
      " Learning rate 4.6223954176149606e-05, Model learning rate 4.622395499609411e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/391 [=======>......................] - ETA: 20s - loss: 0.7323 - acc: 0.7948\n",
      " Optimizer iteration 3632, batch 113\n",
      "\n",
      " Learning rate 4.588626657877898e-05, Model learning rate 4.5886266889283434e-05\n",
      "114/391 [=======>......................] - ETA: 20s - loss: 0.7334 - acc: 0.7945\n",
      " Optimizer iteration 3633, batch 114\n",
      "\n",
      " Learning rate 4.5549757659102795e-05, Model learning rate 4.554975748760626e-05\n",
      "115/391 [=======>......................] - ETA: 20s - loss: 0.7320 - acc: 0.7951\n",
      " Optimizer iteration 3634, batch 115\n",
      "\n",
      " Learning rate 4.521442829054856e-05, Model learning rate 4.521442679106258e-05\n",
      "116/391 [=======>......................] - ETA: 20s - loss: 0.7322 - acc: 0.7950\n",
      " Optimizer iteration 3635, batch 116\n",
      "\n",
      " Learning rate 4.488027934348271e-05, Model learning rate 4.4880278437631205e-05\n",
      "117/391 [=======>......................] - ETA: 20s - loss: 0.7328 - acc: 0.7948\n",
      " Optimizer iteration 3636, batch 117\n",
      "\n",
      " Learning rate 4.4547311685207536e-05, Model learning rate 4.4547312427312136e-05\n",
      "118/391 [========>.....................] - ETA: 20s - loss: 0.7322 - acc: 0.7952\n",
      " Optimizer iteration 3637, batch 118\n",
      "\n",
      " Learning rate 4.4215526179959165e-05, Model learning rate 4.4215525122126564e-05\n",
      "119/391 [========>.....................] - ETA: 20s - loss: 0.7314 - acc: 0.7953\n",
      " Optimizer iteration 3638, batch 119\n",
      "\n",
      " Learning rate 4.388492368890568e-05, Model learning rate 4.3884923798032105e-05\n",
      "\n",
      " Optimizer iteration 3639, batch 120\n",
      "\n",
      " Learning rate 4.3555505070144276e-05, Model learning rate 4.355550481704995e-05\n",
      "121/391 [========>.....................] - ETA: 20s - loss: 0.7307 - acc: 0.7957\n",
      " Optimizer iteration 3640, batch 121\n",
      "\n",
      " Learning rate 4.322727117869951e-05, Model learning rate 4.322727181715891e-05\n",
      "122/391 [========>.....................] - ETA: 19s - loss: 0.7301 - acc: 0.7960\n",
      " Optimizer iteration 3641, batch 122\n",
      "\n",
      " Learning rate 4.2900222866521014e-05, Model learning rate 4.290022116038017e-05\n",
      "\n",
      " Optimizer iteration 3642, batch 123\n",
      "\n",
      " Learning rate 4.257436098248091e-05, Model learning rate 4.257436012267135e-05\n",
      "124/391 [========>.....................] - ETA: 19s - loss: 0.7307 - acc: 0.7961\n",
      " Optimizer iteration 3643, batch 124\n",
      "\n",
      " Learning rate 4.224968637237198e-05, Model learning rate 4.2249685066053644e-05\n",
      "125/391 [========>.....................] - ETA: 19s - loss: 0.7296 - acc: 0.7964\n",
      " Optimizer iteration 3644, batch 125\n",
      "\n",
      " Learning rate 4.192619987890556e-05, Model learning rate 4.1926199628505856e-05\n",
      "126/391 [========>.....................] - ETA: 19s - loss: 0.7293 - acc: 0.7966\n",
      " Optimizer iteration 3645, batch 126\n",
      "\n",
      " Learning rate 4.16039023417088e-05, Model learning rate 4.160390381002799e-05\n",
      "127/391 [========>.....................] - ETA: 19s - loss: 0.7294 - acc: 0.7964\n",
      " Optimizer iteration 3646, batch 127\n",
      "\n",
      " Learning rate 4.128279459732326e-05, Model learning rate 4.128279397264123e-05\n",
      "\n",
      " Optimizer iteration 3647, batch 128\n",
      "\n",
      " Learning rate 4.096287747920202e-05, Model learning rate 4.09628773923032e-05\n",
      "129/391 [========>.....................] - ETA: 19s - loss: 0.7283 - acc: 0.7968\n",
      " Optimizer iteration 3648, batch 129\n",
      "\n",
      " Learning rate 4.0644151817707866e-05, Model learning rate 4.0644150431035087e-05\n",
      "130/391 [========>.....................] - ETA: 19s - loss: 0.7282 - acc: 0.7969\n",
      " Optimizer iteration 3649, batch 130\n",
      "\n",
      " Learning rate 4.0326618440111316e-05, Model learning rate 4.03266167268157e-05\n",
      "131/391 [=========>....................] - ETA: 19s - loss: 0.7280 - acc: 0.7969\n",
      " Optimizer iteration 3650, batch 131\n",
      "\n",
      " Learning rate 4.001027817058789e-05, Model learning rate 4.001027991762385e-05\n",
      "\n",
      " Optimizer iteration 3651, batch 132\n",
      "\n",
      " Learning rate 3.9695131830216616e-05, Model learning rate 3.9695132727501914e-05\n",
      "133/391 [=========>....................] - ETA: 19s - loss: 0.7268 - acc: 0.7971\n",
      " Optimizer iteration 3652, batch 133\n",
      "\n",
      " Learning rate 3.938118023697762e-05, Model learning rate 3.9381178794428706e-05\n",
      "\n",
      " Optimizer iteration 3653, batch 134\n",
      "\n",
      " Learning rate 3.90684242057498e-05, Model learning rate 3.906842539436184e-05\n",
      "135/391 [=========>....................] - ETA: 19s - loss: 0.7262 - acc: 0.7973\n",
      " Optimizer iteration 3654, batch 135\n",
      "\n",
      " Learning rate 3.8756864548308845e-05, Model learning rate 3.87568652513437e-05\n",
      "\n",
      " Optimizer iteration 3655, batch 136\n",
      "\n",
      " Learning rate 3.844650207332562e-05, Model learning rate 3.844650200335309e-05\n",
      "137/391 [=========>....................] - ETA: 18s - loss: 0.7247 - acc: 0.7977\n",
      " Optimizer iteration 3656, batch 137\n",
      "\n",
      " Learning rate 3.8137337586363063e-05, Model learning rate 3.813733928836882e-05\n",
      "138/391 [=========>....................] - ETA: 18s - loss: 0.7237 - acc: 0.7982\n",
      " Optimizer iteration 3657, batch 138\n",
      "\n",
      " Learning rate 3.782937188987523e-05, Model learning rate 3.7829373468412086e-05\n",
      "139/391 [=========>....................] - ETA: 18s - loss: 0.7239 - acc: 0.7985\n",
      " Optimizer iteration 3658, batch 139\n",
      "\n",
      " Learning rate 3.7522605783204264e-05, Model learning rate 3.7522604543482885e-05\n",
      "\n",
      " Optimizer iteration 3659, batch 140\n",
      "\n",
      " Learning rate 3.7217040062578756e-05, Model learning rate 3.721703978953883e-05\n",
      "141/391 [=========>....................] - ETA: 18s - loss: 0.7233 - acc: 0.7984\n",
      " Optimizer iteration 3660, batch 141\n",
      "\n",
      " Learning rate 3.691267552111183e-05, Model learning rate 3.6912675568601117e-05\n",
      "\n",
      " Optimizer iteration 3661, batch 142\n",
      "\n",
      " Learning rate 3.660951294879855e-05, Model learning rate 3.660951188066974e-05\n",
      "143/391 [=========>....................] - ETA: 18s - loss: 0.7221 - acc: 0.7987\n",
      " Optimizer iteration 3662, batch 143\n",
      "\n",
      " Learning rate 3.6307553132514546e-05, Model learning rate 3.6307552363723516e-05\n",
      "\n",
      " Optimizer iteration 3663, batch 144\n",
      "\n",
      " Learning rate 3.6006796856013493e-05, Model learning rate 3.600679701776244e-05\n",
      "145/391 [==========>...................] - ETA: 18s - loss: 0.7206 - acc: 0.7991\n",
      " Optimizer iteration 3664, batch 145\n",
      "\n",
      " Learning rate 3.5707244899925164e-05, Model learning rate 3.5707245842786506e-05\n",
      "146/391 [==========>...................] - ETA: 18s - loss: 0.7212 - acc: 0.7990\n",
      " Optimizer iteration 3665, batch 146\n",
      "\n",
      " Learning rate 3.54088980417534e-05, Model learning rate 3.540889883879572e-05\n",
      "147/391 [==========>...................] - ETA: 18s - loss: 0.7213 - acc: 0.7988\n",
      " Optimizer iteration 3666, batch 147\n",
      "\n",
      " Learning rate 3.5111757055874326e-05, Model learning rate 3.5111756005790085e-05\n",
      "\n",
      " Optimizer iteration 3667, batch 148\n",
      "\n",
      " Learning rate 3.4815822713533954e-05, Model learning rate 3.48158209817484e-05\n",
      "149/391 [==========>...................] - ETA: 18s - loss: 0.7203 - acc: 0.7993\n",
      " Optimizer iteration 3668, batch 149\n",
      "\n",
      " Learning rate 3.4521095782846624e-05, Model learning rate 3.452109740464948e-05\n",
      "150/391 [==========>...................] - ETA: 18s - loss: 0.7198 - acc: 0.7995\n",
      " Optimizer iteration 3669, batch 150\n",
      "\n",
      " Learning rate 3.422757702879259e-05, Model learning rate 3.422757799853571e-05\n",
      "\n",
      " Optimizer iteration 3670, batch 151\n",
      "\n",
      " Learning rate 3.393526721321616e-05, Model learning rate 3.393526640138589e-05\n",
      "152/391 [==========>...................] - ETA: 17s - loss: 0.7191 - acc: 0.7997\n",
      " Optimizer iteration 3671, batch 152\n",
      "\n",
      " Learning rate 3.3644167094823985e-05, Model learning rate 3.364416625117883e-05\n",
      "153/391 [==========>...................] - ETA: 17s - loss: 0.7188 - acc: 0.7998\n",
      " Optimizer iteration 3672, batch 153\n",
      "\n",
      " Learning rate 3.335427742918262e-05, Model learning rate 3.3354277547914535e-05\n",
      "154/391 [==========>...................] - ETA: 17s - loss: 0.7186 - acc: 0.8000\n",
      " Optimizer iteration 3673, batch 154\n",
      "\n",
      " Learning rate 3.3065598968717137e-05, Model learning rate 3.3065600291593e-05\n",
      "155/391 [==========>...................] - ETA: 17s - loss: 0.7192 - acc: 0.7997\n",
      " Optimizer iteration 3674, batch 155\n",
      "\n",
      " Learning rate 3.277813246270872e-05, Model learning rate 3.277813084423542e-05\n",
      "156/391 [==========>...................] - ETA: 17s - loss: 0.7181 - acc: 0.8001\n",
      " Optimizer iteration 3675, batch 156\n",
      "\n",
      " Learning rate 3.249187865729264e-05, Model learning rate 3.2491880119778216e-05\n",
      "157/391 [===========>..................] - ETA: 17s - loss: 0.7178 - acc: 0.8001\n",
      " Optimizer iteration 3676, batch 157\n",
      "\n",
      " Learning rate 3.220683829545678e-05, Model learning rate 3.2206837204284966e-05\n",
      "\n",
      " Optimizer iteration 3677, batch 158\n",
      "\n",
      " Learning rate 3.192301211703952e-05, Model learning rate 3.192301301169209e-05\n",
      "159/391 [===========>..................] - ETA: 17s - loss: 0.7180 - acc: 0.8000\n",
      " Optimizer iteration 3678, batch 159\n",
      "\n",
      " Learning rate 3.164040085872755e-05, Model learning rate 3.164040026604198e-05\n",
      "160/391 [===========>..................] - ETA: 17s - loss: 0.7173 - acc: 0.8002\n",
      " Optimizer iteration 3679, batch 160\n",
      "\n",
      " Learning rate 3.1359005254054274e-05, Model learning rate 3.135900624329224e-05\n",
      "161/391 [===========>..................] - ETA: 17s - loss: 0.7170 - acc: 0.8003\n",
      " Optimizer iteration 3680, batch 161\n",
      "\n",
      " Learning rate 3.107882603339785e-05, Model learning rate 3.1078827305464074e-05\n",
      "162/391 [===========>..................] - ETA: 17s - loss: 0.7175 - acc: 0.8001\n",
      " Optimizer iteration 3681, batch 162\n",
      "\n",
      " Learning rate 3.079986392397899e-05, Model learning rate 3.0799863452557474e-05\n",
      "163/391 [===========>..................] - ETA: 17s - loss: 0.7180 - acc: 0.7999\n",
      " Optimizer iteration 3682, batch 163\n",
      "\n",
      " Learning rate 3.052211964985974e-05, Model learning rate 3.052211832255125e-05\n",
      "\n",
      " Optimizer iteration 3683, batch 164\n",
      "\n",
      " Learning rate 3.024559393194076e-05, Model learning rate 3.0245593734434806e-05\n",
      "165/391 [===========>..................] - ETA: 17s - loss: 0.7173 - acc: 0.8003\n",
      " Optimizer iteration 3684, batch 165\n",
      "\n",
      " Learning rate 2.9970287487960158e-05, Model learning rate 2.9970287869218737e-05\n",
      "\n",
      " Optimizer iteration 3685, batch 166\n",
      "\n",
      " Learning rate 2.9696201032491433e-05, Model learning rate 2.9696200726903044e-05\n",
      "167/391 [===========>..................] - ETA: 16s - loss: 0.7164 - acc: 0.8007\n",
      " Optimizer iteration 3686, batch 167\n",
      "\n",
      " Learning rate 2.942333527694113e-05, Model learning rate 2.9423335945466533e-05\n",
      "\n",
      " Optimizer iteration 3687, batch 168\n",
      "\n",
      " Learning rate 2.9151690929547726e-05, Model learning rate 2.9151691705919802e-05\n",
      "169/391 [===========>..................] - ETA: 16s - loss: 0.7169 - acc: 0.8008\n",
      " Optimizer iteration 3688, batch 169\n",
      "\n",
      " Learning rate 2.8881268695379436e-05, Model learning rate 2.888126800826285e-05\n",
      "170/391 [============>.................] - ETA: 16s - loss: 0.7164 - acc: 0.8010\n",
      " Optimizer iteration 3689, batch 170\n",
      "\n",
      " Learning rate 2.8612069276332253e-05, Model learning rate 2.8612068490474485e-05\n",
      "171/391 [============>.................] - ETA: 16s - loss: 0.7162 - acc: 0.8012\n",
      " Optimizer iteration 3690, batch 171\n",
      "\n",
      " Learning rate 2.834409337112842e-05, Model learning rate 2.8344093152554706e-05\n",
      "172/391 [============>.................] - ETA: 16s - loss: 0.7168 - acc: 0.8011\n",
      " Optimizer iteration 3691, batch 172\n",
      "\n",
      " Learning rate 2.807734167531456e-05, Model learning rate 2.8077341994503513e-05\n",
      "173/391 [============>.................] - ETA: 16s - loss: 0.7163 - acc: 0.8012\n",
      " Optimizer iteration 3692, batch 173\n",
      "\n",
      " Learning rate 2.78118148812595e-05, Model learning rate 2.7811815016320907e-05\n",
      "\n",
      " Optimizer iteration 3693, batch 174\n",
      "\n",
      " Learning rate 2.7547513678153e-05, Model learning rate 2.754751403699629e-05\n",
      "175/391 [============>.................] - ETA: 16s - loss: 0.7161 - acc: 0.8009\n",
      " Optimizer iteration 3694, batch 175\n",
      "\n",
      " Learning rate 2.7284438752003758e-05, Model learning rate 2.7284439056529664e-05\n",
      "\n",
      " Optimizer iteration 3695, batch 176\n",
      "\n",
      " Learning rate 2.70225907856374e-05, Model learning rate 2.7022590074921027e-05\n",
      "177/391 [============>.................] - ETA: 16s - loss: 0.7159 - acc: 0.8013\n",
      " Optimizer iteration 3696, batch 177\n",
      "\n",
      " Learning rate 2.6761970458695107e-05, Model learning rate 2.6761970730149187e-05\n",
      "178/391 [============>.................] - ETA: 16s - loss: 0.7157 - acc: 0.8014\n",
      " Optimizer iteration 3697, batch 178\n",
      "\n",
      " Learning rate 2.65025784476316e-05, Model learning rate 2.650257920322474e-05\n",
      "179/391 [============>.................] - ETA: 15s - loss: 0.7159 - acc: 0.8012\n",
      " Optimizer iteration 3698, batch 179\n",
      "\n",
      " Learning rate 2.6244415425713264e-05, Model learning rate 2.6244415494147688e-05\n",
      "\n",
      " Optimizer iteration 3699, batch 180\n",
      "\n",
      " Learning rate 2.598748206301682e-05, Model learning rate 2.5987481421907432e-05\n",
      "181/391 [============>.................] - ETA: 15s - loss: 0.7157 - acc: 0.8014\n",
      " Optimizer iteration 3700, batch 181\n",
      "\n",
      " Learning rate 2.573177902642726e-05, Model learning rate 2.5731778805493377e-05\n",
      "\n",
      " Optimizer iteration 3701, batch 182\n",
      "\n",
      " Learning rate 2.547730697963607e-05, Model learning rate 2.5477307644905522e-05\n",
      "183/391 [=============>................] - ETA: 15s - loss: 0.7140 - acc: 0.8020\n",
      " Optimizer iteration 3702, batch 183\n",
      "\n",
      " Learning rate 2.522406658313997e-05, Model learning rate 2.5224066121154465e-05\n",
      "184/391 [=============>................] - ETA: 15s - loss: 0.7141 - acc: 0.8020\n",
      " Optimizer iteration 3703, batch 184\n",
      "\n",
      " Learning rate 2.4972058494238337e-05, Model learning rate 2.497205787221901e-05\n",
      "185/391 [=============>................] - ETA: 15s - loss: 0.7140 - acc: 0.8020\n",
      " Optimizer iteration 3704, batch 185\n",
      "\n",
      " Learning rate 2.4721283367032387e-05, Model learning rate 2.472128289809916e-05\n",
      "186/391 [=============>................] - ETA: 15s - loss: 0.7141 - acc: 0.8020\n",
      " Optimizer iteration 3705, batch 186\n",
      "\n",
      " Learning rate 2.4471741852423235e-05, Model learning rate 2.4471741198794916e-05\n",
      "187/391 [=============>................] - ETA: 15s - loss: 0.7139 - acc: 0.8021\n",
      " Optimizer iteration 3706, batch 187\n",
      "\n",
      " Learning rate 2.422343459810966e-05, Model learning rate 2.422343459329568e-05\n",
      "188/391 [=============>................] - ETA: 15s - loss: 0.7140 - acc: 0.8020\n",
      " Optimizer iteration 3707, batch 188\n",
      "\n",
      " Learning rate 2.3976362248587293e-05, Model learning rate 2.397636308160145e-05\n",
      "189/391 [=============>................] - ETA: 15s - loss: 0.7137 - acc: 0.8020\n",
      " Optimizer iteration 3708, batch 189\n",
      "\n",
      " Learning rate 2.3730525445146145e-05, Model learning rate 2.3730524844722822e-05\n",
      "190/391 [=============>................] - ETA: 15s - loss: 0.7138 - acc: 0.8018\n",
      " Optimizer iteration 3709, batch 190\n",
      "\n",
      " Learning rate 2.348592482586942e-05, Model learning rate 2.348592533962801e-05\n",
      "191/391 [=============>................] - ETA: 15s - loss: 0.7141 - acc: 0.8018\n",
      " Optimizer iteration 3710, batch 191\n",
      "\n",
      " Learning rate 2.324256102563188e-05, Model learning rate 2.3242560928338207e-05\n",
      "192/391 [=============>................] - ETA: 15s - loss: 0.7137 - acc: 0.8019\n",
      " Optimizer iteration 3711, batch 192\n",
      "\n",
      " Learning rate 2.3000434676097803e-05, Model learning rate 2.300043524883222e-05\n",
      "193/391 [=============>................] - ETA: 15s - loss: 0.7137 - acc: 0.8019\n",
      " Optimizer iteration 3712, batch 193\n",
      "\n",
      " Learning rate 2.275954640571981e-05, Model learning rate 2.275954648212064e-05\n",
      "194/391 [=============>................] - ETA: 14s - loss: 0.7136 - acc: 0.8020\n",
      " Optimizer iteration 3713, batch 194\n",
      "\n",
      " Learning rate 2.2519896839737097e-05, Model learning rate 2.2519896447192878e-05\n",
      "195/391 [=============>................] - ETA: 14s - loss: 0.7133 - acc: 0.8021\n",
      " Optimizer iteration 3714, batch 195\n",
      "\n",
      " Learning rate 2.2281486600173206e-05, Model learning rate 2.2281486963038333e-05\n",
      "\n",
      " Optimizer iteration 3715, batch 196\n",
      "\n",
      " Learning rate 2.2044316305835478e-05, Model learning rate 2.2044316210667603e-05\n",
      "197/391 [==============>...............] - ETA: 14s - loss: 0.7142 - acc: 0.8019\n",
      " Optimizer iteration 3716, batch 197\n",
      "\n",
      " Learning rate 2.180838657231282e-05, Model learning rate 2.180838600907009e-05\n",
      "198/391 [==============>...............] - ETA: 14s - loss: 0.7140 - acc: 0.8021\n",
      " Optimizer iteration 3717, batch 198\n",
      "\n",
      " Learning rate 2.1573698011973954e-05, Model learning rate 2.15736981772352e-05\n",
      "\n",
      " Optimizer iteration 3718, batch 199\n",
      "\n",
      " Learning rate 2.134025123396638e-05, Model learning rate 2.134025089617353e-05\n",
      "200/391 [==============>...............] - ETA: 14s - loss: 0.7145 - acc: 0.8017\n",
      " Optimizer iteration 3719, batch 200\n",
      "\n",
      " Learning rate 2.1108046844214192e-05, Model learning rate 2.110804598487448e-05\n",
      "201/391 [==============>...............] - ETA: 14s - loss: 0.7139 - acc: 0.8019\n",
      " Optimizer iteration 3720, batch 201\n",
      "\n",
      " Learning rate 2.087708544541689e-05, Model learning rate 2.0877085262327455e-05\n",
      "202/391 [==============>...............] - ETA: 14s - loss: 0.7142 - acc: 0.8020\n",
      " Optimizer iteration 3721, batch 202\n",
      "\n",
      " Learning rate 2.0647367637047887e-05, Model learning rate 2.0647366909543052e-05\n",
      "\n",
      " Optimizer iteration 3722, batch 203\n",
      "\n",
      " Learning rate 2.041889401535252e-05, Model learning rate 2.041889456450008e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/391 [==============>...............] - ETA: 14s - loss: 0.7137 - acc: 0.8021\n",
      " Optimizer iteration 3723, batch 204\n",
      "\n",
      " Learning rate 2.019166517334703e-05, Model learning rate 2.0191664589219727e-05\n",
      "205/391 [==============>...............] - ETA: 14s - loss: 0.7135 - acc: 0.8021\n",
      " Optimizer iteration 3724, batch 205\n",
      "\n",
      " Learning rate 1.9965681700816584e-05, Model learning rate 1.9965682440670207e-05\n",
      "206/391 [==============>...............] - ETA: 14s - loss: 0.7139 - acc: 0.8019\n",
      " Optimizer iteration 3725, batch 206\n",
      "\n",
      " Learning rate 1.974094418431388e-05, Model learning rate 1.9740944480872713e-05\n",
      "\n",
      " Optimizer iteration 3726, batch 207\n",
      "\n",
      " Learning rate 1.9517453207157864e-05, Model learning rate 1.9517452528816648e-05\n",
      "208/391 [==============>...............] - ETA: 13s - loss: 0.7141 - acc: 0.8019\n",
      " Optimizer iteration 3727, batch 208\n",
      "\n",
      " Learning rate 1.929520934943191e-05, Model learning rate 1.929521022248082e-05\n",
      "209/391 [===============>..............] - ETA: 13s - loss: 0.7139 - acc: 0.8020\n",
      " Optimizer iteration 3728, batch 209\n",
      "\n",
      " Learning rate 1.9074213187982415e-05, Model learning rate 1.907421392388642e-05\n",
      "210/391 [===============>..............] - ETA: 13s - loss: 0.7133 - acc: 0.8023\n",
      " Optimizer iteration 3729, batch 210\n",
      "\n",
      " Learning rate 1.885446529641732e-05, Model learning rate 1.885446545202285e-05\n",
      "211/391 [===============>..............] - ETA: 13s - loss: 0.7132 - acc: 0.8022\n",
      " Optimizer iteration 3730, batch 211\n",
      "\n",
      " Learning rate 1.8635966245104663e-05, Model learning rate 1.863596662587952e-05\n",
      "212/391 [===============>..............] - ETA: 13s - loss: 0.7132 - acc: 0.8022\n",
      " Optimizer iteration 3731, batch 212\n",
      "\n",
      " Learning rate 1.841871660117095e-05, Model learning rate 1.8418717445456423e-05\n",
      "213/391 [===============>..............] - ETA: 13s - loss: 0.7134 - acc: 0.8020\n",
      " Optimizer iteration 3732, batch 213\n",
      "\n",
      " Learning rate 1.820271692849984e-05, Model learning rate 1.820271609176416e-05\n",
      "214/391 [===============>..............] - ETA: 13s - loss: 0.7132 - acc: 0.8021\n",
      " Optimizer iteration 3733, batch 214\n",
      "\n",
      " Learning rate 1.7987967787730542e-05, Model learning rate 1.798796802177094e-05\n",
      "\n",
      " Optimizer iteration 3734, batch 215\n",
      "\n",
      " Learning rate 1.7774469736256683e-05, Model learning rate 1.7774469597497955e-05\n",
      "216/391 [===============>..............] - ETA: 13s - loss: 0.7131 - acc: 0.8021\n",
      " Optimizer iteration 3735, batch 216\n",
      "\n",
      " Learning rate 1.7562223328224324e-05, Model learning rate 1.756222263793461e-05\n",
      "217/391 [===============>..............] - ETA: 13s - loss: 0.7130 - acc: 0.8021\n",
      " Optimizer iteration 3736, batch 217\n",
      "\n",
      " Learning rate 1.735122911453091e-05, Model learning rate 1.735122896207031e-05\n",
      "218/391 [===============>..............] - ETA: 13s - loss: 0.7138 - acc: 0.8018\n",
      " Optimizer iteration 3737, batch 218\n",
      "\n",
      " Learning rate 1.7141487642823806e-05, Model learning rate 1.7141486750915647e-05\n",
      "\n",
      " Optimizer iteration 3738, batch 219\n",
      "\n",
      " Learning rate 1.693299945749882e-05, Model learning rate 1.693299964244943e-05\n",
      "220/391 [===============>..............] - ETA: 12s - loss: 0.7138 - acc: 0.8017\n",
      " Optimizer iteration 3739, batch 220\n",
      "\n",
      " Learning rate 1.6725765099698698e-05, Model learning rate 1.672576581768226e-05\n",
      "221/391 [===============>..............] - ETA: 12s - loss: 0.7136 - acc: 0.8016\n",
      " Optimizer iteration 3740, batch 221\n",
      "\n",
      " Learning rate 1.651978510731189e-05, Model learning rate 1.651978527661413e-05\n",
      "\n",
      " Optimizer iteration 3741, batch 222\n",
      "\n",
      " Learning rate 1.6315060014971016e-05, Model learning rate 1.6315059838234447e-05\n",
      "223/391 [================>.............] - ETA: 12s - loss: 0.7134 - acc: 0.8018\n",
      " Optimizer iteration 3742, batch 223\n",
      "\n",
      " Learning rate 1.6111590354051464e-05, Model learning rate 1.611158950254321e-05\n",
      "\n",
      " Optimizer iteration 3743, batch 224\n",
      "\n",
      " Learning rate 1.5909376652670282e-05, Model learning rate 1.5909376088529825e-05\n",
      "225/391 [================>.............] - ETA: 12s - loss: 0.7139 - acc: 0.8016\n",
      " Optimizer iteration 3744, batch 225\n",
      "\n",
      " Learning rate 1.5708419435684463e-05, Model learning rate 1.570841959619429e-05\n",
      "226/391 [================>.............] - ETA: 12s - loss: 0.7140 - acc: 0.8016\n",
      " Optimizer iteration 3745, batch 226\n",
      "\n",
      " Learning rate 1.5508719224689714e-05, Model learning rate 1.5508720025536604e-05\n",
      "227/391 [================>.............] - ETA: 12s - loss: 0.7139 - acc: 0.8018\n",
      " Optimizer iteration 3746, batch 227\n",
      "\n",
      " Learning rate 1.5310276538019196e-05, Model learning rate 1.531027737655677e-05\n",
      "228/391 [================>.............] - ETA: 12s - loss: 0.7139 - acc: 0.8017\n",
      " Optimizer iteration 3747, batch 228\n",
      "\n",
      " Learning rate 1.5113091890741948e-05, Model learning rate 1.5113091649254784e-05\n",
      "229/391 [================>.............] - ETA: 12s - loss: 0.7138 - acc: 0.8018\n",
      " Optimizer iteration 3748, batch 229\n",
      "\n",
      " Learning rate 1.4917165794661846e-05, Model learning rate 1.4917165572114754e-05\n",
      "230/391 [================>.............] - ETA: 12s - loss: 0.7138 - acc: 0.8018\n",
      " Optimizer iteration 3749, batch 230\n",
      "\n",
      " Learning rate 1.4722498758316161e-05, Model learning rate 1.472249914513668e-05\n",
      "231/391 [================>.............] - ETA: 12s - loss: 0.7137 - acc: 0.8018\n",
      " Optimizer iteration 3750, batch 231\n",
      "\n",
      " Learning rate 1.4529091286973995e-05, Model learning rate 1.452909145882586e-05\n",
      "\n",
      " Optimizer iteration 3751, batch 232\n",
      "\n",
      " Learning rate 1.4336943882635345e-05, Model learning rate 1.4336944332171697e-05\n",
      "233/391 [================>.............] - ETA: 12s - loss: 0.7135 - acc: 0.8018\n",
      " Optimizer iteration 3752, batch 233\n",
      "\n",
      " Learning rate 1.414605704402966e-05, Model learning rate 1.414605685567949e-05\n",
      "234/391 [================>.............] - ETA: 11s - loss: 0.7134 - acc: 0.8017\n",
      " Optimizer iteration 3753, batch 234\n",
      "\n",
      " Learning rate 1.3956431266614278e-05, Model learning rate 1.3956430848338641e-05\n",
      "235/391 [=================>............] - ETA: 11s - loss: 0.7128 - acc: 0.8018\n",
      " Optimizer iteration 3754, batch 235\n",
      "\n",
      " Learning rate 1.3768067042573662e-05, Model learning rate 1.3768067219643854e-05\n",
      "236/391 [=================>............] - ETA: 11s - loss: 0.7129 - acc: 0.8018\n",
      " Optimizer iteration 3755, batch 236\n",
      "\n",
      " Learning rate 1.3580964860817779e-05, Model learning rate 1.3580965060100425e-05\n",
      "237/391 [=================>............] - ETA: 11s - loss: 0.7131 - acc: 0.8016\n",
      " Optimizer iteration 3756, batch 237\n",
      "\n",
      " Learning rate 1.3395125206980774e-05, Model learning rate 1.3395125279203057e-05\n",
      "\n",
      " Optimizer iteration 3757, batch 238\n",
      "\n",
      " Learning rate 1.3210548563419855e-05, Model learning rate 1.3210548786446452e-05\n",
      "239/391 [=================>............] - ETA: 11s - loss: 0.7128 - acc: 0.8017\n",
      " Optimizer iteration 3758, batch 239\n",
      "\n",
      " Learning rate 1.3027235409214189e-05, Model learning rate 1.302723558183061e-05\n",
      "\n",
      " Optimizer iteration 3759, batch 240\n",
      "\n",
      " Learning rate 1.2845186220163286e-05, Model learning rate 1.2845186574850231e-05\n",
      "241/391 [=================>............] - ETA: 11s - loss: 0.7125 - acc: 0.8020\n",
      " Optimizer iteration 3760, batch 241\n",
      "\n",
      " Learning rate 1.2664401468786114e-05, Model learning rate 1.2664401765505318e-05\n",
      "242/391 [=================>............] - ETA: 11s - loss: 0.7124 - acc: 0.8020\n",
      " Optimizer iteration 3761, batch 242\n",
      "\n",
      " Learning rate 1.2484881624319489e-05, Model learning rate 1.248488206329057e-05\n",
      "243/391 [=================>............] - ETA: 11s - loss: 0.7127 - acc: 0.8017\n",
      " Optimizer iteration 3762, batch 243\n",
      "\n",
      " Learning rate 1.2306627152717408e-05, Model learning rate 1.2306627468205988e-05\n",
      "\n",
      " Optimizer iteration 3763, batch 244\n",
      "\n",
      " Learning rate 1.2129638516649278e-05, Model learning rate 1.2129638889746275e-05\n",
      "245/391 [=================>............] - ETA: 11s - loss: 0.7129 - acc: 0.8015\n",
      " Optimizer iteration 3764, batch 245\n",
      "\n",
      " Learning rate 1.1953916175499068e-05, Model learning rate 1.1953916327911429e-05\n",
      "246/391 [=================>............] - ETA: 11s - loss: 0.7133 - acc: 0.8015\n",
      " Optimizer iteration 3765, batch 246\n",
      "\n",
      " Learning rate 1.1779460585363943e-05, Model learning rate 1.1779460692196153e-05\n",
      "\n",
      " Optimizer iteration 3766, batch 247\n",
      "\n",
      " Learning rate 1.1606272199053247e-05, Model learning rate 1.1606271982600447e-05\n",
      "248/391 [==================>...........] - ETA: 10s - loss: 0.7128 - acc: 0.8018\n",
      " Optimizer iteration 3767, batch 248\n",
      "\n",
      " Learning rate 1.1434351466087178e-05, Model learning rate 1.1434351108619012e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/391 [==================>...........] - ETA: 10s - loss: 0.7130 - acc: 0.8016\n",
      " Optimizer iteration 3768, batch 249\n",
      "\n",
      " Learning rate 1.1263698832695512e-05, Model learning rate 1.126369897974655e-05\n",
      "\n",
      " Optimizer iteration 3769, batch 250\n",
      "\n",
      " Learning rate 1.1094314741816935e-05, Model learning rate 1.109431468648836e-05\n",
      "251/391 [==================>...........] - ETA: 10s - loss: 0.7133 - acc: 0.8014\n",
      " Optimizer iteration 3770, batch 251\n",
      "\n",
      " Learning rate 1.0926199633097156e-05, Model learning rate 1.0926200047833845e-05\n",
      "252/391 [==================>...........] - ETA: 10s - loss: 0.7138 - acc: 0.8013\n",
      " Optimizer iteration 3771, batch 252\n",
      "\n",
      " Learning rate 1.0759353942888573e-05, Model learning rate 1.0759354154288303e-05\n",
      "253/391 [==================>...........] - ETA: 10s - loss: 0.7133 - acc: 0.8015\n",
      " Optimizer iteration 3772, batch 253\n",
      "\n",
      " Learning rate 1.0593778104248441e-05, Model learning rate 1.0593777915346436e-05\n",
      "254/391 [==================>...........] - ETA: 10s - loss: 0.7131 - acc: 0.8016\n",
      " Optimizer iteration 3773, batch 254\n",
      "\n",
      " Learning rate 1.0429472546938157e-05, Model learning rate 1.0429472240502946e-05\n",
      "255/391 [==================>...........] - ETA: 10s - loss: 0.7129 - acc: 0.8018\n",
      " Optimizer iteration 3774, batch 255\n",
      "\n",
      " Learning rate 1.0266437697422026e-05, Model learning rate 1.0266438039252535e-05\n",
      "256/391 [==================>...........] - ETA: 10s - loss: 0.7129 - acc: 0.8018\n",
      " Optimizer iteration 3775, batch 256\n",
      "\n",
      " Learning rate 1.0104673978866164e-05, Model learning rate 1.01046744021005e-05\n",
      "257/391 [==================>...........] - ETA: 10s - loss: 0.7128 - acc: 0.8019\n",
      " Optimizer iteration 3776, batch 257\n",
      "\n",
      " Learning rate 9.94418181113732e-06, Model learning rate 9.944182238541543e-06\n",
      "258/391 [==================>...........] - ETA: 10s - loss: 0.7128 - acc: 0.8020\n",
      " Optimizer iteration 3777, batch 258\n",
      "\n",
      " Learning rate 9.784961610802113e-06, Model learning rate 9.784961548575666e-06\n",
      "259/391 [==================>...........] - ETA: 10s - loss: 0.7130 - acc: 0.8020\n",
      " Optimizer iteration 3778, batch 259\n",
      "\n",
      " Learning rate 9.627013791125295e-06, Model learning rate 9.62701415119227e-06\n",
      "260/391 [==================>...........] - ETA: 9s - loss: 0.7127 - acc: 0.8021 \n",
      " Optimizer iteration 3779, batch 260\n",
      "\n",
      " Learning rate 9.470338762069431e-06, Model learning rate 9.470339136896655e-06\n",
      "261/391 [===================>..........] - ETA: 9s - loss: 0.7128 - acc: 0.8021\n",
      " Optimizer iteration 3780, batch 261\n",
      "\n",
      " Learning rate 9.314936930293284e-06, Model learning rate 9.31493650568882e-06\n",
      "262/391 [===================>..........] - ETA: 9s - loss: 0.7130 - acc: 0.8020\n",
      " Optimizer iteration 3781, batch 262\n",
      "\n",
      " Learning rate 9.16080869915109e-06, Model learning rate 9.16080898605287e-06\n",
      "\n",
      " Optimizer iteration 3782, batch 263\n",
      "\n",
      " Learning rate 9.007954468691294e-06, Model learning rate 9.007954758999404e-06\n",
      "264/391 [===================>..........] - ETA: 9s - loss: 0.7129 - acc: 0.8021\n",
      " Optimizer iteration 3783, batch 264\n",
      "\n",
      " Learning rate 8.856374635655695e-06, Model learning rate 8.85637473402312e-06\n",
      "265/391 [===================>..........] - ETA: 9s - loss: 0.7125 - acc: 0.8022\n",
      " Optimizer iteration 3784, batch 265\n",
      "\n",
      " Learning rate 8.706069593478139e-06, Model learning rate 8.706069820618723e-06\n",
      "266/391 [===================>..........] - ETA: 9s - loss: 0.7121 - acc: 0.8021\n",
      " Optimizer iteration 3785, batch 266\n",
      "\n",
      " Learning rate 8.557039732283945e-06, Model learning rate 8.55704001878621e-06\n",
      "\n",
      " Optimizer iteration 3786, batch 267\n",
      "\n",
      " Learning rate 8.409285438888358e-06, Model learning rate 8.409285328525584e-06\n",
      "268/391 [===================>..........] - ETA: 9s - loss: 0.7122 - acc: 0.8023\n",
      " Optimizer iteration 3787, batch 268\n",
      "\n",
      " Learning rate 8.262807096795999e-06, Model learning rate 8.262806659331545e-06\n",
      "269/391 [===================>..........] - ETA: 9s - loss: 0.7124 - acc: 0.8022\n",
      " Optimizer iteration 3788, batch 269\n",
      "\n",
      " Learning rate 8.117605086199687e-06, Model learning rate 8.117604920698795e-06\n",
      "\n",
      " Optimizer iteration 3789, batch 270\n",
      "\n",
      " Learning rate 7.973679783979337e-06, Model learning rate 7.973680112627335e-06\n",
      "271/391 [===================>..........] - ETA: 9s - loss: 0.7117 - acc: 0.8023\n",
      " Optimizer iteration 3790, batch 271\n",
      "\n",
      " Learning rate 7.83103156370113e-06, Model learning rate 7.831031325622462e-06\n",
      "272/391 [===================>..........] - ETA: 9s - loss: 0.7118 - acc: 0.8022\n",
      " Optimizer iteration 3791, batch 272\n",
      "\n",
      " Learning rate 7.689660795616559e-06, Model learning rate 7.68966037867358e-06\n",
      "273/391 [===================>..........] - ETA: 9s - loss: 0.7118 - acc: 0.8023\n",
      " Optimizer iteration 3792, batch 273\n",
      "\n",
      " Learning rate 7.549567846661387e-06, Model learning rate 7.549567726528039e-06\n",
      "\n",
      " Optimizer iteration 3793, batch 274\n",
      "\n",
      " Learning rate 7.410753080454746e-06, Model learning rate 7.41075291443849e-06\n",
      "275/391 [====================>.........] - ETA: 8s - loss: 0.7117 - acc: 0.8023\n",
      " Optimizer iteration 3794, batch 275\n",
      "\n",
      " Learning rate 7.2732168572981485e-06, Model learning rate 7.273216851899633e-06\n",
      "276/391 [====================>.........] - ETA: 8s - loss: 0.7121 - acc: 0.8023\n",
      " Optimizer iteration 3795, batch 276\n",
      "\n",
      " Learning rate 7.136959534174592e-06, Model learning rate 7.136959538911469e-06\n",
      "277/391 [====================>.........] - ETA: 8s - loss: 0.7120 - acc: 0.8023\n",
      " Optimizer iteration 3796, batch 277\n",
      "\n",
      " Learning rate 7.001981464747565e-06, Model learning rate 7.001981430221349e-06\n",
      "278/391 [====================>.........] - ETA: 8s - loss: 0.7120 - acc: 0.8022\n",
      " Optimizer iteration 3797, batch 278\n",
      "\n",
      " Learning rate 6.868282999360265e-06, Model learning rate 6.868282980576623e-06\n",
      "279/391 [====================>.........] - ETA: 8s - loss: 0.7122 - acc: 0.8023\n",
      " Optimizer iteration 3798, batch 279\n",
      "\n",
      " Learning rate 6.735864485034493e-06, Model learning rate 6.735864644724643e-06\n",
      "\n",
      " Optimizer iteration 3799, batch 280\n",
      "\n",
      " Learning rate 6.604726265470096e-06, Model learning rate 6.604726422665408e-06\n",
      "281/391 [====================>.........] - ETA: 8s - loss: 0.7118 - acc: 0.8024\n",
      " Optimizer iteration 3800, batch 281\n",
      "\n",
      " Learning rate 6.474868681043577e-06, Model learning rate 6.474868769146269e-06\n",
      "\n",
      " Optimizer iteration 3801, batch 282\n",
      "\n",
      " Learning rate 6.346292068807602e-06, Model learning rate 6.346292138914578e-06\n",
      "283/391 [====================>.........] - ETA: 8s - loss: 0.7114 - acc: 0.8025\n",
      " Optimizer iteration 3802, batch 283\n",
      "\n",
      " Learning rate 6.2189967624899925e-06, Model learning rate 6.218996986717684e-06\n",
      "284/391 [====================>.........] - ETA: 8s - loss: 0.7109 - acc: 0.8028\n",
      " Optimizer iteration 3803, batch 284\n",
      "\n",
      " Learning rate 6.092983092492843e-06, Model learning rate 6.092983312555589e-06\n",
      "285/391 [====================>.........] - ETA: 8s - loss: 0.7109 - acc: 0.8029\n",
      " Optimizer iteration 3804, batch 285\n",
      "\n",
      " Learning rate 5.968251385891743e-06, Model learning rate 5.968251571175642e-06\n",
      "\n",
      " Optimizer iteration 3805, batch 286\n",
      "\n",
      " Learning rate 5.844801966434832e-06, Model learning rate 5.844801762577845e-06\n",
      "287/391 [=====================>........] - ETA: 7s - loss: 0.7108 - acc: 0.8029\n",
      " Optimizer iteration 3806, batch 287\n",
      "\n",
      " Learning rate 5.722635154541967e-06, Model learning rate 5.722635251004249e-06\n",
      "\n",
      " Optimizer iteration 3807, batch 288\n",
      "\n",
      " Learning rate 5.601751267304056e-06, Model learning rate 5.601751126960153e-06\n",
      "289/391 [=====================>........] - ETA: 7s - loss: 0.7108 - acc: 0.8030\n",
      " Optimizer iteration 3808, batch 289\n",
      "\n",
      " Learning rate 5.482150618481952e-06, Model learning rate 5.482150754687609e-06\n",
      "\n",
      " Optimizer iteration 3809, batch 290\n",
      "\n",
      " Learning rate 5.363833518505834e-06, Model learning rate 5.363833679439267e-06\n",
      "291/391 [=====================>........] - ETA: 7s - loss: 0.7109 - acc: 0.8030\n",
      " Optimizer iteration 3810, batch 291\n",
      "\n",
      " Learning rate 5.2468002744744395e-06, Model learning rate 5.246800355962478e-06\n",
      "292/391 [=====================>........] - ETA: 7s - loss: 0.7113 - acc: 0.8030\n",
      " Optimizer iteration 3811, batch 292\n",
      "\n",
      " Learning rate 5.131051190154113e-06, Model learning rate 5.1310512390045915e-06\n",
      "293/391 [=====================>........] - ETA: 7s - loss: 0.7114 - acc: 0.8029\n",
      " Optimizer iteration 3812, batch 293\n",
      "\n",
      " Learning rate 5.016586565978087e-06, Model learning rate 5.01658678331296e-06\n",
      "294/391 [=====================>........] - ETA: 7s - loss: 0.7117 - acc: 0.8029\n",
      " Optimizer iteration 3813, batch 294\n",
      "\n",
      " Learning rate 4.9034066990457095e-06, Model learning rate 4.903406534140231e-06\n",
      "295/391 [=====================>........] - ETA: 7s - loss: 0.7115 - acc: 0.8030\n",
      " Optimizer iteration 3814, batch 295\n",
      "\n",
      " Learning rate 4.791511883121713e-06, Model learning rate 4.791511855728459e-06\n",
      "296/391 [=====================>........] - ETA: 7s - loss: 0.7118 - acc: 0.8028\n",
      " Optimizer iteration 3815, batch 296\n",
      "\n",
      " Learning rate 4.680902408635335e-06, Model learning rate 4.680902293330291e-06\n",
      "297/391 [=====================>........] - ETA: 7s - loss: 0.7122 - acc: 0.8026\n",
      " Optimizer iteration 3816, batch 297\n",
      "\n",
      " Learning rate 4.571578562679757e-06, Model learning rate 4.571578756440431e-06\n",
      "298/391 [=====================>........] - ETA: 7s - loss: 0.7124 - acc: 0.8025\n",
      " Optimizer iteration 3817, batch 298\n",
      "\n",
      " Learning rate 4.463540629010998e-06, Model learning rate 4.4635407903115265e-06\n",
      "299/391 [=====================>........] - ETA: 7s - loss: 0.7127 - acc: 0.8025\n",
      " Optimizer iteration 3818, batch 299\n",
      "\n",
      " Learning rate 4.356788888047747e-06, Model learning rate 4.356788849690929e-06\n",
      "300/391 [======================>.......] - ETA: 6s - loss: 0.7125 - acc: 0.8026\n",
      " Optimizer iteration 3819, batch 300\n",
      "\n",
      " Learning rate 4.2513236168700845e-06, Model learning rate 4.25132384407334e-06\n",
      "301/391 [======================>.......] - ETA: 6s - loss: 0.7126 - acc: 0.8025\n",
      " Optimizer iteration 3820, batch 301\n",
      "\n",
      " Learning rate 4.147145089218984e-06, Model learning rate 4.1471448639640585e-06\n",
      "302/391 [======================>.......] - ETA: 6s - loss: 0.7125 - acc: 0.8025\n",
      " Optimizer iteration 3821, batch 302\n",
      "\n",
      " Learning rate 4.04425357549576e-06, Model learning rate 4.044253728352487e-06\n",
      "303/391 [======================>.......] - ETA: 6s - loss: 0.7126 - acc: 0.8025\n",
      " Optimizer iteration 3822, batch 303\n",
      "\n",
      " Learning rate 3.942649342761117e-06, Model learning rate 3.942649527743924e-06\n",
      "304/391 [======================>.......] - ETA: 6s - loss: 0.7126 - acc: 0.8025\n",
      " Optimizer iteration 3823, batch 304\n",
      "\n",
      " Learning rate 3.842332654734437e-06, Model learning rate 3.842332716885721e-06\n",
      "305/391 [======================>.......] - ETA: 6s - loss: 0.7127 - acc: 0.8024\n",
      " Optimizer iteration 3824, batch 305\n",
      "\n",
      " Learning rate 3.7433037717933828e-06, Model learning rate 3.7433037505252287e-06\n",
      "306/391 [======================>.......] - ETA: 6s - loss: 0.7124 - acc: 0.8026\n",
      " Optimizer iteration 3825, batch 306\n",
      "\n",
      " Learning rate 3.645562950973014e-06, Model learning rate 3.645562856036122e-06\n",
      "307/391 [======================>.......] - ETA: 6s - loss: 0.7118 - acc: 0.8028\n",
      " Optimizer iteration 3826, batch 307\n",
      "\n",
      " Learning rate 3.5491104459650646e-06, Model learning rate 3.549110488165752e-06\n",
      "308/391 [======================>.......] - ETA: 6s - loss: 0.7119 - acc: 0.8029\n",
      " Optimizer iteration 3827, batch 308\n",
      "\n",
      " Learning rate 3.453946507117445e-06, Model learning rate 3.4539464195404435e-06\n",
      "309/391 [======================>.......] - ETA: 6s - loss: 0.7120 - acc: 0.8029\n",
      " Optimizer iteration 3828, batch 309\n",
      "\n",
      " Learning rate 3.3600713814335158e-06, Model learning rate 3.3600713322812226e-06\n",
      "310/391 [======================>.......] - ETA: 6s - loss: 0.7121 - acc: 0.8028\n",
      " Optimizer iteration 3829, batch 310\n",
      "\n",
      " Learning rate 3.2674853125714276e-06, Model learning rate 3.2674852263880894e-06\n",
      "\n",
      " Optimizer iteration 3830, batch 311\n",
      "\n",
      " Learning rate 3.1761885408435056e-06, Model learning rate 3.1761885566083947e-06\n",
      "312/391 [======================>.......] - ETA: 6s - loss: 0.7123 - acc: 0.8028\n",
      " Optimizer iteration 3831, batch 312\n",
      "\n",
      " Learning rate 3.0861813032156404e-06, Model learning rate 3.0861813229421386e-06\n",
      "313/391 [=======================>......] - ETA: 5s - loss: 0.7125 - acc: 0.8027\n",
      " Optimizer iteration 3832, batch 313\n",
      "\n",
      " Learning rate 2.997463833306735e-06, Model learning rate 2.9974637527629966e-06\n",
      "314/391 [=======================>......] - ETA: 5s - loss: 0.7125 - acc: 0.8027\n",
      " Optimizer iteration 3833, batch 314\n",
      "\n",
      " Learning rate 2.9100363613879243e-06, Model learning rate 2.9100363008183194e-06\n",
      "315/391 [=======================>......] - ETA: 5s - loss: 0.7121 - acc: 0.8030\n",
      " Optimizer iteration 3834, batch 315\n",
      "\n",
      " Learning rate 2.823899114382078e-06, Model learning rate 2.8238991944817826e-06\n",
      "316/391 [=======================>......] - ETA: 5s - loss: 0.7121 - acc: 0.8028\n",
      " Optimizer iteration 3835, batch 316\n",
      "\n",
      " Learning rate 2.739052315863355e-06, Model learning rate 2.7390522063797107e-06\n",
      "317/391 [=======================>......] - ETA: 5s - loss: 0.7118 - acc: 0.8030\n",
      " Optimizer iteration 3836, batch 317\n",
      "\n",
      " Learning rate 2.655496186056261e-06, Model learning rate 2.6554962460068054e-06\n",
      "318/391 [=======================>......] - ETA: 5s - loss: 0.7119 - acc: 0.8029\n",
      " Optimizer iteration 3837, batch 318\n",
      "\n",
      " Learning rate 2.573230941835536e-06, Model learning rate 2.573230858615716e-06\n",
      "\n",
      " Optimizer iteration 3838, batch 319\n",
      "\n",
      " Learning rate 2.492256796725212e-06, Model learning rate 2.4922567263274686e-06\n",
      "320/391 [=======================>......] - ETA: 5s - loss: 0.7124 - acc: 0.8029\n",
      " Optimizer iteration 3839, batch 320\n",
      "\n",
      " Learning rate 2.4125739608981124e-06, Model learning rate 2.4125738491420634e-06\n",
      "321/391 [=======================>......] - ETA: 5s - loss: 0.7123 - acc: 0.8027\n",
      " Optimizer iteration 3840, batch 321\n",
      "\n",
      " Learning rate 2.334182641175686e-06, Model learning rate 2.334182681806851e-06\n",
      "322/391 [=======================>......] - ETA: 5s - loss: 0.7125 - acc: 0.8027\n",
      " Optimizer iteration 3841, batch 322\n",
      "\n",
      " Learning rate 2.2570830410268973e-06, Model learning rate 2.2570829969481565e-06\n",
      "323/391 [=======================>......] - ETA: 5s - loss: 0.7129 - acc: 0.8026\n",
      " Optimizer iteration 3842, batch 323\n",
      "\n",
      " Learning rate 2.181275360568169e-06, Model learning rate 2.1812752493133303e-06\n",
      "324/391 [=======================>......] - ETA: 5s - loss: 0.7129 - acc: 0.8026\n",
      " Optimizer iteration 3843, batch 324\n",
      "\n",
      " Learning rate 2.106759796562496e-06, Model learning rate 2.1067598936497234e-06\n",
      "325/391 [=======================>......] - ETA: 5s - loss: 0.7130 - acc: 0.8026\n",
      " Optimizer iteration 3844, batch 325\n",
      "\n",
      " Learning rate 2.0335365424192786e-06, Model learning rate 2.033536475209985e-06\n",
      "326/391 [========================>.....] - ETA: 4s - loss: 0.7128 - acc: 0.8027\n",
      " Optimizer iteration 3845, batch 326\n",
      "\n",
      " Learning rate 1.9616057881935434e-06, Model learning rate 1.9616056761151413e-06\n",
      "327/391 [========================>.....] - ETA: 4s - loss: 0.7130 - acc: 0.8027\n",
      " Optimizer iteration 3846, batch 327\n",
      "\n",
      " Learning rate 1.890967720585668e-06, Model learning rate 1.8909677237388678e-06\n",
      "328/391 [========================>.....] - ETA: 4s - loss: 0.7132 - acc: 0.8027\n",
      " Optimizer iteration 3847, batch 328\n",
      "\n",
      " Learning rate 1.8216225229406026e-06, Model learning rate 1.8216225043943268e-06\n",
      "329/391 [========================>.....] - ETA: 4s - loss: 0.7130 - acc: 0.8028\n",
      " Optimizer iteration 3848, batch 329\n",
      "\n",
      " Learning rate 1.753570375247815e-06, Model learning rate 1.7535703591420315e-06\n",
      "330/391 [========================>.....] - ETA: 4s - loss: 0.7128 - acc: 0.8029\n",
      " Optimizer iteration 3849, batch 330\n",
      "\n",
      " Learning rate 1.6868114541404577e-06, Model learning rate 1.6868114016688196e-06\n",
      "\n",
      " Optimizer iteration 3850, batch 331\n",
      "\n",
      " Learning rate 1.6213459328950354e-06, Model learning rate 1.6213459730352042e-06\n",
      "332/391 [========================>.....] - ETA: 4s - loss: 0.7125 - acc: 0.8030\n",
      " Optimizer iteration 3851, batch 332\n",
      "\n",
      " Learning rate 1.5571739814309594e-06, Model learning rate 1.5571739595543477e-06\n",
      "333/391 [========================>.....] - ETA: 4s - loss: 0.7123 - acc: 0.8030\n",
      " Optimizer iteration 3852, batch 333\n",
      "\n",
      " Learning rate 1.494295766310161e-06, Model learning rate 1.4942958159736008e-06\n",
      "334/391 [========================>.....] - ETA: 4s - loss: 0.7121 - acc: 0.8031\n",
      " Optimizer iteration 3853, batch 334\n",
      "\n",
      " Learning rate 1.4327114507365346e-06, Model learning rate 1.432711428606126e-06\n",
      "335/391 [========================>.....] - ETA: 4s - loss: 0.7121 - acc: 0.8031\n",
      " Optimizer iteration 3854, batch 335\n",
      "\n",
      " Learning rate 1.372421194555773e-06, Model learning rate 1.3724211385124363e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/391 [========================>.....] - ETA: 4s - loss: 0.7121 - acc: 0.8031\n",
      " Optimizer iteration 3855, batch 336\n",
      "\n",
      " Learning rate 1.3134251542544773e-06, Model learning rate 1.3134251730662072e-06\n",
      "337/391 [========================>.....] - ETA: 4s - loss: 0.7122 - acc: 0.8030\n",
      " Optimizer iteration 3856, batch 337\n",
      "\n",
      " Learning rate 1.2557234829601582e-06, Model learning rate 1.2557235322674387e-06\n",
      "\n",
      " Optimizer iteration 3857, batch 338\n",
      "\n",
      " Learning rate 1.1993163304409027e-06, Model learning rate 1.1993163298029685e-06\n",
      "339/391 [=========================>....] - ETA: 3s - loss: 0.7121 - acc: 0.8031\n",
      " Optimizer iteration 3858, batch 339\n",
      "\n",
      " Learning rate 1.1442038431044854e-06, Model learning rate 1.1442037930464721e-06\n",
      "340/391 [=========================>....] - ETA: 3s - loss: 0.7116 - acc: 0.8033\n",
      " Optimizer iteration 3859, batch 340\n",
      "\n",
      " Learning rate 1.0903861639985914e-06, Model learning rate 1.090386149371625e-06\n",
      "341/391 [=========================>....] - ETA: 3s - loss: 0.7118 - acc: 0.8032\n",
      " Optimizer iteration 3860, batch 341\n",
      "\n",
      " Learning rate 1.0378634328099267e-06, Model learning rate 1.037863398778427e-06\n",
      "342/391 [=========================>....] - ETA: 3s - loss: 0.7121 - acc: 0.8031\n",
      " Optimizer iteration 3861, batch 342\n",
      "\n",
      " Learning rate 9.866357858642206e-07, Model learning rate 9.866357686405536e-07\n",
      "343/391 [=========================>....] - ETA: 3s - loss: 0.7121 - acc: 0.8031\n",
      " Optimizer iteration 3862, batch 343\n",
      "\n",
      " Learning rate 9.367033561257233e-07, Model learning rate 9.367033726448426e-07\n",
      "344/391 [=========================>....] - ETA: 3s - loss: 0.7119 - acc: 0.8032\n",
      " Optimizer iteration 3863, batch 344\n",
      "\n",
      " Learning rate 8.880662731968747e-07, Model learning rate 8.880662676347129e-07\n",
      "345/391 [=========================>....] - ETA: 3s - loss: 0.7119 - acc: 0.8031\n",
      " Optimizer iteration 3864, batch 345\n",
      "\n",
      " Learning rate 8.4072466331786e-07, Model learning rate 8.407246809838398e-07\n",
      "346/391 [=========================>....] - ETA: 3s - loss: 0.7119 - acc: 0.8032\n",
      " Optimizer iteration 3865, batch 346\n",
      "\n",
      " Learning rate 7.946786493666647e-07, Model learning rate 7.946786695356423e-07\n",
      "\n",
      " Optimizer iteration 3866, batch 347\n",
      "\n",
      " Learning rate 7.499283508581311e-07, Model learning rate 7.49928346976958e-07\n",
      "348/391 [=========================>....] - ETA: 3s - loss: 0.7120 - acc: 0.8031\n",
      " Optimizer iteration 3867, batch 348\n",
      "\n",
      " Learning rate 7.064738839442364e-07, Model learning rate 7.064738838380435e-07\n",
      "349/391 [=========================>....] - ETA: 3s - loss: 0.7125 - acc: 0.8030\n",
      " Optimizer iteration 3868, batch 349\n",
      "\n",
      " Learning rate 6.643153614134811e-07, Model learning rate 6.643153369623178e-07\n",
      "350/391 [=========================>....] - ETA: 3s - loss: 0.7120 - acc: 0.8031\n",
      " Optimizer iteration 3869, batch 350\n",
      "\n",
      " Learning rate 6.234528926907234e-07, Model learning rate 6.234528768800374e-07\n",
      "351/391 [=========================>....] - ETA: 3s - loss: 0.7120 - acc: 0.8031\n",
      " Optimizer iteration 3870, batch 351\n",
      "\n",
      " Learning rate 5.838865838366791e-07, Model learning rate 5.838865604346211e-07\n",
      "352/391 [==========================>...] - ETA: 2s - loss: 0.7123 - acc: 0.8030\n",
      " Optimizer iteration 3871, batch 352\n",
      "\n",
      " Learning rate 5.456165375480882e-07, Model learning rate 5.456165581563255e-07\n",
      "353/391 [==========================>...] - ETA: 2s - loss: 0.7119 - acc: 0.8031\n",
      " Optimizer iteration 3872, batch 353\n",
      "\n",
      " Learning rate 5.08642853156882e-07, Model learning rate 5.086428700451506e-07\n",
      "354/391 [==========================>...] - ETA: 2s - loss: 0.7118 - acc: 0.8032\n",
      " Optimizer iteration 3873, batch 354\n",
      "\n",
      " Learning rate 4.729656266304061e-07, Model learning rate 4.7296563820964366e-07\n",
      "355/391 [==========================>...] - ETA: 2s - loss: 0.7119 - acc: 0.8032\n",
      " Optimizer iteration 3874, batch 355\n",
      "\n",
      " Learning rate 4.3858495057080837e-07, Model learning rate 4.3858494791493285e-07\n",
      "356/391 [==========================>...] - ETA: 2s - loss: 0.7117 - acc: 0.8034\n",
      " Optimizer iteration 3875, batch 356\n",
      "\n",
      " Learning rate 4.055009142152066e-07, Model learning rate 4.0550091284785594e-07\n",
      "357/391 [==========================>...] - ETA: 2s - loss: 0.7119 - acc: 0.8034\n",
      " Optimizer iteration 3876, batch 357\n",
      "\n",
      " Learning rate 3.737136034349109e-07, Model learning rate 3.737135898518318e-07\n",
      "358/391 [==========================>...] - ETA: 2s - loss: 0.7116 - acc: 0.8035\n",
      " Optimizer iteration 3877, batch 358\n",
      "\n",
      " Learning rate 3.432231007358122e-07, Model learning rate 3.432230926136981e-07\n",
      "359/391 [==========================>...] - ETA: 2s - loss: 0.7117 - acc: 0.8035\n",
      " Optimizer iteration 3878, batch 359\n",
      "\n",
      " Learning rate 3.1402948525766086e-07, Model learning rate 3.1402947797687375e-07\n",
      "\n",
      " Optimizer iteration 3879, batch 360\n",
      "\n",
      " Learning rate 2.861328327741219e-07, Model learning rate 2.86132831206487e-07\n",
      "361/391 [==========================>...] - ETA: 2s - loss: 0.7115 - acc: 0.8036\n",
      " Optimizer iteration 3880, batch 361\n",
      "\n",
      " Learning rate 2.595332156925534e-07, Model learning rate 2.595332091459568e-07\n",
      "362/391 [==========================>...] - ETA: 2s - loss: 0.7117 - acc: 0.8035\n",
      " Optimizer iteration 3881, batch 362\n",
      "\n",
      " Learning rate 2.3423070305367278e-07, Model learning rate 2.3423069706041133e-07\n",
      "\n",
      " Optimizer iteration 3882, batch 363\n",
      "\n",
      " Learning rate 2.1022536053166842e-07, Model learning rate 2.1022536600412423e-07\n",
      "364/391 [==========================>...] - ETA: 2s - loss: 0.7118 - acc: 0.8034\n",
      " Optimizer iteration 3883, batch 364\n",
      "\n",
      " Learning rate 1.8751725043375523e-07, Model learning rate 1.875172443988049e-07\n",
      "365/391 [===========================>..] - ETA: 1s - loss: 0.7118 - acc: 0.8034\n",
      " Optimizer iteration 3884, batch 365\n",
      "\n",
      " Learning rate 1.6610643170000827e-07, Model learning rate 1.6610643172043638e-07\n",
      "366/391 [===========================>..] - ETA: 1s - loss: 0.7118 - acc: 0.8034\n",
      " Optimizer iteration 3885, batch 366\n",
      "\n",
      " Learning rate 1.4599295990352924e-07, Model learning rate 1.4599295639072807e-07\n",
      "\n",
      " Optimizer iteration 3886, batch 367\n",
      "\n",
      " Learning rate 1.271768872498913e-07, Model learning rate 1.2717688946395356e-07\n",
      "368/391 [===========================>..] - ETA: 1s - loss: 0.7120 - acc: 0.8032\n",
      " Optimizer iteration 3887, batch 368\n",
      "\n",
      " Learning rate 1.096582625772502e-07, Model learning rate 1.0965825936182227e-07\n",
      "369/391 [===========================>..] - ETA: 1s - loss: 0.7116 - acc: 0.8034\n",
      " Optimizer iteration 3888, batch 369\n",
      "\n",
      " Learning rate 9.343713135623322e-08, Model learning rate 9.343713003318044e-08\n",
      "370/391 [===========================>..] - ETA: 1s - loss: 0.7118 - acc: 0.8034\n",
      " Optimizer iteration 3889, batch 370\n",
      "\n",
      " Learning rate 7.851353568971708e-08, Model learning rate 7.851353700516484e-08\n",
      "371/391 [===========================>..] - ETA: 1s - loss: 0.7120 - acc: 0.8034\n",
      " Optimizer iteration 3890, batch 371\n",
      "\n",
      " Learning rate 6.488751431266149e-08, Model learning rate 6.488751580491225e-08\n",
      "\n",
      " Optimizer iteration 3891, batch 372\n",
      "\n",
      " Learning rate 5.2559102592164565e-08, Model learning rate 5.2559101959559484e-08\n",
      "373/391 [===========================>..] - ETA: 1s - loss: 0.7121 - acc: 0.8033\n",
      " Optimizer iteration 3892, batch 373\n",
      "\n",
      " Learning rate 4.15283325274074e-08, Model learning rate 4.152833099624331e-08\n",
      "374/391 [===========================>..] - ETA: 1s - loss: 0.7122 - acc: 0.8033\n",
      " Optimizer iteration 3893, batch 374\n",
      "\n",
      " Learning rate 3.179523274932094e-08, Model learning rate 3.179523133667317e-08\n",
      "375/391 [===========================>..] - ETA: 1s - loss: 0.7119 - acc: 0.8034\n",
      " Optimizer iteration 3894, batch 375\n",
      "\n",
      " Learning rate 2.3359828520641556e-08, Model learning rate 2.3359827849844805e-08\n",
      "376/391 [===========================>..] - ETA: 1s - loss: 0.7118 - acc: 0.8034\n",
      " Optimizer iteration 3895, batch 376\n",
      "\n",
      " Learning rate 1.622214173602199e-08, Model learning rate 1.6222141852040295e-08\n",
      "377/391 [===========================>..] - ETA: 1s - loss: 0.7119 - acc: 0.8033\n",
      " Optimizer iteration 3896, batch 377\n",
      "\n",
      " Learning rate 1.0382190921753854e-08, Model learning rate 1.0382191106828031e-08\n",
      "378/391 [============================>.] - ETA: 1s - loss: 0.7119 - acc: 0.8032\n",
      " Optimizer iteration 3897, batch 378\n",
      "\n",
      " Learning rate 5.839991235656594e-09, Model learning rate 5.83999115733036e-09\n",
      "\n",
      " Optimizer iteration 3898, batch 379\n",
      "\n",
      " Learning rate 2.5955544673550437e-09, Model learning rate 2.5955544380451556e-09\n",
      "380/391 [============================>.] - ETA: 0s - loss: 0.7117 - acc: 0.8033\n",
      " Optimizer iteration 3899, batch 380\n",
      "\n",
      " Learning rate 6.48889037890843e-10, Model learning rate 6.488890536004988e-10\n",
      "381/391 [============================>.] - ETA: 0s - loss: 0.7117 - acc: 0.8033\n",
      " Optimizer iteration 3900, batch 381\n",
      "\n",
      " Learning rate 0.001, Model learning rate 0.0010000000474974513\n",
      "382/391 [============================>.] - ETA: 0s - loss: 0.7114 - acc: 0.8034\n",
      " Optimizer iteration 3901, batch 382\n",
      "\n",
      " Learning rate 0.0009999993511109622, Model learning rate 0.0009999993490055203\n",
      "383/391 [============================>.] - ETA: 0s - loss: 0.7113 - acc: 0.8034\n",
      " Optimizer iteration 3902, batch 383\n",
      "\n",
      " Learning rate 0.0009999974044455327, Model learning rate 0.0009999973699450493\n",
      "384/391 [============================>.] - ETA: 0s - loss: 0.7111 - acc: 0.8035\n",
      " Optimizer iteration 3903, batch 384\n",
      "\n",
      " Learning rate 0.0009999941600087644, Model learning rate 0.0009999941103160381\n",
      "385/391 [============================>.] - ETA: 0s - loss: 0.7111 - acc: 0.8035\n",
      " Optimizer iteration 3904, batch 385\n",
      "\n",
      " Learning rate 0.0009999896178090784, Model learning rate 0.0009999895701184869\n",
      "386/391 [============================>.] - ETA: 0s - loss: 0.7112 - acc: 0.8034\n",
      " Optimizer iteration 3905, batch 386\n",
      "\n",
      " Learning rate 0.000999983777858264, Model learning rate 0.0009999837493523955\n",
      "387/391 [============================>.] - ETA: 0s - loss: 0.7116 - acc: 0.8033\n",
      " Optimizer iteration 3906, batch 387\n",
      "\n",
      " Learning rate 0.0009999766401714793, Model learning rate 0.000999976648017764\n",
      "\n",
      " Optimizer iteration 3907, batch 388\n",
      "\n",
      " Learning rate 0.0009999682047672506, Model learning rate 0.0009999681496992707\n",
      "389/391 [============================>.] - ETA: 0s - loss: 0.7119 - acc: 0.8031\n",
      " Optimizer iteration 3908, batch 389\n",
      "\n",
      " Learning rate 0.0009999584716674725, Model learning rate 0.000999958487227559\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.7119 - acc: 0.8030\n",
      " Optimizer iteration 3909, batch 390\n",
      "\n",
      " Learning rate 0.000999947440897408, Model learning rate 0.0009999474277719855\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.7121 - acc: 0.8029 - val_loss: 0.9825 - val_acc: 0.7160\n",
      "\n",
      "Epoch 00010: saving model to /home/ubuntu/Projects/hybrid-ensemble/model/run_200/cifar10_ResNet20v1_model-0010.h5\n",
      "Saving epoch training log...\n",
      "Saving batch training log...\n",
      "Writing index file and predict files...\n",
      "Saving target file...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Trains a ResNet on the CIFAR10 dataset.\n",
    "\n",
    "ResNet v1\n",
    "[a] Deep Residual Learning for Image Recognition\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "\n",
    "ResNet v2\n",
    "[b] Identity Mappings in Deep Residual Networks\n",
    "https://arxiv.org/pdf/1603.05027.pdf\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import Callback\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "dirpath = os.getcwd()\n",
    "\n",
    "seed = 22\n",
    "resample = True\n",
    "save_dir = '/home/ubuntu/Projects/hybrid-ensemble/model/run_200'\n",
    "datafile = '/home/ubuntu/Projects/hybrid-ensemble/data/cifar10_balance/DS3'\n",
    "top_k = 2\n",
    "\n",
    "# Set random seed\n",
    "if seed is not None:\n",
    "    import tensorflow as tf\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 128  # orig paper trained all networks with batch_size=128\n",
    "epochs = 10 # orig paper epochs = 200\n",
    "data_augmentation = True\n",
    "num_classes = 10\n",
    "initial_lr = 1e-3\n",
    "snapshot_window_size = int(math.ceil(epochs/top_k))\n",
    "\n",
    "# Subtracting pixel mean improves accuracy\n",
    "subtract_pixel_mean = True\n",
    "\n",
    "# Model parameter\n",
    "# ----------------------------------------------------------------------------\n",
    "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
    "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
    "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
    "# ----------------------------------------------------------------------------\n",
    "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
    "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
    "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
    "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
    "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
    "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
    "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
    "# ---------------------------------------------------------------------------\n",
    "n = 3\n",
    "\n",
    "# Model version\n",
    "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
    "version = 1\n",
    "\n",
    "# Computed depth from supplied model parameter n\n",
    "if version == 1:\n",
    "    depth = n * 6 + 2\n",
    "elif version == 2:\n",
    "    depth = n * 9 + 2\n",
    "\n",
    "# Model name, depth and version\n",
    "model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "# Load the CIFAR10 data.\n",
    "print('Loading data...')\n",
    "with open(datafile, 'rb') as f:\n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test) = pickle.load(f)\n",
    "\n",
    "# Resample the training data set from training+validating data set with the same class distribution with the loaded ones\n",
    "if resample:\n",
    "    print('Resampling training and validating data sets...')\n",
    "    x_tv = np.concatenate((x_train, x_valid), axis=0)\n",
    "    y_tv = np.concatenate((y_train, y_valid), axis=0)\n",
    "    index_dict = defaultdict(list)\n",
    "    for i in range(len(y_tv)):\n",
    "        index_dict[y_tv[i][0]].append(i)\n",
    "    valid_index_dict = defaultdict(list)\n",
    "    for i in range(len(y_valid)):\n",
    "        valid_index_dict[y_valid[i][0]].append(i)\n",
    "    valid_index = []\n",
    "    for c in valid_index_dict.keys():\n",
    "        valid_index.extend(np.random.choice(index_dict[c], size=len(valid_index_dict[c]), replace=False))\n",
    "    train_index = np.setdiff1d(range(len(y_tv)), valid_index)\n",
    "\n",
    "    x_train, y_train = x_tv[train_index], y_tv[train_index]\n",
    "    x_valid, y_valid = x_tv[valid_index], y_tv[valid_index]\n",
    "    \n",
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "x_valid = x_valid.astype('float32') / 255\n",
    "\n",
    "# If subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "    x_valid -= x_train_mean\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)\n",
    "print(x_valid.shape[0], 'valid samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_valid = keras.utils.to_categorical(y_valid, num_classes)\n",
    "\n",
    "def next_run_dir(path):\n",
    "    \"\"\"\n",
    "    Naive (slow) version of next_path\n",
    "    \"\"\"\n",
    "    i = 1\n",
    "    while os.path.exists('{}_{}'.format(path, i)):\n",
    "        i += 1\n",
    "    return '{}_{}'.format(path, i)\n",
    "\n",
    "def cyclic_cosine_anneal_schedule(initial_lr=1e-3, update_window_size=40):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with cosine annealing schedule.\n",
    "    '''\n",
    "    def lr_schedule(epoch):\n",
    "        \"\"\"Learning Rate Schedule\n",
    "\n",
    "        Learning rate is scheduled to be updated per epoch with a cosine function per epoch. \n",
    "        Learning rate is raised to initial_lr every snapshot_window_size.\n",
    "\n",
    "        # Arguments\n",
    "            epoch (int): The number of epochs\n",
    "\n",
    "        # Returns\n",
    "            lr (float32): learning rate\n",
    "        \"\"\"\n",
    "        lr = initial_lr / 2 * (math.cos(math.pi * ((epoch % update_window_size) / update_window_size)) + 1)\n",
    "        print('Learning rate: ', lr)\n",
    "        return lr\n",
    "    \n",
    "    return LearningRateScheduler(lr_schedule)\n",
    "\n",
    "class MyCallback(Callback):\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        iteration = self.model.optimizer.iterations   \n",
    "        update_window_size = self.params.update_window_size\n",
    "        lr = initial_lr / 2 * (math.cos(math.pi * ((K.eval(iteration) % update_window_size) / update_window_size)) + 1)\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        print('\\n batch {}, lr {}, iterations {}'.format(batch, K.eval(lr), K.eval(iteration)))\n",
    "        \n",
    "def cyclic_cosine_anneal_schedule_itr(batch_logs, initial_lr=1e-3, update_window_size=15600):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with cosine annealing schedule per iteration.\n",
    "    '''\n",
    "    def lr_schedule(batch, logs):\n",
    "        \"\"\"Learning Rate Schedule\n",
    "\n",
    "        Learning rate is scheduled to be updated per epoch with a cosine function per iteration. \n",
    "        Learning rate is raised to initial_lr every snapshot_window_size.\n",
    "\n",
    "        # Arguments\n",
    "            epoch (int): The number of epochs\n",
    "\n",
    "        # Returns\n",
    "            lr (float32): learning rate\n",
    "        \"\"\"\n",
    "        iteration = model.optimizer.iterations\n",
    "        print('\\n Optimizer iteration {}, batch {}'.format(K.eval(iteration), batch))\n",
    "        lr = initial_lr / 2 * (math.cos(math.pi * ((K.eval(iteration) % update_window_size) / update_window_size)) + 1)\n",
    "        K.set_value(model.optimizer.lr, lr)\n",
    "        print('\\n Learning rate {}, Model learning rate {}'.format(lr, K.eval(model.optimizer.lr)))\n",
    "    \n",
    "    def batch_log(batch, logs):\n",
    "        batch_logs['iteration'].append(K.eval(model.optimizer.iterations))\n",
    "        batch_logs['lr'].append(K.eval(model.optimizer.lr))\n",
    "        batch_logs['loss'].append(logs['loss'])\n",
    "        batch_logs['acc'].append(logs['acc'])\n",
    "        \n",
    "    \n",
    "    return LambdaCallback(on_batch_begin=lr_schedule, on_batch_end=batch_log)\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            activation-bn-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet_v2(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "print('Building model...')\n",
    "if version == 2:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "else:\n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=initial_lr),\n",
    "              metrics=['accuracy'])\n",
    "# model.summary()\n",
    "print(model_type)\n",
    "\n",
    "# Prepare model model saving directory.\n",
    "if not save_dir:\n",
    "    save_dir = next_run_dir('{}/../model/run'.format(dirpath))\n",
    "os.makedirs(save_dir)\n",
    "model_name = 'cifar10_%s_model-{epoch:04d}.h5' % model_type\n",
    "# model_name = 'cifar10_%s_model-{epoch:04d}-{val_acc:.5f}.h5' % model_type\n",
    "# model_name = 'cifar10_{}_model.h5'.format(model_type)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "print('Preparing callbacks...')\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=False,\n",
    "                             mode='max')\n",
    "# Learning rate updater\n",
    "batch_num = int(x_train.shape[0]/batch_size)\n",
    "update_window_size = int(math.ceil(epochs*batch_num/top_k))\n",
    "batch_logs = {'iteration':[], 'lr':[], 'loss':[], 'acc':[]}\n",
    "lr_scheduler = cyclic_cosine_anneal_schedule_itr(initial_lr=initial_lr, \n",
    "                                                 update_window_size=update_window_size, batch_logs=batch_logs)\n",
    "\n",
    "# Training log writer\n",
    "csvlog = CSVLogger('callback_training_log.csv', separator=',', append=False)\n",
    "\n",
    "\n",
    "callbacks = [checkpoint, lr_scheduler, csvlog]\n",
    "\n",
    "# callbacks = [MyCallback()]\n",
    "        \n",
    "# Run training, with or without data augmentation.\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    print('Training...')\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_valid, y_valid),\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    print('Training...')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False)\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    steps_per_epoch = math.ceil(len(x_train) / batch_size)\n",
    "    history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        validation_data=(x_valid, y_valid),\n",
    "                        epochs=epochs, verbose=1, workers=4,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "# Score trained model.\n",
    "# scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "# print('Test loss:', scores[0])\n",
    "# print('Test accuracy:', scores[1])\n",
    "\n",
    "# Save training log\n",
    "print('Saving epoch training log...')\n",
    "train_error = history.history['loss']\n",
    "valid_accuracy = history.history['val_acc']\n",
    "logfile = '{}/training_log.csv'.format(save_dir)\n",
    "f = open(logfile, 'w')\n",
    "f.write('current_epoch,total_epochs,train_loss,validation_accuracy\\n')\n",
    "for i in range(len(train_error)):\n",
    "    f.write('{},{},{},{}\\n'.format(i+1, epochs, train_error[i], valid_accuracy[i]))\n",
    "f.close()\n",
    "\n",
    "print('Saving batch training log...')\n",
    "f = open('batch_training_log.csv', 'w')\n",
    "f.write('current_iteration,total_iteration,learning_rate,train_loss,train_accuracy\\n')\n",
    "total_iteration = int(math.ceil(x_train.shape[0]/batch_size)*epochs)\n",
    "for i in range(len(batch_logs['iteration'])):\n",
    "    f.write('{},{},{},{},{}\\n'.format(batch_logs['iteration'][i], \n",
    "                                   total_iteration, \n",
    "                                   batch_logs['lr'][i], \n",
    "                                   batch_logs['loss'][i], \n",
    "                                   batch_logs['acc'][i]))\n",
    "f.close()\n",
    "\n",
    "import csv\n",
    "with open('batch_training_log.csv', 'w') as f:  \n",
    "    w = csv.DictWriter(f, batch_logs.keys())\n",
    "    w.writeheader()\n",
    "    w.writerow(batch_logs)\n",
    "\n",
    "# Save index for combination\n",
    "print('Writing index file and predict files...')\n",
    "indexfile = '{}/index.csv'.format(save_dir)\n",
    "f = open(indexfile, 'w')\n",
    "window_size = int(epochs/top_k)\n",
    "top_x = []\n",
    "for i in range(1, top_k):\n",
    "    top_x.append(np.argmax(valid_accuracy[i*snapshot_window_size:(i+1)*snapshot_window_size]) + i*snapshot_window_size)\n",
    "top_v = [valid_accuracy[i] for i in top_x]\n",
    "for x,v in zip(top_x, top_v):\n",
    "    name = 'cifar10_{}_model-{:04d}.h5'.format(model_type, x+1)\n",
    "    weight = v\n",
    "    f.write('{},{}\\n'.format(name, weight))\n",
    "    # predicting\n",
    "    filepath = os.path.join(save_dir, name)\n",
    "    model.load_weights(filepath)\n",
    "    predicts = model.predict(x_test)\n",
    "    # Save predicts\n",
    "    predictfile = '{}/prediction_{:04d}.csv'.format(save_dir, x+1)\n",
    "    f1 = open(predictfile,'w')\n",
    "    header = '0,1,2,3,4,5,6,7,8,9\\n'\n",
    "    f1.write(header)\n",
    "    np.savetxt(f1, predicts, delimiter=\",\")\n",
    "    f1.close()\n",
    "f.close()\n",
    "\n",
    "# # Delete unwanted model files\n",
    "# print('Deleting unwanted model files...')\n",
    "# no_top_x = range(len(valid_accuracy))\n",
    "# no_top_x = list(set(no_top_x) - set(top_x))\n",
    "# for no_top_x_index in no_top_x:\n",
    "#     remove_filename = '{}/cifar10_{}_model-{:04d}.h5'.format(save_dir, model_type, no_top_x_index+1)\n",
    "#     os.remove(remove_filename)\n",
    "\n",
    "# Save targets\n",
    "print('Saving target file...')\n",
    "targetfile = '{}/target.csv'.format(save_dir)\n",
    "f2 = open(targetfile,'w')\n",
    "header = '0,1,2,3,4,5,6,7,8,9\\n'\n",
    "f2.write(header)\n",
    "np.savetxt(f2, y_test, delimiter=\",\")\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500/4500 [==============================] - 2s 337us/step\n",
      "Test loss: 0.6096683431466421\n",
      "Test accuracy: 0.8351111112170749\n",
      "4500/4500 [==============================] - 1s 142us/step\n",
      "Test loss: 0.5120277782943514\n",
      "Test accuracy: 0.8691111110051473\n",
      "4500/4500 [==============================] - 1s 142us/step\n",
      "Test loss: 0.48847858993212384\n",
      "Test accuracy: 0.8855555554495917\n",
      "4500/4500 [==============================] - 1s 148us/step\n",
      "Test loss: 0.4810516203906801\n",
      "Test accuracy: 0.8931111111111111\n",
      "4500/4500 [==============================] - 1s 148us/step\n",
      "Test loss: 0.48305441366301644\n",
      "Test accuracy: 0.8942222222222223\n",
      "4500/4500 [==============================] - 1s 143us/step\n",
      "Test loss: 0.48185164595974816\n",
      "Test accuracy: 0.9008888888888889\n",
      "4500/4500 [==============================] - 1s 141us/step\n",
      "Test loss: 0.47659654211997987\n",
      "Test accuracy: 0.9022222222222223\n",
      "4500/4500 [==============================] - 1s 142us/step\n",
      "Test loss: 0.49599758842256336\n",
      "Test accuracy: 0.8997777777777778\n",
      "4500/4500 [==============================] - 1s 142us/step\n",
      "Test loss: 0.4874440699285931\n",
      "Test accuracy: 0.9046666666666666\n",
      "4500/4500 [==============================] - 1s 141us/step\n",
      "Test loss: 0.48776051804754467\n",
      "Test accuracy: 0.9044444444444445\n"
     ]
    }
   ],
   "source": [
    "logfile = '../model/run_100/training_log.csv'\n",
    "df = pd.read_csv(logfile, header=0)\n",
    "valid_accuracy = df['validation_accuracy'].values.tolist()\n",
    "top_x = []\n",
    "for i in range(0, 10):\n",
    "    top_x.append(np.argmax(valid_accuracy[i*snapshot_window_size:(i+1)*snapshot_window_size]) + i*snapshot_window_size)\n",
    "\n",
    "\n",
    "if version == 2:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "else:\n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=initial_lr),\n",
    "              metrics=['accuracy'])\n",
    "test_scores = []\n",
    "for k in top_x:\n",
    "    saved_model = '../model/run_100/cifar10_ResNet20v1_model-{:04d}.h5'.format(k+1)\n",
    "    model.load_weights(saved_model)\n",
    "    scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    test_scores.append(scores[1])\n",
    "test_scores_1 = test_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "4500/4500 [==============================] - 2s 411us/step\n",
      "Test loss: 0.623283628516727\n",
      "Test accuracy: 0.8286666666136847\n",
      "4500/4500 [==============================] - 1s 151us/step\n",
      "Test loss: 0.5248715903229184\n",
      "Test accuracy: 0.864\n",
      "4500/4500 [==============================] - 1s 149us/step\n",
      "Test loss: 0.4969917231135898\n",
      "Test accuracy: 0.8771111111111111\n",
      "4500/4500 [==============================] - 1s 152us/step\n",
      "Test loss: 0.4798820976946089\n",
      "Test accuracy: 0.8871111111111111\n",
      "4500/4500 [==============================] - 1s 149us/step\n",
      "Test loss: 0.49182780093616907\n",
      "Test accuracy: 0.8908888888888888\n",
      "4500/4500 [==============================] - 1s 163us/step\n",
      "Test loss: 0.4949813829925325\n",
      "Test accuracy: 0.8993333333333333\n",
      "4500/4500 [==============================] - 1s 151us/step\n",
      "Test loss: 0.500278210149871\n",
      "Test accuracy: 0.9004444444444445\n",
      "4500/4500 [==============================] - 1s 148us/step\n",
      "Test loss: 0.5235393349462085\n",
      "Test accuracy: 0.8928888888888888\n",
      "4500/4500 [==============================] - 1s 148us/step\n",
      "Test loss: 0.4998899468051063\n",
      "Test accuracy: 0.9042222222222223\n",
      "4500/4500 [==============================] - 1s 149us/step\n",
      "Test loss: 0.5158649944994185\n",
      "Test accuracy: 0.9002222222222223\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "if version == 2:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "else:\n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=initial_lr),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "logfile = '../model/run_200/training_log.csv'\n",
    "df = pd.read_csv(logfile, header=0)\n",
    "valid_accuracy = df['validation_accuracy'].values.tolist()\n",
    "version = 1\n",
    "snapshot_window_size = 20\n",
    "top_x = []\n",
    "for i in range(0, 10):\n",
    "    top_x.append(np.argmax(valid_accuracy[i*snapshot_window_size:(i+1)*snapshot_window_size]) + i*snapshot_window_size)\n",
    "\n",
    "\n",
    "if version == 2:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "else:\n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=initial_lr),\n",
    "              metrics=['accuracy'])\n",
    "test_scores = []\n",
    "for k in top_x:\n",
    "    saved_model = '../model/run_200/cifar10_ResNet20v1_model-{:04d}.h5'.format(k+1)\n",
    "    model.load_weights(saved_model)\n",
    "    scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    test_scores.append(scores[1])\n",
    "test_scores_2 = test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:51: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 86.4\n",
      "Test accuracy: 87.71111111111111\n",
      "Test accuracy: 88.71111111111111\n",
      "Test accuracy: 89.08888888888889\n",
      "Test accuracy: 89.93333333333334\n",
      "Test accuracy: 90.04444444444445\n",
      "Test accuracy: 89.28888888888889\n",
      "Test accuracy: 90.42222222222223\n",
      "Test accuracy: 90.02222222222223\n"
     ]
    }
   ],
   "source": [
    "import common_functions as cf\n",
    "logfile = '../model/run_200/training_log.csv'\n",
    "df = pd.read_csv(logfile, header=0)\n",
    "valid_accuracy = df['validation_accuracy'].values.tolist()\n",
    "version = 1\n",
    "snapshot_window_size = 20\n",
    "top_x = []\n",
    "for i in range(1, 10):\n",
    "    top_x.append(np.argmax(valid_accuracy[i*snapshot_window_size:(i+1)*snapshot_window_size]) + i*snapshot_window_size)\n",
    "\n",
    "groundfile = '../model/run_200/target.csv' \n",
    "df_g = pd.read_csv(groundfile,header=0)\n",
    "test_scores = []\n",
    "for k in top_x:\n",
    "    predfile = '../model/run_200/prediction_{:04d}.csv'.format(k+1)  \n",
    "    df_p = pd.read_csv(predfile,header=0)     \n",
    "    # compute confusion matrix\n",
    "    cm = cf.confusion_matrix(df_g, df_p)\n",
    "    tp, total = 0, 0\n",
    "    for i in range(len(cm)):\n",
    "        tp += cm[i,i]\n",
    "        total += np.sum(cm[i])\n",
    "    ea = tp/total*100\n",
    "    print('Test accuracy:', ea)\n",
    "    test_scores.append(ea)\n",
    "test_scores_3 = test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_error = history.history['loss']\n",
    "# valid_accuracy = history.history['val_acc']\n",
    "snapshot_window_size = 20\n",
    "top_k = 10\n",
    "\n",
    "logfile = '../model/run_100/training_log.csv'\n",
    "df = pd.read_csv(logfile, header=0)\n",
    "train_error = df['train_loss'].values.tolist()\n",
    "valid_accuracy = df['validation_accuracy'].values.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "lns1 = ax.plot(train_error, color='red', label='training error')\n",
    "ax.set_ylabel('Training error', fontsize=24)\n",
    "ax.xaxis.set_tick_params(labelsize=16)\n",
    "ax.yaxis.set_tick_params(labelsize=16)\n",
    "ax.set_xlabel('Epochs', fontsize=24)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "lns2 = ax2.plot(valid_accuracy, color='blue', label='validation accuracy')\n",
    "ax2.set_ylabel('Validation accuracy', fontsize=24)\n",
    "ax2.xaxis.set_tick_params(labelsize=16)\n",
    "ax2.yaxis.set_tick_params(labelsize=16)\n",
    "\n",
    "epochs = len(train_error)\n",
    "locs = range(0,epochs+1,snapshot_window_size)\n",
    "for x in locs:\n",
    "    ax.axvline(x,linestyle='--',color='gray')\n",
    "\n",
    "top_x = []\n",
    "for i in range(1, top_k):\n",
    "    top_x.append(int(np.argmax(valid_accuracy[i*snapshot_window_size:(i+1)*snapshot_window_size-1]) + i*snapshot_window_size))\n",
    "top_v = [valid_accuracy[i] for i in top_x]\n",
    "for x,y in zip(top_x, top_v):\n",
    "    ax2.plot(x,y,color='orange',marker='d',markersize=8)\n",
    "    \n",
    "lns = lns1+lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labs, loc='right', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../evaluation/figure/snapshot-A.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_error = history.history['loss']\n",
    "# valid_accuracy = history.history['val_acc']\n",
    "snapshot_window_size = 20\n",
    "top_k = 10\n",
    "\n",
    "logfile = '../model/run_200/training_log.csv'\n",
    "df = pd.read_csv(logfile, header=0)\n",
    "train_error = df['train_loss'].values.tolist()\n",
    "valid_accuracy = df['validation_accuracy'].values.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "lns1 = ax.plot(train_error, color='red', label='training error')\n",
    "ax.set_ylabel('Training error', fontsize=24)\n",
    "ax.xaxis.set_tick_params(labelsize=16)\n",
    "ax.yaxis.set_tick_params(labelsize=16)\n",
    "ax.set_xlabel('Epochs', fontsize=24)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "lns2 = ax2.plot(valid_accuracy, color='blue', label='validation accuracy')\n",
    "ax2.set_ylabel('Validation accuracy', fontsize=24)\n",
    "ax2.xaxis.set_tick_params(labelsize=16)\n",
    "ax2.yaxis.set_tick_params(labelsize=16)\n",
    "\n",
    "epochs = len(train_error)\n",
    "locs = range(0,epochs+1,snapshot_window_size)\n",
    "for x in locs:\n",
    "    ax.axvline(x,linestyle='--',color='gray')\n",
    "\n",
    "top_x = []\n",
    "for i in range(1, top_k):\n",
    "    top_x.append(int(np.argmax(valid_accuracy[i*snapshot_window_size:(i+1)*snapshot_window_size-1]) + i*snapshot_window_size))\n",
    "top_v = [valid_accuracy[i] for i in top_x]\n",
    "for x,y in zip(top_x, top_v):\n",
    "    ax2.plot(x,y,color='orange',marker='d',markersize=8)\n",
    "    \n",
    "lns = lns1+lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labs, loc='right', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../evaluation/figure/snapshot.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "snapshot_window_size = 20\n",
    "top_k = 10\n",
    "\n",
    "logfile = '../model/batch_training_log.csv'\n",
    "df = pd.read_csv(logfile, header=0)\n",
    "train_loss = df['train_loss'].values.tolist()\n",
    "train_accuracy = df['train_accuracy'].values.tolist()\n",
    "lr = df['learning_rate'].values.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "lns1 = ax.plot(train_loss, color='red', label='training error')\n",
    "ax.set_ylabel('Training error', fontsize=24)\n",
    "ax.xaxis.set_tick_params(labelsize=16)\n",
    "ax.yaxis.set_tick_params(labelsize=16)\n",
    "ax.set_xlabel('Iterations', fontsize=24)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "lns2 = ax2.plot(lr, color='c', label='learning rate')\n",
    "ax2.set_ylabel('Learning rate', fontsize=24)\n",
    "ax2.xaxis.set_tick_params(labelsize=16)\n",
    "ax2.yaxis.set_tick_params(labelsize=16)\n",
    "\n",
    "    \n",
    "lns = lns1+lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labs, loc='right', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.title(r'$\\alpha_0=0.001$, cosine cyclic annealing schedule, update per iteration')\n",
    "plt.savefig('../evaluation/figure/lr-snapshot.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "snapshot_window_size = 20\n",
    "top_k = 10\n",
    "\n",
    "logfile = '../model/epoch_training_log.csv'\n",
    "df = pd.read_csv(logfile, header=0)\n",
    "train_loss = df['loss'].values.tolist()\n",
    "train_accuracy = df['acc'].values.tolist()\n",
    "lr = df['lr'].values.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "lns1 = ax.plot(train_loss, color='red', label='training error')\n",
    "ax.set_ylabel('Training error', fontsize=24)\n",
    "ax.xaxis.set_tick_params(labelsize=16)\n",
    "ax.yaxis.set_tick_params(labelsize=16)\n",
    "ax.set_xlabel('Iterations', fontsize=24)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "lns2 = ax2.plot(lr, color='c', label='learning rate')\n",
    "ax2.set_ylabel('Learning rate', fontsize=24)\n",
    "ax2.xaxis.set_tick_params(labelsize=16)\n",
    "ax2.yaxis.set_tick_params(labelsize=16)\n",
    "\n",
    "    \n",
    "lns = lns1+lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labs, loc='right', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.title(r'$\\alpha_0=0.001$, cosine cyclic annealing schedule, update per epoch')\n",
    "plt.savefig('../evaluation/figure/lr-snapshotA.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "snapshot_window_size = 20\n",
    "top_k = 10\n",
    "\n",
    "logfile = '../model/standard_training_log.csv'\n",
    "df = pd.read_csv(logfile, header=0)\n",
    "train_loss = df['loss'].values.tolist()\n",
    "train_accuracy = df['acc'].values.tolist()\n",
    "lr = df['lr'].values.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "lns1 = ax.plot(train_loss, color='red', label='training error')\n",
    "ax.set_ylabel('Training error', fontsize=24)\n",
    "ax.xaxis.set_tick_params(labelsize=16)\n",
    "ax.yaxis.set_tick_params(labelsize=16)\n",
    "ax.set_xlabel('Iterations', fontsize=24)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "lns2 = ax2.plot(lr, color='c', label='learning rate')\n",
    "ax2.set_ylabel('Learning rate', fontsize=24)\n",
    "ax2.xaxis.set_tick_params(labelsize=16)\n",
    "ax2.yaxis.set_tick_params(labelsize=16)\n",
    "\n",
    "    \n",
    "lns = lns1+lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labs, loc='right', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.title(r'$\\alpha_0=0.001$, step-decay schedule (at 80, 120, 160 epochs)')\n",
    "plt.savefig('../evaluation/figure/lr-snapshotB.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "datafile = '/mnt/data4/hallw60/cifar10/hybrid/evaluation/data/cifar10_imbalance/all-avg.csv'\n",
    "df = pd.read_csv(datafile, header=0, delimiter='\\t')\n",
    "nrow = 3\n",
    "ncol = 1\n",
    "fig, ax = plt.subplots(nrow, ncol, sharex='col', sharey='row', figsize=(12,9))\n",
    "\n",
    "ind = np.array([1, 3.8, 6.6, 9.4])\n",
    "color = ['grey','b','g','r','y','c']\n",
    "width = 0.3\n",
    "\n",
    "metrics = ['ea','er','ea_er']\n",
    "for i in range(nrow):\n",
    "    metric = metrics[i]\n",
    "    data_b = df[(df['Model']=='Baseline')][metric]\n",
    "    data_sime = df[(df['Model']=='Simple-ensemble')][metric]\n",
    "    data_s = df[(df['Model']=='Snapshot')][metric]\n",
    "    data_sa = df[(df['Model']=='Snapshot-A')][metric]\n",
    "    data_sb = df[(df['Model']=='Snapshot-B')][metric]\n",
    "    data_supe = df[(df['Model']=='Super-ensemble')][metric]\n",
    "\n",
    "    ax[i,].bar(ind, data_b, width=width, edgecolor=color[0], lw=2, color='white', label='Baseline')\n",
    "    ax[i,].bar(ind+width, data_sime, width=width, edgecolor=color[1], lw=2, color='white', label='Simple-ensemble')\n",
    "    ax[i,].bar(ind+2*width, data_s, width=width, edgecolor=color[2], lw=2, color='white', label='Snapshot')\n",
    "    ax[i,].bar(ind+3*width, data_sa, width=width, edgecolor=color[3], lw=2, color='white', label='Snapshot-A')\n",
    "    ax[i,].bar(ind+4*width, data_sb, width=width, edgecolor=color[4], lw=2, color='white', label='Snapshot-B')\n",
    "    ax[i,].bar(ind+5*width, data_supe, width=width, edgecolor=color[5], lw=2, color='white', label='Super-ensemble')\n",
    "\n",
    "    ax[i,].set_xlim([0, 12.2])\n",
    "    ax[i,].set_yscale('log')\n",
    "    ax[i,].yaxis.set_minor_formatter(ScalarFormatter())\n",
    "    \n",
    "ax[2,].set_xticks(ind+2*width)\n",
    "ax[2,].set_xticklabels(('Majority_voting', 'Weighted_voting', 'Averaging', 'Weighted_averaging'),fontsize=14)\n",
    "ax[0,].set_ylabel('Accuracy(%)',fontsize=14)\n",
    "ax[1,].set_ylabel('Reproducibility(%)',fontsize=14)\n",
    "ax[2,].set_ylabel('Correct-Reproducibility(%)',fontsize=14)\n",
    "ax[0,].legend(ncol=6, loc='upper center',bbox_to_anchor=(0.5, 1.2))\n",
    "\n",
    "plt.savefig('../evaluation/figure/cifar10-result-imbalance.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datafile = '/mnt/data4/hallw60/cifar10/hybrid/evaluation/data/cifar10_imbalance/all-avg.csv'\n",
    "df = pd.read_csv(datafile, header=0, delimiter='\\t')\n",
    "nrow = 2\n",
    "ncol = 4\n",
    "fig, ax = plt.subplots(nrow, ncol, sharex='col', sharey='row', figsize=(12,6))\n",
    "\n",
    "color = ['grey','b','g','r','y','c']\n",
    "xind = [1, 2, 3, 4, 5, 6]\n",
    "markers=['v','^','o']\n",
    "\n",
    "combinations = ['majority_voting', 'weighted_voting', 'averaging', 'weighted_averaging']\n",
    "metrics = ['majority_ea','minority_ea','ea']\n",
    "for j in range(ncol):\n",
    "    data_b = df[(df['Model']=='Baseline') & (df['Combination']==combinations[j])][metrics]\n",
    "    data_sime = df[(df['Model']=='Simple-ensemble') & (df['Combination']==combinations[j])][metrics]\n",
    "    data_s = df[(df['Model']=='Snapshot') & (df['Combination']==combinations[j])][metrics]\n",
    "    data_sa = df[(df['Model']=='Snapshot-A') & (df['Combination']==combinations[j])][metrics]\n",
    "    data_sb = df[(df['Model']=='Snapshot-B') & (df['Combination']==combinations[j])][metrics]\n",
    "    data_supe = df[(df['Model']=='Super-ensemble') & (df['Combination']==combinations[j])][metrics]\n",
    "\n",
    "    \n",
    "    ax[0,j].plot(np.repeat(xind[0],3), data_b.values[0], marker='s', markeredgecolor=color[0], markerfacecolor='white', color=color[0], label='Baseline')\n",
    "    ax[0,j].plot(np.repeat(xind[1],3), data_sime.values[0], marker='s', markeredgecolor=color[1], markerfacecolor='white', color=color[1], label='Simple-ensemble')\n",
    "    ax[0,j].plot(np.repeat(xind[2],3), data_s.values[0], marker='s', markeredgecolor=color[2], markerfacecolor='white', color=color[2], label='Snapshot')\n",
    "    ax[0,j].plot(np.repeat(xind[3],3), data_sa.values[0], marker='s', markeredgecolor=color[3], markerfacecolor='white', color=color[3], label='Snapshot-A')\n",
    "    ax[0,j].plot(np.repeat(xind[4],3), data_sb.values[0], marker='s', markeredgecolor=color[4], markerfacecolor='white', color=color[4], label='Snapshot-B')\n",
    "    ax[0,j].plot(np.repeat(xind[5],3), data_supe.values[0], marker='s', markeredgecolor=color[5], markerfacecolor='white', color=color[5], label='Super-ensemble')\n",
    "    ax[0,j].set_xticks([])\n",
    "\n",
    "    \n",
    "metrics = ['majority_er','minority_er','er']\n",
    "for j in range(ncol):\n",
    "    data_b = df[(df['Model']=='Baseline') & (df['Combination']==combinations[j])][metrics]\n",
    "    data_sime = df[(df['Model']=='Simple-ensemble') & (df['Combination']==combinations[j])][metrics]\n",
    "    data_s = df[(df['Model']=='Snapshot') & (df['Combination']==combinations[j])][metrics]\n",
    "    data_sa = df[(df['Model']=='Snapshot-A') & (df['Combination']==combinations[j])][metrics]\n",
    "    data_sb = df[(df['Model']=='Snapshot-B') & (df['Combination']==combinations[j])][metrics]\n",
    "    data_supe = df[(df['Model']=='Super-ensemble') & (df['Combination']==combinations[j])][metrics]\n",
    "\n",
    "    \n",
    "    ax[1,j].plot(np.repeat(xind[0],3), data_b.values[0], marker='o', markeredgecolor=color[0], markerfacecolor='white', color=color[0], label='Baseline')\n",
    "    ax[1,j].plot(np.repeat(xind[1],3), data_sime.values[0], marker='o', markeredgecolor=color[1], markerfacecolor='white', color=color[1], label='Simple-ensemble')\n",
    "    ax[1,j].plot(np.repeat(xind[2],3), data_s.values[0], marker='o', markeredgecolor=color[2], markerfacecolor='white', color=color[2], label='Snapshot')\n",
    "    ax[1,j].plot(np.repeat(xind[3],3), data_sa.values[0], marker='o', markeredgecolor=color[3], markerfacecolor='white', color=color[3], label='Snapshot-A')\n",
    "    ax[1,j].plot(np.repeat(xind[4],3), data_sb.values[0], marker='o', markeredgecolor=color[4], markerfacecolor='white', color=color[4], label='Snapshot-B')\n",
    "    ax[1,j].plot(np.repeat(xind[5],3), data_supe.values[0], marker='o', markeredgecolor=color[5], markerfacecolor='white', color=color[5], label='Super-ensemble')\n",
    "    ax[1,j].set_xticks([])\n",
    "    ax[1,j].set_xlabel(combinations[j], fontsize=14) \n",
    "    \n",
    "ax[0,0].set_ylabel('Accuracy(%)',fontsize=14)\n",
    "ax[1,0].set_ylabel('Reproducibility(%)',fontsize=14)\n",
    "ax[0,0].legend(ncol=6, loc='upper center',bbox_to_anchor=(2.4, 1.2))\n",
    "\n",
    "\n",
    "plt.savefig('../evaluation/figure/cifar10-result-MM-imbalance.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datafile = '/mnt/data4/hallw60/cifar10/hybrid/evaluation/data/cifar10_imbalance/sensitivity-all-avg-avg.csv'\n",
    "df = pd.read_csv(datafile, header=0, delimiter='\\t')\n",
    "nrow = 1\n",
    "ncol = 4\n",
    "fig, ax = plt.subplots(nrow, ncol, sharex='col', sharey='row', figsize=(12,3))\n",
    "\n",
    "color = ['grey','b','g','r','y','c']\n",
    "xind = [1, 2, 3, 4, 5, 6]\n",
    "markers=['v','^','o']\n",
    "\n",
    "combinations = ['majority_voting', 'weighted_voting', 'averaging', 'weighted_averaging']\n",
    "metrics = ['majority_ea','minority_ea','ea']\n",
    "ticks = range(1,7)\n",
    "for j in range(ncol):\n",
    "    ea = df[(df['Combination']==combinations[j])]['ea']\n",
    "    er = df[(df['Combination']==combinations[j])]['er']\n",
    "    ea_er = df[(df['Combination']==combinations[j])]['ea_er']\n",
    "    \n",
    "    ax[j].plot(ticks, ea, marker='s', markeredgecolor=color[3], markerfacecolor='white', color=color[3], label='Accuracy')\n",
    "    ax[j].plot(ticks, er, marker='o', markeredgecolor=color[1], markerfacecolor='white', color=color[1], label='Reproducibility')\n",
    "    ax[j].plot(ticks, ea_er, marker='v', markeredgecolor=color[5], markerfacecolor='white', color=color[5], label='Correct-Reproducibility')\n",
    "    ax[j].set_xticks(ticks)\n",
    "    ax[j].set_xticklabels(['1','5','10','20','40','80'])\n",
    "    ax[j].set_xlabel(combinations[j], fontsize=14)\n",
    "    \n",
    "ax[0].set_ylabel('Percentage(%)',fontsize=14)\n",
    "ax[0].legend(ncol=6, loc='upper center',bbox_to_anchor=(2.4, 1.2))\n",
    "\n",
    "plt.savefig('../evaluation/figure/cifar10-sensitivity-imbalance.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methods & ER & CR-2 & Pearson & Cosine & EA-ER & Majority-ER & Minority-ER \\\\\n",
      "Baseline & 85.38 & 98.55 & 0.87 & 0.89 & 79.86 & 85.88 & 85.10 \\\\\n",
      "Simple-ensemble \\\\\n",
      " \\hspace{3mm}  majority-voting & 92.01 & -1.00 & -1.00 & -1.00 & 85.21 & 92.26 & 91.87 \\\\\n",
      " \\hspace{3mm}  weighted-voting & 92.03 & -1.00 & -1.00 & -1.00 & 85.46 & 92.59 & 91.71 \\\\\n",
      " \\hspace{3mm}  averaging & 92.84 & 99.68 & 0.96 & 0.97 & 86.12 & 92.82 & 92.85 \\\\\n",
      " \\hspace{3mm}  weighted-averaging & 92.83 & 99.68 & 0.96 & 0.97 & 86.12 & 92.82 & 92.84 \\\\\n",
      "Snapshot \\\\\n",
      " \\hspace{3mm}  majority-voting & 93.13 & -1.00 & -1.00 & -1.00 & 85.57 & 93.71 & 92.78 \\\\\n",
      " \\hspace{3mm}  weighted-voting & 93.14 & -1.00 & -1.00 & -1.00 & 85.66 & 93.83 & 92.74 \\\\\n",
      " \\hspace{3mm}  averaging & 93.33 & 99.76 & 0.97 & 0.97 & 85.82 & 93.59 & 93.18 \\\\\n",
      " \\hspace{3mm}  weighted-averaging & 93.34 & 99.76 & 0.97 & 0.97 & 85.84 & 93.63 & 93.17 \\\\\n",
      "Snapshot-A \\\\\n",
      " \\hspace{3mm}  majority-voting & 93.21 & -1.00 & -1.00 & -1.00 & 86.01 & 93.80 & 92.87 \\\\\n",
      " \\hspace{3mm}  weighted-voting & 93.21 & -1.00 & -1.00 & -1.00 & 86.07 & 93.73 & 92.90 \\\\\n",
      " \\hspace{3mm}  averaging & 93.57 & 99.87 & 0.97 & 0.97 & 86.33 & 94.23 & 93.19 \\\\\n",
      " \\hspace{3mm}  weighted-averaging & 93.56 & 99.87 & 0.97 & 0.97 & 86.33 & 94.22 & 93.16 \\\\\n",
      "Snapshot-B \\\\\n",
      " \\hspace{3mm}  majority-voting & 92.38 & -1.00 & -1.00 & -1.00 & 85.45 & 92.15 & 92.51 \\\\\n",
      " \\hspace{3mm}  weighted-voting & 92.27 & -1.00 & -1.00 & -1.00 & 85.50 & 92.07 & 92.40 \\\\\n",
      " \\hspace{3mm}  averaging & 92.96 & 99.74 & 0.96 & 0.97 & 86.18 & 92.53 & 93.22 \\\\\n",
      " \\hspace{3mm}  weighted-averaging & 92.95 & 99.74 & 0.96 & 0.97 & 86.17 & 92.49 & 93.22 \\\\\n",
      "Super-ensemble \\\\\n",
      " \\hspace{3mm}  majority-voting & 95.11 & -1.00 & -1.00 & -1.00 & 87.37 & 95.14 & 95.09 \\\\\n",
      " \\hspace{3mm}  weighted-voting & 95.12 & -1.00 & -1.00 & -1.00 & 87.42 & 95.08 & 95.14 \\\\\n",
      " \\hspace{3mm}  averaging & 95.24 & 99.90 & 0.98 & 0.98 & 87.50 & 95.12 & 95.32 \\\\\n",
      " \\hspace{3mm}  weighted-averaging & 95.24 & 99.90 & 0.98 & 0.98 & 87.48 & 95.10 & 95.32 \\\\\n"
     ]
    }
   ],
   "source": [
    "file = '../evaluation/data/cifar10_imbalance/all-avg.csv'\n",
    "df = pd.read_csv(file, delimiter='\\t')\n",
    "\n",
    "metrics = ['er','crl-2','pearson','cosine','ea_er','majority_er','minority_er']\n",
    "print('Methods & ER & CR-2 & Pearson & Cosine & EA-ER & Majority-ER & Minority-ER \\\\\\\\')\n",
    "baseline = df[(df.Model=='Baseline')&(df.Combination=='majority_voting')][metrics].values.tolist()[0]\n",
    "print('Baseline & {:.2f} & {:.2f} & {:.2f} & {:.2f} & {:.2f} & {:.2f} & {:.2f} \\\\\\\\'.format(baseline[0],baseline[1],baseline[2],\n",
    "                                                                baseline[3],baseline[4],baseline[5],\n",
    "                                                                baseline[6]))\n",
    "methods = ['majority_voting', 'weighted_voting', 'averaging', 'weighted_averaging']\n",
    "# models = ['n1','n5','n10','n20','n40','n80']\n",
    "models = ['Simple-ensemble','Snapshot','Snapshot-A','Snapshot-B','Super-ensemble']\n",
    "labels = ['majority-voting', 'weighted-voting', 'averaging', 'weighted-averaging']\n",
    "for model in models:\n",
    "    print('{} \\\\\\\\'.format(model))\n",
    "    for i in range(len(methods)):\n",
    "        method = methods[i]\n",
    "        label = labels[i]\n",
    "        df_new = df[(df.Model==model) & (df.Combination==method)][metrics]  \n",
    "        er = df_new[metrics[0]].values[0]\n",
    "        if 'voting' in method:\n",
    "            cr, pearson, cosine = -1, -1, -1\n",
    "        else:\n",
    "            cr = df_new[metrics[1]].values[0]\n",
    "            pearson = df_new[metrics[2]].values[0]\n",
    "            cosine = df_new[metrics[3]].values[0]\n",
    "        ea_er = df_new[metrics[4]].values[0]\n",
    "        mj_er = df_new[metrics[5]].values[0]\n",
    "        mi_er = df_new[metrics[6]].values[0]\n",
    "        print(' \\\\hspace{{3mm}}  {} & {:.2f} & {:.2f} & {:.2f} & {:.2f} & {:.2f} & {:.2f} & {:.2f} \\\\\\\\'.format(label, \n",
    "                                                                                                   er, cr, pearson, \n",
    "                                                                                                   cosine, ea_er,\n",
    "                                                                                                  mj_er, mi_er))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methods & EA & CA-5 & Majority-EA & Minority-EA \\\\\n",
      "Baseline & 85.84 & 94.62 & 91.05 & 83.23 \\\\\n",
      "Simple-ensemble \\\\\n",
      " \\hspace{3mm}  majority-voting & 88.46 & -1.00 & 94.00 & 85.69 \\\\\n",
      " \\hspace{3mm}  weighted-voting & 88.76 & -1.00 & 93.67 & 86.31 \\\\\n",
      " \\hspace{3mm}  averaging & 89.14 & 96.59 & 93.82 & 86.80 \\\\\n",
      " \\hspace{3mm}  weighted-averaging & 89.14 & 96.59 & 93.80 & 86.81 \\\\\n",
      "Snapshot \\\\\n",
      " \\hspace{3mm}  majority-voting & 88.40 & -1.00 & 93.53 & 85.83 \\\\\n",
      " \\hspace{3mm}  weighted-voting & 88.52 & -1.00 & 93.56 & 86.00 \\\\\n",
      " \\hspace{3mm}  averaging & 88.59 & 96.37 & 93.62 & 86.08 \\\\\n",
      " \\hspace{3mm}  weighted-averaging & 88.60 & 96.36 & 93.62 & 86.09 \\\\\n",
      "Snapshot-A \\\\\n",
      " \\hspace{3mm}  majority-voting & 88.82 & -1.00 & 94.16 & 86.16 \\\\\n",
      " \\hspace{3mm}  weighted-voting & 88.92 & -1.00 & 94.11 & 86.32 \\\\\n",
      " \\hspace{3mm}  averaging & 89.01 & 96.56 & 94.16 & 86.43 \\\\\n",
      " \\hspace{3mm}  weighted-averaging & 89.01 & 96.55 & 94.16 & 86.44 \\\\\n",
      "Snapshot-B \\\\\n",
      " \\hspace{3mm}  majority-voting & 88.55 & -1.00 & 93.73 & 85.96 \\\\\n",
      " \\hspace{3mm}  weighted-voting & 88.66 & -1.00 & 93.64 & 86.17 \\\\\n",
      " \\hspace{3mm}  averaging & 89.01 & 96.33 & 93.96 & 86.54 \\\\\n",
      " \\hspace{3mm}  weighted-averaging & 89.01 & 96.34 & 93.96 & 86.54 \\\\\n",
      "Super-ensemble \\\\\n",
      " \\hspace{3mm}  majority-voting & 89.39 & -1.00 & 94.38 & 86.89 \\\\\n",
      " \\hspace{3mm}  weighted-voting & 89.43 & -1.00 & 94.36 & 86.97 \\\\\n",
      " \\hspace{3mm}  averaging & 89.44 & 96.87 & 94.33 & 87.00 \\\\\n",
      " \\hspace{3mm}  weighted-averaging & 89.43 & 96.85 & 94.31 & 86.99 \\\\\n"
     ]
    }
   ],
   "source": [
    "file = '../evaluation/data/cifar10_imbalance/all-avg.csv'\n",
    "df = pd.read_csv(file, delimiter='\\t')\n",
    "\n",
    "metrics = ['ea','ca-2','majority_ea','minority_ea']\n",
    "print('Methods & EA & CA-5 & Majority-EA & Minority-EA \\\\\\\\')\n",
    "baseline = df[(df.Model=='Baseline')&(df.Combination=='majority_voting')][metrics].values.tolist()[0]\n",
    "print('Baseline & {:.2f} & {:.2f} & {:.2f} & {:.2f} \\\\\\\\'.format(baseline[0],baseline[1],baseline[2],\n",
    "                                                                baseline[3]))\n",
    "methods = ['majority_voting', 'weighted_voting', 'averaging', 'weighted_averaging']\n",
    "# models = ['n1','n5','n10','n20','n40','n80']\n",
    "models = ['Simple-ensemble','Snapshot','Snapshot-A','Snapshot-B','Super-ensemble']\n",
    "labels = ['majority-voting', 'weighted-voting', 'averaging', 'weighted-averaging']\n",
    "for model in models:\n",
    "    print('{} \\\\\\\\'.format(model))\n",
    "    for i in range(len(methods)):\n",
    "        method = methods[i]\n",
    "        label = labels[i]\n",
    "        df_new = df[(df.Model==model) & (df.Combination==method)][metrics]  \n",
    "        ea = df_new[metrics[0]].values[0]\n",
    "        ca = df_new[metrics[1]].values[0]\n",
    "        mj_ea = df_new[metrics[2]].values[0]\n",
    "        mi_ea = df_new[metrics[3]].values[0]\n",
    "        print(' \\\\hspace{{3mm}}  {} & {:.2f} & {:.2f} & {:.2f} & {:.2f} \\\\\\\\'.format(label, ea, ca, mj_ea, mi_ea))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
